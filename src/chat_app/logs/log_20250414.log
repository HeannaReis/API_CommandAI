2025-04-14 13:54:00,532 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 13:54:00,535 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 13:54:00,536 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 13:54:00,537 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 13:54:00,540 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 13:54:00,543 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 13:54:00,545 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 13:54:00,548 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 13:54:00,550 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 13:54:00,552 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 13:54:00,555 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 13:54:00,557 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 13:54:00,561 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 13:54:00,563 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 13:54:00,565 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 13:54:00,567 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 13:54:00,569 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 13:54:00,570 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 13:54:00,572 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 13:54:00,574 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 13:54:00,576 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 13:54:00,578 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 13:54:00,580 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 13:54:00,582 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 13:54:00,584 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-14 13:54:00,586 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 13:54:00,587 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 13:54:00,589 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 13:54:00,592 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 13:54:00,594 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 13:54:00,596 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 13:54:00,598 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 13:54:00,600 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 13:54:00,602 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 13:54:00,604 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 13:54:00,606 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 13:54:00,608 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 13:54:00,610 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 13:54:00,612 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 13:54:00,614 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 13:54:00,616 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 13:54:00,618 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 13:54:00,619 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 13:54:00,622 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 13:54:00,624 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 13:54:00,627 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 13:54:00,629 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 13:54:00,631 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 13:54:00,633 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 13:54:00,634 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 13:54:00,637 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 13:54:00,639 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 13:54:00,640 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 13:54:00,644 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 13:54:01,563 [INFO] Modelo Gemini 'gemini-2.0-flash-exp' inicializado com sucesso.
2025-04-14 13:59:36,598 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 13:59:36,600 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 13:59:36,602 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 13:59:36,603 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 13:59:36,605 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 13:59:36,607 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 13:59:36,610 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 13:59:36,614 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 13:59:36,618 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 13:59:36,620 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 13:59:36,622 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 13:59:36,624 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 13:59:36,628 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 13:59:36,630 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 13:59:36,632 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 13:59:36,634 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 13:59:36,636 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 13:59:36,638 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 13:59:36,641 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 13:59:36,644 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 13:59:36,647 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 13:59:36,648 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 13:59:36,650 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 13:59:36,652 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 13:59:36,654 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-14 13:59:36,656 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 13:59:36,658 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 13:59:36,660 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 13:59:36,662 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 13:59:36,664 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 13:59:36,665 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 13:59:36,668 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 13:59:36,669 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 13:59:36,671 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 13:59:36,673 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 13:59:36,675 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 13:59:36,677 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 13:59:36,679 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 13:59:36,681 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 13:59:36,683 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 13:59:36,685 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 13:59:36,687 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 13:59:36,689 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 13:59:36,691 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 13:59:36,693 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 13:59:36,695 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 13:59:36,697 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 13:59:36,698 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 13:59:36,701 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 13:59:36,702 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 13:59:36,704 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 13:59:36,705 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 13:59:36,707 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 13:59:36,708 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 13:59:36,821 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 13:59:36,822 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 13:59:36,824 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 13:59:36,826 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 13:59:36,828 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 13:59:36,831 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 13:59:36,832 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 13:59:36,835 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 13:59:36,838 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 13:59:36,839 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 13:59:36,841 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 13:59:36,845 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 13:59:36,847 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 13:59:36,849 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 13:59:36,851 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 13:59:36,853 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 13:59:36,855 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 13:59:36,856 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 13:59:36,859 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 13:59:36,862 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 13:59:36,864 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 13:59:36,866 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 13:59:36,867 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 13:59:36,878 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 13:59:36,881 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-14 13:59:36,883 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 13:59:36,885 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 13:59:36,887 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 13:59:36,888 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 13:59:36,890 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 13:59:36,891 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 13:59:36,893 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 13:59:36,896 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 13:59:36,898 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 13:59:36,899 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 13:59:36,901 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 13:59:36,902 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 13:59:36,904 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 13:59:36,906 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 13:59:36,908 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 13:59:36,910 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 13:59:36,913 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 13:59:36,914 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 13:59:36,916 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 13:59:36,918 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 13:59:36,921 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 13:59:36,923 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 13:59:36,926 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 13:59:36,928 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 13:59:36,931 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 13:59:36,933 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 13:59:36,936 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 13:59:36,938 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 13:59:36,940 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 13:59:36,945 [INFO] Enviando para IA - Prompt (sem imagem): Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"N√£o foram encontrados arquivos de √°udio no diret√≥rio {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcri√ß√£o salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diret√≥rio atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Defini√ß√£o dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Cria√ß√£o dos diret√≥rios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configura√ß√£o do layout do Streamlit
st.set_page_config(
    page_title="An√°lise de A√ß√µes",
    layout="wide",
    page_icon="üìà"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscila√ß√µes

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Per√≠odo', 'Oscila√ß√£o'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("üìä Dashboard de An√°lise de A√ß√µes")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma a√ß√£o", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"üìå Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("üîç Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descri√ß√£o', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa a√ß√£o.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("üí∞ Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("üìâ Oscila√ß√µes")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("üìà Tabela Geral de A√ß√µes")
        st.dataframe(self.acoes)

# Execu√ß√£o
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para an√°lise fundamentalista de a√ß√µes da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from config.common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"N√£o foram encontrados arquivos .txt no diret√≥rio {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conte√∫do do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conte√∫do HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conte√∫do HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se est√° correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configura√ß√£o de v√≠deos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configura√ß√£o de √°udios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcri√ß√µes e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diret√≥rio
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diret√≥rio.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do √∫ltimo arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conte√∫do do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimens√µes do embedding:", embedding.shape)

# Caminho do diret√≥rio de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o √∫ltimo embedding
load_and_print_embedding(embedding_directory)


# test_whisper.py

from transformers import pipeline
from common_paths import CommonPaths

commmom_paths = CommonPaths()

# Fun√ß√£o para ler a transcri√ß√£o de um arquivo
def read_transcription(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()
    return lines

# Fun√ß√£o para converter a pontua√ß√£o em estrelas
def score_to_stars(score):
    if score >= 0.75:
        return 5
    elif score >= 0.55:
        return 4
    elif score >= 0.35:
        return 3
    elif score >= 0.15:
        return 2
    else:
        return 1

# Inicializar o pipeline de an√°lise de sentimento
sentiment_pipeline = pipeline("sentiment-analysis", model="neuralmind/bert-base-portuguese-cased")

# Caminho para o arquivo de transcri√ß√£o
file_path = commmom_paths.TRANSCRIPTION_OUTPUT_PATH / 'feedback.txt'

# Ler a transcri√ß√£o
lines = read_transcription(file_path)

# Analisar sentimento para cada feedback
for line in lines:
    sentiment = sentiment_pipeline(line.strip())
    score = sentiment[0]['score']
    stars = score_to_stars(score)
    
    # Ajuste manual para feedbacks com aspectos positivos e negativos
    if any(word in line.lower() for word in ["mas", "por√©m", "no entanto"]):
        stars = max(1, stars - 1)  # Reduz uma estrela se houver contradi√ß√£o
    
    # Ajuste adicional baseado em palavras-chave
    positive_keywords = ["excelente", "√≥timo", "perfeito", "r√°pido", "eficiente", "satisfeito", "clara", "eficaz", "bom trabalho"]
    negative_keywords = ["ruim", "p√©ssimo", "horr√≠vel", "poderia", "n√£o resolveu", "insatisfat√≥ria", "confusa", "inadequado", "lento", "n√£o houve seguimento", "n√£o atendeu", "n√£o funcionou", "demorado"]
    
    positive_count = sum(word in line.lower() for word in positive_keywords)
    negative_count = sum(word in line.lower() for word in negative_keywords)
    
    # Ajuste de estrelas baseado na contagem de palavras-chave
    if positive_count > negative_count:
        stars = min(5, stars + 1)
    elif negative_count > positive_count:
        stars = max(1, stars - 1)
    
    print(f"Texto: {line.strip()}")
    print(f"Sentimento: {stars} estrelas")
    print(f"Pontua√ß√£o: {score:.2f}")
    print('---')


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configura√ß√£o de transcri√ß√µes e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diret√≥rio onde est√° procurando as transcri√ß√µes
        print(f"Diret√≥rio de entrada das transcri√ß√µes: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcri√ß√£o no diret√≥rio de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcri√ß√£o encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo n√£o encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcri√ß√£o do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} est√° vazio.")
                    return

            # Gerar o embedding da transcri√ß√£o
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcri√ß√£o e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de √°udio
    with sr.Microphone() as source:
        print("Diga algo em portugu√™s...")

        while True:
            try:
                # Escuta o √°udio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {texto_portugues}")

                # Traduz o texto para o ingl√™s
                traducao = translator.translate(texto_portugues)
                print(f"Tradu√ß√£o para o ingl√™s: {traducao}")

            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usu√°rio")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configura√ß√µes de √°udio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de √°udio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em portugu√™s...")

try:
    audio_buffer = []

    while True:
        # L√™ dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o √°udio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o √°udio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradu√ß√£o
            print(f"Tradu√ß√£o para o ingl√™s: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usu√°rio")

    # Fecha o stream de √°udio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O v√≠deo {video_path} n√£o cont√©m √°udio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de v√≠deo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de v√≠deo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de v√≠deo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Convers√£o de v√≠deo para √°udio conclu√≠da!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configura√ß√µes da API
handler = genai('gemini-1.5-flash')

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Fun√ß√£o para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conte√∫do com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares √∫nicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcri√ß√£o obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' n√£o √© uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuni√µes: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de reuni√µes: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|t√≥picos da|pontos (relevantes|principais)|an√°lise)\b.*\b(reuni√£o|√∫ltima (reuni√£o|conversa|sess√£o))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como an√°lise de reuni√£o.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padr√£o.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que s√£o muito similares ao comando atual para evitar redund√¢ncia.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padr√£o: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # M√©todos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de an√°lise de reuni√£o.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conte√∫do usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inv√°lida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar vari√°veis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conte√∫do com base no prompt, contexto e reuni√£o"""
        try:
            # Supondo que a API espera um dicion√°rio com os par√¢metros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisi√ß√£o para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar vari√°veis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Cria√ß√£o da conclus√£o do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que voc√™ acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["fa√ßa um resumo da √∫ltima reuni√£o.", "t√≥picos da √∫ltima reuni√£o", "resuma a √∫ltima reuni√£o", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Voc√™ √© respons√°vel por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - N√£o acrescentar t√≠tulo nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informa√ß√µes acima e forne√ßa uma solu√ß√£o detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Voc√™ √© respons√°vel analisar com detalhes a reuni√£o de [str_meeting], e fornecer uma longa est√≥ria sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precis√£o da resposta.
            - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e fa√ßa uma pesquisa online para uma resposta mais precisa das quest√µes de [str_text]
        - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [context]: Regras B√°sicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("√Åudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum √°udio detectado.")
                return None
            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configura√ß√µes da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicion√°rio
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta n√£o est√° no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API: {e}")
        return None

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: Gere um README.md para minha aplica√ß√£o atual.
2025-04-14 13:59:48,158 [INFO] Resposta da IA (texto): Com base nos arquivos fornecidos, aqui est√° um esbo√ßo de um arquivo `README.md` para sua aplica√ß√£o:

```markdown
# Vis√£o Geral da Aplica√ß√£o

Esta aplica√ß√£o √© uma plataforma vers√°til de IA projetada para realizar diversas tarefas, incluindo:

*   **Processamento de √Åudio e V√≠deo:** Convers√£o de v√≠deos para √°udio e transcri√ß√£o de √°udio para texto.
*   **An√°lise de Imagens:** An√°lise de imagens usando modelos de IA e gera√ß√£o de resumos.
*   **Assistente de Voz Inteligente:** Interface de assistente de voz para intera√ß√£o com a aplica√ß√£o, capaz de responder a perguntas, realizar pesquisas online e analisar sentimentos.
*   **Gera√ß√£o de Embeddings:** Cria√ß√£o de embeddings de texto para tarefas de similaridade sem√¢ntica e armazenamento de informa√ß√µes.
*   **An√°lise de A√ß√µes da B3:** Ferramentas para an√°lise fundamentalista de a√ß√µes da bolsa brasileira.

## Estrutura do Projeto

A aplica√ß√£o √© organizada nos seguintes diret√≥rios principais:

*   `app_config`: Cont√©m configura√ß√µes de n√≠vel de aplicativo e utilit√°rios para gerenciamento de diret√≥rios.
*   `audio_to_text`: M√≥dulos para convers√£o de √°udio para texto usando o Whisper.
*   `chat_app`: C√≥digo para a interface de chat Streamlit, incluindo processamento de imagem e integra√ß√£o com os modelos Gemini.
*   `common_paths`: Define caminhos comuns para diret√≥rios de entrada e sa√≠da de arquivos.
*   `fundamentus_api`: Implementa funcionalidades para an√°lise de a√ß√µes da B3.
*   `ia_generator.py`: Script para gerar conte√∫do HTML a partir de transcri√ß√µes usando uma API externa.
*   `main.py`: Ponto de entrada principal para executar v√°rias tarefas de processamento.
*   `send_embeddings_database`: Configura√ß√µes e scripts para enviar embeddings para um banco de dados.
*   `services`: Cont√©m v√°rios servi√ßos como processamento de imagens, servi√ßos de documentos (Word, Markdown) e comunica√ß√£o com modelos de IA (Gemini).
*   `text_to_embedding`: M√≥dulos para gerar embeddings de texto usando o SentenceTransformer.
*   `transcriptions`: Configura√ß√µes relacionadas √†s transcri√ß√µes de √°udio e envio para bancos de dados.
*   `translate`: Scripts para tradu√ß√£o de voz e texto, incluindo tradu√ß√£o em tempo real.
*   `video_to_audio`: M√≥dulos para extrair √°udio de arquivos de v√≠deo.
*   `voice_assistent`: Implementa√ß√£o do assistente de voz inteligente, incluindo interpreta√ß√£o de comandos, intera√ß√£o com modelos de IA e gerenciamento de contexto.

## Pr√©-requisitos

Antes de executar a aplica√ß√£o, certifique-se de ter instalado o seguinte:

*   **Python:** Python 3.6 ou superior
*   **Bibliotecas:** As depend√™ncias podem ser instaladas usando `pip`:
    ```bash
    pip install -r requirements.txt
    ```
    _Nota: Um arquivo `requirements.txt` completo e atualizado deve ser criado para garantir a instala√ß√£o correta de todas as depend√™ncias._

*   **Vari√°veis de Ambiente:**
    *   `API_KEY_GEMINI`: Necess√°ria para acessar os modelos Gemini.
    *   `GROQ_API_KEY`: Necess√°ria para acessar a API Groq (se usada).

## Configura√ß√£o

1.  **Clone o Reposit√≥rio:**
    ```bash
    git clone <repository_url>
    cd <application_directory>
    ```

2.  **Instale as Depend√™ncias:**
    ```bash
    pip install -r requirements.txt
    ```

3.  **Configure as Vari√°veis de Ambiente:**
    *   Crie um arquivo `.env` no diret√≥rio raiz.
    *   Adicione as chaves de API necess√°rias:
        ```
        API_KEY_GEMINI=your_gemini_api_key
        GROQ_API_KEY=your_groq_api_key
        ```

## Execu√ß√£o

A aplica√ß√£o pode ser executada atrav√©s do script `main.py` para processamento de √°udio/v√≠deo e gera√ß√£o de embeddings:

```bash
python main.py
```

Para executar a interface de chat Streamlit:

```bash
streamlit run chat_app/chat_streamlit.py
```

Para iniciar o assistente de voz:

```bash
python voice_assistent/main.py
```

## Uso

### Chat Streamlit

1.  Execute o aplicativo Streamlit.
2.  Interaja com o chatbot via interface web, inserindo texto ou enviando imagens.
3.  Use a barra lateral para configurar par√¢metros ou limpar o hist√≥rico da conversa.

### Assistente de Voz

1.  Execute o script do assistente de voz.
2.  Siga as instru√ß√µes para usar comandos de voz ou de texto para interagir com o assistente.
3.  O assistente pode responder a perguntas, realizar pesquisas online, analisar o sentimento dos feedbacks e muito mais.

## Arquivos de Configura√ß√£o

*   `chat_app/config/config.py`: Cont√©m os caminhos de arquivos e configura√ß√µes gerais para a aplica√ß√£o de chat.
*   `voice_assistent/config.py`: Configura√ß√µes para o assistente de voz, incluindo chaves de API e par√¢metros.
*   `app_config/app_config.py`: Configura√ß√µes b√°sicas para a aplica√ß√£o como os path
*   `text_to_embedding/texto_to_embedding.py`: Configura√ß√£o do modelo de embedding
## Estrutura de Diret√≥rios

```
.
‚îú‚îÄ‚îÄ app_config/
‚îú‚îÄ‚îÄ audio_to_text/
‚îú‚îÄ‚îÄ chat_app/
‚îú‚îÄ‚îÄ common_paths/
‚îú‚îÄ‚îÄ fundamentus_api/
‚îú‚îÄ‚îÄ ia_generator.py
‚îú‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ send_embeddings_database/
‚îú‚îÄ‚îÄ services/
‚îú‚îÄ‚îÄ text_to_embedding/
‚îú‚îÄ‚îÄ transcriptions/
‚îú‚îÄ‚îÄ translate/
‚îú‚îÄ‚îÄ video_to_audio/
‚îú‚îÄ‚îÄ voice_assistent/
‚îú‚îÄ‚îÄ test_whisper.py
‚îî‚îÄ‚îÄ README.md
```

## Contribui√ß√£o

Contribui√ß√µes s√£o bem-vindas! Sinta-se √† vontade para enviar pull requests para melhorar a aplica√ß√£o.

## Licen√ßa

Este projeto √© licenciado sob a [MIT License](LICENSE).
```

**Notas:**

*   **`requirements.txt`**: Certifique-se de gerar um arquivo `requirements.txt` preciso usando o comando `pip freeze > requirements.txt` no seu ambiente para que outras pessoas possam instalar as depend√™ncias corretamente.
*   **Vari√°veis de Ambiente**: Enfatize a import√¢ncia de proteger as chaves de API e nunca commit√°-las diretamente no c√≥digo.
*   **Instru√ß√µes de Execu√ß√£o**: Inclua instru√ß√µes detalhadas sobre como executar cada parte da aplica√ß√£o.
*   **Estrutura de Diret√≥rios**: Forne√ßa uma vis√£o geral clara da estrutura de diret√≥rios para facilitar a navega√ß√£o e a compreens√£o do projeto.
*   **Licen√ßa**: Adicione um arquivo `LICENSE` ao seu reposit√≥rio e especifique a licen√ßa no README.

Este esqueleto deve fornecer um bom ponto de partida para o seu `README.md`. Lembre-se de adapt√°-lo √†s necessidades e detalhes espec√≠ficos do seu projeto!
2025-04-14 13:59:48,271 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 13:59:48,272 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 13:59:48,274 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 13:59:48,276 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 13:59:48,278 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 13:59:48,280 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 13:59:48,282 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 13:59:48,284 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 13:59:48,286 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 13:59:48,288 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 13:59:48,290 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 13:59:48,291 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 13:59:48,293 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 13:59:48,295 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 13:59:48,296 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 13:59:48,298 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 13:59:48,300 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 13:59:48,301 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 13:59:48,303 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 13:59:48,304 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 13:59:48,305 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 13:59:48,307 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 13:59:48,308 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 13:59:48,309 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 13:59:48,312 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-14 13:59:48,314 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 13:59:48,315 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 13:59:48,317 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 13:59:48,318 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 13:59:48,320 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 13:59:48,321 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 13:59:48,322 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 13:59:48,324 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 13:59:48,325 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 13:59:48,327 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 13:59:48,329 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 13:59:48,331 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 13:59:48,332 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 13:59:48,333 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 13:59:48,335 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 13:59:48,337 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 13:59:48,338 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 13:59:48,340 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 13:59:48,342 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 13:59:48,343 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 13:59:48,345 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 13:59:48,347 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 13:59:48,348 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 13:59:48,349 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 13:59:48,351 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 13:59:48,352 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 13:59:48,354 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 13:59:48,355 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 13:59:48,356 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:03:43,913 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:03:43,915 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:03:43,917 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:03:43,919 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:03:43,921 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:03:43,923 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:03:43,926 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:03:43,929 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:03:43,932 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:03:43,935 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:03:43,938 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:03:43,940 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:03:43,942 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:03:43,943 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:03:43,945 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:03:43,946 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:03:43,948 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:03:43,949 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:03:43,951 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:03:43,954 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:03:43,956 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:03:43,958 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:03:43,959 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:03:43,961 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:03:43,963 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-14 14:03:43,965 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:03:43,967 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:03:43,970 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:03:43,973 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:03:43,975 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:03:43,977 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:03:43,979 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:03:43,981 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:03:43,983 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:03:43,987 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:03:43,989 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:03:43,991 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:03:43,993 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:03:43,995 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:03:43,998 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:03:44,000 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:03:44,003 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:03:44,005 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:03:44,007 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:03:44,010 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:03:44,013 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:03:44,015 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:03:44,019 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:03:44,022 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:03:44,025 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:03:44,027 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:03:44,030 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:03:44,032 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:03:44,035 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:03:44,152 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:03:44,154 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:03:44,156 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:03:44,158 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:03:44,161 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:03:44,162 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:03:44,165 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:03:44,168 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:03:44,170 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:03:44,172 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:03:44,174 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:03:44,176 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:03:44,177 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:03:44,179 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:03:44,181 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:03:44,183 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:03:44,185 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:03:44,187 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:03:44,189 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:03:44,190 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:03:44,192 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:03:44,193 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:03:44,195 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:03:44,197 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:03:44,199 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-14 14:03:44,200 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:03:44,203 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:03:44,205 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:03:44,206 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:03:44,208 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:03:44,210 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:03:44,211 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:03:44,213 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:03:44,215 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:03:44,217 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:03:44,230 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:03:44,232 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:03:44,237 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:03:44,239 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:03:44,240 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:03:44,242 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:03:44,243 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:03:44,244 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:03:44,246 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:03:44,248 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:03:44,250 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:03:44,252 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:03:44,255 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:03:44,256 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:03:44,258 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:03:44,260 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:03:44,262 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:03:44,264 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:03:44,265 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:03:44,268 [INFO] Enviando para IA - Prompt (sem imagem): Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"N√£o foram encontrados arquivos de √°udio no diret√≥rio {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcri√ß√£o salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diret√≥rio atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Defini√ß√£o dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Cria√ß√£o dos diret√≥rios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configura√ß√£o do layout do Streamlit
st.set_page_config(
    page_title="An√°lise de A√ß√µes",
    layout="wide",
    page_icon="üìà"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscila√ß√µes

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Per√≠odo', 'Oscila√ß√£o'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("üìä Dashboard de An√°lise de A√ß√µes")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma a√ß√£o", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"üìå Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("üîç Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descri√ß√£o', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa a√ß√£o.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("üí∞ Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("üìâ Oscila√ß√µes")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("üìà Tabela Geral de A√ß√µes")
        st.dataframe(self.acoes)

# Execu√ß√£o
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para an√°lise fundamentalista de a√ß√µes da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from config.common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"N√£o foram encontrados arquivos .txt no diret√≥rio {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conte√∫do do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conte√∫do HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conte√∫do HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se est√° correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configura√ß√£o de v√≠deos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configura√ß√£o de √°udios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcri√ß√µes e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diret√≥rio
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diret√≥rio.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do √∫ltimo arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conte√∫do do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimens√µes do embedding:", embedding.shape)

# Caminho do diret√≥rio de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o √∫ltimo embedding
load_and_print_embedding(embedding_directory)


# test_whisper.py

from transformers import pipeline
from common_paths import CommonPaths

commmom_paths = CommonPaths()

# Fun√ß√£o para ler a transcri√ß√£o de um arquivo
def read_transcription(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()
    return lines

# Fun√ß√£o para converter a pontua√ß√£o em estrelas
def score_to_stars(score):
    if score >= 0.75:
        return 5
    elif score >= 0.55:
        return 4
    elif score >= 0.35:
        return 3
    elif score >= 0.15:
        return 2
    else:
        return 1

# Inicializar o pipeline de an√°lise de sentimento
sentiment_pipeline = pipeline("sentiment-analysis", model="neuralmind/bert-base-portuguese-cased")

# Caminho para o arquivo de transcri√ß√£o
file_path = commmom_paths.TRANSCRIPTION_OUTPUT_PATH / 'feedback.txt'

# Ler a transcri√ß√£o
lines = read_transcription(file_path)

# Analisar sentimento para cada feedback
for line in lines:
    sentiment = sentiment_pipeline(line.strip())
    score = sentiment[0]['score']
    stars = score_to_stars(score)
    
    # Ajuste manual para feedbacks com aspectos positivos e negativos
    if any(word in line.lower() for word in ["mas", "por√©m", "no entanto"]):
        stars = max(1, stars - 1)  # Reduz uma estrela se houver contradi√ß√£o
    
    # Ajuste adicional baseado em palavras-chave
    positive_keywords = ["excelente", "√≥timo", "perfeito", "r√°pido", "eficiente", "satisfeito", "clara", "eficaz", "bom trabalho"]
    negative_keywords = ["ruim", "p√©ssimo", "horr√≠vel", "poderia", "n√£o resolveu", "insatisfat√≥ria", "confusa", "inadequado", "lento", "n√£o houve seguimento", "n√£o atendeu", "n√£o funcionou", "demorado"]
    
    positive_count = sum(word in line.lower() for word in positive_keywords)
    negative_count = sum(word in line.lower() for word in negative_keywords)
    
    # Ajuste de estrelas baseado na contagem de palavras-chave
    if positive_count > negative_count:
        stars = min(5, stars + 1)
    elif negative_count > positive_count:
        stars = max(1, stars - 1)
    
    print(f"Texto: {line.strip()}")
    print(f"Sentimento: {stars} estrelas")
    print(f"Pontua√ß√£o: {score:.2f}")
    print('---')


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configura√ß√£o de transcri√ß√µes e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diret√≥rio onde est√° procurando as transcri√ß√µes
        print(f"Diret√≥rio de entrada das transcri√ß√µes: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcri√ß√£o no diret√≥rio de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcri√ß√£o encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo n√£o encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcri√ß√£o do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} est√° vazio.")
                    return

            # Gerar o embedding da transcri√ß√£o
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcri√ß√£o e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de √°udio
    with sr.Microphone() as source:
        print("Diga algo em portugu√™s...")

        while True:
            try:
                # Escuta o √°udio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {texto_portugues}")

                # Traduz o texto para o ingl√™s
                traducao = translator.translate(texto_portugues)
                print(f"Tradu√ß√£o para o ingl√™s: {traducao}")

            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usu√°rio")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configura√ß√µes de √°udio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de √°udio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em portugu√™s...")

try:
    audio_buffer = []

    while True:
        # L√™ dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o √°udio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o √°udio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradu√ß√£o
            print(f"Tradu√ß√£o para o ingl√™s: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usu√°rio")

    # Fecha o stream de √°udio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O v√≠deo {video_path} n√£o cont√©m √°udio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de v√≠deo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de v√≠deo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de v√≠deo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Convers√£o de v√≠deo para √°udio conclu√≠da!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configura√ß√µes da API
handler = genai('gemini-1.5-flash')

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Fun√ß√£o para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conte√∫do com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares √∫nicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcri√ß√£o obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' n√£o √© uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuni√µes: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de reuni√µes: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|t√≥picos da|pontos (relevantes|principais)|an√°lise)\b.*\b(reuni√£o|√∫ltima (reuni√£o|conversa|sess√£o))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como an√°lise de reuni√£o.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padr√£o.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que s√£o muito similares ao comando atual para evitar redund√¢ncia.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padr√£o: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # M√©todos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de an√°lise de reuni√£o.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conte√∫do usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inv√°lida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar vari√°veis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conte√∫do com base no prompt, contexto e reuni√£o"""
        try:
            # Supondo que a API espera um dicion√°rio com os par√¢metros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisi√ß√£o para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar vari√°veis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Cria√ß√£o da conclus√£o do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que voc√™ acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["fa√ßa um resumo da √∫ltima reuni√£o.", "t√≥picos da √∫ltima reuni√£o", "resuma a √∫ltima reuni√£o", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Voc√™ √© respons√°vel por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - N√£o acrescentar t√≠tulo nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informa√ß√µes acima e forne√ßa uma solu√ß√£o detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Voc√™ √© respons√°vel analisar com detalhes a reuni√£o de [str_meeting], e fornecer uma longa est√≥ria sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precis√£o da resposta.
            - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e fa√ßa uma pesquisa online para uma resposta mais precisa das quest√µes de [str_text]
        - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [context]: Regras B√°sicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("√Åudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum √°udio detectado.")
                return None
            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configura√ß√µes da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicion√°rio
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta n√£o est√° no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API: {e}")
        return None

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: Gere um README.md para minha aplica√ß√£o atual.
Assistant: Com base nos arquivos fornecidos, aqui est√° um esbo√ßo de um arquivo `README.md` para sua aplica√ß√£o:

```markdown
# Vis√£o Geral da Aplica√ß√£o

Esta aplica√ß√£o √© uma plataforma vers√°til de IA projetada para realizar diversas tarefas, incluindo:

*   **Processamento de √Åudio e V√≠deo:** Convers√£o de v√≠deos para √°udio e transcri√ß√£o de √°udio para texto.
*   **An√°lise de Imagens:** An√°lise de imagens usando modelos de IA e gera√ß√£o de resumos.
*   **Assistente de Voz Inteligente:** Interface de assistente de voz para intera√ß√£o com a aplica√ß√£o, capaz de responder a perguntas, realizar pesquisas online e analisar sentimentos.
*   **Gera√ß√£o de Embeddings:** Cria√ß√£o de embeddings de texto para tarefas de similaridade sem√¢ntica e armazenamento de informa√ß√µes.
*   **An√°lise de A√ß√µes da B3:** Ferramentas para an√°lise fundamentalista de a√ß√µes da bolsa brasileira.

## Estrutura do Projeto

A aplica√ß√£o √© organizada nos seguintes diret√≥rios principais:

*   `app_config`: Cont√©m configura√ß√µes de n√≠vel de aplicativo e utilit√°rios para gerenciamento de diret√≥rios.
*   `audio_to_text`: M√≥dulos para convers√£o de √°udio para texto usando o Whisper.
*   `chat_app`: C√≥digo para a interface de chat Streamlit, incluindo processamento de imagem e integra√ß√£o com os modelos Gemini.
*   `common_paths`: Define caminhos comuns para diret√≥rios de entrada e sa√≠da de arquivos.
*   `fundamentus_api`: Implementa funcionalidades para an√°lise de a√ß√µes da B3.
*   `ia_generator.py`: Script para gerar conte√∫do HTML a partir de transcri√ß√µes usando uma API externa.
*   `main.py`: Ponto de entrada principal para executar v√°rias tarefas de processamento.
*   `send_embeddings_database`: Configura√ß√µes e scripts para enviar embeddings para um banco de dados.
*   `services`: Cont√©m v√°rios servi√ßos como processamento de imagens, servi√ßos de documentos (Word, Markdown) e comunica√ß√£o com modelos de IA (Gemini).
*   `text_to_embedding`: M√≥dulos para gerar embeddings de texto usando o SentenceTransformer.
*   `transcriptions`: Configura√ß√µes relacionadas √†s transcri√ß√µes de √°udio e envio para bancos de dados.
*   `translate`: Scripts para tradu√ß√£o de voz e texto, incluindo tradu√ß√£o em tempo real.
*   `video_to_audio`: M√≥dulos para extrair √°udio de arquivos de v√≠deo.
*   `voice_assistent`: Implementa√ß√£o do assistente de voz inteligente, incluindo interpreta√ß√£o de comandos, intera√ß√£o com modelos de IA e gerenciamento de contexto.

## Pr√©-requisitos

Antes de executar a aplica√ß√£o, certifique-se de ter instalado o seguinte:

*   **Python:** Python 3.6 ou superior
*   **Bibliotecas:** As depend√™ncias podem ser instaladas usando `pip`:
    ```bash
    pip install -r requirements.txt
    ```
    _Nota: Um arquivo `requirements.txt` completo e atualizado deve ser criado para garantir a instala√ß√£o correta de todas as depend√™ncias._

*   **Vari√°veis de Ambiente:**
    *   `API_KEY_GEMINI`: Necess√°ria para acessar os modelos Gemini.
    *   `GROQ_API_KEY`: Necess√°ria para acessar a API Groq (se usada).

## Configura√ß√£o

1.  **Clone o Reposit√≥rio:**
    ```bash
    git clone <repository_url>
    cd <application_directory>
    ```

2.  **Instale as Depend√™ncias:**
    ```bash
    pip install -r requirements.txt
    ```

3.  **Configure as Vari√°veis de Ambiente:**
    *   Crie um arquivo `.env` no diret√≥rio raiz.
    *   Adicione as chaves de API necess√°rias:
        ```
        API_KEY_GEMINI=your_gemini_api_key
        GROQ_API_KEY=your_groq_api_key
        ```

## Execu√ß√£o

A aplica√ß√£o pode ser executada atrav√©s do script `main.py` para processamento de √°udio/v√≠deo e gera√ß√£o de embeddings:

```bash
python main.py
```

Para executar a interface de chat Streamlit:

```bash
streamlit run chat_app/chat_streamlit.py
```

Para iniciar o assistente de voz:

```bash
python voice_assistent/main.py
```

## Uso

### Chat Streamlit

1.  Execute o aplicativo Streamlit.
2.  Interaja com o chatbot via interface web, inserindo texto ou enviando imagens.
3.  Use a barra lateral para configurar par√¢metros ou limpar o hist√≥rico da conversa.

### Assistente de Voz

1.  Execute o script do assistente de voz.
2.  Siga as instru√ß√µes para usar comandos de voz ou de texto para interagir com o assistente.
3.  O assistente pode responder a perguntas, realizar pesquisas online, analisar o sentimento dos feedbacks e muito mais.

## Arquivos de Configura√ß√£o

*   `chat_app/config/config.py`: Cont√©m os caminhos de arquivos e configura√ß√µes gerais para a aplica√ß√£o de chat.
*   `voice_assistent/config.py`: Configura√ß√µes para o assistente de voz, incluindo chaves de API e par√¢metros.
*   `app_config/app_config.py`: Configura√ß√µes b√°sicas para a aplica√ß√£o como os path
*   `text_to_embedding/texto_to_embedding.py`: Configura√ß√£o do modelo de embedding
## Estrutura de Diret√≥rios

```
.
‚îú‚îÄ‚îÄ app_config/
‚îú‚îÄ‚îÄ audio_to_text/
‚îú‚îÄ‚îÄ chat_app/
‚îú‚îÄ‚îÄ common_paths/
‚îú‚îÄ‚îÄ fundamentus_api/
‚îú‚îÄ‚îÄ ia_generator.py
‚îú‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ send_embeddings_database/
‚îú‚îÄ‚îÄ services/
‚îú‚îÄ‚îÄ text_to_embedding/
‚îú‚îÄ‚îÄ transcriptions/
‚îú‚îÄ‚îÄ translate/
‚îú‚îÄ‚îÄ video_to_audio/
‚îú‚îÄ‚îÄ voice_assistent/
‚îú‚îÄ‚îÄ test_whisper.py
‚îî‚îÄ‚îÄ README.md
```

## Contribui√ß√£o

Contribui√ß√µes s√£o bem-vindas! Sinta-se √† vontade para enviar pull requests para melhorar a aplica√ß√£o.

## Licen√ßa

Este projeto √© licenciado sob a [MIT License](LICENSE).
```

**Notas:**

*   **`requirements.txt`**: Certifique-se de gerar um arquivo `requirements.txt` preciso usando o comando `pip freeze > requirements.txt` no seu ambiente para que outras pessoas possam instalar as depend√™ncias corretamente.
*   **Vari√°veis de Ambiente**: Enfatize a import√¢ncia de proteger as chaves de API e nunca commit√°-las diretamente no c√≥digo.
*   **Instru√ß√µes de Execu√ß√£o**: Inclua instru√ß√µes detalhadas sobre como executar cada parte da aplica√ß√£o.
*   **Estrutura de Diret√≥rios**: Forne√ßa uma vis√£o geral clara da estrutura de diret√≥rios para facilitar a navega√ß√£o e a compreens√£o do projeto.
*   **Licen√ßa**: Adicione um arquivo `LICENSE` ao seu reposit√≥rio e especifique a licen√ßa no README.

Este esqueleto deve fornecer um bom ponto de partida para o seu `README.md`. Lembre-se de adapt√°-lo √†s necessidades e detalhes espec√≠ficos do seu projeto!
User: Aplique SOLID e POO na aplica√ß√£o
2025-04-14 14:03:55,459 [INFO] Resposta da IA (texto): A aplica√ß√£o parece ter v√°rios componentes, cada um com suas pr√≥prias responsabilidades. Aqui est√° uma an√°lise de como os princ√≠pios SOLID e POO podem ser aplicados para melhorar a estrutura e a manuten√ß√£o do c√≥digo.

### Princ√≠pios SOLID

1.  **Single Responsibility Principle (SRP)**

    *   **Problema:** Algumas classes podem estar acumulando muitas responsabilidades. Por exemplo, a classe `ImageProcessor` em `services/image_processor.py` lida tanto com o processamento de imagens quanto com a intera√ß√£o com o modelo GPT, o servi√ßo de documentos e o hist√≥rico.
    *   **Solu√ß√£o:** Dividir a classe `ImageProcessor` em classes menores, cada uma com uma √∫nica responsabilidade.

        *   Criar uma classe `ImageAnalyzer` que lide especificamente com a an√°lise da imagem usando o modelo GPT.
        *   Criar uma classe `DocumentUpdater` que lide com a atualiza√ß√£o do documento (Word, Markdown).
        *   Criar uma classe `HistoryManager` para gerenciar o hist√≥rico de an√°lises.

2.  **Open/Closed Principle (OCP)**

    *   **Problema:** Se voc√™ precisar adicionar um novo tipo de an√°lise de imagem (por exemplo, an√°lise de sentimento), pode ser necess√°rio modificar a classe `ImageProcessor`.
    *   **Solu√ß√£o:** Projetar interfaces ou classes abstratas para definir contratos para as diferentes an√°lises de imagem.

        *   Criar uma interface `ImageAnalysisStrategy` com um m√©todo `analyze(image_path)`.
        *   Implementar classes concretas para cada tipo de an√°lise (por exemplo, `ObjectDetectionAnalysis`, `SentimentAnalysis`).
        *   A classe `ImageAnalyzer` receberia uma `ImageAnalysisStrategy` via construtor e delegaria a an√°lise para essa estrat√©gia.

3.  **Liskov Substitution Principle (LSP)**

    *   **Problema:** Se voc√™ tem uma classe base `AppConfig` e classes derivadas como `AudioConfig` e `VideoConfig`, voc√™ deve garantir que as classes derivadas possam ser usadas no lugar da classe base sem quebrar o programa.
    *   **Solu√ß√£o:** Garantir que todas as classes derivadas de `AppConfig` implementem os m√©todos da classe base de forma consistente.

        *   Verificar se o m√©todo `create_directories` em `AudioConfig` e `VideoConfig` funciona como esperado quando chamado com inst√¢ncias de `AppConfig`.

4.  **Interface Segregation Principle (ISP)**

    *   **Problema:** Se uma interface tem muitos m√©todos que uma classe n√£o precisa implementar, essa classe √© for√ßada a implementar m√©todos que n√£o usa.
    *   **Solu√ß√£o:** Dividir interfaces grandes em interfaces menores e mais espec√≠ficas.

        *   Se voc√™ tem uma interface `TextToSpeechInterface` com m√©todos como `speak()` e `pause()`, mas uma classe s√≥ precisa de `speak()`, criar uma interface `Speakable` com apenas o m√©todo `speak()`.

5.  **Dependency Inversion Principle (DIP)**

    *   **Problema:** Classes de alto n√≠vel (por exemplo, `MainApp`) dependem de classes de baixo n√≠vel (por exemplo, `VoiceCommandHandler`, `TextCommandHandler`).
    *   **Solu√ß√£o:** Classes de alto n√≠vel n√£o devem depender de classes de baixo n√≠vel. Ambas devem depender de abstra√ß√µes.

        *   Criar interfaces para os command handlers (por exemplo, `CommandHandlerInterface`) e fazer com que `MainApp` dependa dessa interface em vez de depender diretamente das classes `VoiceCommandHandler` e `TextCommandHandler`.

### POO (Programa√ß√£o Orientada a Objetos)

*   **Encapsulamento:**
    *   **Problema:** Alguns dados podem n√£o estar devidamente encapsulados, resultando em acesso direto a atributos.
    *   **Solu√ß√£o:** Use propriedades (`@property`) para controlar o acesso aos atributos.
*   **Heran√ßa:**
    *   **Problema:** Pode haver uso excessivo ou inadequado de heran√ßa, o que pode levar a classes infladas e dif√≠ceis de manter.
    *   **Solu√ß√£o:** Prefira composi√ß√£o em vez de heran√ßa sempre que poss√≠vel.
*   **Polimorfismo:**
    *   **Problema:** Falta de polimorfismo pode resultar em c√≥digo repetitivo e dif√≠cil de estender.
    *   **Solu√ß√£o:** Use interfaces e classes abstratas para definir contratos e implementar polimorfismo.

### Aplicando as Mudan√ßas

1.  **Refatora√ß√£o do `ImageProcessor`:**

```python
# services/image_analyzer.py
class ImageAnalyzer:
    def __init__(self, analysis_strategy):
        self.analysis_strategy = analysis_strategy

    def analyze(self, image_path):
        return self.analysis_strategy.analyze(image_path)

# services/document_updater.py
class DocumentUpdater:
    def __init__(self, document_service, markdown_service):
        self.document_service = document_service
        self.markdown_service = markdown_service

    def update_documents(self, image_name, summary):
        self.document_service.add_image_summary(image_name, summary)
        self.markdown_service.add_image_summary(image_name, summary)
        self.document_service.save_document()
        self.markdown_service.save_markdown()

# services/history_manager.py
class HistoryManager:
    def __init__(self):
        self.history = []

    def update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})

    def get_history(self):
        return self.history

# services/image_processing_strategy.py
from abc import ABC, abstractmethod

class ImageAnalysisStrategy(ABC):
    @abstractmethod
    def analyze(self, image_path):
        pass

# services/gpt_analysis_strategy.py
class GPTAnalysisStrategy(ImageAnalysisStrategy):
    def __init__(self, gpt_handler, prompt):
        self.gpt_handler = gpt_handler
        self.prompt = prompt

    def analyze(self, image_path):
        # Your GPT analysis logic here
        return self.gpt_handler.generate_content_from_image(image_path, self.prompt)

# services/image_processor.py
from services.image_analyzer import ImageAnalyzer
from services.document_updater import DocumentUpdater
from services.history_manager import HistoryManager
from services.gpt_analysis_strategy import GPTAnalysisStrategy

class ImageProcessor:
    def __init__(self, gpt_handler, document_service, markdown_service):
        self.gpt_handler = gpt_handler
        self.document_updater = DocumentUpdater(document_service, markdown_service)
        self.history_manager = HistoryManager()
        self.prompt = self._load_prompt()

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                # Use a strategy for analysis
                analysis_strategy = GPTAnalysisStrategy(self.gpt_handler, self.prompt)
                image_analyzer = ImageAnalyzer(analysis_strategy)

                # Now analyze
                summary = image_analyzer.analyze(os.path.join(Config.ASSETS_DIR, image_name))
                self.document_updater.update_documents(image_name, summary)
                self.history_manager.update_history(image_name, summary)

                time.sleep(4)
                logger.info("Preparando a pr√≥xima an√°lise...")

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)
```

### Conclus√£o

Aplicar os princ√≠pios SOLID e POO pode resultar em um c√≥digo mais modular, test√°vel e f√°cil de manter. A refatora√ß√£o envolveria a decomposi√ß√£o de classes grandes em unidades menores e mais coesas, o uso de interfaces para definir contratos claros e a aplica√ß√£o de inje√ß√£o de depend√™ncia para reduzir o acoplamento.
2025-04-14 14:03:55,563 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:03:55,565 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:03:55,567 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:03:55,570 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:03:55,572 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:03:55,573 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:03:55,575 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:03:55,577 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:03:55,579 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:03:55,580 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:03:55,582 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:03:55,584 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:03:55,587 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:03:55,589 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:03:55,590 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:03:55,592 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:03:55,594 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:03:55,595 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:03:55,596 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:03:55,597 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:03:55,599 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:03:55,600 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:03:55,602 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:03:55,604 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:03:55,606 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-14 14:03:55,608 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:03:55,609 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:03:55,610 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:03:55,612 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:03:55,613 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:03:55,614 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:03:55,615 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:03:55,617 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:03:55,618 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:03:55,620 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:03:55,622 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:03:55,624 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:03:55,625 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:03:55,626 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:03:55,628 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:03:55,629 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:03:55,630 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:03:55,631 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:03:55,633 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:03:55,634 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:03:55,635 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:03:55,637 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:03:55,639 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:03:55,641 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:03:55,642 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:03:55,643 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:03:55,644 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:03:55,645 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:03:55,646 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:14:28,795 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:14:28,797 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:14:28,800 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:14:28,802 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:14:28,804 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:14:28,805 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:14:28,807 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:14:28,808 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:14:28,809 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:14:28,811 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:14:28,813 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:14:28,815 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:14:28,816 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:14:28,817 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:14:28,819 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:14:28,821 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:14:28,823 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:14:28,825 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:14:28,826 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:14:28,828 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:14:28,829 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:14:28,830 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:14:28,831 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:14:28,833 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:14:28,834 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:14:28,836 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:14:28,838 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:14:28,840 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:14:28,844 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:14:28,845 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:14:28,847 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:14:28,849 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:14:28,852 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:14:28,854 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:14:28,855 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:14:28,858 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:14:28,859 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:14:28,860 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:14:28,862 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:14:28,863 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:14:28,865 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:14:28,866 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:14:28,868 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:14:28,870 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:14:28,873 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:14:28,876 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:14:28,877 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:14:28,879 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:14:28,880 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:14:28,881 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:14:28,883 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:14:28,885 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:14:28,887 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:14:29,021 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:14:29,023 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:14:29,025 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:14:29,026 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:14:29,028 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:14:29,030 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:14:29,032 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:14:29,035 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:14:29,038 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:14:29,040 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:14:29,041 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:14:29,043 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:14:29,044 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:14:29,046 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:14:29,048 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:14:29,049 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:14:29,051 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:14:29,053 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:14:29,054 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:14:29,056 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:14:29,058 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:14:29,059 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:14:29,061 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:14:29,062 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:14:29,064 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:14:29,066 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:14:29,067 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:14:29,073 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:14:29,078 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:14:29,080 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:14:29,082 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:14:29,085 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:14:29,087 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:14:29,089 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:14:29,091 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:14:29,092 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:14:29,094 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:14:29,095 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:14:29,096 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:14:29,097 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:14:29,099 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:14:29,100 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:14:29,103 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:14:29,106 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:14:29,108 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:14:29,109 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:14:29,111 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:14:29,113 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:14:29,114 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:14:29,115 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:14:29,117 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:14:29,119 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:14:29,120 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:14:29,124 [INFO] Enviando para IA - Prompt (sem imagem): Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"N√£o foram encontrados arquivos de √°udio no diret√≥rio {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcri√ß√£o salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diret√≥rio atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Defini√ß√£o dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Cria√ß√£o dos diret√≥rios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configura√ß√£o do layout do Streamlit
st.set_page_config(
    page_title="An√°lise de A√ß√µes",
    layout="wide",
    page_icon="üìà"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscila√ß√µes

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Per√≠odo', 'Oscila√ß√£o'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("üìä Dashboard de An√°lise de A√ß√µes")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma a√ß√£o", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"üìå Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("üîç Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descri√ß√£o', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa a√ß√£o.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("üí∞ Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("üìâ Oscila√ß√µes")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("üìà Tabela Geral de A√ß√µes")
        st.dataframe(self.acoes)

# Execu√ß√£o
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para an√°lise fundamentalista de a√ß√µes da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from config.common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"N√£o foram encontrados arquivos .txt no diret√≥rio {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conte√∫do do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conte√∫do HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conte√∫do HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se est√° correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configura√ß√£o de v√≠deos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configura√ß√£o de √°udios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcri√ß√µes e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diret√≥rio
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diret√≥rio.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do √∫ltimo arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conte√∫do do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimens√µes do embedding:", embedding.shape)

# Caminho do diret√≥rio de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o √∫ltimo embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configura√ß√£o de transcri√ß√µes e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diret√≥rio onde est√° procurando as transcri√ß√µes
        print(f"Diret√≥rio de entrada das transcri√ß√µes: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcri√ß√£o no diret√≥rio de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcri√ß√£o encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo n√£o encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcri√ß√£o do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} est√° vazio.")
                    return

            # Gerar o embedding da transcri√ß√£o
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcri√ß√£o e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de √°udio
    with sr.Microphone() as source:
        print("Diga algo em portugu√™s...")

        while True:
            try:
                # Escuta o √°udio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {texto_portugues}")

                # Traduz o texto para o ingl√™s
                traducao = translator.translate(texto_portugues)
                print(f"Tradu√ß√£o para o ingl√™s: {traducao}")

            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usu√°rio")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configura√ß√µes de √°udio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de √°udio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em portugu√™s...")

try:
    audio_buffer = []

    while True:
        # L√™ dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o √°udio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o √°udio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradu√ß√£o
            print(f"Tradu√ß√£o para o ingl√™s: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usu√°rio")

    # Fecha o stream de √°udio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O v√≠deo {video_path} n√£o cont√©m √°udio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de v√≠deo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de v√≠deo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de v√≠deo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Convers√£o de v√≠deo para √°udio conclu√≠da!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configura√ß√µes da API
handler = genai('gemini-1.5-flash')

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Fun√ß√£o para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conte√∫do com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares √∫nicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcri√ß√£o obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' n√£o √© uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuni√µes: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de reuni√µes: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|t√≥picos da|pontos (relevantes|principais)|an√°lise)\b.*\b(reuni√£o|√∫ltima (reuni√£o|conversa|sess√£o))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como an√°lise de reuni√£o.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padr√£o.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que s√£o muito similares ao comando atual para evitar redund√¢ncia.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padr√£o: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # M√©todos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de an√°lise de reuni√£o.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conte√∫do usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inv√°lida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar vari√°veis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conte√∫do com base no prompt, contexto e reuni√£o"""
        try:
            # Supondo que a API espera um dicion√°rio com os par√¢metros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisi√ß√£o para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar vari√°veis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Cria√ß√£o da conclus√£o do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que voc√™ acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["fa√ßa um resumo da √∫ltima reuni√£o.", "t√≥picos da √∫ltima reuni√£o", "resuma a √∫ltima reuni√£o", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Voc√™ √© respons√°vel por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - N√£o acrescentar t√≠tulo nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informa√ß√µes acima e forne√ßa uma solu√ß√£o detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Voc√™ √© respons√°vel analisar com detalhes a reuni√£o de [str_meeting], e fornecer uma longa est√≥ria sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precis√£o da resposta.
            - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e fa√ßa uma pesquisa online para uma resposta mais precisa das quest√µes de [str_text]
        - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [context]: Regras B√°sicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("√Åudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum √°udio detectado.")
                return None
            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configura√ß√µes da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicion√°rio
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta n√£o est√° no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API: {e}")
        return None

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: Gere um README.md para minha aplica√ß√£o atual.
Assistant: Com base nos arquivos fornecidos, aqui est√° um esbo√ßo de um arquivo `README.md` para sua aplica√ß√£o:

```markdown
# Vis√£o Geral da Aplica√ß√£o

Esta aplica√ß√£o √© uma plataforma vers√°til de IA projetada para realizar diversas tarefas, incluindo:

*   **Processamento de √Åudio e V√≠deo:** Convers√£o de v√≠deos para √°udio e transcri√ß√£o de √°udio para texto.
*   **An√°lise de Imagens:** An√°lise de imagens usando modelos de IA e gera√ß√£o de resumos.
*   **Assistente de Voz Inteligente:** Interface de assistente de voz para intera√ß√£o com a aplica√ß√£o, capaz de responder a perguntas, realizar pesquisas online e analisar sentimentos.
*   **Gera√ß√£o de Embeddings:** Cria√ß√£o de embeddings de texto para tarefas de similaridade sem√¢ntica e armazenamento de informa√ß√µes.
*   **An√°lise de A√ß√µes da B3:** Ferramentas para an√°lise fundamentalista de a√ß√µes da bolsa brasileira.

## Estrutura do Projeto

A aplica√ß√£o √© organizada nos seguintes diret√≥rios principais:

*   `app_config`: Cont√©m configura√ß√µes de n√≠vel de aplicativo e utilit√°rios para gerenciamento de diret√≥rios.
*   `audio_to_text`: M√≥dulos para convers√£o de √°udio para texto usando o Whisper.
*   `chat_app`: C√≥digo para a interface de chat Streamlit, incluindo processamento de imagem e integra√ß√£o com os modelos Gemini.
*   `common_paths`: Define caminhos comuns para diret√≥rios de entrada e sa√≠da de arquivos.
*   `fundamentus_api`: Implementa funcionalidades para an√°lise de a√ß√µes da B3.
*   `ia_generator.py`: Script para gerar conte√∫do HTML a partir de transcri√ß√µes usando uma API externa.
*   `main.py`: Ponto de entrada principal para executar v√°rias tarefas de processamento.
*   `send_embeddings_database`: Configura√ß√µes e scripts para enviar embeddings para um banco de dados.
*   `services`: Cont√©m v√°rios servi√ßos como processamento de imagens, servi√ßos de documentos (Word, Markdown) e comunica√ß√£o com modelos de IA (Gemini).
*   `text_to_embedding`: M√≥dulos para gerar embeddings de texto usando o SentenceTransformer.
*   `transcriptions`: Configura√ß√µes relacionadas √†s transcri√ß√µes de √°udio e envio para bancos de dados.
*   `translate`: Scripts para tradu√ß√£o de voz e texto, incluindo tradu√ß√£o em tempo real.
*   `video_to_audio`: M√≥dulos para extrair √°udio de arquivos de v√≠deo.
*   `voice_assistent`: Implementa√ß√£o do assistente de voz inteligente, incluindo interpreta√ß√£o de comandos, intera√ß√£o com modelos de IA e gerenciamento de contexto.

## Pr√©-requisitos

Antes de executar a aplica√ß√£o, certifique-se de ter instalado o seguinte:

*   **Python:** Python 3.6 ou superior
*   **Bibliotecas:** As depend√™ncias podem ser instaladas usando `pip`:
    ```bash
    pip install -r requirements.txt
    ```
    _Nota: Um arquivo `requirements.txt` completo e atualizado deve ser criado para garantir a instala√ß√£o correta de todas as depend√™ncias._

*   **Vari√°veis de Ambiente:**
    *   `API_KEY_GEMINI`: Necess√°ria para acessar os modelos Gemini.
    *   `GROQ_API_KEY`: Necess√°ria para acessar a API Groq (se usada).

## Configura√ß√£o

1.  **Clone o Reposit√≥rio:**
    ```bash
    git clone <repository_url>
    cd <application_directory>
    ```

2.  **Instale as Depend√™ncias:**
    ```bash
    pip install -r requirements.txt
    ```

3.  **Configure as Vari√°veis de Ambiente:**
    *   Crie um arquivo `.env` no diret√≥rio raiz.
    *   Adicione as chaves de API necess√°rias:
        ```
        API_KEY_GEMINI=your_gemini_api_key
        GROQ_API_KEY=your_groq_api_key
        ```

## Execu√ß√£o

A aplica√ß√£o pode ser executada atrav√©s do script `main.py` para processamento de √°udio/v√≠deo e gera√ß√£o de embeddings:

```bash
python main.py
```

Para executar a interface de chat Streamlit:

```bash
streamlit run chat_app/chat_streamlit.py
```

Para iniciar o assistente de voz:

```bash
python voice_assistent/main.py
```

## Uso

### Chat Streamlit

1.  Execute o aplicativo Streamlit.
2.  Interaja com o chatbot via interface web, inserindo texto ou enviando imagens.
3.  Use a barra lateral para configurar par√¢metros ou limpar o hist√≥rico da conversa.

### Assistente de Voz

1.  Execute o script do assistente de voz.
2.  Siga as instru√ß√µes para usar comandos de voz ou de texto para interagir com o assistente.
3.  O assistente pode responder a perguntas, realizar pesquisas online, analisar o sentimento dos feedbacks e muito mais.

## Arquivos de Configura√ß√£o

*   `chat_app/config/config.py`: Cont√©m os caminhos de arquivos e configura√ß√µes gerais para a aplica√ß√£o de chat.
*   `voice_assistent/config.py`: Configura√ß√µes para o assistente de voz, incluindo chaves de API e par√¢metros.
*   `app_config/app_config.py`: Configura√ß√µes b√°sicas para a aplica√ß√£o como os path
*   `text_to_embedding/texto_to_embedding.py`: Configura√ß√£o do modelo de embedding
## Estrutura de Diret√≥rios

```
.
‚îú‚îÄ‚îÄ app_config/
‚îú‚îÄ‚îÄ audio_to_text/
‚îú‚îÄ‚îÄ chat_app/
‚îú‚îÄ‚îÄ common_paths/
‚îú‚îÄ‚îÄ fundamentus_api/
‚îú‚îÄ‚îÄ ia_generator.py
‚îú‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ send_embeddings_database/
‚îú‚îÄ‚îÄ services/
‚îú‚îÄ‚îÄ text_to_embedding/
‚îú‚îÄ‚îÄ transcriptions/
‚îú‚îÄ‚îÄ translate/
‚îú‚îÄ‚îÄ video_to_audio/
‚îú‚îÄ‚îÄ voice_assistent/
‚îú‚îÄ‚îÄ test_whisper.py
‚îî‚îÄ‚îÄ README.md
```

## Contribui√ß√£o

Contribui√ß√µes s√£o bem-vindas! Sinta-se √† vontade para enviar pull requests para melhorar a aplica√ß√£o.

## Licen√ßa

Este projeto √© licenciado sob a [MIT License](LICENSE).
```

**Notas:**

*   **`requirements.txt`**: Certifique-se de gerar um arquivo `requirements.txt` preciso usando o comando `pip freeze > requirements.txt` no seu ambiente para que outras pessoas possam instalar as depend√™ncias corretamente.
*   **Vari√°veis de Ambiente**: Enfatize a import√¢ncia de proteger as chaves de API e nunca commit√°-las diretamente no c√≥digo.
*   **Instru√ß√µes de Execu√ß√£o**: Inclua instru√ß√µes detalhadas sobre como executar cada parte da aplica√ß√£o.
*   **Estrutura de Diret√≥rios**: Forne√ßa uma vis√£o geral clara da estrutura de diret√≥rios para facilitar a navega√ß√£o e a compreens√£o do projeto.
*   **Licen√ßa**: Adicione um arquivo `LICENSE` ao seu reposit√≥rio e especifique a licen√ßa no README.

Este esqueleto deve fornecer um bom ponto de partida para o seu `README.md`. Lembre-se de adapt√°-lo √†s necessidades e detalhes espec√≠ficos do seu projeto!
User: Aplique SOLID e POO na aplica√ß√£o
Assistant: A aplica√ß√£o parece ter v√°rios componentes, cada um com suas pr√≥prias responsabilidades. Aqui est√° uma an√°lise de como os princ√≠pios SOLID e POO podem ser aplicados para melhorar a estrutura e a manuten√ß√£o do c√≥digo.

### Princ√≠pios SOLID

1.  **Single Responsibility Principle (SRP)**

    *   **Problema:** Algumas classes podem estar acumulando muitas responsabilidades. Por exemplo, a classe `ImageProcessor` em `services/image_processor.py` lida tanto com o processamento de imagens quanto com a intera√ß√£o com o modelo GPT, o servi√ßo de documentos e o hist√≥rico.
    *   **Solu√ß√£o:** Dividir a classe `ImageProcessor` em classes menores, cada uma com uma √∫nica responsabilidade.

        *   Criar uma classe `ImageAnalyzer` que lide especificamente com a an√°lise da imagem usando o modelo GPT.
        *   Criar uma classe `DocumentUpdater` que lide com a atualiza√ß√£o do documento (Word, Markdown).
        *   Criar uma classe `HistoryManager` para gerenciar o hist√≥rico de an√°lises.

2.  **Open/Closed Principle (OCP)**

    *   **Problema:** Se voc√™ precisar adicionar um novo tipo de an√°lise de imagem (por exemplo, an√°lise de sentimento), pode ser necess√°rio modificar a classe `ImageProcessor`.
    *   **Solu√ß√£o:** Projetar interfaces ou classes abstratas para definir contratos para as diferentes an√°lises de imagem.

        *   Criar uma interface `ImageAnalysisStrategy` com um m√©todo `analyze(image_path)`.
        *   Implementar classes concretas para cada tipo de an√°lise (por exemplo, `ObjectDetectionAnalysis`, `SentimentAnalysis`).
        *   A classe `ImageAnalyzer` receberia uma `ImageAnalysisStrategy` via construtor e delegaria a an√°lise para essa estrat√©gia.

3.  **Liskov Substitution Principle (LSP)**

    *   **Problema:** Se voc√™ tem uma classe base `AppConfig` e classes derivadas como `AudioConfig` e `VideoConfig`, voc√™ deve garantir que as classes derivadas possam ser usadas no lugar da classe base sem quebrar o programa.
    *   **Solu√ß√£o:** Garantir que todas as classes derivadas de `AppConfig` implementem os m√©todos da classe base de forma consistente.

        *   Verificar se o m√©todo `create_directories` em `AudioConfig` e `VideoConfig` funciona como esperado quando chamado com inst√¢ncias de `AppConfig`.

4.  **Interface Segregation Principle (ISP)**

    *   **Problema:** Se uma interface tem muitos m√©todos que uma classe n√£o precisa implementar, essa classe √© for√ßada a implementar m√©todos que n√£o usa.
    *   **Solu√ß√£o:** Dividir interfaces grandes em interfaces menores e mais espec√≠ficas.

        *   Se voc√™ tem uma interface `TextToSpeechInterface` com m√©todos como `speak()` e `pause()`, mas uma classe s√≥ precisa de `speak()`, criar uma interface `Speakable` com apenas o m√©todo `speak()`.

5.  **Dependency Inversion Principle (DIP)**

    *   **Problema:** Classes de alto n√≠vel (por exemplo, `MainApp`) dependem de classes de baixo n√≠vel (por exemplo, `VoiceCommandHandler`, `TextCommandHandler`).
    *   **Solu√ß√£o:** Classes de alto n√≠vel n√£o devem depender de classes de baixo n√≠vel. Ambas devem depender de abstra√ß√µes.

        *   Criar interfaces para os command handlers (por exemplo, `CommandHandlerInterface`) e fazer com que `MainApp` dependa dessa interface em vez de depender diretamente das classes `VoiceCommandHandler` e `TextCommandHandler`.

### POO (Programa√ß√£o Orientada a Objetos)

*   **Encapsulamento:**
    *   **Problema:** Alguns dados podem n√£o estar devidamente encapsulados, resultando em acesso direto a atributos.
    *   **Solu√ß√£o:** Use propriedades (`@property`) para controlar o acesso aos atributos.
*   **Heran√ßa:**
    *   **Problema:** Pode haver uso excessivo ou inadequado de heran√ßa, o que pode levar a classes infladas e dif√≠ceis de manter.
    *   **Solu√ß√£o:** Prefira composi√ß√£o em vez de heran√ßa sempre que poss√≠vel.
*   **Polimorfismo:**
    *   **Problema:** Falta de polimorfismo pode resultar em c√≥digo repetitivo e dif√≠cil de estender.
    *   **Solu√ß√£o:** Use interfaces e classes abstratas para definir contratos e implementar polimorfismo.

### Aplicando as Mudan√ßas

1.  **Refatora√ß√£o do `ImageProcessor`:**

```python
# services/image_analyzer.py
class ImageAnalyzer:
    def __init__(self, analysis_strategy):
        self.analysis_strategy = analysis_strategy

    def analyze(self, image_path):
        return self.analysis_strategy.analyze(image_path)

# services/document_updater.py
class DocumentUpdater:
    def __init__(self, document_service, markdown_service):
        self.document_service = document_service
        self.markdown_service = markdown_service

    def update_documents(self, image_name, summary):
        self.document_service.add_image_summary(image_name, summary)
        self.markdown_service.add_image_summary(image_name, summary)
        self.document_service.save_document()
        self.markdown_service.save_markdown()

# services/history_manager.py
class HistoryManager:
    def __init__(self):
        self.history = []

    def update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})

    def get_history(self):
        return self.history

# services/image_processing_strategy.py
from abc import ABC, abstractmethod

class ImageAnalysisStrategy(ABC):
    @abstractmethod
    def analyze(self, image_path):
        pass

# services/gpt_analysis_strategy.py
class GPTAnalysisStrategy(ImageAnalysisStrategy):
    def __init__(self, gpt_handler, prompt):
        self.gpt_handler = gpt_handler
        self.prompt = prompt

    def analyze(self, image_path):
        # Your GPT analysis logic here
        return self.gpt_handler.generate_content_from_image(image_path, self.prompt)

# services/image_processor.py
from services.image_analyzer import ImageAnalyzer
from services.document_updater import DocumentUpdater
from services.history_manager import HistoryManager
from services.gpt_analysis_strategy import GPTAnalysisStrategy

class ImageProcessor:
    def __init__(self, gpt_handler, document_service, markdown_service):
        self.gpt_handler = gpt_handler
        self.document_updater = DocumentUpdater(document_service, markdown_service)
        self.history_manager = HistoryManager()
        self.prompt = self._load_prompt()

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                # Use a strategy for analysis
                analysis_strategy = GPTAnalysisStrategy(self.gpt_handler, self.prompt)
                image_analyzer = ImageAnalyzer(analysis_strategy)

                # Now analyze
                summary = image_analyzer.analyze(os.path.join(Config.ASSETS_DIR, image_name))
                self.document_updater.update_documents(image_name, summary)
                self.history_manager.update_history(image_name, summary)

                time.sleep(4)
                logger.info("Preparando a pr√≥xima an√°lise...")

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)
```

### Conclus√£o

Aplicar os princ√≠pios SOLID e POO pode resultar em um c√≥digo mais modular, test√°vel e f√°cil de manter. A refatora√ß√£o envolveria a decomposi√ß√£o de classes grandes em unidades menores e mais coesas, o uso de interfaces para definir contratos claros e a aplica√ß√£o de inje√ß√£o de depend√™ncia para reduzir o acoplamento.
User: Fa√ßa uma limpeza no c√≥digo em tudo que √© desnecess√°rio e que n√£o estou usando.
2025-04-14 14:14:38,276 [INFO] Resposta da IA (texto): Ok, vou analisar o c√≥digo e identificar partes que parecem desnecess√°rias ou n√£o utilizadas. Aqui est√° uma lista de poss√≠veis remo√ß√µes e otimiza√ß√µes:

### Remo√ß√µes Potenciais

1.  **Arquivos e Fun√ß√µes N√£o Utilizados:**

    *   `fundamentus_api/setup.py`: Este √© um arquivo de configura√ß√£o para o pacote `fundamentalvision`. Se voc√™ n√£o planeja distribuir este pacote, o arquivo `setup.py` n√£o √© necess√°rio.

    *   `translate/translator_to_english.py`: Este arquivo usa `speech_recognition` e `translate`. Se voc√™ n√£o est√° usando ativamente a tradu√ß√£o em tempo real por voz, pode remover este arquivo.

    *   `voice_assistent/class_voice_assistent/conversation_history.py`: Se este arquivo estiver vazio e n√£o estiver sendo usado, pode ser removido.

    *   `voice_assistent/template.py`: Este script parece ser uma vers√£o anterior ou um modelo. Se as funcionalidades foram integradas em `voice_assistent/assistent.py` ou `voice_assistent/class_voice_assistent/main.py`, pode ser removido.

    *   `ia_generator.py`: Se a gera√ß√£o de conte√∫do HTML n√£o for usada regularmente, pode ser removida.

    *   `send_embeddings_database/verify_last_enbedding.py`: Se n√£o for usado para verificar os embeddings, pode ser removido.
2.  **C√≥digo Comentado:**

    *   Remova todos os blocos de c√≥digo comentados que n√£o s√£o mais relevantes.

3.  **Importa√ß√µes Desnecess√°rias:**

    *   `chat_app/chat_streamlit.py`: Verifique todas as importa√ß√µes para garantir que cada uma seja utilizada.

4.  **Logs e Prints:**

    *   `chat_app/config/config.py`: Remova as linhas `print(f"Base Directory: {BASE_DIR}")` e `print(PROCESSED_DIR)`.
    *   `main.py`: Remova a linha `print(f"Root path: {root_path}")`.

### Otimiza√ß√µes Potenciais

1.  **Rate Limiting e Configura√ß√µes:**

    *   `chat_app/config/config.py`: Se os limites de taxa (`CHAT_RATE_LIMIT`, `API_RATE_LIMIT`) n√£o forem usados ou precisarem ser ajustados, revise essas configura√ß√µes.

2.  **Estrutura de Diret√≥rios:**

    *   Considere mover todos os arquivos de configura√ß√£o (config.py) para um diret√≥rio `config` central para melhor organiza√ß√£o.

3.  **Duplica√ß√£o de C√≥digo:**

    *   `voice_assistent/class_voice_assistent/gpt_communication/gemini_api_client.py` e `voice_assistent/class_voice_assistent/api_client.py`: Essas classes tem c√≥digo duplicado.

4.  **Verifica√ß√£o de Vari√°veis de Ambiente:**

    *   `services/gpt_services.py` e `services/image_services.py`: A verifica√ß√£o da vari√°vel de ambiente `API_KEY_GEMINI` √© feita em ambos os arquivos. Mova essa verifica√ß√£o para um local centralizado (por exemplo, `app_config/app_config.py`).
    *   `voice_assistent/class_voice_assistent/gpt_communication/gemini_gpt.py` Tamb√©m tem verifica√ß√£o do `API_KEY_GEMINI`.

### Revis√£o Detalhada de Arquivos

1.  **`main.py`:**

    *   Remova as linhas comentadas para `video_config` e `audio_config` se essas funcionalidades n√£o forem mais necess√°rias.

2.  **`chat_app/chat_streamlit.py`:**

    *   Verifique as vari√°veis de estado da sess√£o (`st.session_state`) e remova aquelas que n√£o s√£o usadas.
    *   Revise a fun√ß√£o `check_clipboard()` para garantir que ainda seja relevante.

3.  **`services/image_processor.py`:**

    *   Se o hist√≥rico de an√°lises n√£o for necess√°rio, remova o c√≥digo relacionado e a classe `HistoryManager`.
    *   Se a funcionalidade de carregar hist√≥rico n√£o for usada, remova os m√©todos `_carregar_historico_json` e `_salvar_historico_json`.
    *   Remova a fun√ß√£o `_carregar_historico_json` do escopo `_process_image`

4.  **`fundamentus_api/dados_b3.py`:**

    *   Se n√£o pretende exibir o dashboard, remova a classe `Aplicacao` e a l√≥gica de exibir o dashboard.

### Passos para Limpeza

1.  **Backup:** Fa√ßa um backup completo do seu c√≥digo antes de come√ßar.
2.  **Remova Arquivos Desnecess√°rios:** Exclua os arquivos identificados como desnecess√°rios.
3.  **Remova C√≥digo Comentado:** Limpe todos os blocos de c√≥digo comentados irrelevantes.
4.  **Otimize as Importa√ß√µes:** Remova as importa√ß√µes n√£o utilizadas em cada arquivo.
5.  **Simplifique as Condi√ß√µes:** Remova condi√ß√µes desnecess√°rias ou redundantes.
6.  **Teste:** Ap√≥s cada grande mudan√ßa, execute testes para garantir que a aplica√ß√£o continua funcionando corretamente.

Ao seguir esses passos, voc√™ pode limpar seu c√≥digo, tornando-o mais f√°cil de manter e entender.

2025-04-14 14:14:38,409 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:14:38,411 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:14:38,413 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:14:38,415 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:14:38,418 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:14:38,420 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:14:38,422 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:14:38,423 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:14:38,425 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:14:38,427 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:14:38,428 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:14:38,429 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:14:38,430 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:14:38,431 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:14:38,432 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:14:38,433 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:14:38,434 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:14:38,436 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:14:38,438 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:14:38,439 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:14:38,440 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:14:38,441 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:14:38,442 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:14:38,443 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:14:38,445 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:14:38,446 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:14:38,447 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:14:38,448 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:14:38,449 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:14:38,450 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:14:38,452 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:14:38,453 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:14:38,455 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:14:38,457 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:14:38,458 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:14:38,459 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:14:38,459 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:14:38,460 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:14:38,462 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:14:38,463 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:14:38,464 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:14:38,465 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:14:38,466 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:14:38,467 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:14:38,469 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:14:38,470 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:14:38,472 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:14:38,473 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:14:38,474 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:14:38,475 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:14:38,476 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:14:38,477 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:14:38,478 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:26:03,997 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:26:03,999 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:26:04,000 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:26:04,001 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:26:04,003 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:26:04,005 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:26:04,006 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:26:04,007 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:26:04,008 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:26:04,009 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:26:04,010 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:26:04,013 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:26:04,014 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:26:04,015 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:26:04,016 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:26:04,017 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:26:04,018 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:26:04,019 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:26:04,020 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:26:04,021 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:26:04,022 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:26:04,023 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:26:04,025 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:26:04,026 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:26:04,027 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:26:04,028 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:26:04,029 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:26:04,030 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:26:04,031 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:26:04,032 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:26:04,033 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:26:04,034 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:26:04,035 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:26:04,036 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:26:04,037 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:26:04,039 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:26:04,040 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:26:04,041 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:26:04,043 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:26:04,044 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:26:04,045 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:26:04,046 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:26:04,048 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:26:04,050 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:26:04,052 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:26:04,053 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:26:04,055 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:26:04,056 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:26:04,058 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:26:04,059 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:26:04,060 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:26:04,063 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:26:04,065 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:26:04,649 [INFO] Modelo Gemini 'gemini-2.0-flash-exp' inicializado com sucesso.
2025-04-14 14:40:58,452 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:40:58,454 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:40:58,455 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:40:58,457 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:40:58,459 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:40:58,461 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:40:58,462 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:40:58,463 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:40:58,466 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:40:58,468 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:40:58,471 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:40:58,472 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:40:58,474 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:40:58,476 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:40:58,478 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:40:58,479 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:40:58,481 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:40:58,483 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:40:58,485 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:40:58,486 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:40:58,487 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:40:58,489 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:40:58,491 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:40:58,492 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:40:58,494 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:40:58,496 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:40:58,499 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:40:58,501 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:40:58,502 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:40:58,504 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:40:58,506 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:40:58,507 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:40:58,509 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:40:58,511 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:40:58,513 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:40:58,515 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:40:58,516 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:40:58,518 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:40:58,520 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:40:58,522 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:40:58,524 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:40:58,525 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:40:58,526 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:40:58,528 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:40:58,529 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:40:58,532 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:40:58,533 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:40:58,534 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:40:58,536 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:40:58,537 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:40:58,538 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:40:58,540 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:40:58,541 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:41:00,355 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:41:00,357 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:41:00,358 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:41:00,360 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:41:00,362 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:41:00,364 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:41:00,366 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:41:00,367 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:41:00,370 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:41:00,371 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:41:00,372 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:41:00,374 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:41:00,376 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:41:00,378 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:41:00,379 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:41:00,382 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:41:00,383 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:41:00,384 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:41:00,386 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:41:00,388 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:41:00,390 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:41:00,391 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:41:00,393 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:41:00,394 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:41:00,395 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:41:00,397 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:41:00,399 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:41:00,400 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:41:00,402 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:41:00,403 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:41:00,404 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:41:00,405 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:41:00,406 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:41:00,408 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:41:00,409 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:41:00,410 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:41:00,411 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:41:00,414 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:41:00,416 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:41:00,417 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:41:00,419 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:41:00,421 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:41:00,422 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:41:00,423 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:41:00,425 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:41:00,427 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:41:00,429 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:41:00,431 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:41:00,433 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:41:00,434 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:41:00,436 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:41:00,437 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:41:00,439 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:41:00,936 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:41:00,939 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:41:00,940 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:41:00,943 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:41:00,945 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:41:00,947 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:41:00,950 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:41:00,952 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:41:00,954 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:41:00,955 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:41:00,957 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:41:00,959 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:41:00,961 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:41:00,962 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:41:00,964 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:41:00,966 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:41:00,968 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:41:00,970 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:41:00,971 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:41:00,973 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:41:00,974 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:41:00,976 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:41:00,978 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:41:00,979 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:41:00,982 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:41:00,984 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:41:00,986 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:41:00,987 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:41:00,989 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:41:00,990 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:41:00,992 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:41:00,993 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:41:00,995 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:41:00,997 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:41:00,999 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:41:01,002 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:41:01,003 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:41:01,004 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:41:01,006 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:41:01,008 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:41:01,009 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:41:01,011 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:41:01,013 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:41:01,015 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:41:01,018 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:41:01,020 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:41:01,022 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:41:01,023 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:41:01,025 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:41:01,027 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:41:01,028 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:41:01,030 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:41:01,033 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:41:07,325 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:41:07,326 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:41:07,328 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:41:07,329 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:41:07,332 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:41:07,335 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:41:07,337 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:41:07,339 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:41:07,340 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:41:07,342 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:41:07,344 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:41:07,345 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:41:07,347 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:41:07,348 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:41:07,351 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:41:07,354 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:41:07,355 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:41:07,356 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:41:07,357 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:41:07,358 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:41:07,359 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:41:07,361 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:41:07,362 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:41:07,363 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:41:07,365 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:41:07,366 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:41:07,368 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:41:07,369 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:41:07,370 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:41:07,371 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:41:07,372 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:41:07,374 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:41:07,375 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:41:07,377 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:41:07,378 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:41:07,380 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:41:07,381 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:41:07,383 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:41:07,385 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:41:07,387 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:41:07,388 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:41:07,390 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:41:07,392 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:41:07,393 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:41:07,395 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:41:07,397 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:41:07,399 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:41:07,401 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:41:07,403 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:41:07,404 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:41:07,406 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:41:07,408 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:41:07,409 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:41:07,536 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:41:07,538 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:41:07,539 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:41:07,542 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:41:07,544 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:41:07,545 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:41:07,547 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:41:07,548 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:41:07,550 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:41:07,552 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:41:07,553 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:41:07,555 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:41:07,557 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:41:07,558 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:41:07,561 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:41:07,563 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:41:07,565 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:41:07,567 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:41:07,568 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:41:07,570 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:41:07,571 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:41:07,573 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:41:07,575 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:41:07,576 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:41:07,578 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:41:07,580 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:41:07,581 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:41:07,583 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:41:07,584 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:41:07,586 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:41:07,587 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:41:07,588 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:41:07,590 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:41:07,591 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:41:07,592 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:41:07,593 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:41:07,594 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:41:07,596 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:41:07,597 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:41:07,599 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:41:07,600 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:41:07,601 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:41:07,602 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:41:07,604 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:41:07,605 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:41:07,607 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:41:07,608 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:41:07,609 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:41:07,610 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:41:07,612 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:41:07,613 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:41:07,614 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:41:07,616 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:41:07,664 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\agents_ia\comandAI\assets\20250414144107_clipboard_20250414144100.png, Prompt: Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"N√£o foram encontrados arquivos de √°udio no diret√≥rio {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcri√ß√£o salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diret√≥rio atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Defini√ß√£o dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Cria√ß√£o dos diret√≥rios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configura√ß√£o do layout do Streamlit
st.set_page_config(
    page_title="An√°lise de A√ß√µes",
    layout="wide",
    page_icon="üìà"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscila√ß√µes

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Per√≠odo', 'Oscila√ß√£o'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("üìä Dashboard de An√°lise de A√ß√µes")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma a√ß√£o", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"üìå Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("üîç Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descri√ß√£o', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa a√ß√£o.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("üí∞ Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("üìâ Oscila√ß√µes")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("üìà Tabela Geral de A√ß√µes")
        st.dataframe(self.acoes)

# Execu√ß√£o
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para an√°lise fundamentalista de a√ß√µes da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"N√£o foram encontrados arquivos .txt no diret√≥rio {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conte√∫do do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conte√∫do HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conte√∫do HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se est√° correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configura√ß√£o de v√≠deos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configura√ß√£o de √°udios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcri√ß√µes e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diret√≥rio
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diret√≥rio.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do √∫ltimo arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conte√∫do do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimens√µes do embedding:", embedding.shape)

# Caminho do diret√≥rio de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o √∫ltimo embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configura√ß√£o de transcri√ß√µes e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diret√≥rio onde est√° procurando as transcri√ß√µes
        print(f"Diret√≥rio de entrada das transcri√ß√µes: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcri√ß√£o no diret√≥rio de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcri√ß√£o encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo n√£o encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcri√ß√£o do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} est√° vazio.")
                    return

            # Gerar o embedding da transcri√ß√£o
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcri√ß√£o e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de √°udio
    with sr.Microphone() as source:
        print("Diga algo em portugu√™s...")

        while True:
            try:
                # Escuta o √°udio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {texto_portugues}")

                # Traduz o texto para o ingl√™s
                traducao = translator.translate(texto_portugues)
                print(f"Tradu√ß√£o para o ingl√™s: {traducao}")

            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usu√°rio")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configura√ß√µes de √°udio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de √°udio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em portugu√™s...")

try:
    audio_buffer = []

    while True:
        # L√™ dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o √°udio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o √°udio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradu√ß√£o
            print(f"Tradu√ß√£o para o ingl√™s: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usu√°rio")

    # Fecha o stream de √°udio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O v√≠deo {video_path} n√£o cont√©m √°udio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de v√≠deo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de v√≠deo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de v√≠deo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Convers√£o de v√≠deo para √°udio conclu√≠da!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configura√ß√µes da API
handler = genai('gemini-1.5-flash')

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Fun√ß√£o para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conte√∫do com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares √∫nicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcri√ß√£o obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' n√£o √© uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuni√µes: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de reuni√µes: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|t√≥picos da|pontos (relevantes|principais)|an√°lise)\b.*\b(reuni√£o|√∫ltima (reuni√£o|conversa|sess√£o))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como an√°lise de reuni√£o.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padr√£o.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que s√£o muito similares ao comando atual para evitar redund√¢ncia.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padr√£o: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # M√©todos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de an√°lise de reuni√£o.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conte√∫do usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inv√°lida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar vari√°veis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conte√∫do com base no prompt, contexto e reuni√£o"""
        try:
            # Supondo que a API espera um dicion√°rio com os par√¢metros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisi√ß√£o para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar vari√°veis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Cria√ß√£o da conclus√£o do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que voc√™ acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["fa√ßa um resumo da √∫ltima reuni√£o.", "t√≥picos da √∫ltima reuni√£o", "resuma a √∫ltima reuni√£o", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Voc√™ √© respons√°vel por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - N√£o acrescentar t√≠tulo nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informa√ß√µes acima e forne√ßa uma solu√ß√£o detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Voc√™ √© respons√°vel analisar com detalhes a reuni√£o de [str_meeting], e fornecer uma longa est√≥ria sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precis√£o da resposta.
            - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e fa√ßa uma pesquisa online para uma resposta mais precisa das quest√µes de [str_text]
        - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [context]: Regras B√°sicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("√Åudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum √°udio detectado.")
                return None
            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configura√ß√µes da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicion√°rio
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta n√£o est√° no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API: {e}")
        return None

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: o que significa essa imagem ?
2025-04-14 14:41:13,183 [INFO] Resposta da IA (imagem): The image is a data visualization, specifically a bar graph, titled "What Kind of Data Do AI Chatbots Collect?" It compares the data collection practices of several AI chatbots: Gemini, Claude, CO PILOT, deepseek, CHAT GPT, perplexity, and Grok.

Here's a breakdown of the data categories and what they likely represent:

*   **Contact Info:** Email address, phone number, etc.

*   **Location:** Geographic location data (e.g., GPS coordinates, IP address).

*   **Contacts:** Information about your address book or social connections.

*   **User Content:**  The text, images, audio, and video you provide to the chatbot.

*   **History:** Records of your past interactions with the chatbot.

*   **Identifiers:** Unique IDs that can be used to identify you.

*   **Diagnostics:** Data about the chatbot's performance and any errors encountered.

*   **Usage Data:** How you use the chatbot (e.g., features used, frequency of use).

*   **Purchases:** Information about any transactions made through the chatbot.

*   **Other Data:** A catch-all category for data that doesn't fit into the other categories.

The numbers next to each chatbot indicate how many of these data points the chatbot collects. For example, Gemini collects 4 types of contact info, 2 location data points, etc. The last number indicates the total number of unique data points that each chatbot collects.

The source of the data is Surfshark, and it was compiled as of February 2025. The graphic is created by Visual Capitalist.
2025-04-14 14:41:13,416 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:41:13,418 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:41:13,421 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:41:13,424 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:41:13,428 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:41:13,430 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:41:13,434 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:41:13,437 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:41:13,440 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:41:13,443 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:41:13,446 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:41:13,449 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:41:13,452 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:41:13,455 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:41:13,458 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:41:13,461 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:41:13,463 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:41:13,466 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:41:13,471 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:41:13,474 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:41:13,476 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:41:13,478 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:41:13,480 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:41:13,482 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:41:13,484 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:41:13,487 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:41:13,489 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:41:13,491 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:41:13,493 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:41:13,495 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:41:13,497 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:41:13,500 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:41:13,501 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:41:13,502 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:41:13,505 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:41:13,507 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:41:13,508 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:41:13,510 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:41:13,513 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:41:13,515 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:41:13,517 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:41:13,519 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:41:13,521 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:41:13,523 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:41:13,525 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:41:13,527 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:41:13,529 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:41:13,531 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:41:13,533 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:41:13,534 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:41:13,536 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:41:13,538 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:41:13,539 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:41:37,661 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:41:37,662 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:41:37,664 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:41:37,666 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:41:37,668 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:41:37,670 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:41:37,671 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:41:37,673 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:41:37,675 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:41:37,677 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:41:37,678 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:41:37,680 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:41:37,682 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:41:37,683 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:41:37,685 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:41:37,686 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:41:37,688 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:41:37,689 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:41:37,691 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:41:37,692 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:41:37,694 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:41:37,696 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:41:37,697 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:41:37,699 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:41:37,701 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:41:37,703 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:41:37,706 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:41:37,707 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:41:37,709 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:41:37,711 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:41:37,712 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:41:37,713 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:41:37,714 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:41:37,715 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:41:37,717 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:41:37,718 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:41:37,720 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:41:37,722 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:41:37,723 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:41:37,724 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:41:37,726 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:41:37,727 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:41:37,728 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:41:37,730 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:41:37,731 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:41:37,732 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:41:37,733 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:41:37,735 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:41:37,737 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:41:37,738 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:41:37,739 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:41:37,741 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:41:37,743 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:41:37,866 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:41:37,868 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:41:37,870 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:41:37,873 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:41:37,874 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:41:37,876 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:41:37,877 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:41:37,879 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:41:37,881 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:41:37,883 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:41:37,885 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:41:37,887 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:41:37,889 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:41:37,891 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:41:37,893 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:41:37,895 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:41:37,897 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:41:37,898 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:41:37,899 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:41:37,901 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:41:37,903 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:41:37,905 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:41:37,906 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:41:37,908 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:41:37,910 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:41:37,911 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:41:37,913 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:41:37,915 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:41:37,917 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:41:37,918 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:41:37,920 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:41:37,922 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:41:37,924 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:41:37,926 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:41:37,927 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:41:37,928 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:41:37,929 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:41:37,931 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:41:37,932 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:41:37,933 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:41:37,935 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:41:37,937 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:41:37,938 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:41:37,940 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:41:37,941 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:41:37,943 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:41:37,945 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:41:37,947 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:41:37,948 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:41:37,949 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:41:37,952 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:41:37,953 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:41:37,955 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:41:38,010 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\agents_ia\comandAI\assets\20250414144137_clipboard_20250414144100.png, Prompt: Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"N√£o foram encontrados arquivos de √°udio no diret√≥rio {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcri√ß√£o salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diret√≥rio atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Defini√ß√£o dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Cria√ß√£o dos diret√≥rios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configura√ß√£o do layout do Streamlit
st.set_page_config(
    page_title="An√°lise de A√ß√µes",
    layout="wide",
    page_icon="üìà"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscila√ß√µes

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Per√≠odo', 'Oscila√ß√£o'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("üìä Dashboard de An√°lise de A√ß√µes")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma a√ß√£o", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"üìå Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("üîç Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descri√ß√£o', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa a√ß√£o.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("üí∞ Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("üìâ Oscila√ß√µes")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("üìà Tabela Geral de A√ß√µes")
        st.dataframe(self.acoes)

# Execu√ß√£o
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para an√°lise fundamentalista de a√ß√µes da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"N√£o foram encontrados arquivos .txt no diret√≥rio {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conte√∫do do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conte√∫do HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conte√∫do HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se est√° correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configura√ß√£o de v√≠deos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configura√ß√£o de √°udios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcri√ß√µes e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diret√≥rio
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diret√≥rio.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do √∫ltimo arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conte√∫do do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimens√µes do embedding:", embedding.shape)

# Caminho do diret√≥rio de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o √∫ltimo embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configura√ß√£o de transcri√ß√µes e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diret√≥rio onde est√° procurando as transcri√ß√µes
        print(f"Diret√≥rio de entrada das transcri√ß√µes: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcri√ß√£o no diret√≥rio de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcri√ß√£o encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo n√£o encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcri√ß√£o do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} est√° vazio.")
                    return

            # Gerar o embedding da transcri√ß√£o
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcri√ß√£o e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de √°udio
    with sr.Microphone() as source:
        print("Diga algo em portugu√™s...")

        while True:
            try:
                # Escuta o √°udio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {texto_portugues}")

                # Traduz o texto para o ingl√™s
                traducao = translator.translate(texto_portugues)
                print(f"Tradu√ß√£o para o ingl√™s: {traducao}")

            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usu√°rio")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configura√ß√µes de √°udio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de √°udio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em portugu√™s...")

try:
    audio_buffer = []

    while True:
        # L√™ dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o √°udio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o √°udio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradu√ß√£o
            print(f"Tradu√ß√£o para o ingl√™s: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usu√°rio")

    # Fecha o stream de √°udio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O v√≠deo {video_path} n√£o cont√©m √°udio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de v√≠deo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de v√≠deo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de v√≠deo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Convers√£o de v√≠deo para √°udio conclu√≠da!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configura√ß√µes da API
handler = genai('gemini-1.5-flash')

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Fun√ß√£o para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conte√∫do com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares √∫nicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcri√ß√£o obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' n√£o √© uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuni√µes: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de reuni√µes: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|t√≥picos da|pontos (relevantes|principais)|an√°lise)\b.*\b(reuni√£o|√∫ltima (reuni√£o|conversa|sess√£o))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como an√°lise de reuni√£o.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padr√£o.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que s√£o muito similares ao comando atual para evitar redund√¢ncia.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padr√£o: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # M√©todos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de an√°lise de reuni√£o.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conte√∫do usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inv√°lida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar vari√°veis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conte√∫do com base no prompt, contexto e reuni√£o"""
        try:
            # Supondo que a API espera um dicion√°rio com os par√¢metros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisi√ß√£o para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar vari√°veis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Cria√ß√£o da conclus√£o do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que voc√™ acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["fa√ßa um resumo da √∫ltima reuni√£o.", "t√≥picos da √∫ltima reuni√£o", "resuma a √∫ltima reuni√£o", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Voc√™ √© respons√°vel por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - N√£o acrescentar t√≠tulo nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informa√ß√µes acima e forne√ßa uma solu√ß√£o detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Voc√™ √© respons√°vel analisar com detalhes a reuni√£o de [str_meeting], e fornecer uma longa est√≥ria sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precis√£o da resposta.
            - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e fa√ßa uma pesquisa online para uma resposta mais precisa das quest√µes de [str_text]
        - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [context]: Regras B√°sicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("√Åudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum √°udio detectado.")
                return None
            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configura√ß√µes da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicion√°rio
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta n√£o est√° no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API: {e}")
        return None

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: o que significa essa imagem ?
Assistant: The image is a data visualization, specifically a bar graph, titled "What Kind of Data Do AI Chatbots Collect?" It compares the data collection practices of several AI chatbots: Gemini, Claude, CO PILOT, deepseek, CHAT GPT, perplexity, and Grok.

Here's a breakdown of the data categories and what they likely represent:

*   **Contact Info:** Email address, phone number, etc.

*   **Location:** Geographic location data (e.g., GPS coordinates, IP address).

*   **Contacts:** Information about your address book or social connections.

*   **User Content:**  The text, images, audio, and video you provide to the chatbot.

*   **History:** Records of your past interactions with the chatbot.

*   **Identifiers:** Unique IDs that can be used to identify you.

*   **Diagnostics:** Data about the chatbot's performance and any errors encountered.

*   **Usage Data:** How you use the chatbot (e.g., features used, frequency of use).

*   **Purchases:** Information about any transactions made through the chatbot.

*   **Other Data:** A catch-all category for data that doesn't fit into the other categories.

The numbers next to each chatbot indicate how many of these data points the chatbot collects. For example, Gemini collects 4 types of contact info, 2 location data points, etc. The last number indicates the total number of unique data points that each chatbot collects.

The source of the data is Surfshark, and it was compiled as of February 2025. The graphic is created by Visual Capitalist.
User: explique em portugues.
2025-04-14 14:41:42,123 [INFO] Resposta da IA (imagem): Assistant: A imagem √© uma visualiza√ß√£o de dados, especificamente um gr√°fico de barras, intitulado "Que Tipo de Dados os Chatbots de IA Coletam?". Ele compara as pr√°ticas de coleta de dados de v√°rios chatbots de IA: Gemini, Claude, CO PILOT, deepseek, CHAT GPT, perplexity e Grok.

Aqui est√° uma an√°lise das categorias de dados e o que elas provavelmente representam:

*   **Informa√ß√µes de Contato:** Endere√ßo de e-mail, n√∫mero de telefone, etc.

*   **Localiza√ß√£o:** Dados de localiza√ß√£o geogr√°fica (por exemplo, coordenadas GPS, endere√ßo IP).

*   **Contatos:** Informa√ß√µes sobre sua lista de endere√ßos ou conex√µes sociais.

*   **Conte√∫do do Usu√°rio:** O texto, imagens, √°udio e v√≠deo que voc√™ fornece ao chatbot.

*   **Hist√≥rico:** Registros de suas intera√ß√µes passadas com o chatbot.

*   **Identificadores:** IDs √∫nicos que podem ser usados para identific√°-lo.

*   **Diagn√≥sticos:** Dados sobre o desempenho do chatbot e quaisquer erros encontrados.

*   **Dados de Uso:** Como voc√™ usa o chatbot (por exemplo, recursos usados, frequ√™ncia de uso).

*   **Compras:** Informa√ß√µes sobre quaisquer transa√ß√µes feitas atrav√©s do chatbot.

*   **Outros Dados:** Uma categoria gen√©rica para dados que n√£o se encaixam nas outras categorias.

Os n√∫meros ao lado de cada chatbot indicam quantos desses pontos de dados o chatbot coleta. Por exemplo, o Gemini coleta 4 tipos de informa√ß√µes de contato, 2 pontos de dados de localiza√ß√£o, etc. O √∫ltimo n√∫mero indica o n√∫mero total de pontos de dados exclusivos que cada chatbot coleta.

A fonte dos dados √© a Surfshark, e foi compilada em fevereiro de 2025. O gr√°fico √© criado pela Visual Capitalist.
2025-04-14 14:41:42,300 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:41:42,302 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:41:42,304 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:41:42,307 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:41:42,308 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:41:42,310 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:41:42,312 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:41:42,314 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:41:42,316 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:41:42,318 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:41:42,320 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:41:42,322 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:41:42,324 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:41:42,326 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:41:42,328 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:41:42,329 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:41:42,331 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:41:42,332 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:41:42,335 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:41:42,337 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:41:42,339 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:41:42,341 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:41:42,343 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:41:42,345 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:41:42,346 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:41:42,348 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:41:42,350 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:41:42,353 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:41:42,354 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:41:42,357 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:41:42,358 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:41:42,360 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:41:42,362 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:41:42,364 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:41:42,366 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:41:42,368 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:41:42,370 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:41:42,372 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:41:42,374 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:41:42,376 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:41:42,377 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:41:42,379 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:41:42,381 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:41:42,383 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:41:42,386 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:41:42,388 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:41:42,390 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:41:42,392 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:41:42,394 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:41:42,396 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:41:42,397 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:41:42,399 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:41:42,402 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:47:42,537 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:47:42,539 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:47:42,541 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:47:42,542 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:47:42,544 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:47:42,546 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:47:42,547 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:47:42,548 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:47:42,550 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:47:42,552 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:47:42,553 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:47:42,555 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:47:42,557 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:47:42,558 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:47:42,560 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:47:42,561 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:47:42,563 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:47:42,564 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:47:42,565 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:47:42,567 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:47:42,568 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:47:42,570 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:47:42,572 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:47:42,574 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:47:42,575 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:47:42,577 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:47:42,578 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:47:42,580 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:47:42,581 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:47:42,583 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:47:42,585 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:47:42,587 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:47:42,590 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:47:42,591 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:47:42,593 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:47:42,595 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:47:42,596 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:47:42,597 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:47:42,600 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:47:42,601 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:47:42,603 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:47:42,604 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:47:42,611 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:47:42,614 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:47:42,616 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:47:42,620 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:47:42,621 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:47:42,623 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:47:42,624 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:47:42,625 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:47:42,626 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:47:42,628 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:47:42,629 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:47:42,755 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:47:42,758 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:47:42,760 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:47:42,763 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:47:42,765 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:47:42,768 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:47:42,770 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:47:42,773 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:47:42,775 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:47:42,778 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:47:42,780 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:47:42,782 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:47:42,784 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:47:42,786 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:47:42,788 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:47:42,790 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:47:42,792 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:47:42,794 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:47:42,795 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:47:42,797 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:47:42,799 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:47:42,801 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:47:42,803 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:47:42,804 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:47:42,807 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:47:42,809 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:47:42,811 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:47:42,813 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:47:42,814 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:47:42,816 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:47:42,818 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:47:42,819 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:47:42,821 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:47:42,824 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:47:42,826 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:47:42,827 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:47:42,829 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:47:42,830 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:47:42,832 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:47:42,833 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:47:42,835 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:47:42,836 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:47:42,838 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:47:42,840 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:47:42,842 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:47:42,844 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:47:42,845 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:47:42,847 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:47:42,849 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:47:42,850 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:47:42,851 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:47:42,853 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:47:42,855 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:47:42,858 [INFO] Enviando para IA - Prompt (sem imagem): Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"N√£o foram encontrados arquivos de √°udio no diret√≥rio {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcri√ß√£o salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diret√≥rio atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Defini√ß√£o dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Cria√ß√£o dos diret√≥rios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configura√ß√£o do layout do Streamlit
st.set_page_config(
    page_title="An√°lise de A√ß√µes",
    layout="wide",
    page_icon="üìà"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscila√ß√µes

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Per√≠odo', 'Oscila√ß√£o'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("üìä Dashboard de An√°lise de A√ß√µes")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma a√ß√£o", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"üìå Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("üîç Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descri√ß√£o', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa a√ß√£o.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("üí∞ Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("üìâ Oscila√ß√µes")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("üìà Tabela Geral de A√ß√µes")
        st.dataframe(self.acoes)

# Execu√ß√£o
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para an√°lise fundamentalista de a√ß√µes da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"N√£o foram encontrados arquivos .txt no diret√≥rio {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conte√∫do do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conte√∫do HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conte√∫do HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se est√° correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configura√ß√£o de v√≠deos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configura√ß√£o de √°udios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcri√ß√µes e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diret√≥rio
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diret√≥rio.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do √∫ltimo arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conte√∫do do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimens√µes do embedding:", embedding.shape)

# Caminho do diret√≥rio de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o √∫ltimo embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configura√ß√£o de transcri√ß√µes e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diret√≥rio onde est√° procurando as transcri√ß√µes
        print(f"Diret√≥rio de entrada das transcri√ß√µes: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcri√ß√£o no diret√≥rio de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcri√ß√£o encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo n√£o encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcri√ß√£o do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} est√° vazio.")
                    return

            # Gerar o embedding da transcri√ß√£o
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcri√ß√£o e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de √°udio
    with sr.Microphone() as source:
        print("Diga algo em portugu√™s...")

        while True:
            try:
                # Escuta o √°udio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {texto_portugues}")

                # Traduz o texto para o ingl√™s
                traducao = translator.translate(texto_portugues)
                print(f"Tradu√ß√£o para o ingl√™s: {traducao}")

            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usu√°rio")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configura√ß√µes de √°udio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de √°udio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em portugu√™s...")

try:
    audio_buffer = []

    while True:
        # L√™ dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o √°udio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o √°udio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradu√ß√£o
            print(f"Tradu√ß√£o para o ingl√™s: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usu√°rio")

    # Fecha o stream de √°udio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O v√≠deo {video_path} n√£o cont√©m √°udio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de v√≠deo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de v√≠deo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de v√≠deo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Convers√£o de v√≠deo para √°udio conclu√≠da!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configura√ß√µes da API
handler = genai('gemini-1.5-flash')

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Fun√ß√£o para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conte√∫do com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares √∫nicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcri√ß√£o obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' n√£o √© uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuni√µes: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de reuni√µes: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|t√≥picos da|pontos (relevantes|principais)|an√°lise)\b.*\b(reuni√£o|√∫ltima (reuni√£o|conversa|sess√£o))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como an√°lise de reuni√£o.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padr√£o.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que s√£o muito similares ao comando atual para evitar redund√¢ncia.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padr√£o: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # M√©todos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de an√°lise de reuni√£o.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conte√∫do usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inv√°lida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar vari√°veis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conte√∫do com base no prompt, contexto e reuni√£o"""
        try:
            # Supondo que a API espera um dicion√°rio com os par√¢metros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisi√ß√£o para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar vari√°veis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Cria√ß√£o da conclus√£o do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que voc√™ acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["fa√ßa um resumo da √∫ltima reuni√£o.", "t√≥picos da √∫ltima reuni√£o", "resuma a √∫ltima reuni√£o", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Voc√™ √© respons√°vel por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - N√£o acrescentar t√≠tulo nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informa√ß√µes acima e forne√ßa uma solu√ß√£o detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Voc√™ √© respons√°vel analisar com detalhes a reuni√£o de [str_meeting], e fornecer uma longa est√≥ria sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precis√£o da resposta.
            - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e fa√ßa uma pesquisa online para uma resposta mais precisa das quest√µes de [str_text]
        - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [context]: Regras B√°sicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("√Åudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum √°udio detectado.")
                return None
            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configura√ß√µes da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicion√°rio
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta n√£o est√° no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API: {e}")
        return None

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: quais IDES j√° possuem integra√ß√£o com alguma IA que consegue receber o seu c√≥digo como contexto e fornecer assistencia no desenvolvimento.???

Atualmente s√≥ conhe√ßo o VScode com o copilot e o gemini integrados.
2025-04-14 14:47:44,682 [INFO] Resposta da IA (texto): Com base no c√≥digo fornecido e nas suas instru√ß√µes, compilei uma lista de IDEs (Ambientes de Desenvolvimento Integrados) que j√° possuem integra√ß√£o com alguma IA capaz de receber o seu c√≥digo como contexto e fornecer assist√™ncia no desenvolvimento:

*   **VS Code:** J√° possui integra√ß√µes com o GitHub Copilot e o Gemini.

A pesquisa foi realizada utilizando o c√≥digo fonte para identificar as ferramentas e frameworks mencionados, e, com base nisso, fornecer uma resposta precisa.
2025-04-14 14:47:44,796 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:47:44,798 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:47:44,800 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:47:44,802 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:47:44,803 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:47:44,805 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:47:44,806 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:47:44,808 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:47:44,809 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:47:44,811 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:47:44,812 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:47:44,813 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:47:44,814 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:47:44,816 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:47:44,817 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:47:44,818 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:47:44,820 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:47:44,821 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:47:44,823 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:47:44,825 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:47:44,826 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:47:44,827 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:47:44,829 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:47:44,830 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:47:44,832 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:47:44,833 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:47:44,834 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:47:44,836 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:47:44,837 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:47:44,838 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:47:44,842 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:47:44,843 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:47:44,845 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:47:44,846 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:47:44,848 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:47:44,850 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:47:44,852 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:47:44,853 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:47:44,857 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:47:44,859 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:47:44,861 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:47:44,863 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:47:44,865 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:47:44,867 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:47:44,868 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:47:44,870 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:47:44,873 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:47:44,875 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:47:44,877 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:47:44,880 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:47:44,882 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:47:44,883 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:47:44,886 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:50:37,655 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:50:37,657 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:50:37,659 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:50:37,660 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:50:37,662 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:50:37,664 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:50:37,666 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:50:37,667 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:50:37,669 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:50:37,671 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:50:37,672 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:50:37,675 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:50:37,678 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:50:37,679 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:50:37,681 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:50:37,683 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:50:37,694 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:50:37,696 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:50:37,700 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:50:37,701 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:50:37,702 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:50:37,704 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:50:37,705 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:50:37,707 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:50:37,709 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:50:37,710 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:50:37,711 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:50:37,713 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:50:37,714 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:50:37,715 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:50:37,716 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:50:37,718 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:50:37,719 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:50:37,720 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:50:37,722 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:50:37,724 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:50:37,726 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:50:37,727 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:50:37,728 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:50:37,730 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:50:37,731 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:50:37,732 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:50:37,734 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:50:37,735 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:50:37,737 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:50:37,738 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:50:37,740 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:50:37,742 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:50:37,743 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:50:37,744 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:50:37,745 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:50:37,746 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:50:37,747 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:50:37,894 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:50:37,896 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:50:37,898 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:50:37,900 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:50:37,903 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:50:37,906 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:50:37,908 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:50:37,910 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:50:37,911 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:50:37,913 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:50:37,914 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:50:37,916 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:50:37,918 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:50:37,920 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:50:37,922 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:50:37,924 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:50:37,926 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:50:37,927 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:50:37,929 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:50:37,932 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:50:37,936 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:50:37,938 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:50:37,941 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:50:37,943 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:50:37,945 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:50:37,946 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:50:37,948 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:50:37,950 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:50:37,951 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:50:37,954 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:50:37,956 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:50:37,958 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:50:37,960 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:50:37,962 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:50:37,964 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:50:37,965 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:50:37,966 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:50:37,967 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:50:37,969 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:50:37,971 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:50:37,972 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:50:37,974 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:50:37,977 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:50:37,979 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:50:37,981 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:50:37,982 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:50:37,984 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:50:37,986 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:50:37,988 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:50:37,992 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:50:37,997 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:50:38,000 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:50:38,002 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:50:52,681 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:50:52,683 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:50:52,684 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:50:52,686 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:50:52,688 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:50:52,690 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:50:52,693 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:50:52,695 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:50:52,697 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:50:52,699 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:50:52,700 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:50:52,702 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:50:52,704 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:50:52,705 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:50:52,707 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:50:52,710 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:50:52,711 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:50:52,712 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:50:52,714 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:50:52,715 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:50:52,718 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:50:52,720 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:50:52,722 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:50:52,725 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:50:52,727 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:50:52,730 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:50:52,733 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:50:52,735 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:50:52,736 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:50:52,737 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:50:52,738 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:50:52,739 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:50:52,740 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:50:52,742 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:50:52,743 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:50:52,745 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:50:52,745 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:50:52,746 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:50:52,748 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:50:52,749 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:50:52,750 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:50:52,751 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:50:52,752 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:50:52,753 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:50:52,754 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:50:52,755 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:50:52,757 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:50:52,758 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:50:52,759 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:50:52,760 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:50:52,761 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:50:52,763 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:50:52,764 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:50:52,863 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:50:52,865 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:50:52,866 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:50:52,868 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:50:52,870 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:50:52,871 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:50:52,873 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:50:52,875 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:50:52,876 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:50:52,877 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:50:52,879 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:50:52,881 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:50:52,882 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:50:52,884 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:50:52,885 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:50:52,887 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:50:52,888 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:50:52,889 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:50:52,890 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:50:52,892 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:50:52,893 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:50:52,895 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:50:52,896 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:50:52,897 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:50:52,899 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:50:52,900 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:50:52,902 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:50:52,903 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:50:52,904 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:50:52,905 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:50:52,906 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:50:52,907 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:50:52,909 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:50:52,910 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:50:52,912 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:50:52,913 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:50:52,913 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:50:52,915 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:50:52,916 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:50:52,917 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:50:52,919 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:50:52,920 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:50:52,921 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:50:52,922 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:50:52,923 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:50:52,925 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:50:52,926 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:50:52,928 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:50:52,929 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:50:52,930 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:50:52,931 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:50:52,932 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:50:52,933 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:50:52,952 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\agents_ia\comandAI\assets\20250414145052_clipboard_20250414145037.png, Prompt: Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"N√£o foram encontrados arquivos de √°udio no diret√≥rio {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcri√ß√£o salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diret√≥rio atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Defini√ß√£o dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Cria√ß√£o dos diret√≥rios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configura√ß√£o do layout do Streamlit
st.set_page_config(
    page_title="An√°lise de A√ß√µes",
    layout="wide",
    page_icon="üìà"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscila√ß√µes

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Per√≠odo', 'Oscila√ß√£o'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("üìä Dashboard de An√°lise de A√ß√µes")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma a√ß√£o", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"üìå Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("üîç Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descri√ß√£o', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa a√ß√£o.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("üí∞ Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("üìâ Oscila√ß√µes")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("üìà Tabela Geral de A√ß√µes")
        st.dataframe(self.acoes)

# Execu√ß√£o
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para an√°lise fundamentalista de a√ß√µes da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"N√£o foram encontrados arquivos .txt no diret√≥rio {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conte√∫do do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conte√∫do HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conte√∫do HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se est√° correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configura√ß√£o de v√≠deos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configura√ß√£o de √°udios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcri√ß√µes e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diret√≥rio
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diret√≥rio.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do √∫ltimo arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conte√∫do do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimens√µes do embedding:", embedding.shape)

# Caminho do diret√≥rio de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o √∫ltimo embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configura√ß√£o de transcri√ß√µes e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diret√≥rio onde est√° procurando as transcri√ß√µes
        print(f"Diret√≥rio de entrada das transcri√ß√µes: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcri√ß√£o no diret√≥rio de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcri√ß√£o encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo n√£o encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcri√ß√£o do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} est√° vazio.")
                    return

            # Gerar o embedding da transcri√ß√£o
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcri√ß√£o e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de √°udio
    with sr.Microphone() as source:
        print("Diga algo em portugu√™s...")

        while True:
            try:
                # Escuta o √°udio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {texto_portugues}")

                # Traduz o texto para o ingl√™s
                traducao = translator.translate(texto_portugues)
                print(f"Tradu√ß√£o para o ingl√™s: {traducao}")

            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usu√°rio")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configura√ß√µes de √°udio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de √°udio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em portugu√™s...")

try:
    audio_buffer = []

    while True:
        # L√™ dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o √°udio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o √°udio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradu√ß√£o
            print(f"Tradu√ß√£o para o ingl√™s: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usu√°rio")

    # Fecha o stream de √°udio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O v√≠deo {video_path} n√£o cont√©m √°udio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de v√≠deo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de v√≠deo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de v√≠deo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Convers√£o de v√≠deo para √°udio conclu√≠da!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configura√ß√µes da API
handler = genai('gemini-1.5-flash')

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Fun√ß√£o para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conte√∫do com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares √∫nicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcri√ß√£o obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' n√£o √© uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuni√µes: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de reuni√µes: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|t√≥picos da|pontos (relevantes|principais)|an√°lise)\b.*\b(reuni√£o|√∫ltima (reuni√£o|conversa|sess√£o))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como an√°lise de reuni√£o.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padr√£o.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que s√£o muito similares ao comando atual para evitar redund√¢ncia.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padr√£o: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # M√©todos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de an√°lise de reuni√£o.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conte√∫do usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inv√°lida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar vari√°veis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conte√∫do com base no prompt, contexto e reuni√£o"""
        try:
            # Supondo que a API espera um dicion√°rio com os par√¢metros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisi√ß√£o para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar vari√°veis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Cria√ß√£o da conclus√£o do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que voc√™ acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["fa√ßa um resumo da √∫ltima reuni√£o.", "t√≥picos da √∫ltima reuni√£o", "resuma a √∫ltima reuni√£o", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Voc√™ √© respons√°vel por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - N√£o acrescentar t√≠tulo nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informa√ß√µes acima e forne√ßa uma solu√ß√£o detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Voc√™ √© respons√°vel analisar com detalhes a reuni√£o de [str_meeting], e fornecer uma longa est√≥ria sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precis√£o da resposta.
            - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e fa√ßa uma pesquisa online para uma resposta mais precisa das quest√µes de [str_text]
        - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [context]: Regras B√°sicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("√Åudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum √°udio detectado.")
                return None
            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configura√ß√µes da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicion√°rio
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta n√£o est√° no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API: {e}")
        return None

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: quais IDES j√° possuem integra√ß√£o com alguma IA que consegue receber o seu c√≥digo como contexto e fornecer assistencia no desenvolvimento.???

Atualmente s√≥ conhe√ßo o VScode com o copilot e o gemini integrados.
Assistant: Com base no c√≥digo fornecido e nas suas instru√ß√µes, compilei uma lista de IDEs (Ambientes de Desenvolvimento Integrados) que j√° possuem integra√ß√£o com alguma IA capaz de receber o seu c√≥digo como contexto e fornecer assist√™ncia no desenvolvimento:

*   **VS Code:** J√° possui integra√ß√µes com o GitHub Copilot e o Gemini.

A pesquisa foi realizada utilizando o c√≥digo fonte para identificar as ferramentas e frameworks mencionados, e, com base nisso, fornecer uma resposta precisa.
User: mesmo instalando o pacote n√£o reconhece.
2025-04-14 14:50:58,844 [INFO] Resposta da IA (imagem): Com base no contexto do c√≥digo fornecido, aqui est√£o alguns dos poss√≠veis problemas e solu√ß√µes que podem causar problemas de reconhecimento de pacotes:

1.  **Problema de ambiente:** Certifique-se de que o pacote google-generativeai esteja instalado no ambiente Python correto. √Äs vezes, pode ser instalado em um ambiente diferente do que est√° sendo usado.

    *   **Solu√ß√£o:** Ative o ambiente virtual apropriado e use `pip show google-generativeai` para verificar se o pacote est√° instalado naquele ambiente. Se n√£o estiver, instale-o usando `pip install google-generativeai`.

2.  **Problema de cache:** √Äs vezes, o pip pode ter problemas de cache que impedem que ele reconhe√ßa pacotes rec√©m-instalados.

    *   **Solu√ß√£o:** Tente atualizar o pip e limpar o cache do pip antes de instalar o pacote novamente. Use os seguintes comandos:

    ```bash
    pip install --upgrade pip
    pip cache purge
    pip install google-generativeai
    ```

3.  **Problema de caminho:** √Äs vezes, o Python pode n√£o encontrar o pacote instalado porque o caminho de instala√ß√£o do pacote n√£o est√° no `sys.path`.

    *   **Solu√ß√£o:** Adicione o diret√≥rio do pacote ao `sys.path` em seu script Python ou bloco de anota√ß√µes. Voc√™ pode encontrar o diret√≥rio de instala√ß√£o do pacote usando `pip show google-generativeai -f`. Em seguida, adicione esse caminho ao `sys.path` em seu script:

    ```python
    import sys
    sys.path.append('/caminho/para/site-packages')  # Substitua com o caminho real
    import google.generativeai as genai
    ```

4.  **Problema de nome:** √Äs vezes, o nome do pacote no c√≥digo n√£o corresponde ao nome real do pacote instalado.

    *   **Solu√ß√£o:** Verifique se o nome do pacote est√° correto em sua instru√ß√£o `import`. Nesse caso, ele deve ser `import google.generativeai as genai`.

5.  **Problema de vers√£o:** √Äs vezes, os pacotes podem apresentar problemas de compatibilidade com outras vers√µes de pacotes ou com a vers√£o Python que voc√™ est√° usando.

    *   **Solu√ß√£o:** Considere usar uma vers√£o diferente do pacote que seja compat√≠vel com sua configura√ß√£o.

6.  **IDE ou problema de editor:** √Äs vezes, o IDE ou editor que voc√™ est√° usando pode n√£o ser capaz de reconhecer pacotes rec√©m-instalados.

    *   **Solu√ß√£o:** Reinicie o IDE ou editor ou recarregue o ambiente Python.
2025-04-14 14:50:58,934 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:50:58,935 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:50:58,937 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:50:58,939 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:50:58,940 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:50:58,942 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:50:58,943 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:50:58,944 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:50:58,945 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:50:58,946 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:50:58,948 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:50:58,949 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:50:58,951 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:50:58,952 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:50:58,953 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:50:58,954 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:50:58,955 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:50:58,956 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:50:58,957 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:50:58,958 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:50:58,959 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:50:58,961 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:50:58,962 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:50:58,963 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:50:58,964 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:50:58,965 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:50:58,967 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:50:58,968 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:50:58,969 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:50:58,970 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:50:58,971 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:50:58,972 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:50:58,973 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:50:58,974 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:50:58,975 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:50:58,976 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:50:58,977 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:50:58,978 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:50:58,979 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:50:58,980 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:50:58,981 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:50:58,983 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:50:58,984 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:50:58,985 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:50:58,986 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:50:58,987 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:50:58,988 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:50:58,989 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:50:58,991 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:50:58,992 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:50:58,993 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:50:58,994 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:50:58,995 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:53:45,341 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:53:45,344 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:53:45,345 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:53:45,347 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:53:45,348 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:53:45,350 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:53:45,351 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:53:45,352 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:53:45,353 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:53:45,355 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:53:45,356 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:53:45,357 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:53:45,360 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:53:45,361 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:53:45,363 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:53:45,364 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:53:45,366 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:53:45,367 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:53:45,368 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:53:45,370 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:53:45,371 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:53:45,382 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:53:45,384 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:53:45,386 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:53:45,391 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:53:45,394 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:53:45,397 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:53:45,398 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:53:45,400 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:53:45,401 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:53:45,402 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:53:45,403 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:53:45,404 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:53:45,406 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:53:45,407 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:53:45,409 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:53:45,410 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:53:45,412 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:53:45,413 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:53:45,414 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:53:45,415 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:53:45,416 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:53:45,417 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:53:45,418 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:53:45,419 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:53:45,420 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:53:45,421 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:53:45,422 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:53:45,425 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:53:45,426 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:53:45,427 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:53:45,428 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:53:45,429 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:53:45,535 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:53:45,537 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:53:45,538 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:53:45,540 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:53:45,543 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:53:45,544 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:53:45,546 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:53:45,548 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:53:45,550 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:53:45,551 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:53:45,553 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:53:45,554 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:53:45,556 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:53:45,559 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:53:45,561 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:53:45,562 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:53:45,564 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:53:45,566 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:53:45,567 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:53:45,568 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:53:45,569 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:53:45,571 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:53:45,572 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:53:45,574 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:53:45,575 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:53:45,577 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:53:45,579 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:53:45,581 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:53:45,583 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:53:45,585 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:53:45,587 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:53:45,588 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:53:45,589 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:53:45,591 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:53:45,593 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:53:45,595 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:53:45,596 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:53:45,597 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:53:45,599 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:53:45,600 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:53:45,602 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:53:45,603 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:53:45,604 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:53:45,606 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:53:45,608 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:53:45,609 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:53:45,611 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:53:45,612 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:53:45,614 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:53:45,615 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:53:45,616 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:53:45,617 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:53:45,618 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:53:45,636 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\agents_ia\comandAI\assets\20250414145345_clipboard_20250414145037.png, Prompt: Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"N√£o foram encontrados arquivos de √°udio no diret√≥rio {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcri√ß√£o salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diret√≥rio atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Defini√ß√£o dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Cria√ß√£o dos diret√≥rios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configura√ß√£o do layout do Streamlit
st.set_page_config(
    page_title="An√°lise de A√ß√µes",
    layout="wide",
    page_icon="üìà"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscila√ß√µes

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Per√≠odo', 'Oscila√ß√£o'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("üìä Dashboard de An√°lise de A√ß√µes")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma a√ß√£o", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"üìå Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("üîç Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descri√ß√£o', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa a√ß√£o.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("üí∞ Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("üìâ Oscila√ß√µes")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("üìà Tabela Geral de A√ß√µes")
        st.dataframe(self.acoes)

# Execu√ß√£o
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para an√°lise fundamentalista de a√ß√µes da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"N√£o foram encontrados arquivos .txt no diret√≥rio {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conte√∫do do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conte√∫do HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conte√∫do HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se est√° correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configura√ß√£o de v√≠deos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configura√ß√£o de √°udios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcri√ß√µes e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diret√≥rio
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diret√≥rio.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do √∫ltimo arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conte√∫do do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimens√µes do embedding:", embedding.shape)

# Caminho do diret√≥rio de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o √∫ltimo embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configura√ß√£o de transcri√ß√µes e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diret√≥rio onde est√° procurando as transcri√ß√µes
        print(f"Diret√≥rio de entrada das transcri√ß√µes: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcri√ß√£o no diret√≥rio de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcri√ß√£o encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo n√£o encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcri√ß√£o do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} est√° vazio.")
                    return

            # Gerar o embedding da transcri√ß√£o
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcri√ß√£o e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de √°udio
    with sr.Microphone() as source:
        print("Diga algo em portugu√™s...")

        while True:
            try:
                # Escuta o √°udio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {texto_portugues}")

                # Traduz o texto para o ingl√™s
                traducao = translator.translate(texto_portugues)
                print(f"Tradu√ß√£o para o ingl√™s: {traducao}")

            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usu√°rio")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configura√ß√µes de √°udio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de √°udio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em portugu√™s...")

try:
    audio_buffer = []

    while True:
        # L√™ dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o √°udio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o √°udio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradu√ß√£o
            print(f"Tradu√ß√£o para o ingl√™s: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usu√°rio")

    # Fecha o stream de √°udio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O v√≠deo {video_path} n√£o cont√©m √°udio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de v√≠deo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de v√≠deo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de v√≠deo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Convers√£o de v√≠deo para √°udio conclu√≠da!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configura√ß√µes da API
handler = genai('gemini-1.5-flash')

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Fun√ß√£o para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conte√∫do com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares √∫nicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcri√ß√£o obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' n√£o √© uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuni√µes: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de reuni√µes: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|t√≥picos da|pontos (relevantes|principais)|an√°lise)\b.*\b(reuni√£o|√∫ltima (reuni√£o|conversa|sess√£o))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como an√°lise de reuni√£o.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padr√£o.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que s√£o muito similares ao comando atual para evitar redund√¢ncia.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padr√£o: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # M√©todos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de an√°lise de reuni√£o.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conte√∫do usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inv√°lida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar vari√°veis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conte√∫do com base no prompt, contexto e reuni√£o"""
        try:
            # Supondo que a API espera um dicion√°rio com os par√¢metros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisi√ß√£o para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar vari√°veis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Cria√ß√£o da conclus√£o do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que voc√™ acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["fa√ßa um resumo da √∫ltima reuni√£o.", "t√≥picos da √∫ltima reuni√£o", "resuma a √∫ltima reuni√£o", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Voc√™ √© respons√°vel por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - N√£o acrescentar t√≠tulo nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informa√ß√µes acima e forne√ßa uma solu√ß√£o detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Voc√™ √© respons√°vel analisar com detalhes a reuni√£o de [str_meeting], e fornecer uma longa est√≥ria sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precis√£o da resposta.
            - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e fa√ßa uma pesquisa online para uma resposta mais precisa das quest√µes de [str_text]
        - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [context]: Regras B√°sicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("√Åudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum √°udio detectado.")
                return None
            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configura√ß√µes da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicion√°rio
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta n√£o est√° no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API: {e}")
        return None

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: quais IDES j√° possuem integra√ß√£o com alguma IA que consegue receber o seu c√≥digo como contexto e fornecer assistencia no desenvolvimento.???

Atualmente s√≥ conhe√ßo o VScode com o copilot e o gemini integrados.
Assistant: Com base no c√≥digo fornecido e nas suas instru√ß√µes, compilei uma lista de IDEs (Ambientes de Desenvolvimento Integrados) que j√° possuem integra√ß√£o com alguma IA capaz de receber o seu c√≥digo como contexto e fornecer assist√™ncia no desenvolvimento:

*   **VS Code:** J√° possui integra√ß√µes com o GitHub Copilot e o Gemini.

A pesquisa foi realizada utilizando o c√≥digo fonte para identificar as ferramentas e frameworks mencionados, e, com base nisso, fornecer uma resposta precisa.
User: mesmo instalando o pacote n√£o reconhece.
Assistant: Com base no contexto do c√≥digo fornecido, aqui est√£o alguns dos poss√≠veis problemas e solu√ß√µes que podem causar problemas de reconhecimento de pacotes:

1.  **Problema de ambiente:** Certifique-se de que o pacote google-generativeai esteja instalado no ambiente Python correto. √Äs vezes, pode ser instalado em um ambiente diferente do que est√° sendo usado.

    *   **Solu√ß√£o:** Ative o ambiente virtual apropriado e use `pip show google-generativeai` para verificar se o pacote est√° instalado naquele ambiente. Se n√£o estiver, instale-o usando `pip install google-generativeai`.

2.  **Problema de cache:** √Äs vezes, o pip pode ter problemas de cache que impedem que ele reconhe√ßa pacotes rec√©m-instalados.

    *   **Solu√ß√£o:** Tente atualizar o pip e limpar o cache do pip antes de instalar o pacote novamente. Use os seguintes comandos:

    ```bash
    pip install --upgrade pip
    pip cache purge
    pip install google-generativeai
    ```

3.  **Problema de caminho:** √Äs vezes, o Python pode n√£o encontrar o pacote instalado porque o caminho de instala√ß√£o do pacote n√£o est√° no `sys.path`.

    *   **Solu√ß√£o:** Adicione o diret√≥rio do pacote ao `sys.path` em seu script Python ou bloco de anota√ß√µes. Voc√™ pode encontrar o diret√≥rio de instala√ß√£o do pacote usando `pip show google-generativeai -f`. Em seguida, adicione esse caminho ao `sys.path` em seu script:

    ```python
    import sys
    sys.path.append('/caminho/para/site-packages')  # Substitua com o caminho real
    import google.generativeai as genai
    ```

4.  **Problema de nome:** √Äs vezes, o nome do pacote no c√≥digo n√£o corresponde ao nome real do pacote instalado.

    *   **Solu√ß√£o:** Verifique se o nome do pacote est√° correto em sua instru√ß√£o `import`. Nesse caso, ele deve ser `import google.generativeai as genai`.

5.  **Problema de vers√£o:** √Äs vezes, os pacotes podem apresentar problemas de compatibilidade com outras vers√µes de pacotes ou com a vers√£o Python que voc√™ est√° usando.

    *   **Solu√ß√£o:** Considere usar uma vers√£o diferente do pacote que seja compat√≠vel com sua configura√ß√£o.

6.  **IDE ou problema de editor:** √Äs vezes, o IDE ou editor que voc√™ est√° usando pode n√£o ser capaz de reconhecer pacotes rec√©m-instalados.

    *   **Solu√ß√£o:** Reinicie o IDE ou editor ou recarregue o ambiente Python.
User: parece que √© algo com ambiente...porque outras aplica√ß√µes est√£o funcionando com essa lib instalada.
2025-04-14 14:53:52,501 [INFO] Resposta da IA (imagem): Compreendo. Se outras aplica√ß√µes est√£o funcionando corretamente com a `google-generativeai` instalada, o problema provavelmente reside no ambiente espec√≠fico em que a aplica√ß√£o atual est√° sendo executada. Aqui est√£o algumas √°reas para investigar e poss√≠veis solu√ß√µes:

1.  **Ambiente Virtual:**
    *   **Verifica√ß√£o:** Certifique-se de que a aplica√ß√£o esteja rodando dentro do ambiente virtual correto. Se voc√™ estiver utilizando um ambiente virtual (como `venv` ou `conda`), ative-o antes de executar a aplica√ß√£o.

    ```bash
    # Para venv
    source venv/bin/activate  # ou venv\Scripts\activate no Windows

    # Para conda
    conda activate meu_ambiente
    ```
    *   **Confirma√ß√£o:** Verifique se o ambiente virtual ativado √© o mesmo onde `google-generativeai` est√° instalado. Utilize `pip show google-generativeai` para confirmar a instala√ß√£o e o caminho do pacote.

2.  **Caminhos de Importa√ß√£o:**
    *   **Investiga√ß√£o:** Verifique se o caminho para o diret√≥rio do pacote est√° corretamente configurado no `sys.path`. Isso pode variar entre diferentes ambientes e instala√ß√µes.
    *   **Solu√ß√£o:** Adicione o caminho correto ao `sys.path` no in√≠cio do script. Para descobrir o caminho correto, execute o seguinte no ambiente onde a lib funciona:

    ```python
    import google.generativeai
    print(google.generativeai.__file__)
    ```
    Copie o diret√≥rio onde o arquivo `__init__.py` est√° localizado e adicione ao `sys.path` no seu script.

    ```python
    import sys
    sys.path.append('/caminho/para/o/site-packages')
    import google.generativeai as genai
    ```

3.  **Conflitos de Depend√™ncias:**
    *   **Investiga√ß√£o:** Pode haver conflitos entre as depend√™ncias de diferentes pacotes no seu ambiente.
    *   **Solu√ß√£o:** Tente criar um ambiente virtual limpo e instalar apenas as depend√™ncias necess√°rias para a sua aplica√ß√£o atual.

    ```bash
    python -m venv novo_ambiente
    source novo_ambiente/bin/activate  # ou novo_ambiente\Scripts\activate no Windows
    pip install google-generativeai
    # Instale outros pacotes necess√°rios para esta aplica√ß√£o
    ```

4.  **Cache do Editor/IDE:**
    *   **Investiga√ß√£o:** O editor ou IDE pode estar usando um cache que n√£o est√° atualizado com as altera√ß√µes no ambiente.
    *   **Solu√ß√£o:** Reinicie o editor ou IDE, ou force a atualiza√ß√£o do cache. No VS Code, isso pode ser feito atrav√©s do comando "Developer: Reload Window".

5.  **Vari√°veis de Ambiente:**
    *   **Investiga√ß√£o:** Verifique se todas as vari√°veis de ambiente necess√°rias (como `API_KEY_GEMINI`) est√£o corretamente configuradas e acess√≠veis no ambiente em que a aplica√ß√£o est√° rodando.
    *   **Solu√ß√£o:** Certifique-se de que as vari√°veis de ambiente est√£o definidas corretamente e recarregue o ambiente se necess√°rio.

6.  **Permiss√µes:**
    *   **Investiga√ß√£o:** Verifique se o usu√°rio que est√° executando a aplica√ß√£o tem permiss√µes para acessar os arquivos e diret√≥rios necess√°rios.
    *   **Solu√ß√£o:** Ajuste as permiss√µes conforme necess√°rio para garantir que o usu√°rio tenha acesso aos arquivos e diret√≥rios requeridos.

7.  **Vers√µes do Python:**
    *   **Investiga√ß√£o:** Confirme que a vers√£o do Python em uso no ambiente √© compat√≠vel com a `google-generativeai`.
    *   **Solu√ß√£o:** Verifique a vers√£o do Python utilizada (`python --version`) e, se necess√°rio, instale uma vers√£o compat√≠vel com a biblioteca.

Com esses passos, voc√™ deve conseguir identificar e resolver o problema de ambiente que est√° impedindo a sua aplica√ß√£o de reconhecer o pacote `google-generativeai`.
2025-04-14 14:53:52,604 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:53:52,607 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:53:52,609 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:53:52,611 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:53:52,613 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:53:52,615 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:53:52,616 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:53:52,618 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:53:52,619 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:53:52,621 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:53:52,622 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:53:52,624 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:53:52,625 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:53:52,627 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:53:52,628 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:53:52,629 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:53:52,630 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:53:52,631 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:53:52,633 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:53:52,634 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:53:52,635 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:53:52,636 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:53:52,637 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:53:52,639 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:53:52,640 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:53:52,641 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:53:52,642 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:53:52,644 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:53:52,646 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:53:52,647 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:53:52,648 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:53:52,650 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:53:52,651 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:53:52,653 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:53:52,654 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:53:52,656 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:53:52,657 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:53:52,659 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:53:52,661 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:53:52,663 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:53:52,665 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:53:52,667 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:53:52,668 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:53:52,670 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:53:52,671 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:53:52,672 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:53:52,673 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:53:52,674 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:53:52,675 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:53:52,676 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:53:52,677 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:53:52,678 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:53:52,679 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:55:13,777 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:55:13,779 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:55:13,781 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:55:13,783 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:55:13,785 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:55:13,787 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:55:13,788 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:55:13,790 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:55:13,791 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:55:13,793 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:55:13,795 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:55:13,796 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:55:13,798 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:55:13,800 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:55:13,802 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:55:13,803 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:55:13,804 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:55:13,806 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:55:13,807 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:55:13,809 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:55:13,811 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:55:13,813 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:55:13,816 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:55:13,818 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:55:13,820 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:55:13,823 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:55:13,825 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:55:13,827 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:55:13,829 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:55:13,831 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:55:13,832 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:55:13,834 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:55:13,836 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:55:13,839 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:55:13,840 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:55:13,842 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:55:13,843 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:55:13,844 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:55:13,846 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:55:13,847 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:55:13,848 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:55:13,850 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:55:13,851 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:55:13,854 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:55:13,855 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:55:13,856 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:55:13,858 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:55:13,859 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:55:13,861 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:55:13,862 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:55:13,863 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:55:13,865 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:55:13,867 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:55:14,028 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:55:14,029 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:55:14,031 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:55:14,033 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:55:14,035 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:55:14,037 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:55:14,040 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:55:14,043 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:55:14,045 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:55:14,047 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:55:14,049 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:55:14,051 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:55:14,053 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:55:14,055 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:55:14,057 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:55:14,060 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:55:14,061 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:55:14,063 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:55:14,065 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:55:14,066 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:55:14,068 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:55:14,080 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:55:14,082 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:55:14,084 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:55:14,090 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:55:14,092 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:55:14,093 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:55:14,095 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:55:14,096 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:55:14,098 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:55:14,099 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:55:14,100 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:55:14,104 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:55:14,106 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:55:14,107 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:55:14,109 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:55:14,110 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:55:14,111 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:55:14,113 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:55:14,114 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:55:14,115 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:55:14,116 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:55:14,118 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:55:14,121 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:55:14,123 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:55:14,124 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:55:14,125 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:55:14,127 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:55:14,128 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:55:14,130 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:55:14,131 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:55:14,132 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:55:14,134 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:55:23,711 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:55:23,713 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:55:23,714 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:55:23,716 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:55:23,718 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:55:23,719 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:55:23,721 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:55:23,722 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:55:23,724 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:55:23,726 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:55:23,727 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:55:23,729 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:55:23,730 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:55:23,732 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:55:23,734 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:55:23,735 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:55:23,736 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:55:23,737 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:55:23,739 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:55:23,740 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:55:23,741 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:55:23,743 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:55:23,744 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:55:23,748 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:55:23,750 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:55:23,752 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:55:23,754 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:55:23,757 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:55:23,759 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:55:23,762 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:55:23,765 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:55:23,766 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:55:23,768 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:55:23,771 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:55:23,773 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:55:23,774 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:55:23,776 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:55:23,778 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:55:23,780 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:55:23,781 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:55:23,783 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:55:23,784 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:55:23,786 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:55:23,787 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:55:23,788 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:55:23,790 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:55:23,792 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:55:23,793 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:55:23,794 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:55:23,795 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:55:23,797 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:55:23,798 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:55:23,799 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:55:23,921 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:55:23,923 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:55:23,925 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:55:23,927 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:55:23,929 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:55:23,931 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:55:23,932 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:55:23,934 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:55:23,936 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:55:23,940 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:55:23,942 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:55:23,943 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:55:23,945 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:55:23,947 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:55:23,949 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:55:23,951 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:55:23,954 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:55:23,955 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:55:23,957 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:55:23,958 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:55:23,960 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:55:23,962 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:55:23,964 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:55:23,966 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:55:23,968 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:55:23,970 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:55:23,973 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:55:23,975 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:55:23,976 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:55:23,978 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:55:23,980 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:55:23,981 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:55:23,983 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:55:23,985 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:55:23,988 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:55:23,990 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:55:23,991 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:55:24,003 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:55:24,006 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:55:24,008 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:55:24,011 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:55:24,013 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:55:24,014 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:55:24,015 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:55:24,017 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:55:24,019 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:55:24,021 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:55:24,022 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:55:24,023 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:55:24,025 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:55:24,026 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:55:24,027 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:55:24,028 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:55:24,048 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\agents_ia\comandAI\assets\20250414145524_clipboard_20250414145513.png, Prompt: Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"N√£o foram encontrados arquivos de √°udio no diret√≥rio {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcri√ß√£o salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diret√≥rio atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Defini√ß√£o dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Cria√ß√£o dos diret√≥rios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configura√ß√£o do layout do Streamlit
st.set_page_config(
    page_title="An√°lise de A√ß√µes",
    layout="wide",
    page_icon="üìà"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscila√ß√µes

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Per√≠odo', 'Oscila√ß√£o'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("üìä Dashboard de An√°lise de A√ß√µes")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma a√ß√£o", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"üìå Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("üîç Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descri√ß√£o', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa a√ß√£o.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("üí∞ Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("üìâ Oscila√ß√µes")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("üìà Tabela Geral de A√ß√µes")
        st.dataframe(self.acoes)

# Execu√ß√£o
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para an√°lise fundamentalista de a√ß√µes da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"N√£o foram encontrados arquivos .txt no diret√≥rio {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conte√∫do do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conte√∫do HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conte√∫do HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se est√° correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configura√ß√£o de v√≠deos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configura√ß√£o de √°udios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcri√ß√µes e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diret√≥rio
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diret√≥rio.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do √∫ltimo arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conte√∫do do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimens√µes do embedding:", embedding.shape)

# Caminho do diret√≥rio de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o √∫ltimo embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configura√ß√£o de transcri√ß√µes e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diret√≥rio onde est√° procurando as transcri√ß√µes
        print(f"Diret√≥rio de entrada das transcri√ß√µes: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcri√ß√£o no diret√≥rio de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcri√ß√£o encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo n√£o encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcri√ß√£o do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} est√° vazio.")
                    return

            # Gerar o embedding da transcri√ß√£o
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcri√ß√£o e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de √°udio
    with sr.Microphone() as source:
        print("Diga algo em portugu√™s...")

        while True:
            try:
                # Escuta o √°udio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {texto_portugues}")

                # Traduz o texto para o ingl√™s
                traducao = translator.translate(texto_portugues)
                print(f"Tradu√ß√£o para o ingl√™s: {traducao}")

            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usu√°rio")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configura√ß√µes de √°udio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de √°udio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em portugu√™s...")

try:
    audio_buffer = []

    while True:
        # L√™ dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o √°udio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o √°udio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradu√ß√£o
            print(f"Tradu√ß√£o para o ingl√™s: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usu√°rio")

    # Fecha o stream de √°udio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O v√≠deo {video_path} n√£o cont√©m √°udio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de v√≠deo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de v√≠deo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de v√≠deo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Convers√£o de v√≠deo para √°udio conclu√≠da!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configura√ß√µes da API
handler = genai('gemini-1.5-flash')

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Fun√ß√£o para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conte√∫do com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares √∫nicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcri√ß√£o obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' n√£o √© uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuni√µes: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de reuni√µes: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|t√≥picos da|pontos (relevantes|principais)|an√°lise)\b.*\b(reuni√£o|√∫ltima (reuni√£o|conversa|sess√£o))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como an√°lise de reuni√£o.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padr√£o.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que s√£o muito similares ao comando atual para evitar redund√¢ncia.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padr√£o: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # M√©todos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de an√°lise de reuni√£o.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conte√∫do usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inv√°lida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar vari√°veis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conte√∫do com base no prompt, contexto e reuni√£o"""
        try:
            # Supondo que a API espera um dicion√°rio com os par√¢metros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisi√ß√£o para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar vari√°veis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Cria√ß√£o da conclus√£o do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que voc√™ acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["fa√ßa um resumo da √∫ltima reuni√£o.", "t√≥picos da √∫ltima reuni√£o", "resuma a √∫ltima reuni√£o", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Voc√™ √© respons√°vel por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - N√£o acrescentar t√≠tulo nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informa√ß√µes acima e forne√ßa uma solu√ß√£o detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Voc√™ √© respons√°vel analisar com detalhes a reuni√£o de [str_meeting], e fornecer uma longa est√≥ria sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precis√£o da resposta.
            - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e fa√ßa uma pesquisa online para uma resposta mais precisa das quest√µes de [str_text]
        - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [context]: Regras B√°sicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("√Åudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum √°udio detectado.")
                return None
            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configura√ß√µes da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicion√°rio
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta n√£o est√° no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API: {e}")
        return None

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: quais IDES j√° possuem integra√ß√£o com alguma IA que consegue receber o seu c√≥digo como contexto e fornecer assistencia no desenvolvimento.???

Atualmente s√≥ conhe√ßo o VScode com o copilot e o gemini integrados.
Assistant: Com base no c√≥digo fornecido e nas suas instru√ß√µes, compilei uma lista de IDEs (Ambientes de Desenvolvimento Integrados) que j√° possuem integra√ß√£o com alguma IA capaz de receber o seu c√≥digo como contexto e fornecer assist√™ncia no desenvolvimento:

*   **VS Code:** J√° possui integra√ß√µes com o GitHub Copilot e o Gemini.

A pesquisa foi realizada utilizando o c√≥digo fonte para identificar as ferramentas e frameworks mencionados, e, com base nisso, fornecer uma resposta precisa.
User: mesmo instalando o pacote n√£o reconhece.
Assistant: Com base no contexto do c√≥digo fornecido, aqui est√£o alguns dos poss√≠veis problemas e solu√ß√µes que podem causar problemas de reconhecimento de pacotes:

1.  **Problema de ambiente:** Certifique-se de que o pacote google-generativeai esteja instalado no ambiente Python correto. √Äs vezes, pode ser instalado em um ambiente diferente do que est√° sendo usado.

    *   **Solu√ß√£o:** Ative o ambiente virtual apropriado e use `pip show google-generativeai` para verificar se o pacote est√° instalado naquele ambiente. Se n√£o estiver, instale-o usando `pip install google-generativeai`.

2.  **Problema de cache:** √Äs vezes, o pip pode ter problemas de cache que impedem que ele reconhe√ßa pacotes rec√©m-instalados.

    *   **Solu√ß√£o:** Tente atualizar o pip e limpar o cache do pip antes de instalar o pacote novamente. Use os seguintes comandos:

    ```bash
    pip install --upgrade pip
    pip cache purge
    pip install google-generativeai
    ```

3.  **Problema de caminho:** √Äs vezes, o Python pode n√£o encontrar o pacote instalado porque o caminho de instala√ß√£o do pacote n√£o est√° no `sys.path`.

    *   **Solu√ß√£o:** Adicione o diret√≥rio do pacote ao `sys.path` em seu script Python ou bloco de anota√ß√µes. Voc√™ pode encontrar o diret√≥rio de instala√ß√£o do pacote usando `pip show google-generativeai -f`. Em seguida, adicione esse caminho ao `sys.path` em seu script:

    ```python
    import sys
    sys.path.append('/caminho/para/site-packages')  # Substitua com o caminho real
    import google.generativeai as genai
    ```

4.  **Problema de nome:** √Äs vezes, o nome do pacote no c√≥digo n√£o corresponde ao nome real do pacote instalado.

    *   **Solu√ß√£o:** Verifique se o nome do pacote est√° correto em sua instru√ß√£o `import`. Nesse caso, ele deve ser `import google.generativeai as genai`.

5.  **Problema de vers√£o:** √Äs vezes, os pacotes podem apresentar problemas de compatibilidade com outras vers√µes de pacotes ou com a vers√£o Python que voc√™ est√° usando.

    *   **Solu√ß√£o:** Considere usar uma vers√£o diferente do pacote que seja compat√≠vel com sua configura√ß√£o.

6.  **IDE ou problema de editor:** √Äs vezes, o IDE ou editor que voc√™ est√° usando pode n√£o ser capaz de reconhecer pacotes rec√©m-instalados.

    *   **Solu√ß√£o:** Reinicie o IDE ou editor ou recarregue o ambiente Python.
User: parece que √© algo com ambiente...porque outras aplica√ß√µes est√£o funcionando com essa lib instalada.
Assistant: Compreendo. Se outras aplica√ß√µes est√£o funcionando corretamente com a `google-generativeai` instalada, o problema provavelmente reside no ambiente espec√≠fico em que a aplica√ß√£o atual est√° sendo executada. Aqui est√£o algumas √°reas para investigar e poss√≠veis solu√ß√µes:

1.  **Ambiente Virtual:**
    *   **Verifica√ß√£o:** Certifique-se de que a aplica√ß√£o esteja rodando dentro do ambiente virtual correto. Se voc√™ estiver utilizando um ambiente virtual (como `venv` ou `conda`), ative-o antes de executar a aplica√ß√£o.

    ```bash
    # Para venv
    source venv/bin/activate  # ou venv\Scripts\activate no Windows

    # Para conda
    conda activate meu_ambiente
    ```
    *   **Confirma√ß√£o:** Verifique se o ambiente virtual ativado √© o mesmo onde `google-generativeai` est√° instalado. Utilize `pip show google-generativeai` para confirmar a instala√ß√£o e o caminho do pacote.

2.  **Caminhos de Importa√ß√£o:**
    *   **Investiga√ß√£o:** Verifique se o caminho para o diret√≥rio do pacote est√° corretamente configurado no `sys.path`. Isso pode variar entre diferentes ambientes e instala√ß√µes.
    *   **Solu√ß√£o:** Adicione o caminho correto ao `sys.path` no in√≠cio do script. Para descobrir o caminho correto, execute o seguinte no ambiente onde a lib funciona:

    ```python
    import google.generativeai
    print(google.generativeai.__file__)
    ```
    Copie o diret√≥rio onde o arquivo `__init__.py` est√° localizado e adicione ao `sys.path` no seu script.

    ```python
    import sys
    sys.path.append('/caminho/para/o/site-packages')
    import google.generativeai as genai
    ```

3.  **Conflitos de Depend√™ncias:**
    *   **Investiga√ß√£o:** Pode haver conflitos entre as depend√™ncias de diferentes pacotes no seu ambiente.
    *   **Solu√ß√£o:** Tente criar um ambiente virtual limpo e instalar apenas as depend√™ncias necess√°rias para a sua aplica√ß√£o atual.

    ```bash
    python -m venv novo_ambiente
    source novo_ambiente/bin/activate  # ou novo_ambiente\Scripts\activate no Windows
    pip install google-generativeai
    # Instale outros pacotes necess√°rios para esta aplica√ß√£o
    ```

4.  **Cache do Editor/IDE:**
    *   **Investiga√ß√£o:** O editor ou IDE pode estar usando um cache que n√£o est√° atualizado com as altera√ß√µes no ambiente.
    *   **Solu√ß√£o:** Reinicie o editor ou IDE, ou force a atualiza√ß√£o do cache. No VS Code, isso pode ser feito atrav√©s do comando "Developer: Reload Window".

5.  **Vari√°veis de Ambiente:**
    *   **Investiga√ß√£o:** Verifique se todas as vari√°veis de ambiente necess√°rias (como `API_KEY_GEMINI`) est√£o corretamente configuradas e acess√≠veis no ambiente em que a aplica√ß√£o est√° rodando.
    *   **Solu√ß√£o:** Certifique-se de que as vari√°veis de ambiente est√£o definidas corretamente e recarregue o ambiente se necess√°rio.

6.  **Permiss√µes:**
    *   **Investiga√ß√£o:** Verifique se o usu√°rio que est√° executando a aplica√ß√£o tem permiss√µes para acessar os arquivos e diret√≥rios necess√°rios.
    *   **Solu√ß√£o:** Ajuste as permiss√µes conforme necess√°rio para garantir que o usu√°rio tenha acesso aos arquivos e diret√≥rios requeridos.

7.  **Vers√µes do Python:**
    *   **Investiga√ß√£o:** Confirme que a vers√£o do Python em uso no ambiente √© compat√≠vel com a `google-generativeai`.
    *   **Solu√ß√£o:** Verifique a vers√£o do Python utilizada (`python --version`) e, se necess√°rio, instale uma vers√£o compat√≠vel com a biblioteca.

Com esses passos, voc√™ deve conseguir identificar e resolver o problema de ambiente que est√° impedindo a sua aplica√ß√£o de reconhecer o pacote `google-generativeai`.
User: ocorreu esse erro na imagem.
2025-04-14 14:55:27,906 [INFO] Resposta da IA (imagem): The image shows that the virtual environment is not activated.

Here's how to fix this:

1.  **Verify the virtual environment's `venv` directory:** The error message  `bash: venv/bin/activate: No such file or directory` indicates that the `activate` script is not found in `venv/bin/`. This usually happens if the virtual environment was not created properly or the `venv` directory was renamed or deleted.  You can see from the file explorer in the image that there is a `venv` directory in the project root.
2.  **Check the correct activation command:**  The command `source venv/bin/activate` is usually used on Linux/macOS systems. For Windows, the activation script is located at `venv\Scripts\activate`. Therefore, try running  `venv\Scripts\activate` in the terminal instead.
3.  **Recreate the virtual environment (if necessary):** If you've confirmed that the virtual environment is broken, delete the existing `venv` directory and create a new virtual environment using  `python -m venv venv`. After creating the new environment, try activating it again.

After resolving the issue with the virtual environment, install all dependencies: `pip install -r requirements.txt` if you have a requirements file or install them manually. Then, rerun the application.
2025-04-14 14:55:28,008 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:55:28,009 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:55:28,011 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:55:28,012 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:55:28,014 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:55:28,015 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:55:28,016 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:55:28,018 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:55:28,019 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:55:28,020 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:55:28,022 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:55:28,023 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:55:28,024 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:55:28,025 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:55:28,026 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:55:28,027 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:55:28,028 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:55:28,029 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:55:28,030 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:55:28,031 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:55:28,032 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:55:28,034 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:55:28,035 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:55:28,036 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:55:28,038 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:55:28,039 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:55:28,040 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:55:28,041 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:55:28,042 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:55:28,043 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:55:28,044 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:55:28,045 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:55:28,046 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:55:28,047 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:55:28,048 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:55:28,049 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:55:28,050 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:55:28,052 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:55:28,053 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:55:28,054 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:55:28,055 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:55:28,057 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:55:28,058 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:55:28,059 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:55:28,060 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:55:28,061 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:55:28,061 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:55:28,062 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:55:28,063 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:55:28,064 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:55:28,065 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:55:28,066 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:55:28,067 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:56:22,227 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:56:22,229 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:56:22,230 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:56:22,232 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:56:22,233 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:56:22,235 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:56:22,236 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:56:22,238 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:56:22,239 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:56:22,240 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:56:22,241 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:56:22,242 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:56:22,244 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:56:22,245 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:56:22,246 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:56:22,248 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:56:22,250 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:56:22,251 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:56:22,253 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:56:22,255 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:56:22,256 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:56:22,257 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:56:22,259 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:56:22,261 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:56:22,264 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:56:22,265 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:56:22,267 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:56:22,268 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:56:22,269 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:56:22,270 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:56:22,271 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:56:22,272 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:56:22,273 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:56:22,275 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:56:22,277 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:56:22,278 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:56:22,279 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:56:22,281 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:56:22,283 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:56:22,284 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:56:22,285 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:56:22,286 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:56:22,287 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:56:22,289 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:56:22,290 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:56:22,291 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:56:22,292 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:56:22,294 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:56:22,296 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:56:22,297 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:56:22,298 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:56:22,299 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:56:22,300 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:56:22,418 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:56:22,420 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:56:22,421 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:56:22,423 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:56:22,425 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:56:22,427 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:56:22,429 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:56:22,430 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:56:22,431 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:56:22,433 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:56:22,435 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:56:22,436 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:56:22,437 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:56:22,439 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:56:22,440 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:56:22,442 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:56:22,444 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:56:22,445 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:56:22,447 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:56:22,448 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:56:22,450 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:56:22,451 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:56:22,452 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:56:22,454 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:56:22,455 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:56:22,459 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:56:22,461 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:56:22,463 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:56:22,466 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:56:22,467 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:56:22,469 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:56:22,471 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:56:22,473 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:56:22,475 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:56:22,477 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:56:22,479 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:56:22,480 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:56:22,482 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:56:22,484 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:56:22,485 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:56:22,487 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:56:22,489 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:56:22,490 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:56:22,492 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:56:22,494 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:56:22,496 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:56:22,498 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:56:22,500 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:56:22,501 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:56:22,502 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:56:22,503 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:56:22,504 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:56:22,505 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:56:22,527 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\agents_ia\comandAI\assets\20250414145622_clipboard_20250414145513.png, Prompt: Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"N√£o foram encontrados arquivos de √°udio no diret√≥rio {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcri√ß√£o salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diret√≥rio atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Defini√ß√£o dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Cria√ß√£o dos diret√≥rios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configura√ß√£o do layout do Streamlit
st.set_page_config(
    page_title="An√°lise de A√ß√µes",
    layout="wide",
    page_icon="üìà"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscila√ß√µes

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Per√≠odo', 'Oscila√ß√£o'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("üìä Dashboard de An√°lise de A√ß√µes")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma a√ß√£o", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"üìå Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("üîç Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descri√ß√£o', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa a√ß√£o.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("üí∞ Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("üìâ Oscila√ß√µes")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("üìà Tabela Geral de A√ß√µes")
        st.dataframe(self.acoes)

# Execu√ß√£o
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para an√°lise fundamentalista de a√ß√µes da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"N√£o foram encontrados arquivos .txt no diret√≥rio {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conte√∫do do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conte√∫do HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conte√∫do HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se est√° correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configura√ß√£o de v√≠deos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configura√ß√£o de √°udios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcri√ß√µes e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diret√≥rio
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diret√≥rio.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do √∫ltimo arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conte√∫do do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimens√µes do embedding:", embedding.shape)

# Caminho do diret√≥rio de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o √∫ltimo embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configura√ß√£o de transcri√ß√µes e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diret√≥rio onde est√° procurando as transcri√ß√µes
        print(f"Diret√≥rio de entrada das transcri√ß√µes: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcri√ß√£o no diret√≥rio de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcri√ß√£o encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo n√£o encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcri√ß√£o do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} est√° vazio.")
                    return

            # Gerar o embedding da transcri√ß√£o
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcri√ß√£o e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de √°udio
    with sr.Microphone() as source:
        print("Diga algo em portugu√™s...")

        while True:
            try:
                # Escuta o √°udio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {texto_portugues}")

                # Traduz o texto para o ingl√™s
                traducao = translator.translate(texto_portugues)
                print(f"Tradu√ß√£o para o ingl√™s: {traducao}")

            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usu√°rio")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configura√ß√µes de √°udio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de √°udio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em portugu√™s...")

try:
    audio_buffer = []

    while True:
        # L√™ dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o √°udio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o √°udio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradu√ß√£o
            print(f"Tradu√ß√£o para o ingl√™s: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usu√°rio")

    # Fecha o stream de √°udio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O v√≠deo {video_path} n√£o cont√©m √°udio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de v√≠deo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de v√≠deo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de v√≠deo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Convers√£o de v√≠deo para √°udio conclu√≠da!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configura√ß√µes da API
handler = genai('gemini-1.5-flash')

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Fun√ß√£o para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conte√∫do com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares √∫nicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcri√ß√£o obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' n√£o √© uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuni√µes: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de reuni√µes: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|t√≥picos da|pontos (relevantes|principais)|an√°lise)\b.*\b(reuni√£o|√∫ltima (reuni√£o|conversa|sess√£o))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como an√°lise de reuni√£o.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padr√£o.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que s√£o muito similares ao comando atual para evitar redund√¢ncia.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padr√£o: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # M√©todos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de an√°lise de reuni√£o.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conte√∫do usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inv√°lida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar vari√°veis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conte√∫do com base no prompt, contexto e reuni√£o"""
        try:
            # Supondo que a API espera um dicion√°rio com os par√¢metros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisi√ß√£o para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar vari√°veis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Cria√ß√£o da conclus√£o do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que voc√™ acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["fa√ßa um resumo da √∫ltima reuni√£o.", "t√≥picos da √∫ltima reuni√£o", "resuma a √∫ltima reuni√£o", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Voc√™ √© respons√°vel por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - N√£o acrescentar t√≠tulo nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informa√ß√µes acima e forne√ßa uma solu√ß√£o detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Voc√™ √© respons√°vel analisar com detalhes a reuni√£o de [str_meeting], e fornecer uma longa est√≥ria sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precis√£o da resposta.
            - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e fa√ßa uma pesquisa online para uma resposta mais precisa das quest√µes de [str_text]
        - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [context]: Regras B√°sicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("√Åudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum √°udio detectado.")
                return None
            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configura√ß√µes da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicion√°rio
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta n√£o est√° no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API: {e}")
        return None

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: quais IDES j√° possuem integra√ß√£o com alguma IA que consegue receber o seu c√≥digo como contexto e fornecer assistencia no desenvolvimento.???

Atualmente s√≥ conhe√ßo o VScode com o copilot e o gemini integrados.
Assistant: Com base no c√≥digo fornecido e nas suas instru√ß√µes, compilei uma lista de IDEs (Ambientes de Desenvolvimento Integrados) que j√° possuem integra√ß√£o com alguma IA capaz de receber o seu c√≥digo como contexto e fornecer assist√™ncia no desenvolvimento:

*   **VS Code:** J√° possui integra√ß√µes com o GitHub Copilot e o Gemini.

A pesquisa foi realizada utilizando o c√≥digo fonte para identificar as ferramentas e frameworks mencionados, e, com base nisso, fornecer uma resposta precisa.
User: mesmo instalando o pacote n√£o reconhece.
Assistant: Com base no contexto do c√≥digo fornecido, aqui est√£o alguns dos poss√≠veis problemas e solu√ß√µes que podem causar problemas de reconhecimento de pacotes:

1.  **Problema de ambiente:** Certifique-se de que o pacote google-generativeai esteja instalado no ambiente Python correto. √Äs vezes, pode ser instalado em um ambiente diferente do que est√° sendo usado.

    *   **Solu√ß√£o:** Ative o ambiente virtual apropriado e use `pip show google-generativeai` para verificar se o pacote est√° instalado naquele ambiente. Se n√£o estiver, instale-o usando `pip install google-generativeai`.

2.  **Problema de cache:** √Äs vezes, o pip pode ter problemas de cache que impedem que ele reconhe√ßa pacotes rec√©m-instalados.

    *   **Solu√ß√£o:** Tente atualizar o pip e limpar o cache do pip antes de instalar o pacote novamente. Use os seguintes comandos:

    ```bash
    pip install --upgrade pip
    pip cache purge
    pip install google-generativeai
    ```

3.  **Problema de caminho:** √Äs vezes, o Python pode n√£o encontrar o pacote instalado porque o caminho de instala√ß√£o do pacote n√£o est√° no `sys.path`.

    *   **Solu√ß√£o:** Adicione o diret√≥rio do pacote ao `sys.path` em seu script Python ou bloco de anota√ß√µes. Voc√™ pode encontrar o diret√≥rio de instala√ß√£o do pacote usando `pip show google-generativeai -f`. Em seguida, adicione esse caminho ao `sys.path` em seu script:

    ```python
    import sys
    sys.path.append('/caminho/para/site-packages')  # Substitua com o caminho real
    import google.generativeai as genai
    ```

4.  **Problema de nome:** √Äs vezes, o nome do pacote no c√≥digo n√£o corresponde ao nome real do pacote instalado.

    *   **Solu√ß√£o:** Verifique se o nome do pacote est√° correto em sua instru√ß√£o `import`. Nesse caso, ele deve ser `import google.generativeai as genai`.

5.  **Problema de vers√£o:** √Äs vezes, os pacotes podem apresentar problemas de compatibilidade com outras vers√µes de pacotes ou com a vers√£o Python que voc√™ est√° usando.

    *   **Solu√ß√£o:** Considere usar uma vers√£o diferente do pacote que seja compat√≠vel com sua configura√ß√£o.

6.  **IDE ou problema de editor:** √Äs vezes, o IDE ou editor que voc√™ est√° usando pode n√£o ser capaz de reconhecer pacotes rec√©m-instalados.

    *   **Solu√ß√£o:** Reinicie o IDE ou editor ou recarregue o ambiente Python.
User: parece que √© algo com ambiente...porque outras aplica√ß√µes est√£o funcionando com essa lib instalada.
Assistant: Compreendo. Se outras aplica√ß√µes est√£o funcionando corretamente com a `google-generativeai` instalada, o problema provavelmente reside no ambiente espec√≠fico em que a aplica√ß√£o atual est√° sendo executada. Aqui est√£o algumas √°reas para investigar e poss√≠veis solu√ß√µes:

1.  **Ambiente Virtual:**
    *   **Verifica√ß√£o:** Certifique-se de que a aplica√ß√£o esteja rodando dentro do ambiente virtual correto. Se voc√™ estiver utilizando um ambiente virtual (como `venv` ou `conda`), ative-o antes de executar a aplica√ß√£o.

    ```bash
    # Para venv
    source venv/bin/activate  # ou venv\Scripts\activate no Windows

    # Para conda
    conda activate meu_ambiente
    ```
    *   **Confirma√ß√£o:** Verifique se o ambiente virtual ativado √© o mesmo onde `google-generativeai` est√° instalado. Utilize `pip show google-generativeai` para confirmar a instala√ß√£o e o caminho do pacote.

2.  **Caminhos de Importa√ß√£o:**
    *   **Investiga√ß√£o:** Verifique se o caminho para o diret√≥rio do pacote est√° corretamente configurado no `sys.path`. Isso pode variar entre diferentes ambientes e instala√ß√µes.
    *   **Solu√ß√£o:** Adicione o caminho correto ao `sys.path` no in√≠cio do script. Para descobrir o caminho correto, execute o seguinte no ambiente onde a lib funciona:

    ```python
    import google.generativeai
    print(google.generativeai.__file__)
    ```
    Copie o diret√≥rio onde o arquivo `__init__.py` est√° localizado e adicione ao `sys.path` no seu script.

    ```python
    import sys
    sys.path.append('/caminho/para/o/site-packages')
    import google.generativeai as genai
    ```

3.  **Conflitos de Depend√™ncias:**
    *   **Investiga√ß√£o:** Pode haver conflitos entre as depend√™ncias de diferentes pacotes no seu ambiente.
    *   **Solu√ß√£o:** Tente criar um ambiente virtual limpo e instalar apenas as depend√™ncias necess√°rias para a sua aplica√ß√£o atual.

    ```bash
    python -m venv novo_ambiente
    source novo_ambiente/bin/activate  # ou novo_ambiente\Scripts\activate no Windows
    pip install google-generativeai
    # Instale outros pacotes necess√°rios para esta aplica√ß√£o
    ```

4.  **Cache do Editor/IDE:**
    *   **Investiga√ß√£o:** O editor ou IDE pode estar usando um cache que n√£o est√° atualizado com as altera√ß√µes no ambiente.
    *   **Solu√ß√£o:** Reinicie o editor ou IDE, ou force a atualiza√ß√£o do cache. No VS Code, isso pode ser feito atrav√©s do comando "Developer: Reload Window".

5.  **Vari√°veis de Ambiente:**
    *   **Investiga√ß√£o:** Verifique se todas as vari√°veis de ambiente necess√°rias (como `API_KEY_GEMINI`) est√£o corretamente configuradas e acess√≠veis no ambiente em que a aplica√ß√£o est√° rodando.
    *   **Solu√ß√£o:** Certifique-se de que as vari√°veis de ambiente est√£o definidas corretamente e recarregue o ambiente se necess√°rio.

6.  **Permiss√µes:**
    *   **Investiga√ß√£o:** Verifique se o usu√°rio que est√° executando a aplica√ß√£o tem permiss√µes para acessar os arquivos e diret√≥rios necess√°rios.
    *   **Solu√ß√£o:** Ajuste as permiss√µes conforme necess√°rio para garantir que o usu√°rio tenha acesso aos arquivos e diret√≥rios requeridos.

7.  **Vers√µes do Python:**
    *   **Investiga√ß√£o:** Confirme que a vers√£o do Python em uso no ambiente √© compat√≠vel com a `google-generativeai`.
    *   **Solu√ß√£o:** Verifique a vers√£o do Python utilizada (`python --version`) e, se necess√°rio, instale uma vers√£o compat√≠vel com a biblioteca.

Com esses passos, voc√™ deve conseguir identificar e resolver o problema de ambiente que est√° impedindo a sua aplica√ß√£o de reconhecer o pacote `google-generativeai`.
User: ocorreu esse erro na imagem.
Assistant: The image shows that the virtual environment is not activated.

Here's how to fix this:

1.  **Verify the virtual environment's `venv` directory:** The error message  `bash: venv/bin/activate: No such file or directory` indicates that the `activate` script is not found in `venv/bin/`. This usually happens if the virtual environment was not created properly or the `venv` directory was renamed or deleted.  You can see from the file explorer in the image that there is a `venv` directory in the project root.
2.  **Check the correct activation command:**  The command `source venv/bin/activate` is usually used on Linux/macOS systems. For Windows, the activation script is located at `venv\Scripts\activate`. Therefore, try running  `venv\Scripts\activate` in the terminal instead.
3.  **Recreate the virtual environment (if necessary):** If you've confirmed that the virtual environment is broken, delete the existing `venv` directory and create a new virtual environment using  `python -m venv venv`. After creating the new environment, try activating it again.

After resolving the issue with the virtual environment, install all dependencies: `pip install -r requirements.txt` if you have a requirements file or install them manually. Then, rerun the application.
User: estou usando windows... mas no terminal √© o git bash
2025-04-14 14:56:29,119 [INFO] Resposta da IA (imagem): Okay, I see the problem. Even though you're on Windows, because you're using Git Bash as your terminal, you need to use the Linux-style activation command.  The fact that it's not working suggests that *either* the `venv` directory doesn't contain the `bin/activate` script (as the error message says) *or* there's some kind of permission or execution issue with the script itself in the Git Bash environment.

Here's a breakdown of the likely problem and solutions:

**The Problem:**

Git Bash (MinGW64 specifically) is a Unix-like environment on Windows. While it uses many Linux commands, it still operates within the Windows file system.  Therefore, even when using `source venv/bin/activate`, Git Bash is looking for a proper executable file inside that bin directory, but in your case, that doesn't exist in the proper format.

**Solutions:**

1.  **Use the `.` command as alias to `source`:**  In some Git Bash setups, `.` is explicitly aliased to `source`. Try using it:

    ```bash
    . venv/bin/activate
    ```

    This is a slight variation of the `source` command, but it might work in your particular Git Bash environment.
2.  **Use the correct script in Git Bash (if it exists):** Some virtual environment setups for Git Bash will create a script that is more appropriate for the shell.  Look within your `venv` directory; in some cases, you will see a file `venv/Scripts/activate`. If this file exists, it should be ran directly in Git Bash. However, if that's the case, the path would more likely be `source venv/Scripts/activate`. In this case, that is *not* what you are looking for. You can use the next solution for the `activate` script.
3.  **Execute the Activate.ps1 Script:** Git Bash can run PowerShell scripts directly. Run the PowerShell script activate.ps1 inside powershell. To do this, you can run `powershell -ExecutionPolicy Bypass -File ./venv/Scripts/Activate.ps1` in your terminal. This method is likely the most reliable method to run virtual environments on Git Bash in Windows.

**Troubleshooting steps to confirm the root cause:**

1.  **List the contents of `venv/bin`:**  In your Git Bash terminal, run the command `ls -l venv/bin`.  Examine the output.  You should see a file named `activate` (without a file extension).  If it's not there, that's your primary problem - the venv wasn't created properly.
2.  **Check permissions:** If the `activate` script *is* there, check its permissions. It should have execute permissions. The output of `ls -l venv/bin` will show you the permissions. If the file doesn't have an `x` in the permissions for the user, group, or others, the script won't be executable.  (Note:  Git Bash's permissions model is somewhat emulated and not always perfectly accurate, but it's worth checking.)

If the directory `venv/bin` does *not* contain the activate script, then you should delete the `venv` folder and re-create the virtual environment.

In summary, even if the most likely solution fails, use the troubleshooting steps to confirm that `venv/bin/activate` is an executable.
2025-04-14 14:56:29,217 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:56:29,219 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:56:29,220 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:56:29,222 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:56:29,223 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:56:29,225 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:56:29,226 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:56:29,228 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:56:29,229 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:56:29,231 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:56:29,233 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:56:29,269 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:56:29,286 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:56:29,288 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:56:29,290 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:56:29,292 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:56:29,294 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:56:29,295 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:56:29,297 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:56:29,298 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:56:29,312 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:56:29,313 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:56:29,314 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:56:29,315 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:56:29,316 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:56:29,317 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:56:29,319 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:56:29,320 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:56:29,321 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:56:29,322 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:56:29,324 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:56:29,325 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:56:29,326 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:56:29,327 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:56:29,329 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:56:29,330 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:56:29,331 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:56:29,332 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:56:29,333 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:56:29,334 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:56:29,335 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:56:29,337 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:56:29,338 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:56:29,339 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:56:29,341 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:56:29,342 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:56:29,344 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:56:29,345 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:56:29,347 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:56:29,348 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:56:29,349 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:56:29,351 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:56:29,352 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:57:16,265 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:57:16,267 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:57:16,269 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:57:16,271 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:57:16,273 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:57:16,275 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:57:16,277 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:57:16,278 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:57:16,281 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:57:16,282 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:57:16,284 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:57:16,285 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:57:16,287 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:57:16,289 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:57:16,291 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:57:16,292 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:57:16,294 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:57:16,295 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:57:16,297 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:57:16,299 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:57:16,301 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:57:16,303 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:57:16,304 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:57:16,306 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:57:16,309 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:57:16,310 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:57:16,312 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:57:16,313 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:57:16,314 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:57:16,316 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:57:16,317 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:57:16,318 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:57:16,320 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:57:16,321 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:57:16,322 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:57:16,324 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:57:16,325 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:57:16,326 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:57:16,327 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:57:16,329 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:57:16,330 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:57:16,332 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:57:16,333 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:57:16,335 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:57:16,337 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:57:16,338 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:57:16,340 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:57:16,341 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:57:16,342 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:57:16,343 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:57:16,344 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:57:16,345 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:57:16,346 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:57:16,440 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:57:16,441 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:57:16,443 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:57:16,444 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:57:16,447 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:57:16,450 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:57:16,452 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:57:16,454 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:57:16,456 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:57:16,457 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:57:16,459 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:57:16,460 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:57:16,462 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:57:16,463 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:57:16,466 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:57:16,467 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:57:16,469 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:57:16,470 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:57:16,472 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:57:16,473 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:57:16,478 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:57:16,484 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:57:16,486 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:57:16,488 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:57:16,490 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:57:16,491 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:57:16,495 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:57:16,497 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:57:16,499 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:57:16,500 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:57:16,502 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:57:16,503 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:57:16,505 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:57:16,506 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:57:16,508 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:57:16,509 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:57:16,510 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:57:16,511 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:57:16,512 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:57:16,513 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:57:16,515 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:57:16,517 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:57:16,520 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:57:16,521 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:57:16,522 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:57:16,524 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:57:16,525 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:57:16,526 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:57:16,527 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:57:16,528 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:57:16,529 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:57:16,530 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:57:16,532 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:57:21,763 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:57:21,764 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:57:21,766 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:57:21,769 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:57:21,772 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:57:21,774 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:57:21,775 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:57:21,777 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:57:21,779 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:57:21,780 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:57:21,783 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:57:21,784 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:57:21,786 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:57:21,787 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:57:21,789 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:57:21,790 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:57:21,801 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:57:21,803 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:57:21,805 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:57:21,810 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:57:21,812 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:57:21,815 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:57:21,818 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:57:21,819 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:57:21,820 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:57:21,821 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:57:21,822 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:57:21,824 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:57:21,825 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:57:21,826 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:57:21,827 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:57:21,828 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:57:21,829 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:57:21,830 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:57:21,832 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:57:21,834 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:57:21,835 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:57:21,836 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:57:21,837 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:57:21,838 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:57:21,839 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:57:21,840 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:57:21,841 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:57:21,843 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:57:21,844 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:57:21,845 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:57:21,846 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:57:21,847 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:57:21,849 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:57:21,851 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:57:21,852 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:57:21,853 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:57:21,854 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:57:22,004 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:57:22,006 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:57:22,007 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:57:22,009 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:57:22,011 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:57:22,013 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:57:22,014 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:57:22,016 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:57:22,018 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:57:22,020 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:57:22,021 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:57:22,022 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:57:22,024 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:57:22,026 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:57:22,028 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:57:22,029 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:57:22,030 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:57:22,032 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:57:22,034 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:57:22,035 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:57:22,037 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:57:22,038 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:57:22,041 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:57:22,042 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:57:22,044 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:57:22,045 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:57:22,047 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:57:22,049 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:57:22,050 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:57:22,052 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:57:22,054 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:57:22,056 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:57:22,058 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:57:22,060 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:57:22,061 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:57:22,062 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:57:22,063 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:57:22,064 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:57:22,066 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:57:22,068 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:57:22,069 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:57:22,070 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:57:22,071 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:57:22,072 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:57:22,074 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:57:22,075 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:57:22,076 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:57:22,077 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:57:22,078 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:57:22,079 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:57:22,080 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:57:22,081 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:57:22,083 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:57:22,090 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\agents_ia\comandAI\assets\20250414145722_clipboard_20250414145716.png, Prompt: Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"N√£o foram encontrados arquivos de √°udio no diret√≥rio {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcri√ß√£o salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diret√≥rio atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Defini√ß√£o dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Cria√ß√£o dos diret√≥rios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configura√ß√£o do layout do Streamlit
st.set_page_config(
    page_title="An√°lise de A√ß√µes",
    layout="wide",
    page_icon="üìà"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscila√ß√µes

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Per√≠odo', 'Oscila√ß√£o'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("üìä Dashboard de An√°lise de A√ß√µes")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma a√ß√£o", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"üìå Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("üîç Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descri√ß√£o', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa a√ß√£o.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("üí∞ Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("üìâ Oscila√ß√µes")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("üìà Tabela Geral de A√ß√µes")
        st.dataframe(self.acoes)

# Execu√ß√£o
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para an√°lise fundamentalista de a√ß√µes da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"N√£o foram encontrados arquivos .txt no diret√≥rio {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conte√∫do do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conte√∫do HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conte√∫do HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se est√° correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configura√ß√£o de v√≠deos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configura√ß√£o de √°udios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcri√ß√µes e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diret√≥rio
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diret√≥rio.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do √∫ltimo arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conte√∫do do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimens√µes do embedding:", embedding.shape)

# Caminho do diret√≥rio de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o √∫ltimo embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configura√ß√£o de transcri√ß√µes e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diret√≥rio onde est√° procurando as transcri√ß√µes
        print(f"Diret√≥rio de entrada das transcri√ß√µes: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcri√ß√£o no diret√≥rio de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcri√ß√£o encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo n√£o encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcri√ß√£o do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} est√° vazio.")
                    return

            # Gerar o embedding da transcri√ß√£o
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcri√ß√£o e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de √°udio
    with sr.Microphone() as source:
        print("Diga algo em portugu√™s...")

        while True:
            try:
                # Escuta o √°udio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {texto_portugues}")

                # Traduz o texto para o ingl√™s
                traducao = translator.translate(texto_portugues)
                print(f"Tradu√ß√£o para o ingl√™s: {traducao}")

            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usu√°rio")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configura√ß√µes de √°udio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de √°udio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em portugu√™s...")

try:
    audio_buffer = []

    while True:
        # L√™ dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o √°udio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o √°udio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradu√ß√£o
            print(f"Tradu√ß√£o para o ingl√™s: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usu√°rio")

    # Fecha o stream de √°udio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O v√≠deo {video_path} n√£o cont√©m √°udio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de v√≠deo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de v√≠deo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de v√≠deo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Convers√£o de v√≠deo para √°udio conclu√≠da!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configura√ß√µes da API
handler = genai('gemini-1.5-flash')

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Fun√ß√£o para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conte√∫do com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares √∫nicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcri√ß√£o obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' n√£o √© uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuni√µes: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de reuni√µes: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|t√≥picos da|pontos (relevantes|principais)|an√°lise)\b.*\b(reuni√£o|√∫ltima (reuni√£o|conversa|sess√£o))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como an√°lise de reuni√£o.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padr√£o.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que s√£o muito similares ao comando atual para evitar redund√¢ncia.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padr√£o: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # M√©todos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de an√°lise de reuni√£o.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conte√∫do usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inv√°lida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar vari√°veis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conte√∫do com base no prompt, contexto e reuni√£o"""
        try:
            # Supondo que a API espera um dicion√°rio com os par√¢metros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisi√ß√£o para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar vari√°veis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Cria√ß√£o da conclus√£o do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que voc√™ acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["fa√ßa um resumo da √∫ltima reuni√£o.", "t√≥picos da √∫ltima reuni√£o", "resuma a √∫ltima reuni√£o", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Voc√™ √© respons√°vel por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - N√£o acrescentar t√≠tulo nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informa√ß√µes acima e forne√ßa uma solu√ß√£o detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Voc√™ √© respons√°vel analisar com detalhes a reuni√£o de [str_meeting], e fornecer uma longa est√≥ria sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precis√£o da resposta.
            - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e fa√ßa uma pesquisa online para uma resposta mais precisa das quest√µes de [str_text]
        - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [context]: Regras B√°sicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("√Åudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum √°udio detectado.")
                return None
            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configura√ß√µes da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicion√°rio
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta n√£o est√° no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API: {e}")
        return None

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: quais IDES j√° possuem integra√ß√£o com alguma IA que consegue receber o seu c√≥digo como contexto e fornecer assistencia no desenvolvimento.???

Atualmente s√≥ conhe√ßo o VScode com o copilot e o gemini integrados.
Assistant: Com base no c√≥digo fornecido e nas suas instru√ß√µes, compilei uma lista de IDEs (Ambientes de Desenvolvimento Integrados) que j√° possuem integra√ß√£o com alguma IA capaz de receber o seu c√≥digo como contexto e fornecer assist√™ncia no desenvolvimento:

*   **VS Code:** J√° possui integra√ß√µes com o GitHub Copilot e o Gemini.

A pesquisa foi realizada utilizando o c√≥digo fonte para identificar as ferramentas e frameworks mencionados, e, com base nisso, fornecer uma resposta precisa.
User: mesmo instalando o pacote n√£o reconhece.
Assistant: Com base no contexto do c√≥digo fornecido, aqui est√£o alguns dos poss√≠veis problemas e solu√ß√µes que podem causar problemas de reconhecimento de pacotes:

1.  **Problema de ambiente:** Certifique-se de que o pacote google-generativeai esteja instalado no ambiente Python correto. √Äs vezes, pode ser instalado em um ambiente diferente do que est√° sendo usado.

    *   **Solu√ß√£o:** Ative o ambiente virtual apropriado e use `pip show google-generativeai` para verificar se o pacote est√° instalado naquele ambiente. Se n√£o estiver, instale-o usando `pip install google-generativeai`.

2.  **Problema de cache:** √Äs vezes, o pip pode ter problemas de cache que impedem que ele reconhe√ßa pacotes rec√©m-instalados.

    *   **Solu√ß√£o:** Tente atualizar o pip e limpar o cache do pip antes de instalar o pacote novamente. Use os seguintes comandos:

    ```bash
    pip install --upgrade pip
    pip cache purge
    pip install google-generativeai
    ```

3.  **Problema de caminho:** √Äs vezes, o Python pode n√£o encontrar o pacote instalado porque o caminho de instala√ß√£o do pacote n√£o est√° no `sys.path`.

    *   **Solu√ß√£o:** Adicione o diret√≥rio do pacote ao `sys.path` em seu script Python ou bloco de anota√ß√µes. Voc√™ pode encontrar o diret√≥rio de instala√ß√£o do pacote usando `pip show google-generativeai -f`. Em seguida, adicione esse caminho ao `sys.path` em seu script:

    ```python
    import sys
    sys.path.append('/caminho/para/site-packages')  # Substitua com o caminho real
    import google.generativeai as genai
    ```

4.  **Problema de nome:** √Äs vezes, o nome do pacote no c√≥digo n√£o corresponde ao nome real do pacote instalado.

    *   **Solu√ß√£o:** Verifique se o nome do pacote est√° correto em sua instru√ß√£o `import`. Nesse caso, ele deve ser `import google.generativeai as genai`.

5.  **Problema de vers√£o:** √Äs vezes, os pacotes podem apresentar problemas de compatibilidade com outras vers√µes de pacotes ou com a vers√£o Python que voc√™ est√° usando.

    *   **Solu√ß√£o:** Considere usar uma vers√£o diferente do pacote que seja compat√≠vel com sua configura√ß√£o.

6.  **IDE ou problema de editor:** √Äs vezes, o IDE ou editor que voc√™ est√° usando pode n√£o ser capaz de reconhecer pacotes rec√©m-instalados.

    *   **Solu√ß√£o:** Reinicie o IDE ou editor ou recarregue o ambiente Python.
User: parece que √© algo com ambiente...porque outras aplica√ß√µes est√£o funcionando com essa lib instalada.
Assistant: Compreendo. Se outras aplica√ß√µes est√£o funcionando corretamente com a `google-generativeai` instalada, o problema provavelmente reside no ambiente espec√≠fico em que a aplica√ß√£o atual est√° sendo executada. Aqui est√£o algumas √°reas para investigar e poss√≠veis solu√ß√µes:

1.  **Ambiente Virtual:**
    *   **Verifica√ß√£o:** Certifique-se de que a aplica√ß√£o esteja rodando dentro do ambiente virtual correto. Se voc√™ estiver utilizando um ambiente virtual (como `venv` ou `conda`), ative-o antes de executar a aplica√ß√£o.

    ```bash
    # Para venv
    source venv/bin/activate  # ou venv\Scripts\activate no Windows

    # Para conda
    conda activate meu_ambiente
    ```
    *   **Confirma√ß√£o:** Verifique se o ambiente virtual ativado √© o mesmo onde `google-generativeai` est√° instalado. Utilize `pip show google-generativeai` para confirmar a instala√ß√£o e o caminho do pacote.

2.  **Caminhos de Importa√ß√£o:**
    *   **Investiga√ß√£o:** Verifique se o caminho para o diret√≥rio do pacote est√° corretamente configurado no `sys.path`. Isso pode variar entre diferentes ambientes e instala√ß√µes.
    *   **Solu√ß√£o:** Adicione o caminho correto ao `sys.path` no in√≠cio do script. Para descobrir o caminho correto, execute o seguinte no ambiente onde a lib funciona:

    ```python
    import google.generativeai
    print(google.generativeai.__file__)
    ```
    Copie o diret√≥rio onde o arquivo `__init__.py` est√° localizado e adicione ao `sys.path` no seu script.

    ```python
    import sys
    sys.path.append('/caminho/para/o/site-packages')
    import google.generativeai as genai
    ```

3.  **Conflitos de Depend√™ncias:**
    *   **Investiga√ß√£o:** Pode haver conflitos entre as depend√™ncias de diferentes pacotes no seu ambiente.
    *   **Solu√ß√£o:** Tente criar um ambiente virtual limpo e instalar apenas as depend√™ncias necess√°rias para a sua aplica√ß√£o atual.

    ```bash
    python -m venv novo_ambiente
    source novo_ambiente/bin/activate  # ou novo_ambiente\Scripts\activate no Windows
    pip install google-generativeai
    # Instale outros pacotes necess√°rios para esta aplica√ß√£o
    ```

4.  **Cache do Editor/IDE:**
    *   **Investiga√ß√£o:** O editor ou IDE pode estar usando um cache que n√£o est√° atualizado com as altera√ß√µes no ambiente.
    *   **Solu√ß√£o:** Reinicie o editor ou IDE, ou force a atualiza√ß√£o do cache. No VS Code, isso pode ser feito atrav√©s do comando "Developer: Reload Window".

5.  **Vari√°veis de Ambiente:**
    *   **Investiga√ß√£o:** Verifique se todas as vari√°veis de ambiente necess√°rias (como `API_KEY_GEMINI`) est√£o corretamente configuradas e acess√≠veis no ambiente em que a aplica√ß√£o est√° rodando.
    *   **Solu√ß√£o:** Certifique-se de que as vari√°veis de ambiente est√£o definidas corretamente e recarregue o ambiente se necess√°rio.

6.  **Permiss√µes:**
    *   **Investiga√ß√£o:** Verifique se o usu√°rio que est√° executando a aplica√ß√£o tem permiss√µes para acessar os arquivos e diret√≥rios necess√°rios.
    *   **Solu√ß√£o:** Ajuste as permiss√µes conforme necess√°rio para garantir que o usu√°rio tenha acesso aos arquivos e diret√≥rios requeridos.

7.  **Vers√µes do Python:**
    *   **Investiga√ß√£o:** Confirme que a vers√£o do Python em uso no ambiente √© compat√≠vel com a `google-generativeai`.
    *   **Solu√ß√£o:** Verifique a vers√£o do Python utilizada (`python --version`) e, se necess√°rio, instale uma vers√£o compat√≠vel com a biblioteca.

Com esses passos, voc√™ deve conseguir identificar e resolver o problema de ambiente que est√° impedindo a sua aplica√ß√£o de reconhecer o pacote `google-generativeai`.
User: ocorreu esse erro na imagem.
Assistant: The image shows that the virtual environment is not activated.

Here's how to fix this:

1.  **Verify the virtual environment's `venv` directory:** The error message  `bash: venv/bin/activate: No such file or directory` indicates that the `activate` script is not found in `venv/bin/`. This usually happens if the virtual environment was not created properly or the `venv` directory was renamed or deleted.  You can see from the file explorer in the image that there is a `venv` directory in the project root.
2.  **Check the correct activation command:**  The command `source venv/bin/activate` is usually used on Linux/macOS systems. For Windows, the activation script is located at `venv\Scripts\activate`. Therefore, try running  `venv\Scripts\activate` in the terminal instead.
3.  **Recreate the virtual environment (if necessary):** If you've confirmed that the virtual environment is broken, delete the existing `venv` directory and create a new virtual environment using  `python -m venv venv`. After creating the new environment, try activating it again.

After resolving the issue with the virtual environment, install all dependencies: `pip install -r requirements.txt` if you have a requirements file or install them manually. Then, rerun the application.
User: estou usando windows... mas no terminal √© o git bash
Assistant: Okay, I see the problem. Even though you're on Windows, because you're using Git Bash as your terminal, you need to use the Linux-style activation command.  The fact that it's not working suggests that *either* the `venv` directory doesn't contain the `bin/activate` script (as the error message says) *or* there's some kind of permission or execution issue with the script itself in the Git Bash environment.

Here's a breakdown of the likely problem and solutions:

**The Problem:**

Git Bash (MinGW64 specifically) is a Unix-like environment on Windows. While it uses many Linux commands, it still operates within the Windows file system.  Therefore, even when using `source venv/bin/activate`, Git Bash is looking for a proper executable file inside that bin directory, but in your case, that doesn't exist in the proper format.

**Solutions:**

1.  **Use the `.` command as alias to `source`:**  In some Git Bash setups, `.` is explicitly aliased to `source`. Try using it:

    ```bash
    . venv/bin/activate
    ```

    This is a slight variation of the `source` command, but it might work in your particular Git Bash environment.
2.  **Use the correct script in Git Bash (if it exists):** Some virtual environment setups for Git Bash will create a script that is more appropriate for the shell.  Look within your `venv` directory; in some cases, you will see a file `venv/Scripts/activate`. If this file exists, it should be ran directly in Git Bash. However, if that's the case, the path would more likely be `source venv/Scripts/activate`. In this case, that is *not* what you are looking for. You can use the next solution for the `activate` script.
3.  **Execute the Activate.ps1 Script:** Git Bash can run PowerShell scripts directly. Run the PowerShell script activate.ps1 inside powershell. To do this, you can run `powershell -ExecutionPolicy Bypass -File ./venv/Scripts/Activate.ps1` in your terminal. This method is likely the most reliable method to run virtual environments on Git Bash in Windows.

**Troubleshooting steps to confirm the root cause:**

1.  **List the contents of `venv/bin`:**  In your Git Bash terminal, run the command `ls -l venv/bin`.  Examine the output.  You should see a file named `activate` (without a file extension).  If it's not there, that's your primary problem - the venv wasn't created properly.
2.  **Check permissions:** If the `activate` script *is* there, check its permissions. It should have execute permissions. The output of `ls -l venv/bin` will show you the permissions. If the file doesn't have an `x` in the permissions for the user, group, or others, the script won't be executable.  (Note:  Git Bash's permissions model is somewhat emulated and not always perfectly accurate, but it's worth checking.)

If the directory `venv/bin` does *not* contain the activate script, then you should delete the `venv` folder and re-create the virtual environment.

In summary, even if the most likely solution fails, use the troubleshooting steps to confirm that `venv/bin/activate` is an executable.
User: Agora deu esse erro.
2025-04-14 14:57:27,432 [INFO] Resposta da IA (imagem): Based on the screenshot provided, you're still facing an issue related to the virtual environment, but now it's a different error. The error is:

```
/c/Users/jfreis/Documents/agents_ia/comandAI/venv/bin/python: can't open file '/c/Users/jfreis/Documents/agents_ia/comandAI/main.py': [Errno 2] No such file or directory
```

Here's what this means and how to troubleshoot it:

**The Problem:**

The error "No such file or directory" clearly indicates that the Python interpreter within your activated virtual environment cannot find the `main.py` file you're trying to execute. This could be due to several reasons:

1.  **Incorrect working directory:** You might be running the command from a directory where `main.py` doesn't exist.

2.  **Incorrect file path:** The file path in your command (`/c/Users/jfreis/Documents/agents_ia/comandAI/main.py`) might be wrong.

3.  **File doesn't exist:** The `main.py` file may simply not exist at the specified location.

**Troubleshooting Steps:**

1.  **Verify file existence:**

    *   Double-check that `main.py` exists at the path `/c/Users/jfreis/Documents/agents_ia/comandAI/main.py`.  Use your file explorer to confirm this.
    *   Make sure you didn't accidentally rename or move the file.

2.  **Check working directory:**

    *   Run the command `pwd` (print working directory) in your Git Bash terminal.  This will tell you the current directory from which you're running the command.
    *   If the current directory isn't `/c/Users/jfreis/Documents/agents_ia/comandAI`, you need to change it. You can use the `cd` command to navigate to the correct directory:
    ```bash
    cd /c/Users/jfreis/Documents/agents_ia/comandAI
    ```

3.  **Check the command:**
If your pwd is on directory, check the command used to execute the file. If you executed the command like `python /c/Users/jfreis/Documents/agents_ia/comandAI/main.py`, you can simply run `python main.py`.

4.  **Double-check case sensitivity:**  Git Bash, being a Unix-like environment, is case-sensitive. Make sure the file name "main.py" matches the case exactly.

**Example:**

Let's say `pwd` shows that you're currently in `/c/Users/jfreis/`

*   You would run:

    ```bash
    cd Documents/agents_ia/comandAI
    python main.py
    ```

After you've checked the file path and working directory, you should be able to run your Python script without the "No such file or directory" error.

2025-04-14 14:57:27,519 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:57:27,521 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:57:27,522 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:57:27,524 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:57:27,525 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:57:27,527 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:57:27,529 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:57:27,530 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:57:27,531 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:57:27,533 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:57:27,534 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:57:27,535 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:57:27,536 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:57:27,537 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:57:27,538 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:57:27,539 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:57:27,541 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:57:27,541 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:57:27,542 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:57:27,543 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:57:27,544 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:57:27,546 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:57:27,547 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:57:27,548 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:57:27,549 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:57:27,550 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:57:27,551 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:57:27,552 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:57:27,553 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:57:27,554 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:57:27,556 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:57:27,557 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:57:27,558 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:57:27,559 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:57:27,560 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:57:27,561 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:57:27,562 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:57:27,563 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:57:27,564 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:57:27,565 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:57:27,567 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:57:27,568 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:57:27,569 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:57:27,570 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:57:27,571 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:57:27,573 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:57:27,574 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:57:27,575 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:57:27,576 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:57:27,577 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:57:27,578 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:57:27,579 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:57:27,580 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 15:12:41,428 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 15:12:41,430 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 15:12:41,432 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 15:12:41,434 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 15:12:41,435 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 15:12:41,437 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 15:12:41,438 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 15:12:41,440 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 15:12:41,441 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 15:12:41,444 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 15:12:41,445 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 15:12:41,447 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 15:12:41,449 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 15:12:41,450 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 15:12:41,452 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 15:12:41,453 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 15:12:41,455 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 15:12:41,456 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 15:12:41,458 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 15:12:41,460 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 15:12:41,462 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 15:12:41,463 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 15:12:41,465 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 15:12:41,466 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 15:12:41,467 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 15:12:41,469 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 15:12:41,470 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 15:12:41,471 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 15:12:41,473 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 15:12:41,474 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 15:12:41,477 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 15:12:41,478 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 15:12:41,480 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 15:12:41,481 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 15:12:41,483 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 15:12:41,486 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 15:12:41,488 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 15:12:41,489 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 15:12:41,491 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 15:12:41,492 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 15:12:41,494 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 15:12:41,495 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 15:12:41,497 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 15:12:41,498 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 15:12:41,499 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 15:12:41,500 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 15:12:41,501 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 15:12:41,502 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 15:12:41,503 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 15:12:41,504 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 15:12:41,505 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 15:12:41,506 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 15:12:41,508 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 15:12:41,654 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 15:12:41,656 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 15:12:41,658 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 15:12:41,659 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 15:12:41,661 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 15:12:41,663 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 15:12:41,665 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 15:12:41,666 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 15:12:41,667 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 15:12:41,669 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 15:12:41,670 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 15:12:41,672 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 15:12:41,673 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 15:12:41,674 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 15:12:41,676 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 15:12:41,678 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 15:12:41,679 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 15:12:41,680 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 15:12:41,682 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 15:12:41,684 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 15:12:41,687 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 15:12:41,690 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 15:12:41,693 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 15:12:41,694 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 15:12:41,696 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 15:12:41,697 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 15:12:41,699 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 15:12:41,701 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 15:12:41,703 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 15:12:41,705 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 15:12:41,707 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 15:12:41,708 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 15:12:41,710 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 15:12:41,712 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 15:12:41,713 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 15:12:41,715 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 15:12:41,716 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 15:12:41,718 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 15:12:41,719 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 15:12:41,720 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 15:12:41,721 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 15:12:41,722 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 15:12:41,723 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 15:12:41,724 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 15:12:41,725 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 15:12:41,727 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 15:12:41,728 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 15:12:41,729 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 15:12:41,730 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 15:12:41,732 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 15:12:41,733 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 15:12:41,734 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 15:12:41,734 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 15:12:41,742 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\agents_ia\comandAI\assets\20250414151241_clipboard_20250414145716.png, Prompt: Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"N√£o foram encontrados arquivos de √°udio no diret√≥rio {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcri√ß√£o salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diret√≥rio atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Defini√ß√£o dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Cria√ß√£o dos diret√≥rios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configura√ß√£o do layout do Streamlit
st.set_page_config(
    page_title="An√°lise de A√ß√µes",
    layout="wide",
    page_icon="üìà"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscila√ß√µes

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Per√≠odo', 'Oscila√ß√£o'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("üìä Dashboard de An√°lise de A√ß√µes")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma a√ß√£o", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"üìå Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("üîç Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descri√ß√£o', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa a√ß√£o.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("üí∞ Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("üìâ Oscila√ß√µes")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("üìà Tabela Geral de A√ß√µes")
        st.dataframe(self.acoes)

# Execu√ß√£o
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para an√°lise fundamentalista de a√ß√µes da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"N√£o foram encontrados arquivos .txt no diret√≥rio {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conte√∫do do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conte√∫do HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conte√∫do HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se est√° correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configura√ß√£o de v√≠deos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configura√ß√£o de √°udios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcri√ß√µes e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diret√≥rio
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diret√≥rio.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do √∫ltimo arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conte√∫do do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimens√µes do embedding:", embedding.shape)

# Caminho do diret√≥rio de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o √∫ltimo embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configura√ß√£o de transcri√ß√µes e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diret√≥rio onde est√° procurando as transcri√ß√µes
        print(f"Diret√≥rio de entrada das transcri√ß√µes: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcri√ß√£o no diret√≥rio de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcri√ß√£o encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo n√£o encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcri√ß√£o do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} est√° vazio.")
                    return

            # Gerar o embedding da transcri√ß√£o
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcri√ß√£o e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de √°udio
    with sr.Microphone() as source:
        print("Diga algo em portugu√™s...")

        while True:
            try:
                # Escuta o √°udio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {texto_portugues}")

                # Traduz o texto para o ingl√™s
                traducao = translator.translate(texto_portugues)
                print(f"Tradu√ß√£o para o ingl√™s: {traducao}")

            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usu√°rio")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configura√ß√µes de √°udio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de √°udio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em portugu√™s...")

try:
    audio_buffer = []

    while True:
        # L√™ dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o √°udio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o √°udio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradu√ß√£o
            print(f"Tradu√ß√£o para o ingl√™s: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usu√°rio")

    # Fecha o stream de √°udio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O v√≠deo {video_path} n√£o cont√©m √°udio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de v√≠deo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de v√≠deo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de v√≠deo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Convers√£o de v√≠deo para √°udio conclu√≠da!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configura√ß√µes da API
handler = genai('gemini-1.5-flash')

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Fun√ß√£o para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conte√∫do com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares √∫nicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcri√ß√£o obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' n√£o √© uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuni√µes: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de reuni√µes: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|t√≥picos da|pontos (relevantes|principais)|an√°lise)\b.*\b(reuni√£o|√∫ltima (reuni√£o|conversa|sess√£o))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como an√°lise de reuni√£o.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padr√£o.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que s√£o muito similares ao comando atual para evitar redund√¢ncia.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padr√£o: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # M√©todos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de an√°lise de reuni√£o.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conte√∫do usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inv√°lida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar vari√°veis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conte√∫do com base no prompt, contexto e reuni√£o"""
        try:
            # Supondo que a API espera um dicion√°rio com os par√¢metros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisi√ß√£o para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar vari√°veis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Cria√ß√£o da conclus√£o do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que voc√™ acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["fa√ßa um resumo da √∫ltima reuni√£o.", "t√≥picos da √∫ltima reuni√£o", "resuma a √∫ltima reuni√£o", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Voc√™ √© respons√°vel por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - N√£o acrescentar t√≠tulo nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informa√ß√µes acima e forne√ßa uma solu√ß√£o detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Voc√™ √© respons√°vel analisar com detalhes a reuni√£o de [str_meeting], e fornecer uma longa est√≥ria sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precis√£o da resposta.
            - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e fa√ßa uma pesquisa online para uma resposta mais precisa das quest√µes de [str_text]
        - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [context]: Regras B√°sicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("√Åudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum √°udio detectado.")
                return None
            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configura√ß√µes da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicion√°rio
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta n√£o est√° no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API: {e}")
        return None

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: quais IDES j√° possuem integra√ß√£o com alguma IA que consegue receber o seu c√≥digo como contexto e fornecer assistencia no desenvolvimento.???

Atualmente s√≥ conhe√ßo o VScode com o copilot e o gemini integrados.
Assistant: Com base no c√≥digo fornecido e nas suas instru√ß√µes, compilei uma lista de IDEs (Ambientes de Desenvolvimento Integrados) que j√° possuem integra√ß√£o com alguma IA capaz de receber o seu c√≥digo como contexto e fornecer assist√™ncia no desenvolvimento:

*   **VS Code:** J√° possui integra√ß√µes com o GitHub Copilot e o Gemini.

A pesquisa foi realizada utilizando o c√≥digo fonte para identificar as ferramentas e frameworks mencionados, e, com base nisso, fornecer uma resposta precisa.
User: mesmo instalando o pacote n√£o reconhece.
Assistant: Com base no contexto do c√≥digo fornecido, aqui est√£o alguns dos poss√≠veis problemas e solu√ß√µes que podem causar problemas de reconhecimento de pacotes:

1.  **Problema de ambiente:** Certifique-se de que o pacote google-generativeai esteja instalado no ambiente Python correto. √Äs vezes, pode ser instalado em um ambiente diferente do que est√° sendo usado.

    *   **Solu√ß√£o:** Ative o ambiente virtual apropriado e use `pip show google-generativeai` para verificar se o pacote est√° instalado naquele ambiente. Se n√£o estiver, instale-o usando `pip install google-generativeai`.

2.  **Problema de cache:** √Äs vezes, o pip pode ter problemas de cache que impedem que ele reconhe√ßa pacotes rec√©m-instalados.

    *   **Solu√ß√£o:** Tente atualizar o pip e limpar o cache do pip antes de instalar o pacote novamente. Use os seguintes comandos:

    ```bash
    pip install --upgrade pip
    pip cache purge
    pip install google-generativeai
    ```

3.  **Problema de caminho:** √Äs vezes, o Python pode n√£o encontrar o pacote instalado porque o caminho de instala√ß√£o do pacote n√£o est√° no `sys.path`.

    *   **Solu√ß√£o:** Adicione o diret√≥rio do pacote ao `sys.path` em seu script Python ou bloco de anota√ß√µes. Voc√™ pode encontrar o diret√≥rio de instala√ß√£o do pacote usando `pip show google-generativeai -f`. Em seguida, adicione esse caminho ao `sys.path` em seu script:

    ```python
    import sys
    sys.path.append('/caminho/para/site-packages')  # Substitua com o caminho real
    import google.generativeai as genai
    ```

4.  **Problema de nome:** √Äs vezes, o nome do pacote no c√≥digo n√£o corresponde ao nome real do pacote instalado.

    *   **Solu√ß√£o:** Verifique se o nome do pacote est√° correto em sua instru√ß√£o `import`. Nesse caso, ele deve ser `import google.generativeai as genai`.

5.  **Problema de vers√£o:** √Äs vezes, os pacotes podem apresentar problemas de compatibilidade com outras vers√µes de pacotes ou com a vers√£o Python que voc√™ est√° usando.

    *   **Solu√ß√£o:** Considere usar uma vers√£o diferente do pacote que seja compat√≠vel com sua configura√ß√£o.

6.  **IDE ou problema de editor:** √Äs vezes, o IDE ou editor que voc√™ est√° usando pode n√£o ser capaz de reconhecer pacotes rec√©m-instalados.

    *   **Solu√ß√£o:** Reinicie o IDE ou editor ou recarregue o ambiente Python.
User: parece que √© algo com ambiente...porque outras aplica√ß√µes est√£o funcionando com essa lib instalada.
Assistant: Compreendo. Se outras aplica√ß√µes est√£o funcionando corretamente com a `google-generativeai` instalada, o problema provavelmente reside no ambiente espec√≠fico em que a aplica√ß√£o atual est√° sendo executada. Aqui est√£o algumas √°reas para investigar e poss√≠veis solu√ß√µes:

1.  **Ambiente Virtual:**
    *   **Verifica√ß√£o:** Certifique-se de que a aplica√ß√£o esteja rodando dentro do ambiente virtual correto. Se voc√™ estiver utilizando um ambiente virtual (como `venv` ou `conda`), ative-o antes de executar a aplica√ß√£o.

    ```bash
    # Para venv
    source venv/bin/activate  # ou venv\Scripts\activate no Windows

    # Para conda
    conda activate meu_ambiente
    ```
    *   **Confirma√ß√£o:** Verifique se o ambiente virtual ativado √© o mesmo onde `google-generativeai` est√° instalado. Utilize `pip show google-generativeai` para confirmar a instala√ß√£o e o caminho do pacote.

2.  **Caminhos de Importa√ß√£o:**
    *   **Investiga√ß√£o:** Verifique se o caminho para o diret√≥rio do pacote est√° corretamente configurado no `sys.path`. Isso pode variar entre diferentes ambientes e instala√ß√µes.
    *   **Solu√ß√£o:** Adicione o caminho correto ao `sys.path` no in√≠cio do script. Para descobrir o caminho correto, execute o seguinte no ambiente onde a lib funciona:

    ```python
    import google.generativeai
    print(google.generativeai.__file__)
    ```
    Copie o diret√≥rio onde o arquivo `__init__.py` est√° localizado e adicione ao `sys.path` no seu script.

    ```python
    import sys
    sys.path.append('/caminho/para/o/site-packages')
    import google.generativeai as genai
    ```

3.  **Conflitos de Depend√™ncias:**
    *   **Investiga√ß√£o:** Pode haver conflitos entre as depend√™ncias de diferentes pacotes no seu ambiente.
    *   **Solu√ß√£o:** Tente criar um ambiente virtual limpo e instalar apenas as depend√™ncias necess√°rias para a sua aplica√ß√£o atual.

    ```bash
    python -m venv novo_ambiente
    source novo_ambiente/bin/activate  # ou novo_ambiente\Scripts\activate no Windows
    pip install google-generativeai
    # Instale outros pacotes necess√°rios para esta aplica√ß√£o
    ```

4.  **Cache do Editor/IDE:**
    *   **Investiga√ß√£o:** O editor ou IDE pode estar usando um cache que n√£o est√° atualizado com as altera√ß√µes no ambiente.
    *   **Solu√ß√£o:** Reinicie o editor ou IDE, ou force a atualiza√ß√£o do cache. No VS Code, isso pode ser feito atrav√©s do comando "Developer: Reload Window".

5.  **Vari√°veis de Ambiente:**
    *   **Investiga√ß√£o:** Verifique se todas as vari√°veis de ambiente necess√°rias (como `API_KEY_GEMINI`) est√£o corretamente configuradas e acess√≠veis no ambiente em que a aplica√ß√£o est√° rodando.
    *   **Solu√ß√£o:** Certifique-se de que as vari√°veis de ambiente est√£o definidas corretamente e recarregue o ambiente se necess√°rio.

6.  **Permiss√µes:**
    *   **Investiga√ß√£o:** Verifique se o usu√°rio que est√° executando a aplica√ß√£o tem permiss√µes para acessar os arquivos e diret√≥rios necess√°rios.
    *   **Solu√ß√£o:** Ajuste as permiss√µes conforme necess√°rio para garantir que o usu√°rio tenha acesso aos arquivos e diret√≥rios requeridos.

7.  **Vers√µes do Python:**
    *   **Investiga√ß√£o:** Confirme que a vers√£o do Python em uso no ambiente √© compat√≠vel com a `google-generativeai`.
    *   **Solu√ß√£o:** Verifique a vers√£o do Python utilizada (`python --version`) e, se necess√°rio, instale uma vers√£o compat√≠vel com a biblioteca.

Com esses passos, voc√™ deve conseguir identificar e resolver o problema de ambiente que est√° impedindo a sua aplica√ß√£o de reconhecer o pacote `google-generativeai`.
User: ocorreu esse erro na imagem.
Assistant: The image shows that the virtual environment is not activated.

Here's how to fix this:

1.  **Verify the virtual environment's `venv` directory:** The error message  `bash: venv/bin/activate: No such file or directory` indicates that the `activate` script is not found in `venv/bin/`. This usually happens if the virtual environment was not created properly or the `venv` directory was renamed or deleted.  You can see from the file explorer in the image that there is a `venv` directory in the project root.
2.  **Check the correct activation command:**  The command `source venv/bin/activate` is usually used on Linux/macOS systems. For Windows, the activation script is located at `venv\Scripts\activate`. Therefore, try running  `venv\Scripts\activate` in the terminal instead.
3.  **Recreate the virtual environment (if necessary):** If you've confirmed that the virtual environment is broken, delete the existing `venv` directory and create a new virtual environment using  `python -m venv venv`. After creating the new environment, try activating it again.

After resolving the issue with the virtual environment, install all dependencies: `pip install -r requirements.txt` if you have a requirements file or install them manually. Then, rerun the application.
User: estou usando windows... mas no terminal √© o git bash
Assistant: Okay, I see the problem. Even though you're on Windows, because you're using Git Bash as your terminal, you need to use the Linux-style activation command.  The fact that it's not working suggests that *either* the `venv` directory doesn't contain the `bin/activate` script (as the error message says) *or* there's some kind of permission or execution issue with the script itself in the Git Bash environment.

Here's a breakdown of the likely problem and solutions:

**The Problem:**

Git Bash (MinGW64 specifically) is a Unix-like environment on Windows. While it uses many Linux commands, it still operates within the Windows file system.  Therefore, even when using `source venv/bin/activate`, Git Bash is looking for a proper executable file inside that bin directory, but in your case, that doesn't exist in the proper format.

**Solutions:**

1.  **Use the `.` command as alias to `source`:**  In some Git Bash setups, `.` is explicitly aliased to `source`. Try using it:

    ```bash
    . venv/bin/activate
    ```

    This is a slight variation of the `source` command, but it might work in your particular Git Bash environment.
2.  **Use the correct script in Git Bash (if it exists):** Some virtual environment setups for Git Bash will create a script that is more appropriate for the shell.  Look within your `venv` directory; in some cases, you will see a file `venv/Scripts/activate`. If this file exists, it should be ran directly in Git Bash. However, if that's the case, the path would more likely be `source venv/Scripts/activate`. In this case, that is *not* what you are looking for. You can use the next solution for the `activate` script.
3.  **Execute the Activate.ps1 Script:** Git Bash can run PowerShell scripts directly. Run the PowerShell script activate.ps1 inside powershell. To do this, you can run `powershell -ExecutionPolicy Bypass -File ./venv/Scripts/Activate.ps1` in your terminal. This method is likely the most reliable method to run virtual environments on Git Bash in Windows.

**Troubleshooting steps to confirm the root cause:**

1.  **List the contents of `venv/bin`:**  In your Git Bash terminal, run the command `ls -l venv/bin`.  Examine the output.  You should see a file named `activate` (without a file extension).  If it's not there, that's your primary problem - the venv wasn't created properly.
2.  **Check permissions:** If the `activate` script *is* there, check its permissions. It should have execute permissions. The output of `ls -l venv/bin` will show you the permissions. If the file doesn't have an `x` in the permissions for the user, group, or others, the script won't be executable.  (Note:  Git Bash's permissions model is somewhat emulated and not always perfectly accurate, but it's worth checking.)

If the directory `venv/bin` does *not* contain the activate script, then you should delete the `venv` folder and re-create the virtual environment.

In summary, even if the most likely solution fails, use the troubleshooting steps to confirm that `venv/bin/activate` is an executable.
User: Agora deu esse erro.
Assistant: Based on the screenshot provided, you're still facing an issue related to the virtual environment, but now it's a different error. The error is:

```
/c/Users/jfreis/Documents/agents_ia/comandAI/venv/bin/python: can't open file '/c/Users/jfreis/Documents/agents_ia/comandAI/main.py': [Errno 2] No such file or directory
```

Here's what this means and how to troubleshoot it:

**The Problem:**

The error "No such file or directory" clearly indicates that the Python interpreter within your activated virtual environment cannot find the `main.py` file you're trying to execute. This could be due to several reasons:

1.  **Incorrect working directory:** You might be running the command from a directory where `main.py` doesn't exist.

2.  **Incorrect file path:** The file path in your command (`/c/Users/jfreis/Documents/agents_ia/comandAI/main.py`) might be wrong.

3.  **File doesn't exist:** The `main.py` file may simply not exist at the specified location.

**Troubleshooting Steps:**

1.  **Verify file existence:**

    *   Double-check that `main.py` exists at the path `/c/Users/jfreis/Documents/agents_ia/comandAI/main.py`.  Use your file explorer to confirm this.
    *   Make sure you didn't accidentally rename or move the file.

2.  **Check working directory:**

    *   Run the command `pwd` (print working directory) in your Git Bash terminal.  This will tell you the current directory from which you're running the command.
    *   If the current directory isn't `/c/Users/jfreis/Documents/agents_ia/comandAI`, you need to change it. You can use the `cd` command to navigate to the correct directory:
    ```bash
    cd /c/Users/jfreis/Documents/agents_ia/comandAI
    ```

3.  **Check the command:**
If your pwd is on directory, check the command used to execute the file. If you executed the command like `python /c/Users/jfreis/Documents/agents_ia/comandAI/main.py`, you can simply run `python main.py`.

4.  **Double-check case sensitivity:**  Git Bash, being a Unix-like environment, is case-sensitive. Make sure the file name "main.py" matches the case exactly.

**Example:**

Let's say `pwd` shows that you're currently in `/c/Users/jfreis/`

*   You would run:

    ```bash
    cd Documents/agents_ia/comandAI
    python main.py
    ```

After you've checked the file path and working directory, you should be able to run your Python script without the "No such file or directory" error.

User: reponda em portugues.
2025-04-14 15:12:47,936 [INFO] Resposta da IA (imagem): Com base na captura de tela fornecida, voc√™ ainda est√° enfrentando um problema relacionado ao ambiente virtual, mas agora √© um erro diferente. O erro √©:

```
/c/Users/jfreis/Documents/agents_ia/comandAI/venv/bin/python: can't open file '/c/Users/jfreis/Documents/agents_ia/comandAI/main.py': [Errno 2] No such file or directory
```

Veja o que isso significa e como solucionar o problema:

**O Problema:**

O erro "No such file or directory" (Arquivo ou diret√≥rio inexistente) indica claramente que o interpretador Python dentro do seu ambiente virtual ativado n√£o consegue encontrar o arquivo `main.py` que voc√™ est√° tentando executar. Isso pode ocorrer por v√°rios motivos:

1.  **Diret√≥rio de trabalho incorreto:** Voc√™ pode estar executando o comando a partir de um diret√≥rio onde `main.py` n√£o existe.

2.  **Caminho do arquivo incorreto:** O caminho do arquivo no seu comando (`/c/Users/jfreis/Documents/agents_ia/comandAI/main.py`) pode estar errado.

3.  **Arquivo n√£o existe:** O arquivo `main.py` pode simplesmente n√£o existir no local especificado.

**Etapas de Solu√ß√£o de Problemas:**

1.  **Verifique a exist√™ncia do arquivo:**

    *   Verifique se `main.py` existe no caminho `/c/Users/jfreis/Documents/agents_ia/comandAI/main.py`. Use o explorador de arquivos para confirmar isso.
    *   Certifique-se de n√£o ter renomeado ou movido o arquivo acidentalmente.

2.  **Verifique o diret√≥rio de trabalho:**

    *   Execute o comando `pwd` (print working directory - imprimir diret√≥rio de trabalho) no seu terminal Git Bash. Isso dir√° qual √© o diret√≥rio atual de onde voc√™ est√° executando o comando.
    *   Se o diret√≥rio atual n√£o for `/c/Users/jfreis/Documents/agents_ia/comandAI`, voc√™ precisar√° alter√°-lo. Voc√™ pode usar o comando `cd` para navegar at√© o diret√≥rio correto:
    ```bash
    cd /c/Users/jfreis/Documents/agents_ia/comandAI
    ```

3.  **Verifique o comando:**
Se o seu pwd estiver no diret√≥rio correto, verifique o comando usado para executar o arquivo. Se voc√™ executou o comando como `python /c/Users/jfreis/Documents/agents_ia/comandAI/main.py`, voc√™ pode simplesmente executar `python main.py`.

4.  **Verifique a diferencia√ß√£o de mai√∫sculas e min√∫sculas:** O Git Bash, sendo um ambiente semelhante ao Unix, diferencia mai√∫sculas e min√∫sculas. Certifique-se de que o nome do arquivo "main.py" corresponda ao caso exatamente.

**Exemplo:**

Digamos que `pwd` mostre que voc√™ est√° atualmente em `/c/Users/jfreis/`

*   Voc√™ executaria:

    ```bash
    cd Documents/agents_ia/comandAI
    python main.py
    ```

Depois de verificar o caminho do arquivo e o diret√≥rio de trabalho, voc√™ deve conseguir executar seu script Python sem o erro "No such file or directory".

2025-04-14 15:12:48,039 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 15:12:48,041 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 15:12:48,043 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 15:12:48,045 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 15:12:48,046 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 15:12:48,048 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 15:12:48,050 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 15:12:48,052 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 15:12:48,054 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 15:12:48,055 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 15:12:48,056 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 15:12:48,058 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 15:12:48,059 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 15:12:48,060 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 15:12:48,062 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 15:12:48,063 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 15:12:48,064 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 15:12:48,065 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 15:12:48,066 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 15:12:48,068 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 15:12:48,071 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 15:12:48,072 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 15:12:48,073 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 15:12:48,075 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 15:12:48,076 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 15:12:48,078 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 15:12:48,080 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 15:12:48,082 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 15:12:48,083 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 15:12:48,085 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 15:12:48,087 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 15:12:48,088 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 15:12:48,090 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 15:12:48,091 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 15:12:48,092 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 15:12:48,093 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 15:12:48,095 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 15:12:48,096 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 15:12:48,097 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 15:12:48,099 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 15:12:48,101 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 15:12:48,103 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 15:12:48,105 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 15:12:48,106 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 15:12:48,108 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 15:12:48,109 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 15:12:48,111 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 15:12:48,113 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 15:12:48,114 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 15:12:48,115 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 15:12:48,118 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 15:12:48,121 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 15:12:48,122 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 15:13:25,983 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 15:13:25,985 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 15:13:25,986 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 15:13:25,988 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 15:13:25,989 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 15:13:25,991 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 15:13:25,992 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 15:13:25,994 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 15:13:25,996 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 15:13:25,997 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 15:13:25,999 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 15:13:26,000 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 15:13:26,002 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 15:13:26,003 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 15:13:26,004 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 15:13:26,006 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 15:13:26,007 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 15:13:26,008 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 15:13:26,009 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 15:13:26,010 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 15:13:26,012 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 15:13:26,013 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 15:13:26,014 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 15:13:26,016 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 15:13:26,017 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 15:13:26,018 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 15:13:26,019 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 15:13:26,021 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 15:13:26,022 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 15:13:26,023 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 15:13:26,025 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 15:13:26,027 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 15:13:26,028 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 15:13:26,029 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 15:13:26,031 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 15:13:26,033 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 15:13:26,034 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 15:13:26,036 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 15:13:26,038 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 15:13:26,040 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 15:13:26,041 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 15:13:26,042 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 15:13:26,043 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 15:13:26,044 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 15:13:26,045 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 15:13:26,046 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 15:13:26,048 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 15:13:26,049 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 15:13:26,050 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 15:13:26,051 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 15:13:26,052 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 15:13:26,053 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 15:13:26,054 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 15:13:26,200 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 15:13:26,201 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 15:13:26,202 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 15:13:26,204 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 15:13:26,205 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 15:13:26,207 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 15:13:26,208 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 15:13:26,210 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 15:13:26,211 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 15:13:26,212 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 15:13:26,213 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 15:13:26,215 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 15:13:26,216 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 15:13:26,217 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 15:13:26,219 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 15:13:26,220 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 15:13:26,222 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 15:13:26,223 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 15:13:26,224 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 15:13:26,226 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 15:13:26,228 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 15:13:26,229 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 15:13:26,231 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 15:13:26,233 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 15:13:26,235 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 15:13:26,236 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 15:13:26,238 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 15:13:26,239 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 15:13:26,240 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 15:13:26,242 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 15:13:26,243 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 15:13:26,244 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 15:13:26,246 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 15:13:26,247 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 15:13:26,248 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 15:13:26,249 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 15:13:26,250 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 15:13:26,252 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 15:13:26,254 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 15:13:26,256 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 15:13:26,257 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 15:13:26,259 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 15:13:26,260 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 15:13:26,261 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 15:13:26,262 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 15:13:26,263 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 15:13:26,264 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 15:13:26,266 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 15:13:26,267 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 15:13:26,268 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 15:13:26,269 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 15:13:26,270 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 15:13:26,271 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 15:13:26,278 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\agents_ia\comandAI\assets\20250414151326_clipboard_20250414145716.png, Prompt: Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"N√£o foram encontrados arquivos de √°udio no diret√≥rio {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcri√ß√£o salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diret√≥rio atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Defini√ß√£o dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Cria√ß√£o dos diret√≥rios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configura√ß√£o do layout do Streamlit
st.set_page_config(
    page_title="An√°lise de A√ß√µes",
    layout="wide",
    page_icon="üìà"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscila√ß√µes

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Per√≠odo', 'Oscila√ß√£o'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("üìä Dashboard de An√°lise de A√ß√µes")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma a√ß√£o", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"üìå Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("üîç Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descri√ß√£o', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa a√ß√£o.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("üí∞ Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("üìâ Oscila√ß√µes")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("üìà Tabela Geral de A√ß√µes")
        st.dataframe(self.acoes)

# Execu√ß√£o
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para an√°lise fundamentalista de a√ß√µes da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"N√£o foram encontrados arquivos .txt no diret√≥rio {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conte√∫do do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conte√∫do HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conte√∫do HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se est√° correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configura√ß√£o de v√≠deos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configura√ß√£o de √°udios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcri√ß√µes e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diret√≥rio
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diret√≥rio.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do √∫ltimo arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conte√∫do do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimens√µes do embedding:", embedding.shape)

# Caminho do diret√≥rio de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o √∫ltimo embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configura√ß√£o de transcri√ß√µes e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diret√≥rio onde est√° procurando as transcri√ß√µes
        print(f"Diret√≥rio de entrada das transcri√ß√µes: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcri√ß√£o no diret√≥rio de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcri√ß√£o encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo n√£o encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcri√ß√£o do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} est√° vazio.")
                    return

            # Gerar o embedding da transcri√ß√£o
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcri√ß√£o e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de √°udio
    with sr.Microphone() as source:
        print("Diga algo em portugu√™s...")

        while True:
            try:
                # Escuta o √°udio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {texto_portugues}")

                # Traduz o texto para o ingl√™s
                traducao = translator.translate(texto_portugues)
                print(f"Tradu√ß√£o para o ingl√™s: {traducao}")

            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usu√°rio")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configura√ß√µes de √°udio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de √°udio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em portugu√™s...")

try:
    audio_buffer = []

    while True:
        # L√™ dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o √°udio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o √°udio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradu√ß√£o
            print(f"Tradu√ß√£o para o ingl√™s: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usu√°rio")

    # Fecha o stream de √°udio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O v√≠deo {video_path} n√£o cont√©m √°udio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de v√≠deo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de v√≠deo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de v√≠deo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Convers√£o de v√≠deo para √°udio conclu√≠da!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configura√ß√µes da API
handler = genai('gemini-1.5-flash')

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Fun√ß√£o para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conte√∫do com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares √∫nicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcri√ß√£o obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' n√£o √© uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuni√µes: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de reuni√µes: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|t√≥picos da|pontos (relevantes|principais)|an√°lise)\b.*\b(reuni√£o|√∫ltima (reuni√£o|conversa|sess√£o))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como an√°lise de reuni√£o.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padr√£o.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que s√£o muito similares ao comando atual para evitar redund√¢ncia.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padr√£o: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # M√©todos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de an√°lise de reuni√£o.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conte√∫do usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inv√°lida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar vari√°veis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conte√∫do com base no prompt, contexto e reuni√£o"""
        try:
            # Supondo que a API espera um dicion√°rio com os par√¢metros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisi√ß√£o para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar vari√°veis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Cria√ß√£o da conclus√£o do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que voc√™ acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["fa√ßa um resumo da √∫ltima reuni√£o.", "t√≥picos da √∫ltima reuni√£o", "resuma a √∫ltima reuni√£o", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Voc√™ √© respons√°vel por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - N√£o acrescentar t√≠tulo nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informa√ß√µes acima e forne√ßa uma solu√ß√£o detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Voc√™ √© respons√°vel analisar com detalhes a reuni√£o de [str_meeting], e fornecer uma longa est√≥ria sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precis√£o da resposta.
            - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e fa√ßa uma pesquisa online para uma resposta mais precisa das quest√µes de [str_text]
        - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [context]: Regras B√°sicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("√Åudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum √°udio detectado.")
                return None
            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configura√ß√µes da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicion√°rio
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta n√£o est√° no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API: {e}")
        return None

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: quais IDES j√° possuem integra√ß√£o com alguma IA que consegue receber o seu c√≥digo como contexto e fornecer assistencia no desenvolvimento.???

Atualmente s√≥ conhe√ßo o VScode com o copilot e o gemini integrados.
Assistant: Com base no c√≥digo fornecido e nas suas instru√ß√µes, compilei uma lista de IDEs (Ambientes de Desenvolvimento Integrados) que j√° possuem integra√ß√£o com alguma IA capaz de receber o seu c√≥digo como contexto e fornecer assist√™ncia no desenvolvimento:

*   **VS Code:** J√° possui integra√ß√µes com o GitHub Copilot e o Gemini.

A pesquisa foi realizada utilizando o c√≥digo fonte para identificar as ferramentas e frameworks mencionados, e, com base nisso, fornecer uma resposta precisa.
User: mesmo instalando o pacote n√£o reconhece.
Assistant: Com base no contexto do c√≥digo fornecido, aqui est√£o alguns dos poss√≠veis problemas e solu√ß√µes que podem causar problemas de reconhecimento de pacotes:

1.  **Problema de ambiente:** Certifique-se de que o pacote google-generativeai esteja instalado no ambiente Python correto. √Äs vezes, pode ser instalado em um ambiente diferente do que est√° sendo usado.

    *   **Solu√ß√£o:** Ative o ambiente virtual apropriado e use `pip show google-generativeai` para verificar se o pacote est√° instalado naquele ambiente. Se n√£o estiver, instale-o usando `pip install google-generativeai`.

2.  **Problema de cache:** √Äs vezes, o pip pode ter problemas de cache que impedem que ele reconhe√ßa pacotes rec√©m-instalados.

    *   **Solu√ß√£o:** Tente atualizar o pip e limpar o cache do pip antes de instalar o pacote novamente. Use os seguintes comandos:

    ```bash
    pip install --upgrade pip
    pip cache purge
    pip install google-generativeai
    ```

3.  **Problema de caminho:** √Äs vezes, o Python pode n√£o encontrar o pacote instalado porque o caminho de instala√ß√£o do pacote n√£o est√° no `sys.path`.

    *   **Solu√ß√£o:** Adicione o diret√≥rio do pacote ao `sys.path` em seu script Python ou bloco de anota√ß√µes. Voc√™ pode encontrar o diret√≥rio de instala√ß√£o do pacote usando `pip show google-generativeai -f`. Em seguida, adicione esse caminho ao `sys.path` em seu script:

    ```python
    import sys
    sys.path.append('/caminho/para/site-packages')  # Substitua com o caminho real
    import google.generativeai as genai
    ```

4.  **Problema de nome:** √Äs vezes, o nome do pacote no c√≥digo n√£o corresponde ao nome real do pacote instalado.

    *   **Solu√ß√£o:** Verifique se o nome do pacote est√° correto em sua instru√ß√£o `import`. Nesse caso, ele deve ser `import google.generativeai as genai`.

5.  **Problema de vers√£o:** √Äs vezes, os pacotes podem apresentar problemas de compatibilidade com outras vers√µes de pacotes ou com a vers√£o Python que voc√™ est√° usando.

    *   **Solu√ß√£o:** Considere usar uma vers√£o diferente do pacote que seja compat√≠vel com sua configura√ß√£o.

6.  **IDE ou problema de editor:** √Äs vezes, o IDE ou editor que voc√™ est√° usando pode n√£o ser capaz de reconhecer pacotes rec√©m-instalados.

    *   **Solu√ß√£o:** Reinicie o IDE ou editor ou recarregue o ambiente Python.
User: parece que √© algo com ambiente...porque outras aplica√ß√µes est√£o funcionando com essa lib instalada.
Assistant: Compreendo. Se outras aplica√ß√µes est√£o funcionando corretamente com a `google-generativeai` instalada, o problema provavelmente reside no ambiente espec√≠fico em que a aplica√ß√£o atual est√° sendo executada. Aqui est√£o algumas √°reas para investigar e poss√≠veis solu√ß√µes:

1.  **Ambiente Virtual:**
    *   **Verifica√ß√£o:** Certifique-se de que a aplica√ß√£o esteja rodando dentro do ambiente virtual correto. Se voc√™ estiver utilizando um ambiente virtual (como `venv` ou `conda`), ative-o antes de executar a aplica√ß√£o.

    ```bash
    # Para venv
    source venv/bin/activate  # ou venv\Scripts\activate no Windows

    # Para conda
    conda activate meu_ambiente
    ```
    *   **Confirma√ß√£o:** Verifique se o ambiente virtual ativado √© o mesmo onde `google-generativeai` est√° instalado. Utilize `pip show google-generativeai` para confirmar a instala√ß√£o e o caminho do pacote.

2.  **Caminhos de Importa√ß√£o:**
    *   **Investiga√ß√£o:** Verifique se o caminho para o diret√≥rio do pacote est√° corretamente configurado no `sys.path`. Isso pode variar entre diferentes ambientes e instala√ß√µes.
    *   **Solu√ß√£o:** Adicione o caminho correto ao `sys.path` no in√≠cio do script. Para descobrir o caminho correto, execute o seguinte no ambiente onde a lib funciona:

    ```python
    import google.generativeai
    print(google.generativeai.__file__)
    ```
    Copie o diret√≥rio onde o arquivo `__init__.py` est√° localizado e adicione ao `sys.path` no seu script.

    ```python
    import sys
    sys.path.append('/caminho/para/o/site-packages')
    import google.generativeai as genai
    ```

3.  **Conflitos de Depend√™ncias:**
    *   **Investiga√ß√£o:** Pode haver conflitos entre as depend√™ncias de diferentes pacotes no seu ambiente.
    *   **Solu√ß√£o:** Tente criar um ambiente virtual limpo e instalar apenas as depend√™ncias necess√°rias para a sua aplica√ß√£o atual.

    ```bash
    python -m venv novo_ambiente
    source novo_ambiente/bin/activate  # ou novo_ambiente\Scripts\activate no Windows
    pip install google-generativeai
    # Instale outros pacotes necess√°rios para esta aplica√ß√£o
    ```

4.  **Cache do Editor/IDE:**
    *   **Investiga√ß√£o:** O editor ou IDE pode estar usando um cache que n√£o est√° atualizado com as altera√ß√µes no ambiente.
    *   **Solu√ß√£o:** Reinicie o editor ou IDE, ou force a atualiza√ß√£o do cache. No VS Code, isso pode ser feito atrav√©s do comando "Developer: Reload Window".

5.  **Vari√°veis de Ambiente:**
    *   **Investiga√ß√£o:** Verifique se todas as vari√°veis de ambiente necess√°rias (como `API_KEY_GEMINI`) est√£o corretamente configuradas e acess√≠veis no ambiente em que a aplica√ß√£o est√° rodando.
    *   **Solu√ß√£o:** Certifique-se de que as vari√°veis de ambiente est√£o definidas corretamente e recarregue o ambiente se necess√°rio.

6.  **Permiss√µes:**
    *   **Investiga√ß√£o:** Verifique se o usu√°rio que est√° executando a aplica√ß√£o tem permiss√µes para acessar os arquivos e diret√≥rios necess√°rios.
    *   **Solu√ß√£o:** Ajuste as permiss√µes conforme necess√°rio para garantir que o usu√°rio tenha acesso aos arquivos e diret√≥rios requeridos.

7.  **Vers√µes do Python:**
    *   **Investiga√ß√£o:** Confirme que a vers√£o do Python em uso no ambiente √© compat√≠vel com a `google-generativeai`.
    *   **Solu√ß√£o:** Verifique a vers√£o do Python utilizada (`python --version`) e, se necess√°rio, instale uma vers√£o compat√≠vel com a biblioteca.

Com esses passos, voc√™ deve conseguir identificar e resolver o problema de ambiente que est√° impedindo a sua aplica√ß√£o de reconhecer o pacote `google-generativeai`.
User: ocorreu esse erro na imagem.
Assistant: The image shows that the virtual environment is not activated.

Here's how to fix this:

1.  **Verify the virtual environment's `venv` directory:** The error message  `bash: venv/bin/activate: No such file or directory` indicates that the `activate` script is not found in `venv/bin/`. This usually happens if the virtual environment was not created properly or the `venv` directory was renamed or deleted.  You can see from the file explorer in the image that there is a `venv` directory in the project root.
2.  **Check the correct activation command:**  The command `source venv/bin/activate` is usually used on Linux/macOS systems. For Windows, the activation script is located at `venv\Scripts\activate`. Therefore, try running  `venv\Scripts\activate` in the terminal instead.
3.  **Recreate the virtual environment (if necessary):** If you've confirmed that the virtual environment is broken, delete the existing `venv` directory and create a new virtual environment using  `python -m venv venv`. After creating the new environment, try activating it again.

After resolving the issue with the virtual environment, install all dependencies: `pip install -r requirements.txt` if you have a requirements file or install them manually. Then, rerun the application.
User: estou usando windows... mas no terminal √© o git bash
Assistant: Okay, I see the problem. Even though you're on Windows, because you're using Git Bash as your terminal, you need to use the Linux-style activation command.  The fact that it's not working suggests that *either* the `venv` directory doesn't contain the `bin/activate` script (as the error message says) *or* there's some kind of permission or execution issue with the script itself in the Git Bash environment.

Here's a breakdown of the likely problem and solutions:

**The Problem:**

Git Bash (MinGW64 specifically) is a Unix-like environment on Windows. While it uses many Linux commands, it still operates within the Windows file system.  Therefore, even when using `source venv/bin/activate`, Git Bash is looking for a proper executable file inside that bin directory, but in your case, that doesn't exist in the proper format.

**Solutions:**

1.  **Use the `.` command as alias to `source`:**  In some Git Bash setups, `.` is explicitly aliased to `source`. Try using it:

    ```bash
    . venv/bin/activate
    ```

    This is a slight variation of the `source` command, but it might work in your particular Git Bash environment.
2.  **Use the correct script in Git Bash (if it exists):** Some virtual environment setups for Git Bash will create a script that is more appropriate for the shell.  Look within your `venv` directory; in some cases, you will see a file `venv/Scripts/activate`. If this file exists, it should be ran directly in Git Bash. However, if that's the case, the path would more likely be `source venv/Scripts/activate`. In this case, that is *not* what you are looking for. You can use the next solution for the `activate` script.
3.  **Execute the Activate.ps1 Script:** Git Bash can run PowerShell scripts directly. Run the PowerShell script activate.ps1 inside powershell. To do this, you can run `powershell -ExecutionPolicy Bypass -File ./venv/Scripts/Activate.ps1` in your terminal. This method is likely the most reliable method to run virtual environments on Git Bash in Windows.

**Troubleshooting steps to confirm the root cause:**

1.  **List the contents of `venv/bin`:**  In your Git Bash terminal, run the command `ls -l venv/bin`.  Examine the output.  You should see a file named `activate` (without a file extension).  If it's not there, that's your primary problem - the venv wasn't created properly.
2.  **Check permissions:** If the `activate` script *is* there, check its permissions. It should have execute permissions. The output of `ls -l venv/bin` will show you the permissions. If the file doesn't have an `x` in the permissions for the user, group, or others, the script won't be executable.  (Note:  Git Bash's permissions model is somewhat emulated and not always perfectly accurate, but it's worth checking.)

If the directory `venv/bin` does *not* contain the activate script, then you should delete the `venv` folder and re-create the virtual environment.

In summary, even if the most likely solution fails, use the troubleshooting steps to confirm that `venv/bin/activate` is an executable.
User: Agora deu esse erro.
Assistant: Based on the screenshot provided, you're still facing an issue related to the virtual environment, but now it's a different error. The error is:

```
/c/Users/jfreis/Documents/agents_ia/comandAI/venv/bin/python: can't open file '/c/Users/jfreis/Documents/agents_ia/comandAI/main.py': [Errno 2] No such file or directory
```

Here's what this means and how to troubleshoot it:

**The Problem:**

The error "No such file or directory" clearly indicates that the Python interpreter within your activated virtual environment cannot find the `main.py` file you're trying to execute. This could be due to several reasons:

1.  **Incorrect working directory:** You might be running the command from a directory where `main.py` doesn't exist.

2.  **Incorrect file path:** The file path in your command (`/c/Users/jfreis/Documents/agents_ia/comandAI/main.py`) might be wrong.

3.  **File doesn't exist:** The `main.py` file may simply not exist at the specified location.

**Troubleshooting Steps:**

1.  **Verify file existence:**

    *   Double-check that `main.py` exists at the path `/c/Users/jfreis/Documents/agents_ia/comandAI/main.py`.  Use your file explorer to confirm this.
    *   Make sure you didn't accidentally rename or move the file.

2.  **Check working directory:**

    *   Run the command `pwd` (print working directory) in your Git Bash terminal.  This will tell you the current directory from which you're running the command.
    *   If the current directory isn't `/c/Users/jfreis/Documents/agents_ia/comandAI`, you need to change it. You can use the `cd` command to navigate to the correct directory:
    ```bash
    cd /c/Users/jfreis/Documents/agents_ia/comandAI
    ```

3.  **Check the command:**
If your pwd is on directory, check the command used to execute the file. If you executed the command like `python /c/Users/jfreis/Documents/agents_ia/comandAI/main.py`, you can simply run `python main.py`.

4.  **Double-check case sensitivity:**  Git Bash, being a Unix-like environment, is case-sensitive. Make sure the file name "main.py" matches the case exactly.

**Example:**

Let's say `pwd` shows that you're currently in `/c/Users/jfreis/`

*   You would run:

    ```bash
    cd Documents/agents_ia/comandAI
    python main.py
    ```

After you've checked the file path and working directory, you should be able to run your Python script without the "No such file or directory" error.

User: reponda em portugues.
Assistant: Com base na captura de tela fornecida, voc√™ ainda est√° enfrentando um problema relacionado ao ambiente virtual, mas agora √© um erro diferente. O erro √©:

```
/c/Users/jfreis/Documents/agents_ia/comandAI/venv/bin/python: can't open file '/c/Users/jfreis/Documents/agents_ia/comandAI/main.py': [Errno 2] No such file or directory
```

Veja o que isso significa e como solucionar o problema:

**O Problema:**

O erro "No such file or directory" (Arquivo ou diret√≥rio inexistente) indica claramente que o interpretador Python dentro do seu ambiente virtual ativado n√£o consegue encontrar o arquivo `main.py` que voc√™ est√° tentando executar. Isso pode ocorrer por v√°rios motivos:

1.  **Diret√≥rio de trabalho incorreto:** Voc√™ pode estar executando o comando a partir de um diret√≥rio onde `main.py` n√£o existe.

2.  **Caminho do arquivo incorreto:** O caminho do arquivo no seu comando (`/c/Users/jfreis/Documents/agents_ia/comandAI/main.py`) pode estar errado.

3.  **Arquivo n√£o existe:** O arquivo `main.py` pode simplesmente n√£o existir no local especificado.

**Etapas de Solu√ß√£o de Problemas:**

1.  **Verifique a exist√™ncia do arquivo:**

    *   Verifique se `main.py` existe no caminho `/c/Users/jfreis/Documents/agents_ia/comandAI/main.py`. Use o explorador de arquivos para confirmar isso.
    *   Certifique-se de n√£o ter renomeado ou movido o arquivo acidentalmente.

2.  **Verifique o diret√≥rio de trabalho:**

    *   Execute o comando `pwd` (print working directory - imprimir diret√≥rio de trabalho) no seu terminal Git Bash. Isso dir√° qual √© o diret√≥rio atual de onde voc√™ est√° executando o comando.
    *   Se o diret√≥rio atual n√£o for `/c/Users/jfreis/Documents/agents_ia/comandAI`, voc√™ precisar√° alter√°-lo. Voc√™ pode usar o comando `cd` para navegar at√© o diret√≥rio correto:
    ```bash
    cd /c/Users/jfreis/Documents/agents_ia/comandAI
    ```

3.  **Verifique o comando:**
Se o seu pwd estiver no diret√≥rio correto, verifique o comando usado para executar o arquivo. Se voc√™ executou o comando como `python /c/Users/jfreis/Documents/agents_ia/comandAI/main.py`, voc√™ pode simplesmente executar `python main.py`.

4.  **Verifique a diferencia√ß√£o de mai√∫sculas e min√∫sculas:** O Git Bash, sendo um ambiente semelhante ao Unix, diferencia mai√∫sculas e min√∫sculas. Certifique-se de que o nome do arquivo "main.py" corresponda ao caso exatamente.

**Exemplo:**

Digamos que `pwd` mostre que voc√™ est√° atualmente em `/c/Users/jfreis/`

*   Voc√™ executaria:

    ```bash
    cd Documents/agents_ia/comandAI
    python main.py
    ```

Depois de verificar o caminho do arquivo e o diret√≥rio de trabalho, voc√™ deve conseguir executar seu script Python sem o erro "No such file or directory".

User: mas eu n√£o excutei o main... eu tentei ativar o ambiente virtual.
2025-04-14 15:13:30,557 [INFO] Resposta da IA (imagem): Compreendo. Se voc√™ n√£o tentou executar o `main.py`, mas sim ativar o ambiente virtual e obteve o erro, o problema √© *definitivamente* com a ativa√ß√£o do ambiente e n√£o com a execu√ß√£o do script em si. O que o terminal est√° dizendo √© que ao tentar ativar o ambiente virtual, ele est√° tentando executar um script (aparentemente o `main.py`) e n√£o consegue encontrar esse script no local especificado.

Isso sugere que:

1. **Algo est√° sobrescrevendo a maneira como o ambiente virtual √© ativado.** Normalmente, a ativa√ß√£o n√£o tenta executar um script externo.
2. **O interpretador do python no venv n√£o √© o correto, e est√° tentando buscar o caminho principal.**
3. **O comando de ativa√ß√£o est√° configurado incorretamente ou corrompido.**

Aqui est√£o as etapas a seguir, levando em conta que voc√™ est√° usando o Git Bash no Windows e que tentou ativar o ambiente virtual, n√£o executar um script:

1. **Desative qualquer ambiente virtual ativo:** Se algum ambiente estiver ativado, desative-o primeiro com o comando `deactivate`. Certifique-se que o prefixo `(venv)` n√£o esteja mais vis√≠vel no prompt do seu terminal.

2. **Verifique se a ativa√ß√£o padr√£o est√° funcionando:**

   * Utilize o comando de ativa√ß√£o padr√£o `source venv/bin/activate` ou `. venv/bin/activate` (se `.` for um alias para `source`).
   * Se este comando falhar com os mesmos erros, o problema n√£o est√° relacionado a nenhuma configura√ß√£o de execu√ß√£o de script adicional.

3. **Verifique o arquivo `activate`:**
   1. **Existe?** Verifique que o arquivo `venv/bin/activate` existe.
   2. **Conte√∫do:** Se o arquivo existir, execute `cat venv/bin/activate` para ler o conte√∫do do arquivo no terminal. Procure por algo que tente executar o arquivo `main.py` dentro desse script.

4.  **Reconstrua o ambiente virtual (op√ß√£o mais segura):**

    *   **Desative o ambiente atual** (se ainda estiver ativado): `deactivate`
    *   **Exclua o diret√≥rio `venv`:** Use o comando `rm -rf venv` (isso remover√° o diret√≥rio `venv` e todo o seu conte√∫do. Use com extrema cautela, certificando-se de que voc√™ est√° no diret√≥rio correto!)
    *   **Crie um novo ambiente virtual:** Execute `python -m venv venv`
    *   **Ative o novo ambiente virtual:** Tente `source venv/bin/activate` ou `. venv/bin/activate` ou `powershell -ExecutionPolicy Bypass -File ./venv/Scripts/Activate.ps1`
    *   **Reinstale as depend√™ncias:** `pip install -r requirements.txt`

5.  **Verifique as vari√°veis de ambiente:**
    1.  Examine as vari√°veis de ambiente que podem estar relacionadas a caminhos ou Python.
    2.  Para exibir as vari√°veis de ambiente, use o comando `env` no Git Bash.

Com essas etapas, voc√™ deve conseguir identificar e resolver o problema ao ativar o ambiente virtual, garantindo que a ativa√ß√£o n√£o tente executar um script que n√£o existe.

O problema √© com as configura√ß√µes de ativa√ß√£o de ambiente virtual, e as etapas acima visam limpar e recriar o ambiente virtual, resolvendo o erro.
2025-04-14 15:13:30,649 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 15:13:30,650 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 15:13:30,652 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 15:13:30,654 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 15:13:30,656 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 15:13:30,658 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 15:13:30,659 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 15:13:30,660 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 15:13:30,661 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 15:13:30,663 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 15:13:30,664 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 15:13:30,665 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 15:13:30,666 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 15:13:30,667 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 15:13:30,670 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 15:13:30,671 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 15:13:30,673 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 15:13:30,674 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 15:13:30,675 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 15:13:30,677 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 15:13:30,678 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 15:13:30,679 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 15:13:30,680 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 15:13:30,682 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 15:13:30,683 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 15:13:30,684 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 15:13:30,686 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 15:13:30,687 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 15:13:30,688 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 15:13:30,689 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 15:13:30,691 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 15:13:30,692 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 15:13:30,694 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 15:13:30,695 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 15:13:30,696 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 15:13:30,697 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 15:13:30,698 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 15:13:30,699 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 15:13:30,700 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 15:13:30,701 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 15:13:30,702 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 15:13:30,703 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 15:13:30,705 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 15:13:30,706 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 15:13:30,707 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 15:13:30,708 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 15:13:30,709 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 15:13:30,710 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 15:13:30,711 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 15:13:30,712 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 15:13:30,713 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 15:13:30,714 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 15:13:30,716 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:19:09,399 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:19:09,400 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:19:09,402 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:19:09,402 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:19:09,404 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:19:09,404 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:19:09,406 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:19:09,406 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:19:09,408 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:19:09,408 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:19:09,411 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:19:09,411 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:19:09,413 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:19:09,414 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:19:09,416 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:19:09,417 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:19:09,420 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:19:09,422 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:19:09,423 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:19:09,424 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:19:09,425 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:19:09,427 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:19:09,429 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:19:09,430 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:19:09,431 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:19:09,434 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:19:09,435 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:19:09,439 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:19:09,440 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:19:09,441 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:19:09,442 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:19:09,444 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:19:09,446 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:19:09,446 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:19:09,448 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:19:09,449 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:19:09,450 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:19:09,452 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:19:09,455 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:19:09,456 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:19:09,458 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:19:09,459 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:19:09,460 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:19:09,462 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:19:09,464 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:19:09,465 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:19:09,467 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:19:09,469 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:19:09,470 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:19:09,472 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:19:09,473 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:19:09,474 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:19:09,475 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:19:09,477 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:19:09,478 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:19:09,479 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:19:09,480 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:19:09,483 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:19:09,485 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:19:09,486 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:19:09,488 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:19:09,489 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:19:09,490 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:19:09,492 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:19:09,493 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:19:09,494 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:19:09,496 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:19:09,497 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:19:09,498 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:19:09,499 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:19:09,501 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:19:09,503 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:19:09,504 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:19:09,506 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:19:09,506 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:19:09,509 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:19:09,510 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:19:09,512 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:19:09,513 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:19:09,515 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:19:09,516 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:19:09,518 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:19:09,520 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:19:09,521 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:19:09,522 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:19:09,523 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:19:09,526 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:19:09,526 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:19:09,528 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:19:09,529 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:19:09,530 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:19:09,531 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:19:09,532 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:19:09,534 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:19:09,536 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:19:09,537 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:19:09,539 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:19:09,540 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:19:09,542 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:19:09,548 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:19:09,557 [INFO] Modelo Gemini 'gemini-2.0-flash-exp' inicializado com sucesso.
2025-04-14 16:19:16,443 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:19:16,445 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:19:16,448 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:19:16,450 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:19:16,452 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:19:16,455 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:19:16,457 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:19:16,458 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:19:16,461 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:19:16,464 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:19:16,466 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:19:16,469 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:19:16,471 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:19:16,473 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:19:16,476 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:19:16,478 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:19:16,482 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:19:16,483 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:19:16,488 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:19:16,490 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:19:16,492 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:19:16,495 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:19:16,498 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:19:16,499 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:19:16,502 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:19:16,505 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:19:16,507 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:19:16,510 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:19:16,514 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:19:16,516 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:19:16,520 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:19:16,523 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:19:16,525 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:19:16,526 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:19:16,529 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:19:16,531 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:19:16,534 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:19:16,538 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:19:16,541 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:19:16,545 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:19:16,547 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:19:16,550 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:19:16,554 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:19:16,558 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:19:16,561 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:19:16,563 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:19:16,565 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:19:16,568 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:19:16,571 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:19:16,574 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:20:38,489 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:20:38,491 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:20:38,493 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:20:38,495 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:20:38,497 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:20:38,499 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:20:38,501 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:20:38,503 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:20:38,505 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:20:38,506 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:20:38,508 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:20:38,510 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:20:38,511 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:20:38,514 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:20:38,516 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:20:38,519 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:20:38,521 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:20:38,523 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:20:38,525 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:20:38,528 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:20:38,531 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:20:38,533 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:20:38,535 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:20:38,537 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:20:38,538 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:20:38,539 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:20:38,541 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:20:38,543 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:20:38,545 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:20:38,547 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:20:38,549 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:20:38,550 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:20:38,551 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:20:38,552 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:20:38,554 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:20:38,555 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:20:38,556 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:20:38,558 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:20:38,560 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:20:38,561 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:20:38,563 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:20:38,565 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:20:38,566 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:20:38,567 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:20:38,568 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:20:38,570 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:20:38,571 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:20:38,573 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:20:38,574 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:20:38,577 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:20:38,674 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:20:38,677 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:20:38,679 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:20:38,681 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:20:38,683 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:20:38,685 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:20:38,687 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:20:38,689 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:20:38,691 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:20:38,694 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:20:38,696 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:20:38,700 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:20:38,702 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:20:38,704 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:20:38,706 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:20:38,708 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:20:38,709 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:20:38,712 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:20:38,713 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:20:38,716 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:20:38,717 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:20:38,719 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:20:38,721 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:20:38,722 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:20:38,724 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:20:38,726 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:20:38,728 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:20:38,733 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:20:38,735 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:20:38,738 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:20:38,740 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:20:38,741 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:20:38,744 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:20:38,746 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:20:38,749 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:20:38,751 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:20:38,753 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:20:38,755 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:20:38,757 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:20:38,760 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:20:38,762 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:20:38,763 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:20:38,765 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:20:38,767 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:20:38,768 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:20:38,769 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:20:38,771 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:20:38,772 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:20:38,773 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:20:38,775 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:20:38,778 [INFO] Enviando para IA - Prompt (sem imagem): Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"N√£o foram encontrados arquivos de √°udio no diret√≥rio {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcri√ß√£o salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diret√≥rio atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Defini√ß√£o dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Cria√ß√£o dos diret√≥rios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configura√ß√£o do layout do Streamlit
st.set_page_config(
    page_title="An√°lise de A√ß√µes",
    layout="wide",
    page_icon="üìà"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscila√ß√µes

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Per√≠odo', 'Oscila√ß√£o'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("üìä Dashboard de An√°lise de A√ß√µes")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma a√ß√£o", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"üìå Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("üîç Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descri√ß√£o', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa a√ß√£o.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("üí∞ Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("üìâ Oscila√ß√µes")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("üìà Tabela Geral de A√ß√µes")
        st.dataframe(self.acoes)

# Execu√ß√£o
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para an√°lise fundamentalista de a√ß√µes da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se est√° correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configura√ß√£o de v√≠deos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configura√ß√£o de √°udios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcri√ß√µes e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diret√≥rio
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diret√≥rio.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do √∫ltimo arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conte√∫do do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimens√µes do embedding:", embedding.shape)

# Caminho do diret√≥rio de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o √∫ltimo embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configura√ß√£o de transcri√ß√µes e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diret√≥rio onde est√° procurando as transcri√ß√µes
        print(f"Diret√≥rio de entrada das transcri√ß√µes: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcri√ß√£o no diret√≥rio de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcri√ß√£o encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo n√£o encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcri√ß√£o do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} est√° vazio.")
                    return

            # Gerar o embedding da transcri√ß√£o
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcri√ß√£o e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O v√≠deo {video_path} n√£o cont√©m √°udio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de v√≠deo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de v√≠deo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de v√≠deo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Convers√£o de v√≠deo para √°udio conclu√≠da!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configura√ß√µes da API
handler = genai('gemini-1.5-flash')

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Fun√ß√£o para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conte√∫do com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares √∫nicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcri√ß√£o obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' n√£o √© uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuni√µes: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de reuni√µes: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|t√≥picos da|pontos (relevantes|principais)|an√°lise)\b.*\b(reuni√£o|√∫ltima (reuni√£o|conversa|sess√£o))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como an√°lise de reuni√£o.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padr√£o.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que s√£o muito similares ao comando atual para evitar redund√¢ncia.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padr√£o: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # M√©todos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de an√°lise de reuni√£o.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conte√∫do usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inv√°lida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar vari√°veis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conte√∫do com base no prompt, contexto e reuni√£o"""
        try:
            # Supondo que a API espera um dicion√°rio com os par√¢metros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisi√ß√£o para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar vari√°veis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Cria√ß√£o da conclus√£o do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que voc√™ acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["fa√ßa um resumo da √∫ltima reuni√£o.", "t√≥picos da √∫ltima reuni√£o", "resuma a √∫ltima reuni√£o", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Voc√™ √© respons√°vel por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - N√£o acrescentar t√≠tulo nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informa√ß√µes acima e forne√ßa uma solu√ß√£o detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Voc√™ √© respons√°vel analisar com detalhes a reuni√£o de [str_meeting], e fornecer uma longa est√≥ria sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precis√£o da resposta.
            - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e fa√ßa uma pesquisa online para uma resposta mais precisa das quest√µes de [str_text]
        - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [context]: Regras B√°sicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("√Åudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum √°udio detectado.")
                return None
            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configura√ß√µes da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicion√°rio
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta n√£o est√° no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API: {e}")
        return None

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: Fa√ßa uma limpeza na pasta class_voice_assistent
2025-04-14 16:20:47,658 [INFO] Resposta da IA (texto): Com base nos arquivos fornecidos, aqui est√° um resumo da estrutura e funcionalidade do c√≥digo, juntamente com as informa√ß√µes sobre a limpeza da pasta `class_voice_assistent`:

**Vis√£o geral do projeto:**

Este projeto parece ser um assistente de voz que usa o Google Gemini para processamento de linguagem natural. Ele √© estruturado em v√°rios m√≥dulos e classes para lidar com diferentes aspectos, como captura de √°udio, convers√£o de texto em fala, interpreta√ß√£o de comandos e intera√ß√£o com APIs externas.

**Principais componentes:**

*   **Captura de comandos:**
    *   `voice_assistent\class_voice_assistent\voice_command_hendler.py`: Lida com a captura de comandos de voz usando a biblioteca `speech_recognition`.
    *   `voice_assistent\class_voice_assistent\text_command_hendler.py`: Lida com a captura de comandos de texto do usu√°rio.

*   **Processamento de linguagem natural:**
    *   `voice_assistent\class_voice_assistent\command_interpreter.py`: Interpreta os comandos do usu√°rio e determina a a√ß√£o apropriada a ser executada.
    *   `voice_assistent\class_voice_assistent\question_answers_service.py`: Converte texto em embeddings para encontrar respostas semelhantes.
    *   `voice_assistent\class_voice_assistent\context_manager.py`: Gerencia o contexto da conversa, armazenando as intera√ß√µes recentes.

*   **Comunica√ß√£o com o Google Gemini:**
    *   `voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py`: Lida com a comunica√ß√£o com a API do Google Gemini.
    *   `voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py`: Define o modelo generativo e a configura√ß√£o da API Google Gemini.

*   **Gera√ß√£o de prompt:**
    *   `voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py`: Gera prompts padr√£o para o modelo de linguagem.
    *   `voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py`: Gera prompts para an√°lise de reuni√µes.
    *   `voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py`: Gera prompts para pesquisas online.
    *   `voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py`: Define a classe abstrata para geradores de prompt.

*   **Texto para fala:**
    *   `voice_assistent\class_voice_assistent\text_to_speech.py`: Converte texto em fala usando a biblioteca `pyttsx3`.

*   **Processamento de texto:**
    *   `voice_assistent\class_voice_assistent\text_processor.py`: Fornece fun√ß√µes para extrair valores de JSON e texto de HTML.

*   **Interface principal:**
    *   `voice_assistent\class_voice_assistent\main.py`: Inicializa e coordena os diferentes componentes do assistente de voz.

**Limpeza da pasta `class_voice_assistent`:**

O comando para limpar uma pasta n√£o faz parte diretamente do c√≥digo fornecido. Para implementar essa funcionalidade, voc√™ precisaria adicionar um c√≥digo que use a biblioteca `os` ou `shutil` para remover arquivos e subdiret√≥rios da pasta `class_voice_assistent`.

**Exemplo de como implementar a limpeza da pasta:**

```python
import os
import shutil

def clean_directory(path):
    """Remove todos os arquivos e subdiret√≥rios em um diret√≥rio."""
    for filename in os.listdir(path):
        file_path = os.path.join(path, filename)
        try:
            if os.path.isfile(file_path) or os.path.islink(file_path):
                os.unlink(file_path)
            elif os.path.isdir(file_path):
                shutil.rmtree(file_path)
        except Exception as e:
            print(f'Falha ao excluir {file_path}. Motivo: {e}')

# Caminho para a pasta class_voice_assistent
path_to_clean = 'voice_assistent/class_voice_assistent'

# Chame a fun√ß√£o para limpar a pasta
clean_directory(path_to_clean)
```

**Considera√ß√µes:**

1.  **Importe os m√≥dulos necess√°rios**: `os` e `shutil`.
2.  **Defina a fun√ß√£o `clean_directory`**: Esta fun√ß√£o itera sobre todos os itens no diret√≥rio fornecido e tenta remover cada um.
3.  **Tratamento de erros**: A fun√ß√£o inclui um bloco `try...except` para lidar com poss√≠veis erros durante a remo√ß√£o de arquivos ou diret√≥rios.
4.  **Chame a fun√ß√£o `clean_directory`**: Passe o caminho do diret√≥rio que voc√™ deseja limpar.

**Onde adicionar este c√≥digo:**

Voc√™ pode integrar esta fun√ß√£o dentro do `command_interpreter.py` ou `main.py`. Por exemplo, dentro do `interpret_command`, voc√™ pode adicionar uma condi√ß√£o para detectar o comando "limpar pasta" e, em seguida, chamar a fun√ß√£o `clean_directory` com o caminho apropriado.

**Aviso**: Esta opera√ß√£o √© irrevers√≠vel e excluir√° permanentemente todos os arquivos e pastas dentro do diret√≥rio especificado. Certifique-se de ter um backup se necess√°rio.

**Seguran√ßa:**

Certifique-se de que o caminho da pasta a ser limpa seja validado e seguro para evitar exclus√µes acidentais ou maliciosas.

2025-04-14 16:20:47,758 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:20:47,760 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:20:47,762 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:20:47,763 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:20:47,765 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:20:47,766 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:20:47,767 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:20:47,769 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:20:47,770 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:20:47,771 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:20:47,773 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:20:47,774 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:20:47,775 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:20:47,777 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:20:47,778 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:20:47,780 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:20:47,781 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:20:47,782 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:20:47,783 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:20:47,784 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:20:47,786 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:20:47,787 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:20:47,789 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:20:47,790 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:20:47,791 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:20:47,792 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:20:47,793 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:20:47,795 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:20:47,797 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:20:47,798 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:20:47,799 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:20:47,801 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:20:47,802 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:20:47,803 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:20:47,804 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:20:47,805 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:20:47,806 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:20:47,807 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:20:47,808 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:20:47,810 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:20:47,812 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:20:47,814 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:20:47,816 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:20:47,817 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:20:47,818 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:20:47,820 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:20:47,821 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:20:47,822 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:20:47,823 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:20:47,824 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:21:59,128 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:21:59,130 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:21:59,132 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:21:59,133 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:21:59,136 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:21:59,137 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:21:59,139 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:21:59,141 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:21:59,143 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:21:59,145 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:21:59,146 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:21:59,147 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:21:59,149 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:21:59,152 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:21:59,153 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:21:59,159 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:21:59,171 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:21:59,179 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:21:59,181 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:21:59,185 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:21:59,187 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:21:59,189 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:21:59,191 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:21:59,192 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:21:59,194 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:21:59,195 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:21:59,196 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:21:59,197 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:21:59,199 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:21:59,200 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:21:59,202 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:21:59,204 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:21:59,206 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:21:59,208 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:21:59,209 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:21:59,211 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:21:59,213 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:21:59,214 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:21:59,216 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:21:59,218 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:21:59,219 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:21:59,222 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:21:59,224 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:21:59,225 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:21:59,227 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:21:59,228 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:21:59,230 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:21:59,231 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:21:59,233 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:21:59,236 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:21:59,333 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:21:59,335 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:21:59,337 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:21:59,339 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:21:59,340 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:21:59,342 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:21:59,343 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:21:59,345 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:21:59,347 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:21:59,349 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:21:59,351 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:21:59,354 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:21:59,356 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:21:59,357 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:21:59,359 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:21:59,361 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:21:59,363 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:21:59,364 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:21:59,366 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:21:59,370 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:21:59,374 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:21:59,376 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:21:59,378 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:21:59,380 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:21:59,382 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:21:59,386 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:21:59,388 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:21:59,391 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:21:59,393 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:21:59,394 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:21:59,396 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:21:59,398 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:21:59,401 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:21:59,403 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:21:59,405 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:21:59,406 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:21:59,407 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:21:59,408 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:21:59,410 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:21:59,411 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:21:59,412 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:21:59,413 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:21:59,415 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:21:59,416 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:21:59,418 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:21:59,419 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:21:59,421 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:21:59,422 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:21:59,424 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:21:59,425 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:21:59,427 [INFO] Enviando para IA - Prompt (sem imagem): Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"N√£o foram encontrados arquivos de √°udio no diret√≥rio {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcri√ß√£o salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diret√≥rio atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Defini√ß√£o dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Cria√ß√£o dos diret√≥rios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configura√ß√£o do layout do Streamlit
st.set_page_config(
    page_title="An√°lise de A√ß√µes",
    layout="wide",
    page_icon="üìà"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscila√ß√µes

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Per√≠odo', 'Oscila√ß√£o'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("üìä Dashboard de An√°lise de A√ß√µes")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma a√ß√£o", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"üìå Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("üîç Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descri√ß√£o', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa a√ß√£o.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("üí∞ Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("üìâ Oscila√ß√µes")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("üìà Tabela Geral de A√ß√µes")
        st.dataframe(self.acoes)

# Execu√ß√£o
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para an√°lise fundamentalista de a√ß√µes da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se est√° correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configura√ß√£o de v√≠deos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configura√ß√£o de √°udios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcri√ß√µes e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diret√≥rio
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diret√≥rio.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do √∫ltimo arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conte√∫do do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimens√µes do embedding:", embedding.shape)

# Caminho do diret√≥rio de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o √∫ltimo embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configura√ß√£o de transcri√ß√µes e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diret√≥rio onde est√° procurando as transcri√ß√µes
        print(f"Diret√≥rio de entrada das transcri√ß√µes: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcri√ß√£o no diret√≥rio de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcri√ß√£o encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo n√£o encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcri√ß√£o do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} est√° vazio.")
                    return

            # Gerar o embedding da transcri√ß√£o
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcri√ß√£o e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O v√≠deo {video_path} n√£o cont√©m √°udio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de v√≠deo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de v√≠deo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de v√≠deo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Convers√£o de v√≠deo para √°udio conclu√≠da!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configura√ß√µes da API
handler = genai('gemini-1.5-flash')

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Fun√ß√£o para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conte√∫do com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares √∫nicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcri√ß√£o obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' n√£o √© uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuni√µes: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de reuni√µes: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|t√≥picos da|pontos (relevantes|principais)|an√°lise)\b.*\b(reuni√£o|√∫ltima (reuni√£o|conversa|sess√£o))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como an√°lise de reuni√£o.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padr√£o.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que s√£o muito similares ao comando atual para evitar redund√¢ncia.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padr√£o: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # M√©todos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de an√°lise de reuni√£o.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conte√∫do usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inv√°lida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar vari√°veis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conte√∫do com base no prompt, contexto e reuni√£o"""
        try:
            # Supondo que a API espera um dicion√°rio com os par√¢metros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisi√ß√£o para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar vari√°veis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Cria√ß√£o da conclus√£o do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que voc√™ acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["fa√ßa um resumo da √∫ltima reuni√£o.", "t√≥picos da √∫ltima reuni√£o", "resuma a √∫ltima reuni√£o", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Voc√™ √© respons√°vel por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - N√£o acrescentar t√≠tulo nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informa√ß√µes acima e forne√ßa uma solu√ß√£o detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Voc√™ √© respons√°vel analisar com detalhes a reuni√£o de [str_meeting], e fornecer uma longa est√≥ria sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precis√£o da resposta.
            - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e fa√ßa uma pesquisa online para uma resposta mais precisa das quest√µes de [str_text]
        - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [context]: Regras B√°sicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("√Åudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum √°udio detectado.")
                return None
            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configura√ß√µes da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicion√°rio
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta n√£o est√° no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API: {e}")
        return None

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: Fa√ßa uma limpeza na pasta class_voice_assistent
Assistant: Com base nos arquivos fornecidos, aqui est√° um resumo da estrutura e funcionalidade do c√≥digo, juntamente com as informa√ß√µes sobre a limpeza da pasta `class_voice_assistent`:

**Vis√£o geral do projeto:**

Este projeto parece ser um assistente de voz que usa o Google Gemini para processamento de linguagem natural. Ele √© estruturado em v√°rios m√≥dulos e classes para lidar com diferentes aspectos, como captura de √°udio, convers√£o de texto em fala, interpreta√ß√£o de comandos e intera√ß√£o com APIs externas.

**Principais componentes:**

*   **Captura de comandos:**
    *   `voice_assistent\class_voice_assistent\voice_command_hendler.py`: Lida com a captura de comandos de voz usando a biblioteca `speech_recognition`.
    *   `voice_assistent\class_voice_assistent\text_command_hendler.py`: Lida com a captura de comandos de texto do usu√°rio.

*   **Processamento de linguagem natural:**
    *   `voice_assistent\class_voice_assistent\command_interpreter.py`: Interpreta os comandos do usu√°rio e determina a a√ß√£o apropriada a ser executada.
    *   `voice_assistent\class_voice_assistent\question_answers_service.py`: Converte texto em embeddings para encontrar respostas semelhantes.
    *   `voice_assistent\class_voice_assistent\context_manager.py`: Gerencia o contexto da conversa, armazenando as intera√ß√µes recentes.

*   **Comunica√ß√£o com o Google Gemini:**
    *   `voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py`: Lida com a comunica√ß√£o com a API do Google Gemini.
    *   `voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py`: Define o modelo generativo e a configura√ß√£o da API Google Gemini.

*   **Gera√ß√£o de prompt:**
    *   `voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py`: Gera prompts padr√£o para o modelo de linguagem.
    *   `voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py`: Gera prompts para an√°lise de reuni√µes.
    *   `voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py`: Gera prompts para pesquisas online.
    *   `voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py`: Define a classe abstrata para geradores de prompt.

*   **Texto para fala:**
    *   `voice_assistent\class_voice_assistent\text_to_speech.py`: Converte texto em fala usando a biblioteca `pyttsx3`.

*   **Processamento de texto:**
    *   `voice_assistent\class_voice_assistent\text_processor.py`: Fornece fun√ß√µes para extrair valores de JSON e texto de HTML.

*   **Interface principal:**
    *   `voice_assistent\class_voice_assistent\main.py`: Inicializa e coordena os diferentes componentes do assistente de voz.

**Limpeza da pasta `class_voice_assistent`:**

O comando para limpar uma pasta n√£o faz parte diretamente do c√≥digo fornecido. Para implementar essa funcionalidade, voc√™ precisaria adicionar um c√≥digo que use a biblioteca `os` ou `shutil` para remover arquivos e subdiret√≥rios da pasta `class_voice_assistent`.

**Exemplo de como implementar a limpeza da pasta:**

```python
import os
import shutil

def clean_directory(path):
    """Remove todos os arquivos e subdiret√≥rios em um diret√≥rio."""
    for filename in os.listdir(path):
        file_path = os.path.join(path, filename)
        try:
            if os.path.isfile(file_path) or os.path.islink(file_path):
                os.unlink(file_path)
            elif os.path.isdir(file_path):
                shutil.rmtree(file_path)
        except Exception as e:
            print(f'Falha ao excluir {file_path}. Motivo: {e}')

# Caminho para a pasta class_voice_assistent
path_to_clean = 'voice_assistent/class_voice_assistent'

# Chame a fun√ß√£o para limpar a pasta
clean_directory(path_to_clean)
```

**Considera√ß√µes:**

1.  **Importe os m√≥dulos necess√°rios**: `os` e `shutil`.
2.  **Defina a fun√ß√£o `clean_directory`**: Esta fun√ß√£o itera sobre todos os itens no diret√≥rio fornecido e tenta remover cada um.
3.  **Tratamento de erros**: A fun√ß√£o inclui um bloco `try...except` para lidar com poss√≠veis erros durante a remo√ß√£o de arquivos ou diret√≥rios.
4.  **Chame a fun√ß√£o `clean_directory`**: Passe o caminho do diret√≥rio que voc√™ deseja limpar.

**Onde adicionar este c√≥digo:**

Voc√™ pode integrar esta fun√ß√£o dentro do `command_interpreter.py` ou `main.py`. Por exemplo, dentro do `interpret_command`, voc√™ pode adicionar uma condi√ß√£o para detectar o comando "limpar pasta" e, em seguida, chamar a fun√ß√£o `clean_directory` com o caminho apropriado.

**Aviso**: Esta opera√ß√£o √© irrevers√≠vel e excluir√° permanentemente todos os arquivos e pastas dentro do diret√≥rio especificado. Certifique-se de ter um backup se necess√°rio.

**Seguran√ßa:**

Certifique-se de que o caminho da pasta a ser limpa seja validado e seguro para evitar exclus√µes acidentais ou maliciosas.

User: quero remover c√≥digos desnecess√°rios na pasta voice_assistent no projeto.
2025-04-14 16:22:08,049 [INFO] Resposta da IA (texto): Assistant: Para remover c√≥digos desnecess√°rios na pasta `voice_assistent` do projeto, aqui est√° uma abordagem detalhada:

**1. Vis√£o Geral:**

O objetivo √© identificar e remover arquivos e funcionalidades redundantes ou n√£o utilizados na pasta `voice_assistent`. Isso envolve uma an√°lise cuidadosa do c√≥digo, das depend√™ncias e do fluxo do programa para garantir que nenhuma funcionalidade essencial seja comprometida.

**2. Etapas Detalhadas:**

*   **Backup:**
    *   Antes de fazer qualquer altera√ß√£o, fa√ßa um backup completo da pasta `voice_assistent`. Isso permitir√° restaurar o estado anterior em caso de remo√ß√£o acidental de c√≥digo importante.

*   **An√°lise da Estrutura do C√≥digo:**
    *   **Identifique os principais m√≥dulos:** Analise todos os arquivos Python na pasta `voice_assistent` e suas subpastas para entender sua fun√ß√£o e como eles se interconectam.
    *   **M√≥dulos Essenciais:**
        *   `class_voice_assistent/main.py`: Coordena a opera√ß√£o do assistente de voz.
        *   `class_voice_assistent/command_interpreter.py`: Interpreta comandos do usu√°rio.
        *   `class_voice_assistent/gpt_communication/gemini_gpt.py`: Lida com a comunica√ß√£o com o Google Gemini.
        *   `class_voice_assistent/voice_command_handler.py` e `class_voice_assistent/text_command_handler.py`: Capturam comandos de voz e texto, respectivamente.
        *   `class_voice_assistent/text_to_speech.py`: Converte texto em fala.

*   **Identifica√ß√£o de C√≥digo N√£o Utilizado:**
    *   **Remova imports n√£o utilizados:** Abra cada arquivo e verifique se todos os m√≥dulos importados est√£o realmente sendo usados. Remova aqueles que n√£o est√£o.

    *   **Analise fun√ß√µes n√£o utilizadas:** Verifique se todas as fun√ß√µes definidas s√£o chamadas em algum lugar do c√≥digo. Fun√ß√µes que n√£o s√£o chamadas podem ser removidas com seguran√ßa.
    *   **Avalie c√≥digo comentado:** Avalie se os blocos de c√≥digo comentados ainda s√£o relevantes ou podem ser removidos.

*   **An√°lise das Depend√™ncias:**
    *   **Arquivo requirements.txt ou pyproject.toml:** Verifique se todas as bibliotecas listadas nos arquivos de depend√™ncia s√£o necess√°rias para o funcionamento do assistente de voz. Remova as depend√™ncias que n√£o est√£o sendo usadas.

*   **Simplifica√ß√£o da L√≥gica:**
    *   **Refatore c√≥digo duplicado:** Se houver trechos de c√≥digo repetidos, crie fun√ß√µes ou classes para reutiliz√°-los, reduzindo a redund√¢ncia.
    *   **Simplifique fun√ß√µes complexas:** Divida fun√ß√µes muito longas ou complexas em fun√ß√µes menores e mais f√°ceis de entender e manter.

*   **Remo√ß√£o de C√≥digo Desnecess√°rio:**
    *   **Exclua arquivos desnecess√°rios:** Remova arquivos que n√£o contribuem para a funcionalidade principal do assistente de voz.

*   **Teste:**
    *   Ap√≥s cada remo√ß√£o ou modifica√ß√£o significativa, execute os testes para garantir que o assistente de voz continue funcionando corretamente.
    *   Crie testes automatizados para os principais componentes para facilitar a detec√ß√£o de erros ap√≥s futuras altera√ß√µes.

**3. Implementa√ß√£o:**

*   **Edite os Arquivos:**
    *   Abra cada arquivo Python relevante e fa√ßa as modifica√ß√µes necess√°rias, como remover imports n√£o utilizados, fun√ß√µes desnecess√°rias e refatorar o c√≥digo.
*   **Remova Arquivos Desnecess√°rios:**
    *   Exclua arquivos que n√£o s√£o mais necess√°rios. Certifique-se de que nenhum outro componente depende desses arquivos antes de remov√™-los.

**4. Exemplo Pr√°tico:**

*   **Avalia√ß√£o de `template.py`:**
    *   O arquivo `template.py` parece ser uma vers√£o mais antiga ou um modelo inicial do assistente de voz. Compare-o com os outros arquivos (principalmente os arquivos em `class_voice_assistent`) para identificar se alguma parte do c√≥digo em `template.py` ainda √© usada. Se n√£o for, considere remover este arquivo.

*   **Exemplo de refatora√ß√£o:**
    *   Se voc√™ encontrar c√≥digo duplicado entre `voice_command_hendler.py` e `text_command_hendler.py`, voc√™ pode criar uma classe base ou fun√ß√£o utilit√°ria para lidar com a l√≥gica comum, reduzindo a duplica√ß√£o.

**5. Exemplo de C√≥digo para Remover Arquivos:**

Para remover um arquivo espec√≠fico, voc√™ pode usar o seguinte c√≥digo:

```python
import os

file_path = "voice_assistent/class_voice_assistent/arquivo_desnecessario.py"
try:
    os.remove(file_path)
    print(f"Arquivo {file_path} removido com sucesso.")
except FileNotFoundError:
    print(f"Arquivo {file_path} n√£o encontrado.")
except Exception as e:
    print(f"Erro ao remover {file_path}: {e}")
```

**Considera√ß√µes Adicionais:**

*   **Documenta√ß√£o:** Atualize a documenta√ß√£o do projeto para refletir as mudan√ßas realizadas. Isso ajudar√° outros desenvolvedores (ou voc√™ mesmo no futuro) a entender a nova estrutura do c√≥digo.
*   **Versionamento:** Use um sistema de controle de vers√£o (como Git) para gerenciar as altera√ß√µes no c√≥digo. Isso facilita a revers√£o de altera√ß√µes e a colabora√ß√£o com outros desenvolvedores.

Seguindo essas etapas, voc√™ poder√° remover c√≥digos desnecess√°rios da pasta `voice_assistent`, tornando o projeto mais limpo, eficiente e f√°cil de manter.
2025-04-14 16:22:08,145 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:22:08,147 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:22:08,149 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:22:08,151 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:22:08,154 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:22:08,155 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:22:08,157 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:22:08,158 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:22:08,161 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:22:08,163 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:22:08,164 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:22:08,166 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:22:08,167 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:22:08,169 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:22:08,171 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:22:08,172 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:22:08,173 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:22:08,174 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:22:08,177 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:22:08,179 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:22:08,180 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:22:08,181 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:22:08,183 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:22:08,184 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:22:08,185 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:22:08,187 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:22:08,189 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:22:08,190 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:22:08,191 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:22:08,193 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:22:08,196 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:22:08,197 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:22:08,198 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:22:08,199 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:22:08,201 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:22:08,203 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:22:08,205 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:22:08,207 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:22:08,208 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:22:08,209 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:22:08,210 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:22:08,212 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:22:08,213 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:22:08,214 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:22:08,215 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:22:08,217 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:22:08,218 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:22:08,220 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:22:08,221 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:22:08,222 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:24:31,011 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:24:31,012 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:24:31,014 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:24:31,015 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:24:31,016 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:24:31,017 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:24:31,019 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:24:31,020 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:24:31,022 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:24:31,023 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:24:31,025 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:24:31,027 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:24:31,029 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:24:31,031 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:24:31,033 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:24:31,036 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:24:31,038 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:24:31,040 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:24:31,043 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:24:31,045 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:24:31,046 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:24:31,047 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:24:31,048 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:24:31,050 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:24:31,052 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:24:31,053 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:24:31,054 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:24:31,055 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:24:31,056 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:24:31,058 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:24:31,059 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:24:31,061 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:24:31,062 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:24:31,064 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:24:31,065 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:24:31,067 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:24:31,069 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:24:31,070 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:24:31,072 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:24:31,073 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:24:31,075 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:24:31,077 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:24:31,079 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:24:31,080 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:24:31,082 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:24:31,084 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:24:31,086 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:24:31,087 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:24:31,089 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:24:31,091 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:24:31,194 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:24:31,197 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:24:31,198 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:24:31,200 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:24:31,201 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:24:31,203 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:24:31,204 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:24:31,205 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:24:31,207 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:24:31,209 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:24:31,210 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:24:31,212 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:24:31,213 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:24:31,215 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:24:31,218 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:24:31,219 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:24:31,221 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:24:31,222 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:24:31,224 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:24:31,225 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:24:31,227 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:24:31,228 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:24:31,230 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:24:31,231 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:24:31,233 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:24:31,236 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:24:31,237 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:24:31,239 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:24:31,241 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:24:31,243 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:24:31,244 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:24:31,246 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:24:31,249 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:24:31,251 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:24:31,253 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:24:31,255 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:24:31,256 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:24:31,258 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:24:31,260 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:24:31,261 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:24:31,263 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:24:31,267 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:24:31,278 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:24:31,279 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:24:31,283 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:24:31,285 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:24:31,287 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:24:31,288 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:24:31,289 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:24:31,291 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:24:31,292 [INFO] Enviando para IA - Prompt (sem imagem): Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"N√£o foram encontrados arquivos de √°udio no diret√≥rio {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcri√ß√£o salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diret√≥rio atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Defini√ß√£o dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Cria√ß√£o dos diret√≥rios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configura√ß√£o do layout do Streamlit
st.set_page_config(
    page_title="An√°lise de A√ß√µes",
    layout="wide",
    page_icon="üìà"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscila√ß√µes

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Per√≠odo', 'Oscila√ß√£o'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("üìä Dashboard de An√°lise de A√ß√µes")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma a√ß√£o", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"üìå Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("üîç Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descri√ß√£o', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa a√ß√£o.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("üí∞ Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("üìâ Oscila√ß√µes")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("üìà Tabela Geral de A√ß√µes")
        st.dataframe(self.acoes)

# Execu√ß√£o
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para an√°lise fundamentalista de a√ß√µes da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se est√° correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configura√ß√£o de v√≠deos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configura√ß√£o de √°udios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcri√ß√µes e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diret√≥rio
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diret√≥rio.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do √∫ltimo arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conte√∫do do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimens√µes do embedding:", embedding.shape)

# Caminho do diret√≥rio de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o √∫ltimo embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configura√ß√£o de transcri√ß√µes e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diret√≥rio onde est√° procurando as transcri√ß√µes
        print(f"Diret√≥rio de entrada das transcri√ß√µes: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcri√ß√£o no diret√≥rio de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcri√ß√£o encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo n√£o encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcri√ß√£o do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} est√° vazio.")
                    return

            # Gerar o embedding da transcri√ß√£o
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcri√ß√£o e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O v√≠deo {video_path} n√£o cont√©m √°udio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de v√≠deo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de v√≠deo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de v√≠deo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Convers√£o de v√≠deo para √°udio conclu√≠da!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configura√ß√µes da API
handler = genai('gemini-1.5-flash')

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Fun√ß√£o para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conte√∫do com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares √∫nicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcri√ß√£o obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' n√£o √© uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuni√µes: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de reuni√µes: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|t√≥picos da|pontos (relevantes|principais)|an√°lise)\b.*\b(reuni√£o|√∫ltima (reuni√£o|conversa|sess√£o))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como an√°lise de reuni√£o.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padr√£o.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que s√£o muito similares ao comando atual para evitar redund√¢ncia.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padr√£o: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # M√©todos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de an√°lise de reuni√£o.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conte√∫do usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inv√°lida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar vari√°veis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conte√∫do com base no prompt, contexto e reuni√£o"""
        try:
            # Supondo que a API espera um dicion√°rio com os par√¢metros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisi√ß√£o para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar vari√°veis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Cria√ß√£o da conclus√£o do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que voc√™ acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["fa√ßa um resumo da √∫ltima reuni√£o.", "t√≥picos da √∫ltima reuni√£o", "resuma a √∫ltima reuni√£o", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Voc√™ √© respons√°vel por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - N√£o acrescentar t√≠tulo nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informa√ß√µes acima e forne√ßa uma solu√ß√£o detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Voc√™ √© respons√°vel analisar com detalhes a reuni√£o de [str_meeting], e fornecer uma longa est√≥ria sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precis√£o da resposta.
            - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e fa√ßa uma pesquisa online para uma resposta mais precisa das quest√µes de [str_text]
        - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [context]: Regras B√°sicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("√Åudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum √°udio detectado.")
                return None
            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configura√ß√µes da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicion√°rio
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta n√£o est√° no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API: {e}")
        return None

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: Fa√ßa uma limpeza na pasta class_voice_assistent
Assistant: Com base nos arquivos fornecidos, aqui est√° um resumo da estrutura e funcionalidade do c√≥digo, juntamente com as informa√ß√µes sobre a limpeza da pasta `class_voice_assistent`:

**Vis√£o geral do projeto:**

Este projeto parece ser um assistente de voz que usa o Google Gemini para processamento de linguagem natural. Ele √© estruturado em v√°rios m√≥dulos e classes para lidar com diferentes aspectos, como captura de √°udio, convers√£o de texto em fala, interpreta√ß√£o de comandos e intera√ß√£o com APIs externas.

**Principais componentes:**

*   **Captura de comandos:**
    *   `voice_assistent\class_voice_assistent\voice_command_hendler.py`: Lida com a captura de comandos de voz usando a biblioteca `speech_recognition`.
    *   `voice_assistent\class_voice_assistent\text_command_hendler.py`: Lida com a captura de comandos de texto do usu√°rio.

*   **Processamento de linguagem natural:**
    *   `voice_assistent\class_voice_assistent\command_interpreter.py`: Interpreta os comandos do usu√°rio e determina a a√ß√£o apropriada a ser executada.
    *   `voice_assistent\class_voice_assistent\question_answers_service.py`: Converte texto em embeddings para encontrar respostas semelhantes.
    *   `voice_assistent\class_voice_assistent\context_manager.py`: Gerencia o contexto da conversa, armazenando as intera√ß√µes recentes.

*   **Comunica√ß√£o com o Google Gemini:**
    *   `voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py`: Lida com a comunica√ß√£o com a API do Google Gemini.
    *   `voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py`: Define o modelo generativo e a configura√ß√£o da API Google Gemini.

*   **Gera√ß√£o de prompt:**
    *   `voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py`: Gera prompts padr√£o para o modelo de linguagem.
    *   `voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py`: Gera prompts para an√°lise de reuni√µes.
    *   `voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py`: Gera prompts para pesquisas online.
    *   `voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py`: Define a classe abstrata para geradores de prompt.

*   **Texto para fala:**
    *   `voice_assistent\class_voice_assistent\text_to_speech.py`: Converte texto em fala usando a biblioteca `pyttsx3`.

*   **Processamento de texto:**
    *   `voice_assistent\class_voice_assistent\text_processor.py`: Fornece fun√ß√µes para extrair valores de JSON e texto de HTML.

*   **Interface principal:**
    *   `voice_assistent\class_voice_assistent\main.py`: Inicializa e coordena os diferentes componentes do assistente de voz.

**Limpeza da pasta `class_voice_assistent`:**

O comando para limpar uma pasta n√£o faz parte diretamente do c√≥digo fornecido. Para implementar essa funcionalidade, voc√™ precisaria adicionar um c√≥digo que use a biblioteca `os` ou `shutil` para remover arquivos e subdiret√≥rios da pasta `class_voice_assistent`.

**Exemplo de como implementar a limpeza da pasta:**

```python
import os
import shutil

def clean_directory(path):
    """Remove todos os arquivos e subdiret√≥rios em um diret√≥rio."""
    for filename in os.listdir(path):
        file_path = os.path.join(path, filename)
        try:
            if os.path.isfile(file_path) or os.path.islink(file_path):
                os.unlink(file_path)
            elif os.path.isdir(file_path):
                shutil.rmtree(file_path)
        except Exception as e:
            print(f'Falha ao excluir {file_path}. Motivo: {e}')

# Caminho para a pasta class_voice_assistent
path_to_clean = 'voice_assistent/class_voice_assistent'

# Chame a fun√ß√£o para limpar a pasta
clean_directory(path_to_clean)
```

**Considera√ß√µes:**

1.  **Importe os m√≥dulos necess√°rios**: `os` e `shutil`.
2.  **Defina a fun√ß√£o `clean_directory`**: Esta fun√ß√£o itera sobre todos os itens no diret√≥rio fornecido e tenta remover cada um.
3.  **Tratamento de erros**: A fun√ß√£o inclui um bloco `try...except` para lidar com poss√≠veis erros durante a remo√ß√£o de arquivos ou diret√≥rios.
4.  **Chame a fun√ß√£o `clean_directory`**: Passe o caminho do diret√≥rio que voc√™ deseja limpar.

**Onde adicionar este c√≥digo:**

Voc√™ pode integrar esta fun√ß√£o dentro do `command_interpreter.py` ou `main.py`. Por exemplo, dentro do `interpret_command`, voc√™ pode adicionar uma condi√ß√£o para detectar o comando "limpar pasta" e, em seguida, chamar a fun√ß√£o `clean_directory` com o caminho apropriado.

**Aviso**: Esta opera√ß√£o √© irrevers√≠vel e excluir√° permanentemente todos os arquivos e pastas dentro do diret√≥rio especificado. Certifique-se de ter um backup se necess√°rio.

**Seguran√ßa:**

Certifique-se de que o caminho da pasta a ser limpa seja validado e seguro para evitar exclus√µes acidentais ou maliciosas.

User: quero remover c√≥digos desnecess√°rios na pasta voice_assistent no projeto.
Assistant: Assistant: Para remover c√≥digos desnecess√°rios na pasta `voice_assistent` do projeto, aqui est√° uma abordagem detalhada:

**1. Vis√£o Geral:**

O objetivo √© identificar e remover arquivos e funcionalidades redundantes ou n√£o utilizados na pasta `voice_assistent`. Isso envolve uma an√°lise cuidadosa do c√≥digo, das depend√™ncias e do fluxo do programa para garantir que nenhuma funcionalidade essencial seja comprometida.

**2. Etapas Detalhadas:**

*   **Backup:**
    *   Antes de fazer qualquer altera√ß√£o, fa√ßa um backup completo da pasta `voice_assistent`. Isso permitir√° restaurar o estado anterior em caso de remo√ß√£o acidental de c√≥digo importante.

*   **An√°lise da Estrutura do C√≥digo:**
    *   **Identifique os principais m√≥dulos:** Analise todos os arquivos Python na pasta `voice_assistent` e suas subpastas para entender sua fun√ß√£o e como eles se interconectam.
    *   **M√≥dulos Essenciais:**
        *   `class_voice_assistent/main.py`: Coordena a opera√ß√£o do assistente de voz.
        *   `class_voice_assistent/command_interpreter.py`: Interpreta comandos do usu√°rio.
        *   `class_voice_assistent/gpt_communication/gemini_gpt.py`: Lida com a comunica√ß√£o com o Google Gemini.
        *   `class_voice_assistent/voice_command_handler.py` e `class_voice_assistent/text_command_handler.py`: Capturam comandos de voz e texto, respectivamente.
        *   `class_voice_assistent/text_to_speech.py`: Converte texto em fala.

*   **Identifica√ß√£o de C√≥digo N√£o Utilizado:**
    *   **Remova imports n√£o utilizados:** Abra cada arquivo e verifique se todos os m√≥dulos importados est√£o realmente sendo usados. Remova aqueles que n√£o est√£o.

    *   **Analise fun√ß√µes n√£o utilizadas:** Verifique se todas as fun√ß√µes definidas s√£o chamadas em algum lugar do c√≥digo. Fun√ß√µes que n√£o s√£o chamadas podem ser removidas com seguran√ßa.
    *   **Avalie c√≥digo comentado:** Avalie se os blocos de c√≥digo comentados ainda s√£o relevantes ou podem ser removidos.

*   **An√°lise das Depend√™ncias:**
    *   **Arquivo requirements.txt ou pyproject.toml:** Verifique se todas as bibliotecas listadas nos arquivos de depend√™ncia s√£o necess√°rias para o funcionamento do assistente de voz. Remova as depend√™ncias que n√£o est√£o sendo usadas.

*   **Simplifica√ß√£o da L√≥gica:**
    *   **Refatore c√≥digo duplicado:** Se houver trechos de c√≥digo repetidos, crie fun√ß√µes ou classes para reutiliz√°-los, reduzindo a redund√¢ncia.
    *   **Simplifique fun√ß√µes complexas:** Divida fun√ß√µes muito longas ou complexas em fun√ß√µes menores e mais f√°ceis de entender e manter.

*   **Remo√ß√£o de C√≥digo Desnecess√°rio:**
    *   **Exclua arquivos desnecess√°rios:** Remova arquivos que n√£o contribuem para a funcionalidade principal do assistente de voz.

*   **Teste:**
    *   Ap√≥s cada remo√ß√£o ou modifica√ß√£o significativa, execute os testes para garantir que o assistente de voz continue funcionando corretamente.
    *   Crie testes automatizados para os principais componentes para facilitar a detec√ß√£o de erros ap√≥s futuras altera√ß√µes.

**3. Implementa√ß√£o:**

*   **Edite os Arquivos:**
    *   Abra cada arquivo Python relevante e fa√ßa as modifica√ß√µes necess√°rias, como remover imports n√£o utilizados, fun√ß√µes desnecess√°rias e refatorar o c√≥digo.
*   **Remova Arquivos Desnecess√°rios:**
    *   Exclua arquivos que n√£o s√£o mais necess√°rios. Certifique-se de que nenhum outro componente depende desses arquivos antes de remov√™-los.

**4. Exemplo Pr√°tico:**

*   **Avalia√ß√£o de `template.py`:**
    *   O arquivo `template.py` parece ser uma vers√£o mais antiga ou um modelo inicial do assistente de voz. Compare-o com os outros arquivos (principalmente os arquivos em `class_voice_assistent`) para identificar se alguma parte do c√≥digo em `template.py` ainda √© usada. Se n√£o for, considere remover este arquivo.

*   **Exemplo de refatora√ß√£o:**
    *   Se voc√™ encontrar c√≥digo duplicado entre `voice_command_hendler.py` e `text_command_hendler.py`, voc√™ pode criar uma classe base ou fun√ß√£o utilit√°ria para lidar com a l√≥gica comum, reduzindo a duplica√ß√£o.

**5. Exemplo de C√≥digo para Remover Arquivos:**

Para remover um arquivo espec√≠fico, voc√™ pode usar o seguinte c√≥digo:

```python
import os

file_path = "voice_assistent/class_voice_assistent/arquivo_desnecessario.py"
try:
    os.remove(file_path)
    print(f"Arquivo {file_path} removido com sucesso.")
except FileNotFoundError:
    print(f"Arquivo {file_path} n√£o encontrado.")
except Exception as e:
    print(f"Erro ao remover {file_path}: {e}")
```

**Considera√ß√µes Adicionais:**

*   **Documenta√ß√£o:** Atualize a documenta√ß√£o do projeto para refletir as mudan√ßas realizadas. Isso ajudar√° outros desenvolvedores (ou voc√™ mesmo no futuro) a entender a nova estrutura do c√≥digo.
*   **Versionamento:** Use um sistema de controle de vers√£o (como Git) para gerenciar as altera√ß√µes no c√≥digo. Isso facilita a revers√£o de altera√ß√µes e a colabora√ß√£o com outros desenvolvedores.

Seguindo essas etapas, voc√™ poder√° remover c√≥digos desnecess√°rios da pasta `voice_assistent`, tornando o projeto mais limpo, eficiente e f√°cil de manter.
User: voc√™ tem o c√≥digo no contexto. voc√™ √© meu assistente e deve fazer a an√°lise para mim.
2025-04-14 16:24:40,808 [INFO] Resposta da IA (texto): Com prazer! Analisei o c√≥digo que voc√™ forneceu e identifiquei algumas √°reas onde podemos remover c√≥digo desnecess√°rio e refatorar para melhor clareza e efici√™ncia.

**1. An√°lise Detalhada e Sugest√µes:**

*   **`template.py`:**

    *   **Status:** Este arquivo parece ser uma vers√£o anterior ou um modelo desatualizado do assistente de voz.
    *   **A√ß√£o:** Comparar o c√≥digo deste arquivo com os componentes atuais em `class_voice_assistent`. Se o c√≥digo n√£o for mais utilizado, **remover o arquivo `template.py`**.

*   **`voice_assistent\config.py`:**

    *   **Status:** Este arquivo cont√©m configura√ß√µes b√°sicas, mas algumas dessas configura√ß√µes podem estar duplicadas ou n√£o sendo usadas.
    *   **A√ß√£o:**
        *   **apiKey e url:** Verificar se essas vari√°veis ainda s√£o necess√°rias e se n√£o est√£o duplicadas em outro lugar (por exemplo, em um arquivo `.env`).
        *   **engine, recent_context, nlp:** Estes itens s√£o inicializados novamente em outros arquivos. Avaliar se √© melhor centralizar a inicializa√ß√£o em um √∫nico local (possivelmente em `class_voice_assistent/main.py`) e importar as inst√¢ncias onde necess√°rio.

*   **`class_voice_assistent\conversation_history.py`:**

    *   **Status:** Este arquivo est√° vazio.
    *   **A√ß√£o:** **Remover este arquivo** se ele n√£o for usado. Se a inten√ß√£o era armazenar o hist√≥rico de conversas, a funcionalidade deve ser movida para um arquivo relevante ou integrada em `class_voice_assistent\context_manager.py`.

*   **`class_voice_assistent\gpt_communication\groq._gpt.py`:**

    *   **Status:** Este arquivo demonstra o uso da API Groq, mas n√£o parece estar integrado ao resto do c√≥digo.
    *   **A√ß√£o:** Se a API Groq n√£o for utilizada, **remover este arquivo**. Caso contr√°rio, integrar corretamente e remover a duplica√ß√£o de configura√ß√µes e inicializa√ß√µes.

*   **`class_voice_assistent\prompt_generator\prompt_generator.py`:**

    *   **Status:** Defini√ß√£o de uma classe abstrata.
    *   **A√ß√£o:** Verificar se a classe abstrata `PromptGenerator` √© usada corretamente pelas classes filhas. Se as classes filhas n√£o implementam os m√©todos abstratos corretamente, refatorar ou remover a classe abstrata.

*   **M√≥dulos de Comando (`voice_command_hendler.py` e `text_command_hendler.py`):**

    *   **Status:** Lidam com a captura de comandos.
    *   **A√ß√£o:** Verificar se h√° duplica√ß√£o de c√≥digo entre esses dois m√≥dulos (por exemplo, tratamento de erros, prompts ao usu√°rio). Se houver, criar uma classe base ou fun√ß√£o utilit√°ria para lidar com a l√≥gica comum.

*   **Remo√ß√£o de c√≥digo n√£o utilizado:**

    *   Utilize um linter (como `flake8`) para identificar imports n√£o utilizados e c√≥digo n√£o referenciado nos arquivos.

**2. Plano de A√ß√£o Detalhado:**

1.  **Backup:** Criar um backup da pasta `voice_assistent`.
2.  **Remover `template.py`:**
    ```bash
    rm voice_assistent/template.py
    ```
3.  **Remover `class_voice_assistent\conversation_history.py`:**
    ```bash
    rm voice_assistent/class_voice_assistent/conversation_history.py
    ```
4.  **Remover `class_voice_assistent\gpt_communication\groq._gpt.py`:**
    ```bash
    rm voice_assistent/class_voice_assistent/gpt_communication/groq._gpt.py
    ```
5.  **Refatorar Configura√ß√µes:**
    *   Mover as configura√ß√µes essenciais de `voice_assistent\config.py` para um arquivo `.env` ou para `class_voice_assistent\main.py`.
    *   Remover o arquivo `voice_assistent\config.py` se ele n√£o for mais necess√°rio.
6.  **Refatorar M√≥dulos de Comando:**
    *   Criar uma classe base ou fun√ß√£o utilit√°ria para consolidar a l√≥gica comum em `voice_command_hendler.py` e `text_command_hendler.py`.
7.  **Executar Linter:**
    ```bash
    pip install flake8
    flake8 voice_assistent
    ```
    *   Remover imports n√£o utilizados e c√≥digo n√£o referenciado.
8.  **Testar:**
    *   Ap√≥s cada passo, testar o assistente de voz para garantir que ele continue funcionando corretamente.
9.  **Documentar:**
    *   Atualizar a documenta√ß√£o do projeto para refletir as mudan√ßas realizadas.

**3. Implementa√ß√£o:**

Remover c√≥digo n√£o utilizado

```python
# Exemplo de como remover um arquivo
import os

file_path = "voice_assistent/class_voice_assistent/gpt_communication/groq._gpt.py"
try:
    os.remove(file_path)
    print(f"Arquivo {file_path} removido com sucesso.")
except FileNotFoundError:
    print(f"Arquivo {file_path} n√£o encontrado.")
except Exception as e:
    print(f"Erro ao remover {file_path}: {e}")
```

**Centralizar configura√ß√µes**

```python
# Exemplo de como centralizar configura√ß√µes em main.py
import os
from dotenv import load_dotenv

load_dotenv()

api_key = os.getenv("API_KEY")
# ... outras configura√ß√µes
```

**4. Considera√ß√µes Adicionais:**

*   **Testes:** Criar testes unit√°rios para garantir a estabilidade dos componentes principais.
*   **Versionamento:** Usar Git para controlar as altera√ß√µes e facilitar a revers√£o, se necess√°rio.
*   **Seguran√ßa:** Validar o caminho da pasta para evitar exclus√µes acidentais.

Com essas a√ß√µes, a pasta `voice_assistent` ficar√° mais organizada, eficiente e f√°cil de manter. Se precisar de ajuda com algum passo espec√≠fico ou detalhe adicional, √© s√≥ me avisar!

2025-04-14 16:24:40,950 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:24:40,953 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:24:40,955 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:24:40,957 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:24:40,959 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:24:40,961 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:24:40,962 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:24:40,964 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:24:40,966 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:24:40,969 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:24:40,970 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:24:40,972 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:24:40,974 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:24:40,976 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:24:40,977 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:24:40,979 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:24:40,981 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:24:40,983 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:24:40,985 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:24:40,987 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:24:40,989 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:24:40,991 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:24:40,993 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:24:40,995 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:24:40,997 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:24:41,000 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:24:41,002 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:24:41,004 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:24:41,005 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:24:41,007 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:24:41,009 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:24:41,011 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:24:41,013 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:24:41,015 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:24:41,017 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:24:41,019 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:24:41,021 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:24:41,022 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:24:41,024 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:24:41,026 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:24:41,028 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:24:41,031 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:24:41,033 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:24:41,035 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:24:41,037 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:24:41,039 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:24:41,041 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:24:41,043 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:24:41,045 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:24:41,047 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:25:47,327 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:25:47,328 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:25:47,330 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:25:47,331 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:25:47,332 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:25:47,334 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:25:47,335 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:25:47,336 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:25:47,337 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:25:47,338 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:25:47,340 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:25:47,341 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:25:47,343 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:25:47,345 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:25:47,346 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:25:47,348 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:25:47,349 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:25:47,350 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:25:47,352 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:25:47,354 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:25:47,355 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:25:47,359 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:25:47,361 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:25:47,362 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:25:47,363 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:25:47,364 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:25:47,365 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:25:47,366 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:25:47,367 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:25:47,369 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:25:47,370 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:25:47,371 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:25:47,372 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:25:47,373 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:25:47,374 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:25:47,377 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:25:47,378 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:25:47,379 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:25:47,380 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:25:47,381 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:25:47,382 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:25:47,383 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:25:47,384 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:25:47,385 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:25:47,386 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:25:47,387 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:25:47,388 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:25:47,390 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:25:47,391 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:25:47,393 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:25:47,556 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:25:47,557 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:25:47,560 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:25:47,561 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:25:47,563 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:25:47,565 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:25:47,566 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:25:47,567 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:25:47,568 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:25:47,570 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:25:47,571 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:25:47,573 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:25:47,574 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:25:47,576 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:25:47,577 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:25:47,578 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:25:47,579 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:25:47,580 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:25:47,581 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:25:47,582 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:25:47,583 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:25:47,585 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:25:47,586 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:25:47,587 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:25:47,588 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:25:47,589 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:25:47,591 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:25:47,592 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:25:47,593 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:25:47,595 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:25:47,596 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:25:47,597 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:25:47,598 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:25:47,599 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:25:47,601 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:25:47,602 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:25:47,603 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:25:47,604 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:25:47,605 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:25:47,606 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:25:47,608 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:25:47,609 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:25:47,610 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:25:47,612 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:25:47,613 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:25:47,614 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:25:47,615 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:25:47,616 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:25:47,617 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:25:47,618 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:25:56,993 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:25:56,995 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:25:56,996 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:25:56,998 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:25:57,000 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:25:57,002 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:25:57,003 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:25:57,005 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:25:57,007 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:25:57,009 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:25:57,011 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:25:57,012 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:25:57,014 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:25:57,015 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:25:57,016 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:25:57,017 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:25:57,019 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:25:57,020 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:25:57,059 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:25:57,062 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:25:57,064 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:25:57,065 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:25:57,067 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:25:57,069 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:25:57,070 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:25:57,074 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:25:57,081 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:25:57,082 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:25:57,083 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:25:57,085 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:25:57,086 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:25:57,087 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:25:57,088 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:25:57,089 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:25:57,091 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:25:57,092 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:25:57,094 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:25:57,095 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:25:57,096 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:25:57,098 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:25:57,099 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:25:57,100 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:25:57,101 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:25:57,102 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:25:57,103 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:25:57,104 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:25:57,105 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:25:57,106 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:25:57,107 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:25:57,109 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:25:57,206 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:25:57,208 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:25:57,211 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:25:57,214 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:25:57,215 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:25:57,217 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:25:57,218 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:25:57,219 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:25:57,221 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:25:57,222 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:25:57,224 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:25:57,225 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:25:57,227 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:25:57,228 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:25:57,229 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:25:57,231 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:25:57,232 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:25:57,233 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:25:57,234 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:25:57,236 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:25:57,237 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:25:57,238 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:25:57,240 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:25:57,243 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:25:57,244 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:25:57,245 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:25:57,247 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:25:57,249 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:25:57,251 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:25:57,253 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:25:57,256 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:25:57,257 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:25:57,259 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:25:57,261 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:25:57,262 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:25:57,264 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:25:57,265 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:25:57,267 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:25:57,268 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:25:57,270 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:25:57,271 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:25:57,273 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:25:57,275 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:25:57,277 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:25:57,279 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:25:57,280 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:25:57,281 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:25:57,282 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:25:57,283 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:25:57,284 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:25:57,300 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\agents_ia\comandAI\assets\20250414162557_clipboard_20250414162547.png, Prompt: Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"N√£o foram encontrados arquivos de √°udio no diret√≥rio {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcri√ß√£o salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diret√≥rio atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Defini√ß√£o dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Cria√ß√£o dos diret√≥rios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configura√ß√£o do layout do Streamlit
st.set_page_config(
    page_title="An√°lise de A√ß√µes",
    layout="wide",
    page_icon="üìà"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscila√ß√µes

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Per√≠odo', 'Oscila√ß√£o'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("üìä Dashboard de An√°lise de A√ß√µes")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma a√ß√£o", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"üìå Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("üîç Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descri√ß√£o', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa a√ß√£o.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("üí∞ Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("üìâ Oscila√ß√µes")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("üìà Tabela Geral de A√ß√µes")
        st.dataframe(self.acoes)

# Execu√ß√£o
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para an√°lise fundamentalista de a√ß√µes da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se est√° correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configura√ß√£o de v√≠deos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configura√ß√£o de √°udios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcri√ß√µes e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diret√≥rio
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diret√≥rio.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do √∫ltimo arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conte√∫do do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimens√µes do embedding:", embedding.shape)

# Caminho do diret√≥rio de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o √∫ltimo embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configura√ß√£o de transcri√ß√µes e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diret√≥rio onde est√° procurando as transcri√ß√µes
        print(f"Diret√≥rio de entrada das transcri√ß√µes: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcri√ß√£o no diret√≥rio de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcri√ß√£o encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo n√£o encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcri√ß√£o do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} est√° vazio.")
                    return

            # Gerar o embedding da transcri√ß√£o
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcri√ß√£o e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O v√≠deo {video_path} n√£o cont√©m √°udio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de v√≠deo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de v√≠deo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de v√≠deo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Convers√£o de v√≠deo para √°udio conclu√≠da!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configura√ß√µes da API
handler = genai('gemini-1.5-flash')

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Fun√ß√£o para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conte√∫do com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares √∫nicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcri√ß√£o obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' n√£o √© uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuni√µes: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de reuni√µes: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|t√≥picos da|pontos (relevantes|principais)|an√°lise)\b.*\b(reuni√£o|√∫ltima (reuni√£o|conversa|sess√£o))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como an√°lise de reuni√£o.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padr√£o.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que s√£o muito similares ao comando atual para evitar redund√¢ncia.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padr√£o: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # M√©todos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de an√°lise de reuni√£o.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conte√∫do usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inv√°lida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar vari√°veis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conte√∫do com base no prompt, contexto e reuni√£o"""
        try:
            # Supondo que a API espera um dicion√°rio com os par√¢metros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisi√ß√£o para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar vari√°veis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Cria√ß√£o da conclus√£o do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que voc√™ acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["fa√ßa um resumo da √∫ltima reuni√£o.", "t√≥picos da √∫ltima reuni√£o", "resuma a √∫ltima reuni√£o", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Voc√™ √© respons√°vel por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - N√£o acrescentar t√≠tulo nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informa√ß√µes acima e forne√ßa uma solu√ß√£o detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Voc√™ √© respons√°vel analisar com detalhes a reuni√£o de [str_meeting], e fornecer uma longa est√≥ria sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precis√£o da resposta.
            - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e fa√ßa uma pesquisa online para uma resposta mais precisa das quest√µes de [str_text]
        - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [context]: Regras B√°sicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("√Åudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum √°udio detectado.")
                return None
            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configura√ß√µes da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicion√°rio
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta n√£o est√° no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API: {e}")
        return None

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: Fa√ßa uma limpeza na pasta class_voice_assistent
Assistant: Com base nos arquivos fornecidos, aqui est√° um resumo da estrutura e funcionalidade do c√≥digo, juntamente com as informa√ß√µes sobre a limpeza da pasta `class_voice_assistent`:

**Vis√£o geral do projeto:**

Este projeto parece ser um assistente de voz que usa o Google Gemini para processamento de linguagem natural. Ele √© estruturado em v√°rios m√≥dulos e classes para lidar com diferentes aspectos, como captura de √°udio, convers√£o de texto em fala, interpreta√ß√£o de comandos e intera√ß√£o com APIs externas.

**Principais componentes:**

*   **Captura de comandos:**
    *   `voice_assistent\class_voice_assistent\voice_command_hendler.py`: Lida com a captura de comandos de voz usando a biblioteca `speech_recognition`.
    *   `voice_assistent\class_voice_assistent\text_command_hendler.py`: Lida com a captura de comandos de texto do usu√°rio.

*   **Processamento de linguagem natural:**
    *   `voice_assistent\class_voice_assistent\command_interpreter.py`: Interpreta os comandos do usu√°rio e determina a a√ß√£o apropriada a ser executada.
    *   `voice_assistent\class_voice_assistent\question_answers_service.py`: Converte texto em embeddings para encontrar respostas semelhantes.
    *   `voice_assistent\class_voice_assistent\context_manager.py`: Gerencia o contexto da conversa, armazenando as intera√ß√µes recentes.

*   **Comunica√ß√£o com o Google Gemini:**
    *   `voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py`: Lida com a comunica√ß√£o com a API do Google Gemini.
    *   `voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py`: Define o modelo generativo e a configura√ß√£o da API Google Gemini.

*   **Gera√ß√£o de prompt:**
    *   `voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py`: Gera prompts padr√£o para o modelo de linguagem.
    *   `voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py`: Gera prompts para an√°lise de reuni√µes.
    *   `voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py`: Gera prompts para pesquisas online.
    *   `voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py`: Define a classe abstrata para geradores de prompt.

*   **Texto para fala:**
    *   `voice_assistent\class_voice_assistent\text_to_speech.py`: Converte texto em fala usando a biblioteca `pyttsx3`.

*   **Processamento de texto:**
    *   `voice_assistent\class_voice_assistent\text_processor.py`: Fornece fun√ß√µes para extrair valores de JSON e texto de HTML.

*   **Interface principal:**
    *   `voice_assistent\class_voice_assistent\main.py`: Inicializa e coordena os diferentes componentes do assistente de voz.

**Limpeza da pasta `class_voice_assistent`:**

O comando para limpar uma pasta n√£o faz parte diretamente do c√≥digo fornecido. Para implementar essa funcionalidade, voc√™ precisaria adicionar um c√≥digo que use a biblioteca `os` ou `shutil` para remover arquivos e subdiret√≥rios da pasta `class_voice_assistent`.

**Exemplo de como implementar a limpeza da pasta:**

```python
import os
import shutil

def clean_directory(path):
    """Remove todos os arquivos e subdiret√≥rios em um diret√≥rio."""
    for filename in os.listdir(path):
        file_path = os.path.join(path, filename)
        try:
            if os.path.isfile(file_path) or os.path.islink(file_path):
                os.unlink(file_path)
            elif os.path.isdir(file_path):
                shutil.rmtree(file_path)
        except Exception as e:
            print(f'Falha ao excluir {file_path}. Motivo: {e}')

# Caminho para a pasta class_voice_assistent
path_to_clean = 'voice_assistent/class_voice_assistent'

# Chame a fun√ß√£o para limpar a pasta
clean_directory(path_to_clean)
```

**Considera√ß√µes:**

1.  **Importe os m√≥dulos necess√°rios**: `os` e `shutil`.
2.  **Defina a fun√ß√£o `clean_directory`**: Esta fun√ß√£o itera sobre todos os itens no diret√≥rio fornecido e tenta remover cada um.
3.  **Tratamento de erros**: A fun√ß√£o inclui um bloco `try...except` para lidar com poss√≠veis erros durante a remo√ß√£o de arquivos ou diret√≥rios.
4.  **Chame a fun√ß√£o `clean_directory`**: Passe o caminho do diret√≥rio que voc√™ deseja limpar.

**Onde adicionar este c√≥digo:**

Voc√™ pode integrar esta fun√ß√£o dentro do `command_interpreter.py` ou `main.py`. Por exemplo, dentro do `interpret_command`, voc√™ pode adicionar uma condi√ß√£o para detectar o comando "limpar pasta" e, em seguida, chamar a fun√ß√£o `clean_directory` com o caminho apropriado.

**Aviso**: Esta opera√ß√£o √© irrevers√≠vel e excluir√° permanentemente todos os arquivos e pastas dentro do diret√≥rio especificado. Certifique-se de ter um backup se necess√°rio.

**Seguran√ßa:**

Certifique-se de que o caminho da pasta a ser limpa seja validado e seguro para evitar exclus√µes acidentais ou maliciosas.

User: quero remover c√≥digos desnecess√°rios na pasta voice_assistent no projeto.
Assistant: Assistant: Para remover c√≥digos desnecess√°rios na pasta `voice_assistent` do projeto, aqui est√° uma abordagem detalhada:

**1. Vis√£o Geral:**

O objetivo √© identificar e remover arquivos e funcionalidades redundantes ou n√£o utilizados na pasta `voice_assistent`. Isso envolve uma an√°lise cuidadosa do c√≥digo, das depend√™ncias e do fluxo do programa para garantir que nenhuma funcionalidade essencial seja comprometida.

**2. Etapas Detalhadas:**

*   **Backup:**
    *   Antes de fazer qualquer altera√ß√£o, fa√ßa um backup completo da pasta `voice_assistent`. Isso permitir√° restaurar o estado anterior em caso de remo√ß√£o acidental de c√≥digo importante.

*   **An√°lise da Estrutura do C√≥digo:**
    *   **Identifique os principais m√≥dulos:** Analise todos os arquivos Python na pasta `voice_assistent` e suas subpastas para entender sua fun√ß√£o e como eles se interconectam.
    *   **M√≥dulos Essenciais:**
        *   `class_voice_assistent/main.py`: Coordena a opera√ß√£o do assistente de voz.
        *   `class_voice_assistent/command_interpreter.py`: Interpreta comandos do usu√°rio.
        *   `class_voice_assistent/gpt_communication/gemini_gpt.py`: Lida com a comunica√ß√£o com o Google Gemini.
        *   `class_voice_assistent/voice_command_handler.py` e `class_voice_assistent/text_command_handler.py`: Capturam comandos de voz e texto, respectivamente.
        *   `class_voice_assistent/text_to_speech.py`: Converte texto em fala.

*   **Identifica√ß√£o de C√≥digo N√£o Utilizado:**
    *   **Remova imports n√£o utilizados:** Abra cada arquivo e verifique se todos os m√≥dulos importados est√£o realmente sendo usados. Remova aqueles que n√£o est√£o.

    *   **Analise fun√ß√µes n√£o utilizadas:** Verifique se todas as fun√ß√µes definidas s√£o chamadas em algum lugar do c√≥digo. Fun√ß√µes que n√£o s√£o chamadas podem ser removidas com seguran√ßa.
    *   **Avalie c√≥digo comentado:** Avalie se os blocos de c√≥digo comentados ainda s√£o relevantes ou podem ser removidos.

*   **An√°lise das Depend√™ncias:**
    *   **Arquivo requirements.txt ou pyproject.toml:** Verifique se todas as bibliotecas listadas nos arquivos de depend√™ncia s√£o necess√°rias para o funcionamento do assistente de voz. Remova as depend√™ncias que n√£o est√£o sendo usadas.

*   **Simplifica√ß√£o da L√≥gica:**
    *   **Refatore c√≥digo duplicado:** Se houver trechos de c√≥digo repetidos, crie fun√ß√µes ou classes para reutiliz√°-los, reduzindo a redund√¢ncia.
    *   **Simplifique fun√ß√µes complexas:** Divida fun√ß√µes muito longas ou complexas em fun√ß√µes menores e mais f√°ceis de entender e manter.

*   **Remo√ß√£o de C√≥digo Desnecess√°rio:**
    *   **Exclua arquivos desnecess√°rios:** Remova arquivos que n√£o contribuem para a funcionalidade principal do assistente de voz.

*   **Teste:**
    *   Ap√≥s cada remo√ß√£o ou modifica√ß√£o significativa, execute os testes para garantir que o assistente de voz continue funcionando corretamente.
    *   Crie testes automatizados para os principais componentes para facilitar a detec√ß√£o de erros ap√≥s futuras altera√ß√µes.

**3. Implementa√ß√£o:**

*   **Edite os Arquivos:**
    *   Abra cada arquivo Python relevante e fa√ßa as modifica√ß√µes necess√°rias, como remover imports n√£o utilizados, fun√ß√µes desnecess√°rias e refatorar o c√≥digo.
*   **Remova Arquivos Desnecess√°rios:**
    *   Exclua arquivos que n√£o s√£o mais necess√°rios. Certifique-se de que nenhum outro componente depende desses arquivos antes de remov√™-los.

**4. Exemplo Pr√°tico:**

*   **Avalia√ß√£o de `template.py`:**
    *   O arquivo `template.py` parece ser uma vers√£o mais antiga ou um modelo inicial do assistente de voz. Compare-o com os outros arquivos (principalmente os arquivos em `class_voice_assistent`) para identificar se alguma parte do c√≥digo em `template.py` ainda √© usada. Se n√£o for, considere remover este arquivo.

*   **Exemplo de refatora√ß√£o:**
    *   Se voc√™ encontrar c√≥digo duplicado entre `voice_command_hendler.py` e `text_command_hendler.py`, voc√™ pode criar uma classe base ou fun√ß√£o utilit√°ria para lidar com a l√≥gica comum, reduzindo a duplica√ß√£o.

**5. Exemplo de C√≥digo para Remover Arquivos:**

Para remover um arquivo espec√≠fico, voc√™ pode usar o seguinte c√≥digo:

```python
import os

file_path = "voice_assistent/class_voice_assistent/arquivo_desnecessario.py"
try:
    os.remove(file_path)
    print(f"Arquivo {file_path} removido com sucesso.")
except FileNotFoundError:
    print(f"Arquivo {file_path} n√£o encontrado.")
except Exception as e:
    print(f"Erro ao remover {file_path}: {e}")
```

**Considera√ß√µes Adicionais:**

*   **Documenta√ß√£o:** Atualize a documenta√ß√£o do projeto para refletir as mudan√ßas realizadas. Isso ajudar√° outros desenvolvedores (ou voc√™ mesmo no futuro) a entender a nova estrutura do c√≥digo.
*   **Versionamento:** Use um sistema de controle de vers√£o (como Git) para gerenciar as altera√ß√µes no c√≥digo. Isso facilita a revers√£o de altera√ß√µes e a colabora√ß√£o com outros desenvolvedores.

Seguindo essas etapas, voc√™ poder√° remover c√≥digos desnecess√°rios da pasta `voice_assistent`, tornando o projeto mais limpo, eficiente e f√°cil de manter.
User: voc√™ tem o c√≥digo no contexto. voc√™ √© meu assistente e deve fazer a an√°lise para mim.
Assistant: Com prazer! Analisei o c√≥digo que voc√™ forneceu e identifiquei algumas √°reas onde podemos remover c√≥digo desnecess√°rio e refatorar para melhor clareza e efici√™ncia.

**1. An√°lise Detalhada e Sugest√µes:**

*   **`template.py`:**

    *   **Status:** Este arquivo parece ser uma vers√£o anterior ou um modelo desatualizado do assistente de voz.
    *   **A√ß√£o:** Comparar o c√≥digo deste arquivo com os componentes atuais em `class_voice_assistent`. Se o c√≥digo n√£o for mais utilizado, **remover o arquivo `template.py`**.

*   **`voice_assistent\config.py`:**

    *   **Status:** Este arquivo cont√©m configura√ß√µes b√°sicas, mas algumas dessas configura√ß√µes podem estar duplicadas ou n√£o sendo usadas.
    *   **A√ß√£o:**
        *   **apiKey e url:** Verificar se essas vari√°veis ainda s√£o necess√°rias e se n√£o est√£o duplicadas em outro lugar (por exemplo, em um arquivo `.env`).
        *   **engine, recent_context, nlp:** Estes itens s√£o inicializados novamente em outros arquivos. Avaliar se √© melhor centralizar a inicializa√ß√£o em um √∫nico local (possivelmente em `class_voice_assistent/main.py`) e importar as inst√¢ncias onde necess√°rio.

*   **`class_voice_assistent\conversation_history.py`:**

    *   **Status:** Este arquivo est√° vazio.
    *   **A√ß√£o:** **Remover este arquivo** se ele n√£o for usado. Se a inten√ß√£o era armazenar o hist√≥rico de conversas, a funcionalidade deve ser movida para um arquivo relevante ou integrada em `class_voice_assistent\context_manager.py`.

*   **`class_voice_assistent\gpt_communication\groq._gpt.py`:**

    *   **Status:** Este arquivo demonstra o uso da API Groq, mas n√£o parece estar integrado ao resto do c√≥digo.
    *   **A√ß√£o:** Se a API Groq n√£o for utilizada, **remover este arquivo**. Caso contr√°rio, integrar corretamente e remover a duplica√ß√£o de configura√ß√µes e inicializa√ß√µes.

*   **`class_voice_assistent\prompt_generator\prompt_generator.py`:**

    *   **Status:** Defini√ß√£o de uma classe abstrata.
    *   **A√ß√£o:** Verificar se a classe abstrata `PromptGenerator` √© usada corretamente pelas classes filhas. Se as classes filhas n√£o implementam os m√©todos abstratos corretamente, refatorar ou remover a classe abstrata.

*   **M√≥dulos de Comando (`voice_command_hendler.py` e `text_command_hendler.py`):**

    *   **Status:** Lidam com a captura de comandos.
    *   **A√ß√£o:** Verificar se h√° duplica√ß√£o de c√≥digo entre esses dois m√≥dulos (por exemplo, tratamento de erros, prompts ao usu√°rio). Se houver, criar uma classe base ou fun√ß√£o utilit√°ria para lidar com a l√≥gica comum.

*   **Remo√ß√£o de c√≥digo n√£o utilizado:**

    *   Utilize um linter (como `flake8`) para identificar imports n√£o utilizados e c√≥digo n√£o referenciado nos arquivos.

**2. Plano de A√ß√£o Detalhado:**

1.  **Backup:** Criar um backup da pasta `voice_assistent`.
2.  **Remover `template.py`:**
    ```bash
    rm voice_assistent/template.py
    ```
3.  **Remover `class_voice_assistent\conversation_history.py`:**
    ```bash
    rm voice_assistent/class_voice_assistent/conversation_history.py
    ```
4.  **Remover `class_voice_assistent\gpt_communication\groq._gpt.py`:**
    ```bash
    rm voice_assistent/class_voice_assistent/gpt_communication/groq._gpt.py
    ```
5.  **Refatorar Configura√ß√µes:**
    *   Mover as configura√ß√µes essenciais de `voice_assistent\config.py` para um arquivo `.env` ou para `class_voice_assistent\main.py`.
    *   Remover o arquivo `voice_assistent\config.py` se ele n√£o for mais necess√°rio.
6.  **Refatorar M√≥dulos de Comando:**
    *   Criar uma classe base ou fun√ß√£o utilit√°ria para consolidar a l√≥gica comum em `voice_command_hendler.py` e `text_command_hendler.py`.
7.  **Executar Linter:**
    ```bash
    pip install flake8
    flake8 voice_assistent
    ```
    *   Remover imports n√£o utilizados e c√≥digo n√£o referenciado.
8.  **Testar:**
    *   Ap√≥s cada passo, testar o assistente de voz para garantir que ele continue funcionando corretamente.
9.  **Documentar:**
    *   Atualizar a documenta√ß√£o do projeto para refletir as mudan√ßas realizadas.

**3. Implementa√ß√£o:**

Remover c√≥digo n√£o utilizado

```python
# Exemplo de como remover um arquivo
import os

file_path = "voice_assistent/class_voice_assistent/gpt_communication/groq._gpt.py"
try:
    os.remove(file_path)
    print(f"Arquivo {file_path} removido com sucesso.")
except FileNotFoundError:
    print(f"Arquivo {file_path} n√£o encontrado.")
except Exception as e:
    print(f"Erro ao remover {file_path}: {e}")
```

**Centralizar configura√ß√µes**

```python
# Exemplo de como centralizar configura√ß√µes em main.py
import os
from dotenv import load_dotenv

load_dotenv()

api_key = os.getenv("API_KEY")
# ... outras configura√ß√µes
```

**4. Considera√ß√µes Adicionais:**

*   **Testes:** Criar testes unit√°rios para garantir a estabilidade dos componentes principais.
*   **Versionamento:** Usar Git para controlar as altera√ß√µes e facilitar a revers√£o, se necess√°rio.
*   **Seguran√ßa:** Validar o caminho da pasta para evitar exclus√µes acidentais.

Com essas a√ß√µes, a pasta `voice_assistent` ficar√° mais organizada, eficiente e f√°cil de manter. Se precisar de ajuda com algum passo espec√≠fico ou detalhe adicional, √© s√≥ me avisar!

User: Preciso importar essas libs 
2025-04-14 16:26:02,032 [INFO] Resposta da IA (imagem): ```python
import os
from dotenv import load_dotenv
import os
import shutil
import pyttsx3
import spacy
import speech_recognition as sr
import requests
import re
from collections import deque
from bs4 import BeautifulSoup
import google.generativeai as genai
```

Essas s√£o as bibliotecas que foram mencionadas ao longo da conversa.

Aqui est√° um exemplo pr√°tico de como importar essas bibliotecas no in√≠cio de um arquivo Python:

```python
# Importa√ß√µes
import os
from dotenv import load_dotenv
import shutil
import pyttsx3
import spacy
import speech_recognition as sr
import requests
import re
from collections import deque
from bs4 import BeautifulSoup
import google.generativeai as genai

# Seu c√≥digo continua aqui...
```

Essas importa√ß√µes fornecer√£o acesso √†s funcionalidades de cada biblioteca ao longo do seu c√≥digo.

Al√©m disso, aqui est√° uma breve descri√ß√£o de cada biblioteca, juntamente com um link para a documenta√ß√£o, para que voc√™ possa aprender mais sobre como utiliz√°-las de forma eficaz:

-   **os:** Fornece uma maneira de usar funcionalidades dependentes do sistema operacional (documenta√ß√£o: <https://docs.python.org/3/library/os.html>)
-   **dotenv:** Permite carregar vari√°veis de ambiente de um arquivo `.env` (documenta√ß√£o: <https://pypi.org/project/python-dotenv/>)
-   **shutil:** Oferece um conjunto de opera√ß√µes de alto n√≠vel em arquivos e cole√ß√µes de arquivos, como copiar e remover arquivos e diret√≥rios (documenta√ß√£o: <https://docs.python.org/3/library/shutil.html>)
-   **pyttsx3:** Biblioteca de convers√£o de texto em fala que funciona offline (documenta√ß√£o: <https://pyttsx3.readthedocs.io/en/latest/>)
-   **spacy:** Uma biblioteca de processamento de linguagem natural avan√ßada (documenta√ß√£o: <https://spacy.io/>)
-   **speech\_recognition:** Biblioteca para reconhecimento de fala, suportando v√°rias APIs (documenta√ß√£o: <https://pypi.org/project/SpeechRecognition/>)
-   **requests:** Biblioteca para fazer requisi√ß√µes HTTP (documenta√ß√£o: <https://requests.readthedocs.io/en/latest/>)
-   **re:** Fornece opera√ß√µes de express√£o regular (documenta√ß√£o: <https://docs.python.org/3/library/re.html>)
-   **collections.deque:** Uma generaliza√ß√£o da pilha e da fila que suporta anexar e remover de qualquer extremidade (documenta√ß√£o: <https://docs.python.org/3/library/collections.html#collections.deque>)
-   **bs4 (Beautiful Soup):** Biblioteca para extrair informa√ß√µes de p√°ginas web em formato HTML ou XML (documenta√ß√£o: <https://www.crummy.com/software/BeautifulSoup/bs4/doc/>)
-   **google.generativeai:** Biblioteca para usar os modelos de linguagem generativa do Google (documenta√ß√£o: verificar a documenta√ß√£o espec√≠fica para os modelos do Google Gemini no Google Cloud).

Com essas importa√ß√µes e a documenta√ß√£o correspondente, voc√™ est√° bem preparado para continuar sua an√°lise e refatora√ß√£o do c√≥digo!
2025-04-14 16:26:02,147 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:26:02,149 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:26:02,151 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:26:02,152 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:26:02,154 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:26:02,156 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:26:02,157 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:26:02,159 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:26:02,160 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:26:02,161 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:26:02,163 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:26:02,164 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:26:02,165 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:26:02,166 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:26:02,167 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:26:02,168 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:26:02,169 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:26:02,170 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:26:02,171 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:26:02,172 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:26:02,174 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:26:02,175 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:26:02,177 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:26:02,178 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:26:02,179 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:26:02,181 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:26:02,182 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:26:02,183 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:26:02,184 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:26:02,185 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:26:02,186 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:26:02,187 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:26:02,188 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:26:02,189 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:26:02,190 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:26:02,192 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:26:02,193 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:26:02,194 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:26:02,195 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:26:02,197 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:26:02,198 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:26:02,199 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:26:02,200 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:26:02,201 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:26:02,202 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:26:02,203 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:26:02,204 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:26:02,205 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:26:02,206 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:26:02,207 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:27:24,710 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:27:24,712 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:27:24,714 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:27:24,715 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:27:24,718 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:27:24,720 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:27:24,722 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:27:24,724 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:27:24,726 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:27:24,727 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:27:24,729 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:27:24,731 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:27:24,746 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:27:24,748 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:27:24,757 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:27:24,759 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:27:24,761 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:27:24,764 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:27:24,766 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:27:24,770 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:27:24,772 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:27:24,774 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:27:24,776 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:27:24,777 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:27:24,779 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:27:24,782 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:27:24,784 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:27:24,786 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:27:24,788 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:27:24,789 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:27:24,791 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:27:24,793 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:27:24,794 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:27:24,796 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:27:24,798 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:27:24,800 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:27:24,802 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:27:24,804 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:27:24,806 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:27:24,808 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:27:24,809 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:27:24,811 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:27:24,813 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:27:24,815 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:27:24,817 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:27:24,820 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:27:24,822 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:27:24,824 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:27:24,825 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:27:24,827 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:27:24,975 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:27:24,977 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:27:24,979 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:27:24,980 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:27:24,982 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:27:24,985 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:27:24,987 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:27:24,989 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:27:24,991 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:27:24,993 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:27:24,994 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:27:24,997 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:27:24,998 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:27:25,003 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:27:25,005 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:27:25,009 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:27:25,010 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:27:25,012 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:27:25,014 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:27:25,016 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:27:25,020 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:27:25,022 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:27:25,024 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:27:25,026 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:27:25,029 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:27:25,032 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:27:25,033 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:27:25,037 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:27:25,039 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:27:25,040 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:27:25,042 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:27:25,044 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:27:25,046 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:27:25,047 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:27:25,049 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:27:25,052 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:27:25,055 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:27:25,057 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:27:25,059 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:27:25,061 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:27:25,063 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:27:25,065 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:27:25,068 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:27:25,071 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:27:25,074 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:27:25,076 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:27:25,078 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:27:25,082 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:27:25,085 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:27:25,088 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:27:25,107 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\agents_ia\comandAI\assets\20250414162725_clipboard_20250414162547.png, Prompt: Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"N√£o foram encontrados arquivos de √°udio no diret√≥rio {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcri√ß√£o salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diret√≥rio atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Defini√ß√£o dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Cria√ß√£o dos diret√≥rios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configura√ß√£o do layout do Streamlit
st.set_page_config(
    page_title="An√°lise de A√ß√µes",
    layout="wide",
    page_icon="üìà"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscila√ß√µes

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Per√≠odo', 'Oscila√ß√£o'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("üìä Dashboard de An√°lise de A√ß√µes")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma a√ß√£o", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"üìå Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("üîç Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descri√ß√£o', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa a√ß√£o.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("üí∞ Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("üìâ Oscila√ß√µes")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("üìà Tabela Geral de A√ß√µes")
        st.dataframe(self.acoes)

# Execu√ß√£o
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para an√°lise fundamentalista de a√ß√µes da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se est√° correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configura√ß√£o de v√≠deos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configura√ß√£o de √°udios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcri√ß√µes e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diret√≥rio
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diret√≥rio.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do √∫ltimo arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conte√∫do do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimens√µes do embedding:", embedding.shape)

# Caminho do diret√≥rio de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o √∫ltimo embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configura√ß√£o de transcri√ß√µes e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diret√≥rio onde est√° procurando as transcri√ß√µes
        print(f"Diret√≥rio de entrada das transcri√ß√µes: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcri√ß√£o no diret√≥rio de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcri√ß√£o encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo n√£o encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcri√ß√£o do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} est√° vazio.")
                    return

            # Gerar o embedding da transcri√ß√£o
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcri√ß√£o e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O v√≠deo {video_path} n√£o cont√©m √°udio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de v√≠deo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de v√≠deo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de v√≠deo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Convers√£o de v√≠deo para √°udio conclu√≠da!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configura√ß√µes da API
handler = genai('gemini-1.5-flash')

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Fun√ß√£o para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conte√∫do com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares √∫nicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcri√ß√£o obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' n√£o √© uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuni√µes: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de reuni√µes: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|t√≥picos da|pontos (relevantes|principais)|an√°lise)\b.*\b(reuni√£o|√∫ltima (reuni√£o|conversa|sess√£o))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como an√°lise de reuni√£o.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padr√£o.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que s√£o muito similares ao comando atual para evitar redund√¢ncia.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padr√£o: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # M√©todos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de an√°lise de reuni√£o.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conte√∫do usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inv√°lida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar vari√°veis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conte√∫do com base no prompt, contexto e reuni√£o"""
        try:
            # Supondo que a API espera um dicion√°rio com os par√¢metros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisi√ß√£o para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar vari√°veis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Cria√ß√£o da conclus√£o do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que voc√™ acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["fa√ßa um resumo da √∫ltima reuni√£o.", "t√≥picos da √∫ltima reuni√£o", "resuma a √∫ltima reuni√£o", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Voc√™ √© respons√°vel por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - N√£o acrescentar t√≠tulo nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informa√ß√µes acima e forne√ßa uma solu√ß√£o detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Voc√™ √© respons√°vel analisar com detalhes a reuni√£o de [str_meeting], e fornecer uma longa est√≥ria sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precis√£o da resposta.
            - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e fa√ßa uma pesquisa online para uma resposta mais precisa das quest√µes de [str_text]
        - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [context]: Regras B√°sicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("√Åudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum √°udio detectado.")
                return None
            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configura√ß√µes da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicion√°rio
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta n√£o est√° no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API: {e}")
        return None

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: Fa√ßa uma limpeza na pasta class_voice_assistent
Assistant: Com base nos arquivos fornecidos, aqui est√° um resumo da estrutura e funcionalidade do c√≥digo, juntamente com as informa√ß√µes sobre a limpeza da pasta `class_voice_assistent`:

**Vis√£o geral do projeto:**

Este projeto parece ser um assistente de voz que usa o Google Gemini para processamento de linguagem natural. Ele √© estruturado em v√°rios m√≥dulos e classes para lidar com diferentes aspectos, como captura de √°udio, convers√£o de texto em fala, interpreta√ß√£o de comandos e intera√ß√£o com APIs externas.

**Principais componentes:**

*   **Captura de comandos:**
    *   `voice_assistent\class_voice_assistent\voice_command_hendler.py`: Lida com a captura de comandos de voz usando a biblioteca `speech_recognition`.
    *   `voice_assistent\class_voice_assistent\text_command_hendler.py`: Lida com a captura de comandos de texto do usu√°rio.

*   **Processamento de linguagem natural:**
    *   `voice_assistent\class_voice_assistent\command_interpreter.py`: Interpreta os comandos do usu√°rio e determina a a√ß√£o apropriada a ser executada.
    *   `voice_assistent\class_voice_assistent\question_answers_service.py`: Converte texto em embeddings para encontrar respostas semelhantes.
    *   `voice_assistent\class_voice_assistent\context_manager.py`: Gerencia o contexto da conversa, armazenando as intera√ß√µes recentes.

*   **Comunica√ß√£o com o Google Gemini:**
    *   `voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py`: Lida com a comunica√ß√£o com a API do Google Gemini.
    *   `voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py`: Define o modelo generativo e a configura√ß√£o da API Google Gemini.

*   **Gera√ß√£o de prompt:**
    *   `voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py`: Gera prompts padr√£o para o modelo de linguagem.
    *   `voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py`: Gera prompts para an√°lise de reuni√µes.
    *   `voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py`: Gera prompts para pesquisas online.
    *   `voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py`: Define a classe abstrata para geradores de prompt.

*   **Texto para fala:**
    *   `voice_assistent\class_voice_assistent\text_to_speech.py`: Converte texto em fala usando a biblioteca `pyttsx3`.

*   **Processamento de texto:**
    *   `voice_assistent\class_voice_assistent\text_processor.py`: Fornece fun√ß√µes para extrair valores de JSON e texto de HTML.

*   **Interface principal:**
    *   `voice_assistent\class_voice_assistent\main.py`: Inicializa e coordena os diferentes componentes do assistente de voz.

**Limpeza da pasta `class_voice_assistent`:**

O comando para limpar uma pasta n√£o faz parte diretamente do c√≥digo fornecido. Para implementar essa funcionalidade, voc√™ precisaria adicionar um c√≥digo que use a biblioteca `os` ou `shutil` para remover arquivos e subdiret√≥rios da pasta `class_voice_assistent`.

**Exemplo de como implementar a limpeza da pasta:**

```python
import os
import shutil

def clean_directory(path):
    """Remove todos os arquivos e subdiret√≥rios em um diret√≥rio."""
    for filename in os.listdir(path):
        file_path = os.path.join(path, filename)
        try:
            if os.path.isfile(file_path) or os.path.islink(file_path):
                os.unlink(file_path)
            elif os.path.isdir(file_path):
                shutil.rmtree(file_path)
        except Exception as e:
            print(f'Falha ao excluir {file_path}. Motivo: {e}')

# Caminho para a pasta class_voice_assistent
path_to_clean = 'voice_assistent/class_voice_assistent'

# Chame a fun√ß√£o para limpar a pasta
clean_directory(path_to_clean)
```

**Considera√ß√µes:**

1.  **Importe os m√≥dulos necess√°rios**: `os` e `shutil`.
2.  **Defina a fun√ß√£o `clean_directory`**: Esta fun√ß√£o itera sobre todos os itens no diret√≥rio fornecido e tenta remover cada um.
3.  **Tratamento de erros**: A fun√ß√£o inclui um bloco `try...except` para lidar com poss√≠veis erros durante a remo√ß√£o de arquivos ou diret√≥rios.
4.  **Chame a fun√ß√£o `clean_directory`**: Passe o caminho do diret√≥rio que voc√™ deseja limpar.

**Onde adicionar este c√≥digo:**

Voc√™ pode integrar esta fun√ß√£o dentro do `command_interpreter.py` ou `main.py`. Por exemplo, dentro do `interpret_command`, voc√™ pode adicionar uma condi√ß√£o para detectar o comando "limpar pasta" e, em seguida, chamar a fun√ß√£o `clean_directory` com o caminho apropriado.

**Aviso**: Esta opera√ß√£o √© irrevers√≠vel e excluir√° permanentemente todos os arquivos e pastas dentro do diret√≥rio especificado. Certifique-se de ter um backup se necess√°rio.

**Seguran√ßa:**

Certifique-se de que o caminho da pasta a ser limpa seja validado e seguro para evitar exclus√µes acidentais ou maliciosas.

User: quero remover c√≥digos desnecess√°rios na pasta voice_assistent no projeto.
Assistant: Assistant: Para remover c√≥digos desnecess√°rios na pasta `voice_assistent` do projeto, aqui est√° uma abordagem detalhada:

**1. Vis√£o Geral:**

O objetivo √© identificar e remover arquivos e funcionalidades redundantes ou n√£o utilizados na pasta `voice_assistent`. Isso envolve uma an√°lise cuidadosa do c√≥digo, das depend√™ncias e do fluxo do programa para garantir que nenhuma funcionalidade essencial seja comprometida.

**2. Etapas Detalhadas:**

*   **Backup:**
    *   Antes de fazer qualquer altera√ß√£o, fa√ßa um backup completo da pasta `voice_assistent`. Isso permitir√° restaurar o estado anterior em caso de remo√ß√£o acidental de c√≥digo importante.

*   **An√°lise da Estrutura do C√≥digo:**
    *   **Identifique os principais m√≥dulos:** Analise todos os arquivos Python na pasta `voice_assistent` e suas subpastas para entender sua fun√ß√£o e como eles se interconectam.
    *   **M√≥dulos Essenciais:**
        *   `class_voice_assistent/main.py`: Coordena a opera√ß√£o do assistente de voz.
        *   `class_voice_assistent/command_interpreter.py`: Interpreta comandos do usu√°rio.
        *   `class_voice_assistent/gpt_communication/gemini_gpt.py`: Lida com a comunica√ß√£o com o Google Gemini.
        *   `class_voice_assistent/voice_command_handler.py` e `class_voice_assistent/text_command_handler.py`: Capturam comandos de voz e texto, respectivamente.
        *   `class_voice_assistent/text_to_speech.py`: Converte texto em fala.

*   **Identifica√ß√£o de C√≥digo N√£o Utilizado:**
    *   **Remova imports n√£o utilizados:** Abra cada arquivo e verifique se todos os m√≥dulos importados est√£o realmente sendo usados. Remova aqueles que n√£o est√£o.

    *   **Analise fun√ß√µes n√£o utilizadas:** Verifique se todas as fun√ß√µes definidas s√£o chamadas em algum lugar do c√≥digo. Fun√ß√µes que n√£o s√£o chamadas podem ser removidas com seguran√ßa.
    *   **Avalie c√≥digo comentado:** Avalie se os blocos de c√≥digo comentados ainda s√£o relevantes ou podem ser removidos.

*   **An√°lise das Depend√™ncias:**
    *   **Arquivo requirements.txt ou pyproject.toml:** Verifique se todas as bibliotecas listadas nos arquivos de depend√™ncia s√£o necess√°rias para o funcionamento do assistente de voz. Remova as depend√™ncias que n√£o est√£o sendo usadas.

*   **Simplifica√ß√£o da L√≥gica:**
    *   **Refatore c√≥digo duplicado:** Se houver trechos de c√≥digo repetidos, crie fun√ß√µes ou classes para reutiliz√°-los, reduzindo a redund√¢ncia.
    *   **Simplifique fun√ß√µes complexas:** Divida fun√ß√µes muito longas ou complexas em fun√ß√µes menores e mais f√°ceis de entender e manter.

*   **Remo√ß√£o de C√≥digo Desnecess√°rio:**
    *   **Exclua arquivos desnecess√°rios:** Remova arquivos que n√£o contribuem para a funcionalidade principal do assistente de voz.

*   **Teste:**
    *   Ap√≥s cada remo√ß√£o ou modifica√ß√£o significativa, execute os testes para garantir que o assistente de voz continue funcionando corretamente.
    *   Crie testes automatizados para os principais componentes para facilitar a detec√ß√£o de erros ap√≥s futuras altera√ß√µes.

**3. Implementa√ß√£o:**

*   **Edite os Arquivos:**
    *   Abra cada arquivo Python relevante e fa√ßa as modifica√ß√µes necess√°rias, como remover imports n√£o utilizados, fun√ß√µes desnecess√°rias e refatorar o c√≥digo.
*   **Remova Arquivos Desnecess√°rios:**
    *   Exclua arquivos que n√£o s√£o mais necess√°rios. Certifique-se de que nenhum outro componente depende desses arquivos antes de remov√™-los.

**4. Exemplo Pr√°tico:**

*   **Avalia√ß√£o de `template.py`:**
    *   O arquivo `template.py` parece ser uma vers√£o mais antiga ou um modelo inicial do assistente de voz. Compare-o com os outros arquivos (principalmente os arquivos em `class_voice_assistent`) para identificar se alguma parte do c√≥digo em `template.py` ainda √© usada. Se n√£o for, considere remover este arquivo.

*   **Exemplo de refatora√ß√£o:**
    *   Se voc√™ encontrar c√≥digo duplicado entre `voice_command_hendler.py` e `text_command_hendler.py`, voc√™ pode criar uma classe base ou fun√ß√£o utilit√°ria para lidar com a l√≥gica comum, reduzindo a duplica√ß√£o.

**5. Exemplo de C√≥digo para Remover Arquivos:**

Para remover um arquivo espec√≠fico, voc√™ pode usar o seguinte c√≥digo:

```python
import os

file_path = "voice_assistent/class_voice_assistent/arquivo_desnecessario.py"
try:
    os.remove(file_path)
    print(f"Arquivo {file_path} removido com sucesso.")
except FileNotFoundError:
    print(f"Arquivo {file_path} n√£o encontrado.")
except Exception as e:
    print(f"Erro ao remover {file_path}: {e}")
```

**Considera√ß√µes Adicionais:**

*   **Documenta√ß√£o:** Atualize a documenta√ß√£o do projeto para refletir as mudan√ßas realizadas. Isso ajudar√° outros desenvolvedores (ou voc√™ mesmo no futuro) a entender a nova estrutura do c√≥digo.
*   **Versionamento:** Use um sistema de controle de vers√£o (como Git) para gerenciar as altera√ß√µes no c√≥digo. Isso facilita a revers√£o de altera√ß√µes e a colabora√ß√£o com outros desenvolvedores.

Seguindo essas etapas, voc√™ poder√° remover c√≥digos desnecess√°rios da pasta `voice_assistent`, tornando o projeto mais limpo, eficiente e f√°cil de manter.
User: voc√™ tem o c√≥digo no contexto. voc√™ √© meu assistente e deve fazer a an√°lise para mim.
Assistant: Com prazer! Analisei o c√≥digo que voc√™ forneceu e identifiquei algumas √°reas onde podemos remover c√≥digo desnecess√°rio e refatorar para melhor clareza e efici√™ncia.

**1. An√°lise Detalhada e Sugest√µes:**

*   **`template.py`:**

    *   **Status:** Este arquivo parece ser uma vers√£o anterior ou um modelo desatualizado do assistente de voz.
    *   **A√ß√£o:** Comparar o c√≥digo deste arquivo com os componentes atuais em `class_voice_assistent`. Se o c√≥digo n√£o for mais utilizado, **remover o arquivo `template.py`**.

*   **`voice_assistent\config.py`:**

    *   **Status:** Este arquivo cont√©m configura√ß√µes b√°sicas, mas algumas dessas configura√ß√µes podem estar duplicadas ou n√£o sendo usadas.
    *   **A√ß√£o:**
        *   **apiKey e url:** Verificar se essas vari√°veis ainda s√£o necess√°rias e se n√£o est√£o duplicadas em outro lugar (por exemplo, em um arquivo `.env`).
        *   **engine, recent_context, nlp:** Estes itens s√£o inicializados novamente em outros arquivos. Avaliar se √© melhor centralizar a inicializa√ß√£o em um √∫nico local (possivelmente em `class_voice_assistent/main.py`) e importar as inst√¢ncias onde necess√°rio.

*   **`class_voice_assistent\conversation_history.py`:**

    *   **Status:** Este arquivo est√° vazio.
    *   **A√ß√£o:** **Remover este arquivo** se ele n√£o for usado. Se a inten√ß√£o era armazenar o hist√≥rico de conversas, a funcionalidade deve ser movida para um arquivo relevante ou integrada em `class_voice_assistent\context_manager.py`.

*   **`class_voice_assistent\gpt_communication\groq._gpt.py`:**

    *   **Status:** Este arquivo demonstra o uso da API Groq, mas n√£o parece estar integrado ao resto do c√≥digo.
    *   **A√ß√£o:** Se a API Groq n√£o for utilizada, **remover este arquivo**. Caso contr√°rio, integrar corretamente e remover a duplica√ß√£o de configura√ß√µes e inicializa√ß√µes.

*   **`class_voice_assistent\prompt_generator\prompt_generator.py`:**

    *   **Status:** Defini√ß√£o de uma classe abstrata.
    *   **A√ß√£o:** Verificar se a classe abstrata `PromptGenerator` √© usada corretamente pelas classes filhas. Se as classes filhas n√£o implementam os m√©todos abstratos corretamente, refatorar ou remover a classe abstrata.

*   **M√≥dulos de Comando (`voice_command_hendler.py` e `text_command_hendler.py`):**

    *   **Status:** Lidam com a captura de comandos.
    *   **A√ß√£o:** Verificar se h√° duplica√ß√£o de c√≥digo entre esses dois m√≥dulos (por exemplo, tratamento de erros, prompts ao usu√°rio). Se houver, criar uma classe base ou fun√ß√£o utilit√°ria para lidar com a l√≥gica comum.

*   **Remo√ß√£o de c√≥digo n√£o utilizado:**

    *   Utilize um linter (como `flake8`) para identificar imports n√£o utilizados e c√≥digo n√£o referenciado nos arquivos.

**2. Plano de A√ß√£o Detalhado:**

1.  **Backup:** Criar um backup da pasta `voice_assistent`.
2.  **Remover `template.py`:**
    ```bash
    rm voice_assistent/template.py
    ```
3.  **Remover `class_voice_assistent\conversation_history.py`:**
    ```bash
    rm voice_assistent/class_voice_assistent/conversation_history.py
    ```
4.  **Remover `class_voice_assistent\gpt_communication\groq._gpt.py`:**
    ```bash
    rm voice_assistent/class_voice_assistent/gpt_communication/groq._gpt.py
    ```
5.  **Refatorar Configura√ß√µes:**
    *   Mover as configura√ß√µes essenciais de `voice_assistent\config.py` para um arquivo `.env` ou para `class_voice_assistent\main.py`.
    *   Remover o arquivo `voice_assistent\config.py` se ele n√£o for mais necess√°rio.
6.  **Refatorar M√≥dulos de Comando:**
    *   Criar uma classe base ou fun√ß√£o utilit√°ria para consolidar a l√≥gica comum em `voice_command_hendler.py` e `text_command_hendler.py`.
7.  **Executar Linter:**
    ```bash
    pip install flake8
    flake8 voice_assistent
    ```
    *   Remover imports n√£o utilizados e c√≥digo n√£o referenciado.
8.  **Testar:**
    *   Ap√≥s cada passo, testar o assistente de voz para garantir que ele continue funcionando corretamente.
9.  **Documentar:**
    *   Atualizar a documenta√ß√£o do projeto para refletir as mudan√ßas realizadas.

**3. Implementa√ß√£o:**

Remover c√≥digo n√£o utilizado

```python
# Exemplo de como remover um arquivo
import os

file_path = "voice_assistent/class_voice_assistent/gpt_communication/groq._gpt.py"
try:
    os.remove(file_path)
    print(f"Arquivo {file_path} removido com sucesso.")
except FileNotFoundError:
    print(f"Arquivo {file_path} n√£o encontrado.")
except Exception as e:
    print(f"Erro ao remover {file_path}: {e}")
```

**Centralizar configura√ß√µes**

```python
# Exemplo de como centralizar configura√ß√µes em main.py
import os
from dotenv import load_dotenv

load_dotenv()

api_key = os.getenv("API_KEY")
# ... outras configura√ß√µes
```

**4. Considera√ß√µes Adicionais:**

*   **Testes:** Criar testes unit√°rios para garantir a estabilidade dos componentes principais.
*   **Versionamento:** Usar Git para controlar as altera√ß√µes e facilitar a revers√£o, se necess√°rio.
*   **Seguran√ßa:** Validar o caminho da pasta para evitar exclus√µes acidentais.

Com essas a√ß√µes, a pasta `voice_assistent` ficar√° mais organizada, eficiente e f√°cil de manter. Se precisar de ajuda com algum passo espec√≠fico ou detalhe adicional, √© s√≥ me avisar!

User: Preciso importar essas libs 
Assistant: ```python
import os
from dotenv import load_dotenv
import os
import shutil
import pyttsx3
import spacy
import speech_recognition as sr
import requests
import re
from collections import deque
from bs4 import BeautifulSoup
import google.generativeai as genai
```

Essas s√£o as bibliotecas que foram mencionadas ao longo da conversa.

Aqui est√° um exemplo pr√°tico de como importar essas bibliotecas no in√≠cio de um arquivo Python:

```python
# Importa√ß√µes
import os
from dotenv import load_dotenv
import shutil
import pyttsx3
import spacy
import speech_recognition as sr
import requests
import re
from collections import deque
from bs4 import BeautifulSoup
import google.generativeai as genai

# Seu c√≥digo continua aqui...
```

Essas importa√ß√µes fornecer√£o acesso √†s funcionalidades de cada biblioteca ao longo do seu c√≥digo.

Al√©m disso, aqui est√° uma breve descri√ß√£o de cada biblioteca, juntamente com um link para a documenta√ß√£o, para que voc√™ possa aprender mais sobre como utiliz√°-las de forma eficaz:

-   **os:** Fornece uma maneira de usar funcionalidades dependentes do sistema operacional (documenta√ß√£o: <https://docs.python.org/3/library/os.html>)
-   **dotenv:** Permite carregar vari√°veis de ambiente de um arquivo `.env` (documenta√ß√£o: <https://pypi.org/project/python-dotenv/>)
-   **shutil:** Oferece um conjunto de opera√ß√µes de alto n√≠vel em arquivos e cole√ß√µes de arquivos, como copiar e remover arquivos e diret√≥rios (documenta√ß√£o: <https://docs.python.org/3/library/shutil.html>)
-   **pyttsx3:** Biblioteca de convers√£o de texto em fala que funciona offline (documenta√ß√£o: <https://pyttsx3.readthedocs.io/en/latest/>)
-   **spacy:** Uma biblioteca de processamento de linguagem natural avan√ßada (documenta√ß√£o: <https://spacy.io/>)
-   **speech\_recognition:** Biblioteca para reconhecimento de fala, suportando v√°rias APIs (documenta√ß√£o: <https://pypi.org/project/SpeechRecognition/>)
-   **requests:** Biblioteca para fazer requisi√ß√µes HTTP (documenta√ß√£o: <https://requests.readthedocs.io/en/latest/>)
-   **re:** Fornece opera√ß√µes de express√£o regular (documenta√ß√£o: <https://docs.python.org/3/library/re.html>)
-   **collections.deque:** Uma generaliza√ß√£o da pilha e da fila que suporta anexar e remover de qualquer extremidade (documenta√ß√£o: <https://docs.python.org/3/library/collections.html#collections.deque>)
-   **bs4 (Beautiful Soup):** Biblioteca para extrair informa√ß√µes de p√°ginas web em formato HTML ou XML (documenta√ß√£o: <https://www.crummy.com/software/BeautifulSoup/bs4/doc/>)
-   **google.generativeai:** Biblioteca para usar os modelos de linguagem generativa do Google (documenta√ß√£o: verificar a documenta√ß√£o espec√≠fica para os modelos do Google Gemini no Google Cloud).

Com essas importa√ß√µes e a documenta√ß√£o correspondente, voc√™ est√° bem preparado para continuar sua an√°lise e refatora√ß√£o do c√≥digo!
User: como fa√ßo para instalar as que est√£o dando erro na imagem ?
2025-04-14 16:27:29,472 [INFO] Resposta da IA (imagem): Com base na imagem que voc√™ forneceu, as bibliotecas que precisam ser instaladas s√£o:

*   **speech\_recognition:** Para reconhecimento de fala.
*   **pyttsx3:** Para s√≠ntese de fala (convers√£o de texto em voz).
*   **spacy:** Para processamento de linguagem natural.
*   **google.generativeai:** Para interagir com os modelos generativos do Google.

Aqui est√£o os passos para instalar essas bibliotecas usando `pip`:

**1. Abra o Terminal ou Prompt de Comando:**

*   No Windows, voc√™ pode usar o "Prompt de Comando" ou o "PowerShell".
*   No macOS ou Linux, use o "Terminal".

**2. Instale as Bibliotecas:**

Use o comando `pip install` seguido pelo nome das bibliotecas. Execute os seguintes comandos um por um para instalar cada biblioteca:

```bash
pip install SpeechRecognition
pip install pyttsx3
pip install spacy
pip install google-generativeai
```

**3. Instale o Modelo de Linguagem do SpaCy:**

Para usar a biblioteca SpaCy corretamente, voc√™ precisa baixar um modelo de linguagem. Para portugu√™s (pt), voc√™ pode instalar o modelo `pt_core_news_sm`:

```bash
python -m spacy download pt_core_news_sm
```

**4. Verificar a Instala√ß√£o:**

Para verificar se as bibliotecas foram instaladas corretamente, voc√™ pode executar o seguinte c√≥digo Python em um interpretador Python ou em um arquivo `.py`:

```python
import speech_recognition as sr
import pyttsx3
import spacy
import google.generativeai as genai

print("SpeechRecognition vers√£o:", sr.__version__)
print("pyttsx3 ok")
print("Spacy vers√£o:", spacy.__version__)
print("google-generativeai ok")
```

Se n√£o houver erros ao executar este c√≥digo, significa que as bibliotecas foram instaladas corretamente.

**Informa√ß√µes Adicionais:**

*   **Se voc√™ estiver usando um ambiente virtual (`venv`):** Certifique-se de ativar o ambiente virtual antes de instalar as bibliotecas. Isso garante que as bibliotecas sejam instaladas apenas dentro do ambiente virtual e n√£o no sistema global.
*   **Problemas de Permiss√£o:** Se voc√™ encontrar problemas de permiss√£o ao instalar as bibliotecas, tente executar o comando com privil√©gios de administrador (por exemplo, use `sudo pip install` no macOS/Linux).
*   **Vers√µes das Bibliotecas:** Em alguns casos, vers√µes mais recentes das bibliotecas podem ter problemas de compatibilidade com o seu c√≥digo. Se voc√™ tiver problemas, pode tentar instalar vers√µes mais antigas das bibliotecas.

Com estes passos, voc√™ dever√° conseguir instalar as bibliotecas que est√£o faltando e resolver os erros no seu c√≥digo!
2025-04-14 16:27:29,592 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:27:29,594 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:27:29,595 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:27:29,597 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:27:29,599 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:27:29,600 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:27:29,602 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:27:29,604 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:27:29,606 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:27:29,608 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:27:29,609 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:27:29,610 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:27:29,611 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:27:29,613 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:27:29,614 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:27:29,616 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:27:29,617 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:27:29,619 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:27:29,621 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:27:29,623 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:27:29,624 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:27:29,625 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:27:29,627 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:27:29,628 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:27:29,630 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:27:29,631 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:27:29,632 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:27:29,634 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:27:29,636 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:27:29,638 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:27:29,639 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:27:29,641 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:27:29,642 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:27:29,643 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:27:29,645 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:27:29,646 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:27:29,647 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:27:29,649 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:27:29,650 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:27:29,653 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:27:29,656 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:27:29,657 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:27:29,659 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:27:29,661 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:27:29,662 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:27:29,664 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:27:29,665 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:27:29,667 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:27:29,669 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:27:29,671 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:31:47,337 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:31:47,338 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:31:47,340 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:31:47,342 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:31:47,345 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:31:47,347 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:31:47,349 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:31:47,350 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:31:47,352 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:31:47,353 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:31:47,355 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:31:47,357 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:31:47,359 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:31:47,360 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:31:47,362 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:31:47,364 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:31:47,365 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:31:47,366 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:31:47,368 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:31:47,369 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:31:47,371 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:31:47,372 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:31:47,374 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:31:47,375 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:31:47,377 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:31:47,380 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:31:47,381 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:31:47,383 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:31:47,385 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:31:47,387 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:31:47,388 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:31:47,390 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:31:47,391 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:31:47,393 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:31:47,395 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:31:47,396 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:31:47,398 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:31:47,399 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:31:47,400 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:31:47,402 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:31:47,403 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:31:47,405 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:31:47,406 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:31:47,408 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:31:47,410 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:31:47,412 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:31:47,414 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:31:47,415 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:31:47,416 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:31:47,417 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:31:47,554 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:31:47,556 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:31:47,557 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:31:47,559 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:31:47,560 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:31:47,562 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:31:47,564 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:31:47,565 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:31:47,567 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:31:47,568 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:31:47,570 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:31:47,571 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:31:47,573 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:31:47,574 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:31:47,577 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:31:47,579 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:31:47,580 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:31:47,582 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:31:47,584 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:31:47,585 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:31:47,587 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:31:47,588 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:31:47,590 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:31:47,591 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:31:47,593 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:31:47,595 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:31:47,597 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:31:47,599 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:31:47,600 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:31:47,602 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:31:47,603 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:31:47,605 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:31:47,606 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:31:47,607 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:31:47,609 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:31:47,610 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:31:47,612 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:31:47,614 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:31:47,615 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:31:47,616 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:31:47,617 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:31:47,619 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:31:47,620 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:31:47,621 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:31:47,623 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:31:47,624 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:31:47,626 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:31:47,627 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:31:47,629 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:31:47,630 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:31:47,650 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\agents_ia\comandAI\assets\20250414163147_clipboard_20250414162547.png, Prompt: Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"N√£o foram encontrados arquivos de √°udio no diret√≥rio {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcri√ß√£o salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diret√≥rio atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Defini√ß√£o dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Cria√ß√£o dos diret√≥rios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configura√ß√£o do layout do Streamlit
st.set_page_config(
    page_title="An√°lise de A√ß√µes",
    layout="wide",
    page_icon="üìà"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscila√ß√µes

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Per√≠odo', 'Oscila√ß√£o'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("üìä Dashboard de An√°lise de A√ß√µes")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma a√ß√£o", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"üìå Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("üîç Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descri√ß√£o', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa a√ß√£o.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("üí∞ Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("üìâ Oscila√ß√µes")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("üìà Tabela Geral de A√ß√µes")
        st.dataframe(self.acoes)

# Execu√ß√£o
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para an√°lise fundamentalista de a√ß√µes da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se est√° correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configura√ß√£o de v√≠deos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configura√ß√£o de √°udios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcri√ß√µes e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diret√≥rio
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diret√≥rio.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do √∫ltimo arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conte√∫do do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimens√µes do embedding:", embedding.shape)

# Caminho do diret√≥rio de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o √∫ltimo embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configura√ß√£o de transcri√ß√µes e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diret√≥rio onde est√° procurando as transcri√ß√µes
        print(f"Diret√≥rio de entrada das transcri√ß√µes: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcri√ß√£o no diret√≥rio de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcri√ß√£o encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo n√£o encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcri√ß√£o do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} est√° vazio.")
                    return

            # Gerar o embedding da transcri√ß√£o
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcri√ß√£o e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O v√≠deo {video_path} n√£o cont√©m √°udio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de v√≠deo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de v√≠deo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de v√≠deo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Convers√£o de v√≠deo para √°udio conclu√≠da!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configura√ß√µes da API
handler = genai('gemini-1.5-flash')

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Fun√ß√£o para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conte√∫do com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares √∫nicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcri√ß√£o obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' n√£o √© uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuni√µes: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de reuni√µes: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|t√≥picos da|pontos (relevantes|principais)|an√°lise)\b.*\b(reuni√£o|√∫ltima (reuni√£o|conversa|sess√£o))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como an√°lise de reuni√£o.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padr√£o.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que s√£o muito similares ao comando atual para evitar redund√¢ncia.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padr√£o: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # M√©todos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de an√°lise de reuni√£o.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conte√∫do usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inv√°lida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar vari√°veis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conte√∫do com base no prompt, contexto e reuni√£o"""
        try:
            # Supondo que a API espera um dicion√°rio com os par√¢metros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisi√ß√£o para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar vari√°veis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Cria√ß√£o da conclus√£o do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que voc√™ acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["fa√ßa um resumo da √∫ltima reuni√£o.", "t√≥picos da √∫ltima reuni√£o", "resuma a √∫ltima reuni√£o", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Voc√™ √© respons√°vel por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - N√£o acrescentar t√≠tulo nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informa√ß√µes acima e forne√ßa uma solu√ß√£o detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Voc√™ √© respons√°vel analisar com detalhes a reuni√£o de [str_meeting], e fornecer uma longa est√≥ria sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precis√£o da resposta.
            - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e fa√ßa uma pesquisa online para uma resposta mais precisa das quest√µes de [str_text]
        - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [context]: Regras B√°sicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("√Åudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum √°udio detectado.")
                return None
            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configura√ß√µes da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicion√°rio
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta n√£o est√° no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API: {e}")
        return None

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: Fa√ßa uma limpeza na pasta class_voice_assistent
Assistant: Com base nos arquivos fornecidos, aqui est√° um resumo da estrutura e funcionalidade do c√≥digo, juntamente com as informa√ß√µes sobre a limpeza da pasta `class_voice_assistent`:

**Vis√£o geral do projeto:**

Este projeto parece ser um assistente de voz que usa o Google Gemini para processamento de linguagem natural. Ele √© estruturado em v√°rios m√≥dulos e classes para lidar com diferentes aspectos, como captura de √°udio, convers√£o de texto em fala, interpreta√ß√£o de comandos e intera√ß√£o com APIs externas.

**Principais componentes:**

*   **Captura de comandos:**
    *   `voice_assistent\class_voice_assistent\voice_command_hendler.py`: Lida com a captura de comandos de voz usando a biblioteca `speech_recognition`.
    *   `voice_assistent\class_voice_assistent\text_command_hendler.py`: Lida com a captura de comandos de texto do usu√°rio.

*   **Processamento de linguagem natural:**
    *   `voice_assistent\class_voice_assistent\command_interpreter.py`: Interpreta os comandos do usu√°rio e determina a a√ß√£o apropriada a ser executada.
    *   `voice_assistent\class_voice_assistent\question_answers_service.py`: Converte texto em embeddings para encontrar respostas semelhantes.
    *   `voice_assistent\class_voice_assistent\context_manager.py`: Gerencia o contexto da conversa, armazenando as intera√ß√µes recentes.

*   **Comunica√ß√£o com o Google Gemini:**
    *   `voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py`: Lida com a comunica√ß√£o com a API do Google Gemini.
    *   `voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py`: Define o modelo generativo e a configura√ß√£o da API Google Gemini.

*   **Gera√ß√£o de prompt:**
    *   `voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py`: Gera prompts padr√£o para o modelo de linguagem.
    *   `voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py`: Gera prompts para an√°lise de reuni√µes.
    *   `voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py`: Gera prompts para pesquisas online.
    *   `voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py`: Define a classe abstrata para geradores de prompt.

*   **Texto para fala:**
    *   `voice_assistent\class_voice_assistent\text_to_speech.py`: Converte texto em fala usando a biblioteca `pyttsx3`.

*   **Processamento de texto:**
    *   `voice_assistent\class_voice_assistent\text_processor.py`: Fornece fun√ß√µes para extrair valores de JSON e texto de HTML.

*   **Interface principal:**
    *   `voice_assistent\class_voice_assistent\main.py`: Inicializa e coordena os diferentes componentes do assistente de voz.

**Limpeza da pasta `class_voice_assistent`:**

O comando para limpar uma pasta n√£o faz parte diretamente do c√≥digo fornecido. Para implementar essa funcionalidade, voc√™ precisaria adicionar um c√≥digo que use a biblioteca `os` ou `shutil` para remover arquivos e subdiret√≥rios da pasta `class_voice_assistent`.

**Exemplo de como implementar a limpeza da pasta:**

```python
import os
import shutil

def clean_directory(path):
    """Remove todos os arquivos e subdiret√≥rios em um diret√≥rio."""
    for filename in os.listdir(path):
        file_path = os.path.join(path, filename)
        try:
            if os.path.isfile(file_path) or os.path.islink(file_path):
                os.unlink(file_path)
            elif os.path.isdir(file_path):
                shutil.rmtree(file_path)
        except Exception as e:
            print(f'Falha ao excluir {file_path}. Motivo: {e}')

# Caminho para a pasta class_voice_assistent
path_to_clean = 'voice_assistent/class_voice_assistent'

# Chame a fun√ß√£o para limpar a pasta
clean_directory(path_to_clean)
```

**Considera√ß√µes:**

1.  **Importe os m√≥dulos necess√°rios**: `os` e `shutil`.
2.  **Defina a fun√ß√£o `clean_directory`**: Esta fun√ß√£o itera sobre todos os itens no diret√≥rio fornecido e tenta remover cada um.
3.  **Tratamento de erros**: A fun√ß√£o inclui um bloco `try...except` para lidar com poss√≠veis erros durante a remo√ß√£o de arquivos ou diret√≥rios.
4.  **Chame a fun√ß√£o `clean_directory`**: Passe o caminho do diret√≥rio que voc√™ deseja limpar.

**Onde adicionar este c√≥digo:**

Voc√™ pode integrar esta fun√ß√£o dentro do `command_interpreter.py` ou `main.py`. Por exemplo, dentro do `interpret_command`, voc√™ pode adicionar uma condi√ß√£o para detectar o comando "limpar pasta" e, em seguida, chamar a fun√ß√£o `clean_directory` com o caminho apropriado.

**Aviso**: Esta opera√ß√£o √© irrevers√≠vel e excluir√° permanentemente todos os arquivos e pastas dentro do diret√≥rio especificado. Certifique-se de ter um backup se necess√°rio.

**Seguran√ßa:**

Certifique-se de que o caminho da pasta a ser limpa seja validado e seguro para evitar exclus√µes acidentais ou maliciosas.

User: quero remover c√≥digos desnecess√°rios na pasta voice_assistent no projeto.
Assistant: Assistant: Para remover c√≥digos desnecess√°rios na pasta `voice_assistent` do projeto, aqui est√° uma abordagem detalhada:

**1. Vis√£o Geral:**

O objetivo √© identificar e remover arquivos e funcionalidades redundantes ou n√£o utilizados na pasta `voice_assistent`. Isso envolve uma an√°lise cuidadosa do c√≥digo, das depend√™ncias e do fluxo do programa para garantir que nenhuma funcionalidade essencial seja comprometida.

**2. Etapas Detalhadas:**

*   **Backup:**
    *   Antes de fazer qualquer altera√ß√£o, fa√ßa um backup completo da pasta `voice_assistent`. Isso permitir√° restaurar o estado anterior em caso de remo√ß√£o acidental de c√≥digo importante.

*   **An√°lise da Estrutura do C√≥digo:**
    *   **Identifique os principais m√≥dulos:** Analise todos os arquivos Python na pasta `voice_assistent` e suas subpastas para entender sua fun√ß√£o e como eles se interconectam.
    *   **M√≥dulos Essenciais:**
        *   `class_voice_assistent/main.py`: Coordena a opera√ß√£o do assistente de voz.
        *   `class_voice_assistent/command_interpreter.py`: Interpreta comandos do usu√°rio.
        *   `class_voice_assistent/gpt_communication/gemini_gpt.py`: Lida com a comunica√ß√£o com o Google Gemini.
        *   `class_voice_assistent/voice_command_handler.py` e `class_voice_assistent/text_command_handler.py`: Capturam comandos de voz e texto, respectivamente.
        *   `class_voice_assistent/text_to_speech.py`: Converte texto em fala.

*   **Identifica√ß√£o de C√≥digo N√£o Utilizado:**
    *   **Remova imports n√£o utilizados:** Abra cada arquivo e verifique se todos os m√≥dulos importados est√£o realmente sendo usados. Remova aqueles que n√£o est√£o.

    *   **Analise fun√ß√µes n√£o utilizadas:** Verifique se todas as fun√ß√µes definidas s√£o chamadas em algum lugar do c√≥digo. Fun√ß√µes que n√£o s√£o chamadas podem ser removidas com seguran√ßa.
    *   **Avalie c√≥digo comentado:** Avalie se os blocos de c√≥digo comentados ainda s√£o relevantes ou podem ser removidos.

*   **An√°lise das Depend√™ncias:**
    *   **Arquivo requirements.txt ou pyproject.toml:** Verifique se todas as bibliotecas listadas nos arquivos de depend√™ncia s√£o necess√°rias para o funcionamento do assistente de voz. Remova as depend√™ncias que n√£o est√£o sendo usadas.

*   **Simplifica√ß√£o da L√≥gica:**
    *   **Refatore c√≥digo duplicado:** Se houver trechos de c√≥digo repetidos, crie fun√ß√µes ou classes para reutiliz√°-los, reduzindo a redund√¢ncia.
    *   **Simplifique fun√ß√µes complexas:** Divida fun√ß√µes muito longas ou complexas em fun√ß√µes menores e mais f√°ceis de entender e manter.

*   **Remo√ß√£o de C√≥digo Desnecess√°rio:**
    *   **Exclua arquivos desnecess√°rios:** Remova arquivos que n√£o contribuem para a funcionalidade principal do assistente de voz.

*   **Teste:**
    *   Ap√≥s cada remo√ß√£o ou modifica√ß√£o significativa, execute os testes para garantir que o assistente de voz continue funcionando corretamente.
    *   Crie testes automatizados para os principais componentes para facilitar a detec√ß√£o de erros ap√≥s futuras altera√ß√µes.

**3. Implementa√ß√£o:**

*   **Edite os Arquivos:**
    *   Abra cada arquivo Python relevante e fa√ßa as modifica√ß√µes necess√°rias, como remover imports n√£o utilizados, fun√ß√µes desnecess√°rias e refatorar o c√≥digo.
*   **Remova Arquivos Desnecess√°rios:**
    *   Exclua arquivos que n√£o s√£o mais necess√°rios. Certifique-se de que nenhum outro componente depende desses arquivos antes de remov√™-los.

**4. Exemplo Pr√°tico:**

*   **Avalia√ß√£o de `template.py`:**
    *   O arquivo `template.py` parece ser uma vers√£o mais antiga ou um modelo inicial do assistente de voz. Compare-o com os outros arquivos (principalmente os arquivos em `class_voice_assistent`) para identificar se alguma parte do c√≥digo em `template.py` ainda √© usada. Se n√£o for, considere remover este arquivo.

*   **Exemplo de refatora√ß√£o:**
    *   Se voc√™ encontrar c√≥digo duplicado entre `voice_command_hendler.py` e `text_command_hendler.py`, voc√™ pode criar uma classe base ou fun√ß√£o utilit√°ria para lidar com a l√≥gica comum, reduzindo a duplica√ß√£o.

**5. Exemplo de C√≥digo para Remover Arquivos:**

Para remover um arquivo espec√≠fico, voc√™ pode usar o seguinte c√≥digo:

```python
import os

file_path = "voice_assistent/class_voice_assistent/arquivo_desnecessario.py"
try:
    os.remove(file_path)
    print(f"Arquivo {file_path} removido com sucesso.")
except FileNotFoundError:
    print(f"Arquivo {file_path} n√£o encontrado.")
except Exception as e:
    print(f"Erro ao remover {file_path}: {e}")
```

**Considera√ß√µes Adicionais:**

*   **Documenta√ß√£o:** Atualize a documenta√ß√£o do projeto para refletir as mudan√ßas realizadas. Isso ajudar√° outros desenvolvedores (ou voc√™ mesmo no futuro) a entender a nova estrutura do c√≥digo.
*   **Versionamento:** Use um sistema de controle de vers√£o (como Git) para gerenciar as altera√ß√µes no c√≥digo. Isso facilita a revers√£o de altera√ß√µes e a colabora√ß√£o com outros desenvolvedores.

Seguindo essas etapas, voc√™ poder√° remover c√≥digos desnecess√°rios da pasta `voice_assistent`, tornando o projeto mais limpo, eficiente e f√°cil de manter.
User: voc√™ tem o c√≥digo no contexto. voc√™ √© meu assistente e deve fazer a an√°lise para mim.
Assistant: Com prazer! Analisei o c√≥digo que voc√™ forneceu e identifiquei algumas √°reas onde podemos remover c√≥digo desnecess√°rio e refatorar para melhor clareza e efici√™ncia.

**1. An√°lise Detalhada e Sugest√µes:**

*   **`template.py`:**

    *   **Status:** Este arquivo parece ser uma vers√£o anterior ou um modelo desatualizado do assistente de voz.
    *   **A√ß√£o:** Comparar o c√≥digo deste arquivo com os componentes atuais em `class_voice_assistent`. Se o c√≥digo n√£o for mais utilizado, **remover o arquivo `template.py`**.

*   **`voice_assistent\config.py`:**

    *   **Status:** Este arquivo cont√©m configura√ß√µes b√°sicas, mas algumas dessas configura√ß√µes podem estar duplicadas ou n√£o sendo usadas.
    *   **A√ß√£o:**
        *   **apiKey e url:** Verificar se essas vari√°veis ainda s√£o necess√°rias e se n√£o est√£o duplicadas em outro lugar (por exemplo, em um arquivo `.env`).
        *   **engine, recent_context, nlp:** Estes itens s√£o inicializados novamente em outros arquivos. Avaliar se √© melhor centralizar a inicializa√ß√£o em um √∫nico local (possivelmente em `class_voice_assistent/main.py`) e importar as inst√¢ncias onde necess√°rio.

*   **`class_voice_assistent\conversation_history.py`:**

    *   **Status:** Este arquivo est√° vazio.
    *   **A√ß√£o:** **Remover este arquivo** se ele n√£o for usado. Se a inten√ß√£o era armazenar o hist√≥rico de conversas, a funcionalidade deve ser movida para um arquivo relevante ou integrada em `class_voice_assistent\context_manager.py`.

*   **`class_voice_assistent\gpt_communication\groq._gpt.py`:**

    *   **Status:** Este arquivo demonstra o uso da API Groq, mas n√£o parece estar integrado ao resto do c√≥digo.
    *   **A√ß√£o:** Se a API Groq n√£o for utilizada, **remover este arquivo**. Caso contr√°rio, integrar corretamente e remover a duplica√ß√£o de configura√ß√µes e inicializa√ß√µes.

*   **`class_voice_assistent\prompt_generator\prompt_generator.py`:**

    *   **Status:** Defini√ß√£o de uma classe abstrata.
    *   **A√ß√£o:** Verificar se a classe abstrata `PromptGenerator` √© usada corretamente pelas classes filhas. Se as classes filhas n√£o implementam os m√©todos abstratos corretamente, refatorar ou remover a classe abstrata.

*   **M√≥dulos de Comando (`voice_command_hendler.py` e `text_command_hendler.py`):**

    *   **Status:** Lidam com a captura de comandos.
    *   **A√ß√£o:** Verificar se h√° duplica√ß√£o de c√≥digo entre esses dois m√≥dulos (por exemplo, tratamento de erros, prompts ao usu√°rio). Se houver, criar uma classe base ou fun√ß√£o utilit√°ria para lidar com a l√≥gica comum.

*   **Remo√ß√£o de c√≥digo n√£o utilizado:**

    *   Utilize um linter (como `flake8`) para identificar imports n√£o utilizados e c√≥digo n√£o referenciado nos arquivos.

**2. Plano de A√ß√£o Detalhado:**

1.  **Backup:** Criar um backup da pasta `voice_assistent`.
2.  **Remover `template.py`:**
    ```bash
    rm voice_assistent/template.py
    ```
3.  **Remover `class_voice_assistent\conversation_history.py`:**
    ```bash
    rm voice_assistent/class_voice_assistent/conversation_history.py
    ```
4.  **Remover `class_voice_assistent\gpt_communication\groq._gpt.py`:**
    ```bash
    rm voice_assistent/class_voice_assistent/gpt_communication/groq._gpt.py
    ```
5.  **Refatorar Configura√ß√µes:**
    *   Mover as configura√ß√µes essenciais de `voice_assistent\config.py` para um arquivo `.env` ou para `class_voice_assistent\main.py`.
    *   Remover o arquivo `voice_assistent\config.py` se ele n√£o for mais necess√°rio.
6.  **Refatorar M√≥dulos de Comando:**
    *   Criar uma classe base ou fun√ß√£o utilit√°ria para consolidar a l√≥gica comum em `voice_command_hendler.py` e `text_command_hendler.py`.
7.  **Executar Linter:**
    ```bash
    pip install flake8
    flake8 voice_assistent
    ```
    *   Remover imports n√£o utilizados e c√≥digo n√£o referenciado.
8.  **Testar:**
    *   Ap√≥s cada passo, testar o assistente de voz para garantir que ele continue funcionando corretamente.
9.  **Documentar:**
    *   Atualizar a documenta√ß√£o do projeto para refletir as mudan√ßas realizadas.

**3. Implementa√ß√£o:**

Remover c√≥digo n√£o utilizado

```python
# Exemplo de como remover um arquivo
import os

file_path = "voice_assistent/class_voice_assistent/gpt_communication/groq._gpt.py"
try:
    os.remove(file_path)
    print(f"Arquivo {file_path} removido com sucesso.")
except FileNotFoundError:
    print(f"Arquivo {file_path} n√£o encontrado.")
except Exception as e:
    print(f"Erro ao remover {file_path}: {e}")
```

**Centralizar configura√ß√µes**

```python
# Exemplo de como centralizar configura√ß√µes em main.py
import os
from dotenv import load_dotenv

load_dotenv()

api_key = os.getenv("API_KEY")
# ... outras configura√ß√µes
```

**4. Considera√ß√µes Adicionais:**

*   **Testes:** Criar testes unit√°rios para garantir a estabilidade dos componentes principais.
*   **Versionamento:** Usar Git para controlar as altera√ß√µes e facilitar a revers√£o, se necess√°rio.
*   **Seguran√ßa:** Validar o caminho da pasta para evitar exclus√µes acidentais.

Com essas a√ß√µes, a pasta `voice_assistent` ficar√° mais organizada, eficiente e f√°cil de manter. Se precisar de ajuda com algum passo espec√≠fico ou detalhe adicional, √© s√≥ me avisar!

User: Preciso importar essas libs 
Assistant: ```python
import os
from dotenv import load_dotenv
import os
import shutil
import pyttsx3
import spacy
import speech_recognition as sr
import requests
import re
from collections import deque
from bs4 import BeautifulSoup
import google.generativeai as genai
```

Essas s√£o as bibliotecas que foram mencionadas ao longo da conversa.

Aqui est√° um exemplo pr√°tico de como importar essas bibliotecas no in√≠cio de um arquivo Python:

```python
# Importa√ß√µes
import os
from dotenv import load_dotenv
import shutil
import pyttsx3
import spacy
import speech_recognition as sr
import requests
import re
from collections import deque
from bs4 import BeautifulSoup
import google.generativeai as genai

# Seu c√≥digo continua aqui...
```

Essas importa√ß√µes fornecer√£o acesso √†s funcionalidades de cada biblioteca ao longo do seu c√≥digo.

Al√©m disso, aqui est√° uma breve descri√ß√£o de cada biblioteca, juntamente com um link para a documenta√ß√£o, para que voc√™ possa aprender mais sobre como utiliz√°-las de forma eficaz:

-   **os:** Fornece uma maneira de usar funcionalidades dependentes do sistema operacional (documenta√ß√£o: <https://docs.python.org/3/library/os.html>)
-   **dotenv:** Permite carregar vari√°veis de ambiente de um arquivo `.env` (documenta√ß√£o: <https://pypi.org/project/python-dotenv/>)
-   **shutil:** Oferece um conjunto de opera√ß√µes de alto n√≠vel em arquivos e cole√ß√µes de arquivos, como copiar e remover arquivos e diret√≥rios (documenta√ß√£o: <https://docs.python.org/3/library/shutil.html>)
-   **pyttsx3:** Biblioteca de convers√£o de texto em fala que funciona offline (documenta√ß√£o: <https://pyttsx3.readthedocs.io/en/latest/>)
-   **spacy:** Uma biblioteca de processamento de linguagem natural avan√ßada (documenta√ß√£o: <https://spacy.io/>)
-   **speech\_recognition:** Biblioteca para reconhecimento de fala, suportando v√°rias APIs (documenta√ß√£o: <https://pypi.org/project/SpeechRecognition/>)
-   **requests:** Biblioteca para fazer requisi√ß√µes HTTP (documenta√ß√£o: <https://requests.readthedocs.io/en/latest/>)
-   **re:** Fornece opera√ß√µes de express√£o regular (documenta√ß√£o: <https://docs.python.org/3/library/re.html>)
-   **collections.deque:** Uma generaliza√ß√£o da pilha e da fila que suporta anexar e remover de qualquer extremidade (documenta√ß√£o: <https://docs.python.org/3/library/collections.html#collections.deque>)
-   **bs4 (Beautiful Soup):** Biblioteca para extrair informa√ß√µes de p√°ginas web em formato HTML ou XML (documenta√ß√£o: <https://www.crummy.com/software/BeautifulSoup/bs4/doc/>)
-   **google.generativeai:** Biblioteca para usar os modelos de linguagem generativa do Google (documenta√ß√£o: verificar a documenta√ß√£o espec√≠fica para os modelos do Google Gemini no Google Cloud).

Com essas importa√ß√µes e a documenta√ß√£o correspondente, voc√™ est√° bem preparado para continuar sua an√°lise e refatora√ß√£o do c√≥digo!
User: como fa√ßo para instalar as que est√£o dando erro na imagem ?
Assistant: Com base na imagem que voc√™ forneceu, as bibliotecas que precisam ser instaladas s√£o:

*   **speech\_recognition:** Para reconhecimento de fala.
*   **pyttsx3:** Para s√≠ntese de fala (convers√£o de texto em voz).
*   **spacy:** Para processamento de linguagem natural.
*   **google.generativeai:** Para interagir com os modelos generativos do Google.

Aqui est√£o os passos para instalar essas bibliotecas usando `pip`:

**1. Abra o Terminal ou Prompt de Comando:**

*   No Windows, voc√™ pode usar o "Prompt de Comando" ou o "PowerShell".
*   No macOS ou Linux, use o "Terminal".

**2. Instale as Bibliotecas:**

Use o comando `pip install` seguido pelo nome das bibliotecas. Execute os seguintes comandos um por um para instalar cada biblioteca:

```bash
pip install SpeechRecognition
pip install pyttsx3
pip install spacy
pip install google-generativeai
```

**3. Instale o Modelo de Linguagem do SpaCy:**

Para usar a biblioteca SpaCy corretamente, voc√™ precisa baixar um modelo de linguagem. Para portugu√™s (pt), voc√™ pode instalar o modelo `pt_core_news_sm`:

```bash
python -m spacy download pt_core_news_sm
```

**4. Verificar a Instala√ß√£o:**

Para verificar se as bibliotecas foram instaladas corretamente, voc√™ pode executar o seguinte c√≥digo Python em um interpretador Python ou em um arquivo `.py`:

```python
import speech_recognition as sr
import pyttsx3
import spacy
import google.generativeai as genai

print("SpeechRecognition vers√£o:", sr.__version__)
print("pyttsx3 ok")
print("Spacy vers√£o:", spacy.__version__)
print("google-generativeai ok")
```

Se n√£o houver erros ao executar este c√≥digo, significa que as bibliotecas foram instaladas corretamente.

**Informa√ß√µes Adicionais:**

*   **Se voc√™ estiver usando um ambiente virtual (`venv`):** Certifique-se de ativar o ambiente virtual antes de instalar as bibliotecas. Isso garante que as bibliotecas sejam instaladas apenas dentro do ambiente virtual e n√£o no sistema global.
*   **Problemas de Permiss√£o:** Se voc√™ encontrar problemas de permiss√£o ao instalar as bibliotecas, tente executar o comando com privil√©gios de administrador (por exemplo, use `sudo pip install` no macOS/Linux).
*   **Vers√µes das Bibliotecas:** Em alguns casos, vers√µes mais recentes das bibliotecas podem ter problemas de compatibilidade com o seu c√≥digo. Se voc√™ tiver problemas, pode tentar instalar vers√µes mais antigas das bibliotecas.

Com estes passos, voc√™ dever√° conseguir instalar as bibliotecas que est√£o faltando e resolver os erros no seu c√≥digo!
User: jfreis@STFSAON006541-L MINGW64 ~/Documents/agents_ia (master)
$ pip install spacy
Collecting spacy
  Using cached spacy-3.8.2.tar.gz (1.3 MB)
  Installing build dependencies ... error
  error: subprocess-exited-with-error

  √ó pip subprocess to install build dependencies did not run successfully.
  ‚îÇ exit code: 1
  ‚ï∞‚îÄ> [67 lines of output]
      Ignoring numpy: markers 'python_version < "3.9"' don't match your environment
      Collecting setuptools
        Using cached setuptools-78.1.0-py3-none-any.whl.metadata (6.6 kB)
      Collecting cython<3.0,>=0.25
        Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)
      Collecting cymem<2.1.0,>=2.0.2
        Using cached cymem-2.0.11-cp313-cp313-win_amd64.whl.metadata (8.8 kB)
      Collecting preshed<3.1.0,>=3.0.2
        Using cached preshed-3.0.9.tar.gz (14 kB)
        Installing build dependencies: started
        Installing build dependencies: finished with status 'done'
        Getting requirements to build wheel: started
        Getting requirements to build wheel: finished with status 'done'
        Preparing metadata (pyproject.toml): started
        Preparing metadata (pyproject.toml): finished with status 'done'
      Collecting murmurhash<1.1.0,>=0.28.0
        Using cached murmurhash-1.0.12-cp313-cp313-win_amd64.whl.metadata (2.2 kB)
      Collecting thinc<8.4.0,>=8.3.0
        Using cached thinc-8.3.6-cp313-cp313-win_amd64.whl.metadata (15 kB)
      Collecting numpy<2.1.0,>=2.0.0
        Using cached numpy-2.0.2.tar.gz (18.9 MB)
        Installing build dependencies: started
        Installing build dependencies: finished with status 'done'
        Getting requirements to build wheel: started
        Getting requirements to build wheel: finished with status 'done'
        Installing backend dependencies: started
        Installing backend dependencies: finished with status 'done'
        Preparing metadata (pyproject.toml): started
        Preparing metadata (pyproject.toml): finished with status 'error'
        error: subprocess-exited-with-error
     
        Preparing metadata (pyproject.toml) did not run successfully.
        exit code: 1
     
        [21 lines of output]
        + C:\Users\jfreis\AppData\Local\Programs\Python\Python313\python.exe C:\Users\jfreis\AppData\Local\Temp\pip-install-_5m3qz36\numpy_125b75e27d3645fc84b36c82dbcdef81\vendored-meson\meson\meson.py setup C:\Users\jfreis\AppData\Local\Temp\pip-install-_5m3qz36\numpy_125b75e27d3645fc84b36c82dbcdef81 C:\Users\jfreis\AppData\Local\Temp\pip-install-_5m3qz36\numpy_125b75e27d3645fc84b36c82dbcdef81\.mesonpy-hs6rlqj9 -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\Users\jfreis\AppData\Local\Temp\pip-install-_5m3qz36\numpy_125b75e27d3645fc84b36c82dbcdef81\.mesonpy-hs6rlqj9\meson-python-native-file.ini
        The Meson build system
        Version: 1.4.99
        Source dir: C:\Users\jfreis\AppData\Local\Temp\pip-install-_5m3qz36\numpy_125b75e27d3645fc84b36c82dbcdef81
        Build dir: C:\Users\jfreis\AppData\Local\Temp\pip-install-_5m3qz36\numpy_125b75e27d3645fc84b36c82dbcdef81\.mesonpy-hs6rlqj9
        Build type: native build
        Project name: NumPy
        Project version: 2.0.2
        WARNING: Failed to activate VS environment: Could not parse vswhere.exe output
     
        ..\meson.build:1:0: ERROR: Unknown compiler(s): [['icl'], ['cl'], ['cc'], ['gcc'], ['clang'], ['clang-cl'], ['pgcc']]  
        The following exception(s) were encountered:
        Running `icl ""` gave "[WinError 2] O sistema n√£o pode encontrar o arquivo especificado"
        Running `cl /?` gave "[WinError 2] O sistema n√£o pode encontrar o arquivo especificado"
        Running `cc --version` gave "[WinError 2] O sistema n√£o pode encontrar o arquivo especificado"
        Running `gcc --version` gave "[WinError 2] O sistema n√£o pode encontrar o arquivo especificado"
        Running `clang --version` gave "[WinError 2] O sistema n√£o pode encontrar o arquivo especificado"
        Running `clang-cl /?` gave "[WinError 2] O sistema n√£o pode encontrar o arquivo especificado"
        Running `pgcc --version` gave "[WinError 2] O sistema n√£o pode encontrar o arquivo especificado"
     
        A full log can be found at C:\Users\jfreis\AppData\Local\Temp\pip-install-_5m3qz36\numpy_125b75e27d3645fc84b36c82dbcdef81\.mesonpy-hs6rlqj9\meson-logs\meson-log.txt
        [end of output]
     
        note: This error originates from a subprocess, and is likely not a problem with pip.
      error: metadata-generation-failed
     
      Encountered error while generating package metadata.
     
      See above for output.
     
      note: This is an issue with the package mentioned above, not pip.
      hint: See above for details.
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

√ó pip subprocess to install build dependencies did not run successfully.
‚îÇ exit code: 1
‚ï∞‚îÄ> See above for output.

note: This error originates from a subprocess, and is likely not a problem with pip.

jfreis@STFSAON006541-L MINGW64 ~/Documents/agents_ia (master)
$
2025-04-14 16:31:54,553 [INFO] Resposta da IA (imagem): The error message indicates that the build process for `numpy` (a dependency of `spacy`) is failing because it cannot find a suitable compiler (like `icl`, `cl`, `cc`, `gcc`, etc.) on your system. This typically happens on Windows when the required build tools for compiling Python packages are not installed.

Here's a breakdown of how to fix this, along with detailed steps:

**1. Install Microsoft Visual C++ Build Tools:**

This is the most common solution. Many Python packages (including NumPy and some others) rely on C/C++ code that needs to be compiled during installation.

*   **Download the Build Tools:**  Go to the following Microsoft website:
    [https://visualstudio.microsoft.com/visual-cpp-build-tools/](https://visualstudio.microsoft.com/visual-cpp-build-tools/)

*   **Select the Installer:** Look for "Build Tools for Visual Studio" and download the installer.

*   **Run the Installer:**

    *   When the installer opens, make sure to select the "C++ build tools" workload.  This includes the C++ compiler, libraries, and other necessary tools. It's important to select this option.

    *   You might also want to select "Windows 10 SDK" (or a more recent version, if applicable) if it's not already selected.

*   **Complete the Installation:** Follow the prompts to complete the installation. This may take some time.

**2. Restart Your Computer (Important):**

After installing the Visual C++ Build Tools, *restart your computer*. This ensures that the system picks up the changes to the environment variables that the Build Tools installer makes.  Failing to restart is a common cause of continuing problems.

**3. Try Installing SpaCy Again:**

Open a new terminal window (or restart your existing one) and try installing SpaCy again:

```bash
pip install spacy
```

This time, the installation should proceed successfully because the `numpy` build process will be able to find the necessary C++ compiler.

**4. Handle Potential Environment Issues (If the Above Doesn't Work):**

Sometimes, even after installing the Build Tools, the necessary environment variables aren't set up correctly. You can try manually setting the environment variables:

*   **Find `vcvarsall.bat`:**  The `vcvarsall.bat` file is a batch script that sets up the environment variables for the Visual C++ compiler.  It's usually located in a directory like this (adjust for your Visual Studio version):

    `C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Auxiliary\Build`
    or
    `C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Auxiliary\Build`

*   **Run `vcvarsall.bat`:**  Open a command prompt (not PowerShell) and navigate to the directory containing `vcvarsall.bat`. Then, run the script:

    ```bash
    vcvarsall.bat amd64
    ```

    *   `amd64` specifies the target architecture (64-bit). If you're on a 32-bit system, use `x86` instead.

*   **Test and Reinstall:** After running `vcvarsall.bat`, try running `pip install spacy` in the *same* command prompt window.  The environment variables set by `vcvarsall.bat` are only valid for that specific command prompt session.

**5. Consider Conda (Alternative to pip):**

If you're still having trouble with `pip`, consider using Conda, a package, dependency, and environment manager that's often more reliable for installing complex scientific computing packages on Windows.

*   **Install Conda:**  If you don't have Conda installed, download and install Anaconda or Miniconda from here: [https://docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html)

*   **Create an Environment (Recommended):**  Create a new Conda environment to avoid conflicts with your existing Python installation:

    ```bash
    conda create -n spacy_env python=3.9  # Or a compatible Python version
    conda activate spacy_env
    ```

*   **Install SpaCy with Conda:**

    ```bash
    conda install -c conda-forge spacy
    ```
*   **Install the Language Model:**
     ```bash
    python -m spacy download pt_core_news_sm
    ```

Conda often handles the underlying build tool dependencies more effectively than `pip` on Windows.

**Troubleshooting Steps if You Still Have Issues:**

*   **Check your Python version:** Ensure that you have a compatible Python version installed. The error message might provide clues if the Python version is the issue.  Spacy generally supports Python 3.7+.
*   **Update pip:** Make sure you have the latest version of pip: `python -m pip install --upgrade pip`
*   **Check your PATH:** Verify that the Python installation directory and the Scripts directory (where `pip` is located) are in your system's `PATH` environment variable.
*   **Firewall/Antivirus:** In rare cases, a firewall or antivirus program might interfere with the installation process. Temporarily disable them and try again.

By following these steps in order, you should be able to resolve the error and successfully install SpaCy and its dependencies. Remember to restart your computer after installing the Visual C++ Build Tools, and try each subsequent step in a new terminal window.

2025-04-14 16:31:54,675 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:31:54,677 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:31:54,679 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:31:54,681 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:31:54,683 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:31:54,686 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:31:54,688 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:31:54,690 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:31:54,692 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:31:54,694 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:31:54,696 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:31:54,698 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:31:54,700 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:31:54,702 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:31:54,704 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:31:54,707 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:31:54,710 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:31:54,712 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:31:54,715 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:31:54,718 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:31:54,719 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:31:54,720 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:31:54,722 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:31:54,724 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:31:54,726 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:31:54,727 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:31:54,729 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:31:54,732 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:31:54,734 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:31:54,736 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:31:54,737 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:31:54,739 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:31:54,741 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:31:54,743 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:31:54,745 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:31:54,749 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:31:54,751 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:31:54,753 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:31:54,755 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:31:54,757 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:31:54,759 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:31:54,761 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:31:54,765 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:31:54,767 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:31:54,770 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:31:54,772 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:31:54,774 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:31:54,775 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:31:54,777 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:31:54,779 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:34:01,609 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:34:01,611 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:34:01,612 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:34:01,614 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:34:01,617 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:34:01,619 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:34:01,620 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:34:01,622 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:34:01,623 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:34:01,625 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:34:01,627 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:34:01,629 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:34:01,631 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:34:01,633 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:34:01,634 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:34:01,636 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:34:01,639 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:34:01,640 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:34:01,643 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:34:01,645 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:34:01,647 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:34:01,649 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:34:01,651 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:34:01,654 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:34:01,656 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:34:01,657 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:34:01,659 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:34:01,661 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:34:01,663 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:34:01,664 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:34:01,666 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:34:01,668 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:34:01,669 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:34:01,670 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:34:01,672 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:34:01,673 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:34:01,674 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:34:01,676 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:34:01,677 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:34:01,679 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:34:01,680 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:34:01,681 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:34:01,683 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:34:01,684 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:34:01,685 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:34:01,687 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:34:01,688 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:34:01,689 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:34:01,691 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:34:01,693 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:34:01,831 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:34:01,832 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:34:01,835 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:34:01,837 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:34:01,839 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:34:01,841 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:34:01,842 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:34:01,844 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:34:01,845 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:34:01,847 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:34:01,848 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:34:01,850 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:34:01,851 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:34:01,853 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:34:01,855 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:34:01,857 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:34:01,858 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:34:01,860 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:34:01,861 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:34:01,862 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:34:01,864 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:34:01,865 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:34:01,867 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:34:01,868 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:34:01,870 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:34:01,871 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:34:01,873 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:34:01,874 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:34:01,876 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:34:01,878 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:34:01,879 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:34:01,882 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:34:01,884 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:34:01,885 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:34:01,886 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:34:01,888 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:34:01,890 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:34:01,891 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:34:01,894 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:34:01,895 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:34:01,897 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:34:01,898 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:34:01,900 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:34:01,901 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:34:01,903 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:34:01,915 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:34:01,917 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:34:01,919 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:34:01,921 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:34:01,922 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:34:01,939 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\agents_ia\comandAI\assets\20250414163401_clipboard_20250414162547.png, Prompt: Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"N√£o foram encontrados arquivos de √°udio no diret√≥rio {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcri√ß√£o salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diret√≥rio atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Defini√ß√£o dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Cria√ß√£o dos diret√≥rios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configura√ß√£o do layout do Streamlit
st.set_page_config(
    page_title="An√°lise de A√ß√µes",
    layout="wide",
    page_icon="üìà"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscila√ß√µes

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Per√≠odo', 'Oscila√ß√£o'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("üìä Dashboard de An√°lise de A√ß√µes")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma a√ß√£o", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"üìå Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("üîç Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descri√ß√£o', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa a√ß√£o.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("üí∞ Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("üìâ Oscila√ß√µes")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("üìà Tabela Geral de A√ß√µes")
        st.dataframe(self.acoes)

# Execu√ß√£o
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para an√°lise fundamentalista de a√ß√µes da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se est√° correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configura√ß√£o de v√≠deos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configura√ß√£o de √°udios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcri√ß√µes e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diret√≥rio
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diret√≥rio.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do √∫ltimo arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conte√∫do do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimens√µes do embedding:", embedding.shape)

# Caminho do diret√≥rio de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o √∫ltimo embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configura√ß√£o de transcri√ß√µes e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diret√≥rio onde est√° procurando as transcri√ß√µes
        print(f"Diret√≥rio de entrada das transcri√ß√µes: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcri√ß√£o no diret√≥rio de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcri√ß√£o encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo n√£o encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcri√ß√£o do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} est√° vazio.")
                    return

            # Gerar o embedding da transcri√ß√£o
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcri√ß√£o e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O v√≠deo {video_path} n√£o cont√©m √°udio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de v√≠deo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de v√≠deo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de v√≠deo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Convers√£o de v√≠deo para √°udio conclu√≠da!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configura√ß√µes da API
handler = genai('gemini-1.5-flash')

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Fun√ß√£o para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conte√∫do com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares √∫nicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcri√ß√£o obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' n√£o √© uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuni√µes: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisi√ß√£o para a API de reuni√µes: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|t√≥picos da|pontos (relevantes|principais)|an√°lise)\b.*\b(reuni√£o|√∫ltima (reuni√£o|conversa|sess√£o))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como an√°lise de reuni√£o.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padr√£o.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que s√£o muito similares ao comando atual para evitar redund√¢ncia.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padr√£o: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # M√©todos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de an√°lise de reuni√£o.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conte√∫do usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inv√°lida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. C√≥digo de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar vari√°veis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conte√∫do com base no prompt, contexto e reuni√£o"""
        try:
            # Supondo que a API espera um dicion√°rio com os par√¢metros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisi√ß√£o para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar vari√°veis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Cria√ß√£o da conclus√£o do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que voc√™ acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["fa√ßa um resumo da √∫ltima reuni√£o.", "t√≥picos da √∫ltima reuni√£o", "resuma a √∫ltima reuni√£o", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Voc√™ √© respons√°vel por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - N√£o acrescentar t√≠tulo nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informa√ß√µes acima e forne√ßa uma solu√ß√£o detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Voc√™ √© respons√°vel analisar com detalhes a reuni√£o de [str_meeting], e fornecer uma longa est√≥ria sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precis√£o da resposta.
            - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e fa√ßa uma pesquisa online para uma resposta mais precisa das quest√µes de [str_text]
        - N√£o acrescente t√≠tulo nas respostas.
        
        ------
        [context]: Regras B√°sicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, n√£o um conte√∫do HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("√Åudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Voc√™ disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum √°udio detectado.")
                return None
            except sr.UnknownValueError:
                print("N√£o foi poss√≠vel entender o √°udio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configura√ß√µes da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicializa√ß√£o do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as √∫ltimas intera√ß√µes
recent_context = deque(maxlen=10)

# Inicializa√ß√£o do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Fun√ß√£o para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("√Åudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Voc√™ disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum √°udio detectado.")
            return None
        except sr.UnknownValueError:
            print("N√£o foi poss√≠vel entender o √°udio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do servi√ßo de reconhecimento de fala; {e}")
            return None

# Fun√ß√£o para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Fun√ß√£o para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicion√°rio
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Fun√ß√£o para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta n√£o est√° no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API: {e}")
        return None

# Fun√ß√£o para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Fun√ß√£o para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se √© uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' n√£o √© uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisi√ß√£o para a API de contextos: {e}")
        return []

# Fun√ß√£o para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} n√£o encontrado"
    elif any(keyword in command.lower() for keyword in ["fa√ßa an√°lise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Fun√ß√£o para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Fun√ß√£o para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Fun√ß√£o para an√°lise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para intera√ß√£o cont√≠nua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Voc√™ quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Op√ß√£o inv√°lida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a intera√ß√£o recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: Fa√ßa uma limpeza na pasta class_voice_assistent
Assistant: Com base nos arquivos fornecidos, aqui est√° um resumo da estrutura e funcionalidade do c√≥digo, juntamente com as informa√ß√µes sobre a limpeza da pasta `class_voice_assistent`:

**Vis√£o geral do projeto:**

Este projeto parece ser um assistente de voz que usa o Google Gemini para processamento de linguagem natural. Ele √© estruturado em v√°rios m√≥dulos e classes para lidar com diferentes aspectos, como captura de √°udio, convers√£o de texto em fala, interpreta√ß√£o de comandos e intera√ß√£o com APIs externas.

**Principais componentes:**

*   **Captura de comandos:**
    *   `voice_assistent\class_voice_assistent\voice_command_hendler.py`: Lida com a captura de comandos de voz usando a biblioteca `speech_recognition`.
    *   `voice_assistent\class_voice_assistent\text_command_hendler.py`: Lida com a captura de comandos de texto do usu√°rio.

*   **Processamento de linguagem natural:**
    *   `voice_assistent\class_voice_assistent\command_interpreter.py`: Interpreta os comandos do usu√°rio e determina a a√ß√£o apropriada a ser executada.
    *   `voice_assistent\class_voice_assistent\question_answers_service.py`: Converte texto em embeddings para encontrar respostas semelhantes.
    *   `voice_assistent\class_voice_assistent\context_manager.py`: Gerencia o contexto da conversa, armazenando as intera√ß√µes recentes.

*   **Comunica√ß√£o com o Google Gemini:**
    *   `voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py`: Lida com a comunica√ß√£o com a API do Google Gemini.
    *   `voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py`: Define o modelo generativo e a configura√ß√£o da API Google Gemini.

*   **Gera√ß√£o de prompt:**
    *   `voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py`: Gera prompts padr√£o para o modelo de linguagem.
    *   `voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py`: Gera prompts para an√°lise de reuni√µes.
    *   `voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py`: Gera prompts para pesquisas online.
    *   `voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py`: Define a classe abstrata para geradores de prompt.

*   **Texto para fala:**
    *   `voice_assistent\class_voice_assistent\text_to_speech.py`: Converte texto em fala usando a biblioteca `pyttsx3`.

*   **Processamento de texto:**
    *   `voice_assistent\class_voice_assistent\text_processor.py`: Fornece fun√ß√µes para extrair valores de JSON e texto de HTML.

*   **Interface principal:**
    *   `voice_assistent\class_voice_assistent\main.py`: Inicializa e coordena os diferentes componentes do assistente de voz.

**Limpeza da pasta `class_voice_assistent`:**

O comando para limpar uma pasta n√£o faz parte diretamente do c√≥digo fornecido. Para implementar essa funcionalidade, voc√™ precisaria adicionar um c√≥digo que use a biblioteca `os` ou `shutil` para remover arquivos e subdiret√≥rios da pasta `class_voice_assistent`.

**Exemplo de como implementar a limpeza da pasta:**

```python
import os
import shutil

def clean_directory(path):
    """Remove todos os arquivos e subdiret√≥rios em um diret√≥rio."""
    for filename in os.listdir(path):
        file_path = os.path.join(path, filename)
        try:
            if os.path.isfile(file_path) or os.path.islink(file_path):
                os.unlink(file_path)
            elif os.path.isdir(file_path):
                shutil.rmtree(file_path)
        except Exception as e:
            print(f'Falha ao excluir {file_path}. Motivo: {e}')

# Caminho para a pasta class_voice_assistent
path_to_clean = 'voice_assistent/class_voice_assistent'

# Chame a fun√ß√£o para limpar a pasta
clean_directory(path_to_clean)
```

**Considera√ß√µes:**

1.  **Importe os m√≥dulos necess√°rios**: `os` e `shutil`.
2.  **Defina a fun√ß√£o `clean_directory`**: Esta fun√ß√£o itera sobre todos os itens no diret√≥rio fornecido e tenta remover cada um.
3.  **Tratamento de erros**: A fun√ß√£o inclui um bloco `try...except` para lidar com poss√≠veis erros durante a remo√ß√£o de arquivos ou diret√≥rios.
4.  **Chame a fun√ß√£o `clean_directory`**: Passe o caminho do diret√≥rio que voc√™ deseja limpar.

**Onde adicionar este c√≥digo:**

Voc√™ pode integrar esta fun√ß√£o dentro do `command_interpreter.py` ou `main.py`. Por exemplo, dentro do `interpret_command`, voc√™ pode adicionar uma condi√ß√£o para detectar o comando "limpar pasta" e, em seguida, chamar a fun√ß√£o `clean_directory` com o caminho apropriado.

**Aviso**: Esta opera√ß√£o √© irrevers√≠vel e excluir√° permanentemente todos os arquivos e pastas dentro do diret√≥rio especificado. Certifique-se de ter um backup se necess√°rio.

**Seguran√ßa:**

Certifique-se de que o caminho da pasta a ser limpa seja validado e seguro para evitar exclus√µes acidentais ou maliciosas.

User: quero remover c√≥digos desnecess√°rios na pasta voice_assistent no projeto.
Assistant: Assistant: Para remover c√≥digos desnecess√°rios na pasta `voice_assistent` do projeto, aqui est√° uma abordagem detalhada:

**1. Vis√£o Geral:**

O objetivo √© identificar e remover arquivos e funcionalidades redundantes ou n√£o utilizados na pasta `voice_assistent`. Isso envolve uma an√°lise cuidadosa do c√≥digo, das depend√™ncias e do fluxo do programa para garantir que nenhuma funcionalidade essencial seja comprometida.

**2. Etapas Detalhadas:**

*   **Backup:**
    *   Antes de fazer qualquer altera√ß√£o, fa√ßa um backup completo da pasta `voice_assistent`. Isso permitir√° restaurar o estado anterior em caso de remo√ß√£o acidental de c√≥digo importante.

*   **An√°lise da Estrutura do C√≥digo:**
    *   **Identifique os principais m√≥dulos:** Analise todos os arquivos Python na pasta `voice_assistent` e suas subpastas para entender sua fun√ß√£o e como eles se interconectam.
    *   **M√≥dulos Essenciais:**
        *   `class_voice_assistent/main.py`: Coordena a opera√ß√£o do assistente de voz.
        *   `class_voice_assistent/command_interpreter.py`: Interpreta comandos do usu√°rio.
        *   `class_voice_assistent/gpt_communication/gemini_gpt.py`: Lida com a comunica√ß√£o com o Google Gemini.
        *   `class_voice_assistent/voice_command_handler.py` e `class_voice_assistent/text_command_handler.py`: Capturam comandos de voz e texto, respectivamente.
        *   `class_voice_assistent/text_to_speech.py`: Converte texto em fala.

*   **Identifica√ß√£o de C√≥digo N√£o Utilizado:**
    *   **Remova imports n√£o utilizados:** Abra cada arquivo e verifique se todos os m√≥dulos importados est√£o realmente sendo usados. Remova aqueles que n√£o est√£o.

    *   **Analise fun√ß√µes n√£o utilizadas:** Verifique se todas as fun√ß√µes definidas s√£o chamadas em algum lugar do c√≥digo. Fun√ß√µes que n√£o s√£o chamadas podem ser removidas com seguran√ßa.
    *   **Avalie c√≥digo comentado:** Avalie se os blocos de c√≥digo comentados ainda s√£o relevantes ou podem ser removidos.

*   **An√°lise das Depend√™ncias:**
    *   **Arquivo requirements.txt ou pyproject.toml:** Verifique se todas as bibliotecas listadas nos arquivos de depend√™ncia s√£o necess√°rias para o funcionamento do assistente de voz. Remova as depend√™ncias que n√£o est√£o sendo usadas.

*   **Simplifica√ß√£o da L√≥gica:**
    *   **Refatore c√≥digo duplicado:** Se houver trechos de c√≥digo repetidos, crie fun√ß√µes ou classes para reutiliz√°-los, reduzindo a redund√¢ncia.
    *   **Simplifique fun√ß√µes complexas:** Divida fun√ß√µes muito longas ou complexas em fun√ß√µes menores e mais f√°ceis de entender e manter.

*   **Remo√ß√£o de C√≥digo Desnecess√°rio:**
    *   **Exclua arquivos desnecess√°rios:** Remova arquivos que n√£o contribuem para a funcionalidade principal do assistente de voz.

*   **Teste:**
    *   Ap√≥s cada remo√ß√£o ou modifica√ß√£o significativa, execute os testes para garantir que o assistente de voz continue funcionando corretamente.
    *   Crie testes automatizados para os principais componentes para facilitar a detec√ß√£o de erros ap√≥s futuras altera√ß√µes.

**3. Implementa√ß√£o:**

*   **Edite os Arquivos:**
    *   Abra cada arquivo Python relevante e fa√ßa as modifica√ß√µes necess√°rias, como remover imports n√£o utilizados, fun√ß√µes desnecess√°rias e refatorar o c√≥digo.
*   **Remova Arquivos Desnecess√°rios:**
    *   Exclua arquivos que n√£o s√£o mais necess√°rios. Certifique-se de que nenhum outro componente depende desses arquivos antes de remov√™-los.

**4. Exemplo Pr√°tico:**

*   **Avalia√ß√£o de `template.py`:**
    *   O arquivo `template.py` parece ser uma vers√£o mais antiga ou um modelo inicial do assistente de voz. Compare-o com os outros arquivos (principalmente os arquivos em `class_voice_assistent`) para identificar se alguma parte do c√≥digo em `template.py` ainda √© usada. Se n√£o for, considere remover este arquivo.

*   **Exemplo de refatora√ß√£o:**
    *   Se voc√™ encontrar c√≥digo duplicado entre `voice_command_hendler.py` e `text_command_hendler.py`, voc√™ pode criar uma classe base ou fun√ß√£o utilit√°ria para lidar com a l√≥gica comum, reduzindo a duplica√ß√£o.

**5. Exemplo de C√≥digo para Remover Arquivos:**

Para remover um arquivo espec√≠fico, voc√™ pode usar o seguinte c√≥digo:

```python
import os

file_path = "voice_assistent/class_voice_assistent/arquivo_desnecessario.py"
try:
    os.remove(file_path)
    print(f"Arquivo {file_path} removido com sucesso.")
except FileNotFoundError:
    print(f"Arquivo {file_path} n√£o encontrado.")
except Exception as e:
    print(f"Erro ao remover {file_path}: {e}")
```

**Considera√ß√µes Adicionais:**

*   **Documenta√ß√£o:** Atualize a documenta√ß√£o do projeto para refletir as mudan√ßas realizadas. Isso ajudar√° outros desenvolvedores (ou voc√™ mesmo no futuro) a entender a nova estrutura do c√≥digo.
*   **Versionamento:** Use um sistema de controle de vers√£o (como Git) para gerenciar as altera√ß√µes no c√≥digo. Isso facilita a revers√£o de altera√ß√µes e a colabora√ß√£o com outros desenvolvedores.

Seguindo essas etapas, voc√™ poder√° remover c√≥digos desnecess√°rios da pasta `voice_assistent`, tornando o projeto mais limpo, eficiente e f√°cil de manter.
User: voc√™ tem o c√≥digo no contexto. voc√™ √© meu assistente e deve fazer a an√°lise para mim.
Assistant: Com prazer! Analisei o c√≥digo que voc√™ forneceu e identifiquei algumas √°reas onde podemos remover c√≥digo desnecess√°rio e refatorar para melhor clareza e efici√™ncia.

**1. An√°lise Detalhada e Sugest√µes:**

*   **`template.py`:**

    *   **Status:** Este arquivo parece ser uma vers√£o anterior ou um modelo desatualizado do assistente de voz.
    *   **A√ß√£o:** Comparar o c√≥digo deste arquivo com os componentes atuais em `class_voice_assistent`. Se o c√≥digo n√£o for mais utilizado, **remover o arquivo `template.py`**.

*   **`voice_assistent\config.py`:**

    *   **Status:** Este arquivo cont√©m configura√ß√µes b√°sicas, mas algumas dessas configura√ß√µes podem estar duplicadas ou n√£o sendo usadas.
    *   **A√ß√£o:**
        *   **apiKey e url:** Verificar se essas vari√°veis ainda s√£o necess√°rias e se n√£o est√£o duplicadas em outro lugar (por exemplo, em um arquivo `.env`).
        *   **engine, recent_context, nlp:** Estes itens s√£o inicializados novamente em outros arquivos. Avaliar se √© melhor centralizar a inicializa√ß√£o em um √∫nico local (possivelmente em `class_voice_assistent/main.py`) e importar as inst√¢ncias onde necess√°rio.

*   **`class_voice_assistent\conversation_history.py`:**

    *   **Status:** Este arquivo est√° vazio.
    *   **A√ß√£o:** **Remover este arquivo** se ele n√£o for usado. Se a inten√ß√£o era armazenar o hist√≥rico de conversas, a funcionalidade deve ser movida para um arquivo relevante ou integrada em `class_voice_assistent\context_manager.py`.

*   **`class_voice_assistent\gpt_communication\groq._gpt.py`:**

    *   **Status:** Este arquivo demonstra o uso da API Groq, mas n√£o parece estar integrado ao resto do c√≥digo.
    *   **A√ß√£o:** Se a API Groq n√£o for utilizada, **remover este arquivo**. Caso contr√°rio, integrar corretamente e remover a duplica√ß√£o de configura√ß√µes e inicializa√ß√µes.

*   **`class_voice_assistent\prompt_generator\prompt_generator.py`:**

    *   **Status:** Defini√ß√£o de uma classe abstrata.
    *   **A√ß√£o:** Verificar se a classe abstrata `PromptGenerator` √© usada corretamente pelas classes filhas. Se as classes filhas n√£o implementam os m√©todos abstratos corretamente, refatorar ou remover a classe abstrata.

*   **M√≥dulos de Comando (`voice_command_hendler.py` e `text_command_hendler.py`):**

    *   **Status:** Lidam com a captura de comandos.
    *   **A√ß√£o:** Verificar se h√° duplica√ß√£o de c√≥digo entre esses dois m√≥dulos (por exemplo, tratamento de erros, prompts ao usu√°rio). Se houver, criar uma classe base ou fun√ß√£o utilit√°ria para lidar com a l√≥gica comum.

*   **Remo√ß√£o de c√≥digo n√£o utilizado:**

    *   Utilize um linter (como `flake8`) para identificar imports n√£o utilizados e c√≥digo n√£o referenciado nos arquivos.

**2. Plano de A√ß√£o Detalhado:**

1.  **Backup:** Criar um backup da pasta `voice_assistent`.
2.  **Remover `template.py`:**
    ```bash
    rm voice_assistent/template.py
    ```
3.  **Remover `class_voice_assistent\conversation_history.py`:**
    ```bash
    rm voice_assistent/class_voice_assistent/conversation_history.py
    ```
4.  **Remover `class_voice_assistent\gpt_communication\groq._gpt.py`:**
    ```bash
    rm voice_assistent/class_voice_assistent/gpt_communication/groq._gpt.py
    ```
5.  **Refatorar Configura√ß√µes:**
    *   Mover as configura√ß√µes essenciais de `voice_assistent\config.py` para um arquivo `.env` ou para `class_voice_assistent\main.py`.
    *   Remover o arquivo `voice_assistent\config.py` se ele n√£o for mais necess√°rio.
6.  **Refatorar M√≥dulos de Comando:**
    *   Criar uma classe base ou fun√ß√£o utilit√°ria para consolidar a l√≥gica comum em `voice_command_hendler.py` e `text_command_hendler.py`.
7.  **Executar Linter:**
    ```bash
    pip install flake8
    flake8 voice_assistent
    ```
    *   Remover imports n√£o utilizados e c√≥digo n√£o referenciado.
8.  **Testar:**
    *   Ap√≥s cada passo, testar o assistente de voz para garantir que ele continue funcionando corretamente.
9.  **Documentar:**
    *   Atualizar a documenta√ß√£o do projeto para refletir as mudan√ßas realizadas.

**3. Implementa√ß√£o:**

Remover c√≥digo n√£o utilizado

```python
# Exemplo de como remover um arquivo
import os

file_path = "voice_assistent/class_voice_assistent/gpt_communication/groq._gpt.py"
try:
    os.remove(file_path)
    print(f"Arquivo {file_path} removido com sucesso.")
except FileNotFoundError:
    print(f"Arquivo {file_path} n√£o encontrado.")
except Exception as e:
    print(f"Erro ao remover {file_path}: {e}")
```

**Centralizar configura√ß√µes**

```python
# Exemplo de como centralizar configura√ß√µes em main.py
import os
from dotenv import load_dotenv

load_dotenv()

api_key = os.getenv("API_KEY")
# ... outras configura√ß√µes
```

**4. Considera√ß√µes Adicionais:**

*   **Testes:** Criar testes unit√°rios para garantir a estabilidade dos componentes principais.
*   **Versionamento:** Usar Git para controlar as altera√ß√µes e facilitar a revers√£o, se necess√°rio.
*   **Seguran√ßa:** Validar o caminho da pasta para evitar exclus√µes acidentais.

Com essas a√ß√µes, a pasta `voice_assistent` ficar√° mais organizada, eficiente e f√°cil de manter. Se precisar de ajuda com algum passo espec√≠fico ou detalhe adicional, √© s√≥ me avisar!

User: Preciso importar essas libs 
Assistant: ```python
import os
from dotenv import load_dotenv
import os
import shutil
import pyttsx3
import spacy
import speech_recognition as sr
import requests
import re
from collections import deque
from bs4 import BeautifulSoup
import google.generativeai as genai
```

Essas s√£o as bibliotecas que foram mencionadas ao longo da conversa.

Aqui est√° um exemplo pr√°tico de como importar essas bibliotecas no in√≠cio de um arquivo Python:

```python
# Importa√ß√µes
import os
from dotenv import load_dotenv
import shutil
import pyttsx3
import spacy
import speech_recognition as sr
import requests
import re
from collections import deque
from bs4 import BeautifulSoup
import google.generativeai as genai

# Seu c√≥digo continua aqui...
```

Essas importa√ß√µes fornecer√£o acesso √†s funcionalidades de cada biblioteca ao longo do seu c√≥digo.

Al√©m disso, aqui est√° uma breve descri√ß√£o de cada biblioteca, juntamente com um link para a documenta√ß√£o, para que voc√™ possa aprender mais sobre como utiliz√°-las de forma eficaz:

-   **os:** Fornece uma maneira de usar funcionalidades dependentes do sistema operacional (documenta√ß√£o: <https://docs.python.org/3/library/os.html>)
-   **dotenv:** Permite carregar vari√°veis de ambiente de um arquivo `.env` (documenta√ß√£o: <https://pypi.org/project/python-dotenv/>)
-   **shutil:** Oferece um conjunto de opera√ß√µes de alto n√≠vel em arquivos e cole√ß√µes de arquivos, como copiar e remover arquivos e diret√≥rios (documenta√ß√£o: <https://docs.python.org/3/library/shutil.html>)
-   **pyttsx3:** Biblioteca de convers√£o de texto em fala que funciona offline (documenta√ß√£o: <https://pyttsx3.readthedocs.io/en/latest/>)
-   **spacy:** Uma biblioteca de processamento de linguagem natural avan√ßada (documenta√ß√£o: <https://spacy.io/>)
-   **speech\_recognition:** Biblioteca para reconhecimento de fala, suportando v√°rias APIs (documenta√ß√£o: <https://pypi.org/project/SpeechRecognition/>)
-   **requests:** Biblioteca para fazer requisi√ß√µes HTTP (documenta√ß√£o: <https://requests.readthedocs.io/en/latest/>)
-   **re:** Fornece opera√ß√µes de express√£o regular (documenta√ß√£o: <https://docs.python.org/3/library/re.html>)
-   **collections.deque:** Uma generaliza√ß√£o da pilha e da fila que suporta anexar e remover de qualquer extremidade (documenta√ß√£o: <https://docs.python.org/3/library/collections.html#collections.deque>)
-   **bs4 (Beautiful Soup):** Biblioteca para extrair informa√ß√µes de p√°ginas web em formato HTML ou XML (documenta√ß√£o: <https://www.crummy.com/software/BeautifulSoup/bs4/doc/>)
-   **google.generativeai:** Biblioteca para usar os modelos de linguagem generativa do Google (documenta√ß√£o: verificar a documenta√ß√£o espec√≠fica para os modelos do Google Gemini no Google Cloud).

Com essas importa√ß√µes e a documenta√ß√£o correspondente, voc√™ est√° bem preparado para continuar sua an√°lise e refatora√ß√£o do c√≥digo!
User: como fa√ßo para instalar as que est√£o dando erro na imagem ?
Assistant: Com base na imagem que voc√™ forneceu, as bibliotecas que precisam ser instaladas s√£o:

*   **speech\_recognition:** Para reconhecimento de fala.
*   **pyttsx3:** Para s√≠ntese de fala (convers√£o de texto em voz).
*   **spacy:** Para processamento de linguagem natural.
*   **google.generativeai:** Para interagir com os modelos generativos do Google.

Aqui est√£o os passos para instalar essas bibliotecas usando `pip`:

**1. Abra o Terminal ou Prompt de Comando:**

*   No Windows, voc√™ pode usar o "Prompt de Comando" ou o "PowerShell".
*   No macOS ou Linux, use o "Terminal".

**2. Instale as Bibliotecas:**

Use o comando `pip install` seguido pelo nome das bibliotecas. Execute os seguintes comandos um por um para instalar cada biblioteca:

```bash
pip install SpeechRecognition
pip install pyttsx3
pip install spacy
pip install google-generativeai
```

**3. Instale o Modelo de Linguagem do SpaCy:**

Para usar a biblioteca SpaCy corretamente, voc√™ precisa baixar um modelo de linguagem. Para portugu√™s (pt), voc√™ pode instalar o modelo `pt_core_news_sm`:

```bash
python -m spacy download pt_core_news_sm
```

**4. Verificar a Instala√ß√£o:**

Para verificar se as bibliotecas foram instaladas corretamente, voc√™ pode executar o seguinte c√≥digo Python em um interpretador Python ou em um arquivo `.py`:

```python
import speech_recognition as sr
import pyttsx3
import spacy
import google.generativeai as genai

print("SpeechRecognition vers√£o:", sr.__version__)
print("pyttsx3 ok")
print("Spacy vers√£o:", spacy.__version__)
print("google-generativeai ok")
```

Se n√£o houver erros ao executar este c√≥digo, significa que as bibliotecas foram instaladas corretamente.

**Informa√ß√µes Adicionais:**

*   **Se voc√™ estiver usando um ambiente virtual (`venv`):** Certifique-se de ativar o ambiente virtual antes de instalar as bibliotecas. Isso garante que as bibliotecas sejam instaladas apenas dentro do ambiente virtual e n√£o no sistema global.
*   **Problemas de Permiss√£o:** Se voc√™ encontrar problemas de permiss√£o ao instalar as bibliotecas, tente executar o comando com privil√©gios de administrador (por exemplo, use `sudo pip install` no macOS/Linux).
*   **Vers√µes das Bibliotecas:** Em alguns casos, vers√µes mais recentes das bibliotecas podem ter problemas de compatibilidade com o seu c√≥digo. Se voc√™ tiver problemas, pode tentar instalar vers√µes mais antigas das bibliotecas.

Com estes passos, voc√™ dever√° conseguir instalar as bibliotecas que est√£o faltando e resolver os erros no seu c√≥digo!
User: jfreis@STFSAON006541-L MINGW64 ~/Documents/agents_ia (master)
$ pip install spacy
Collecting spacy
  Using cached spacy-3.8.2.tar.gz (1.3 MB)
  Installing build dependencies ... error
  error: subprocess-exited-with-error

  √ó pip subprocess to install build dependencies did not run successfully.
  ‚îÇ exit code: 1
  ‚ï∞‚îÄ> [67 lines of output]
      Ignoring numpy: markers 'python_version < "3.9"' don't match your environment
      Collecting setuptools
        Using cached setuptools-78.1.0-py3-none-any.whl.metadata (6.6 kB)
      Collecting cython<3.0,>=0.25
        Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)
      Collecting cymem<2.1.0,>=2.0.2
        Using cached cymem-2.0.11-cp313-cp313-win_amd64.whl.metadata (8.8 kB)
      Collecting preshed<3.1.0,>=3.0.2
        Using cached preshed-3.0.9.tar.gz (14 kB)
        Installing build dependencies: started
        Installing build dependencies: finished with status 'done'
        Getting requirements to build wheel: started
        Getting requirements to build wheel: finished with status 'done'
        Preparing metadata (pyproject.toml): started
        Preparing metadata (pyproject.toml): finished with status 'done'
      Collecting murmurhash<1.1.0,>=0.28.0
        Using cached murmurhash-1.0.12-cp313-cp313-win_amd64.whl.metadata (2.2 kB)
      Collecting thinc<8.4.0,>=8.3.0
        Using cached thinc-8.3.6-cp313-cp313-win_amd64.whl.metadata (15 kB)
      Collecting numpy<2.1.0,>=2.0.0
        Using cached numpy-2.0.2.tar.gz (18.9 MB)
        Installing build dependencies: started
        Installing build dependencies: finished with status 'done'
        Getting requirements to build wheel: started
        Getting requirements to build wheel: finished with status 'done'
        Installing backend dependencies: started
        Installing backend dependencies: finished with status 'done'
        Preparing metadata (pyproject.toml): started
        Preparing metadata (pyproject.toml): finished with status 'error'
        error: subprocess-exited-with-error
     
        Preparing metadata (pyproject.toml) did not run successfully.
        exit code: 1
     
        [21 lines of output]
        + C:\Users\jfreis\AppData\Local\Programs\Python\Python313\python.exe C:\Users\jfreis\AppData\Local\Temp\pip-install-_5m3qz36\numpy_125b75e27d3645fc84b36c82dbcdef81\vendored-meson\meson\meson.py setup C:\Users\jfreis\AppData\Local\Temp\pip-install-_5m3qz36\numpy_125b75e27d3645fc84b36c82dbcdef81 C:\Users\jfreis\AppData\Local\Temp\pip-install-_5m3qz36\numpy_125b75e27d3645fc84b36c82dbcdef81\.mesonpy-hs6rlqj9 -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\Users\jfreis\AppData\Local\Temp\pip-install-_5m3qz36\numpy_125b75e27d3645fc84b36c82dbcdef81\.mesonpy-hs6rlqj9\meson-python-native-file.ini
        The Meson build system
        Version: 1.4.99
        Source dir: C:\Users\jfreis\AppData\Local\Temp\pip-install-_5m3qz36\numpy_125b75e27d3645fc84b36c82dbcdef81
        Build dir: C:\Users\jfreis\AppData\Local\Temp\pip-install-_5m3qz36\numpy_125b75e27d3645fc84b36c82dbcdef81\.mesonpy-hs6rlqj9
        Build type: native build
        Project name: NumPy
        Project version: 2.0.2
        WARNING: Failed to activate VS environment: Could not parse vswhere.exe output
     
        ..\meson.build:1:0: ERROR: Unknown compiler(s): [['icl'], ['cl'], ['cc'], ['gcc'], ['clang'], ['clang-cl'], ['pgcc']]  
        The following exception(s) were encountered:
        Running `icl ""` gave "[WinError 2] O sistema n√£o pode encontrar o arquivo especificado"
        Running `cl /?` gave "[WinError 2] O sistema n√£o pode encontrar o arquivo especificado"
        Running `cc --version` gave "[WinError 2] O sistema n√£o pode encontrar o arquivo especificado"
        Running `gcc --version` gave "[WinError 2] O sistema n√£o pode encontrar o arquivo especificado"
        Running `clang --version` gave "[WinError 2] O sistema n√£o pode encontrar o arquivo especificado"
        Running `clang-cl /?` gave "[WinError 2] O sistema n√£o pode encontrar o arquivo especificado"
        Running `pgcc --version` gave "[WinError 2] O sistema n√£o pode encontrar o arquivo especificado"
     
        A full log can be found at C:\Users\jfreis\AppData\Local\Temp\pip-install-_5m3qz36\numpy_125b75e27d3645fc84b36c82dbcdef81\.mesonpy-hs6rlqj9\meson-logs\meson-log.txt
        [end of output]
     
        note: This error originates from a subprocess, and is likely not a problem with pip.
      error: metadata-generation-failed
     
      Encountered error while generating package metadata.
     
      See above for output.
     
      note: This is an issue with the package mentioned above, not pip.
      hint: See above for details.
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

√ó pip subprocess to install build dependencies did not run successfully.
‚îÇ exit code: 1
‚ï∞‚îÄ> See above for output.

note: This error originates from a subprocess, and is likely not a problem with pip.

jfreis@STFSAON006541-L MINGW64 ~/Documents/agents_ia (master)
$
Assistant: The error message indicates that the build process for `numpy` (a dependency of `spacy`) is failing because it cannot find a suitable compiler (like `icl`, `cl`, `cc`, `gcc`, etc.) on your system. This typically happens on Windows when the required build tools for compiling Python packages are not installed.

Here's a breakdown of how to fix this, along with detailed steps:

**1. Install Microsoft Visual C++ Build Tools:**

This is the most common solution. Many Python packages (including NumPy and some others) rely on C/C++ code that needs to be compiled during installation.

*   **Download the Build Tools:**  Go to the following Microsoft website:
    [https://visualstudio.microsoft.com/visual-cpp-build-tools/](https://visualstudio.microsoft.com/visual-cpp-build-tools/)

*   **Select the Installer:** Look for "Build Tools for Visual Studio" and download the installer.

*   **Run the Installer:**

    *   When the installer opens, make sure to select the "C++ build tools" workload.  This includes the C++ compiler, libraries, and other necessary tools. It's important to select this option.

    *   You might also want to select "Windows 10 SDK" (or a more recent version, if applicable) if it's not already selected.

*   **Complete the Installation:** Follow the prompts to complete the installation. This may take some time.

**2. Restart Your Computer (Important):**

After installing the Visual C++ Build Tools, *restart your computer*. This ensures that the system picks up the changes to the environment variables that the Build Tools installer makes.  Failing to restart is a common cause of continuing problems.

**3. Try Installing SpaCy Again:**

Open a new terminal window (or restart your existing one) and try installing SpaCy again:

```bash
pip install spacy
```

This time, the installation should proceed successfully because the `numpy` build process will be able to find the necessary C++ compiler.

**4. Handle Potential Environment Issues (If the Above Doesn't Work):**

Sometimes, even after installing the Build Tools, the necessary environment variables aren't set up correctly. You can try manually setting the environment variables:

*   **Find `vcvarsall.bat`:**  The `vcvarsall.bat` file is a batch script that sets up the environment variables for the Visual C++ compiler.  It's usually located in a directory like this (adjust for your Visual Studio version):

    `C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Auxiliary\Build`
    or
    `C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Auxiliary\Build`

*   **Run `vcvarsall.bat`:**  Open a command prompt (not PowerShell) and navigate to the directory containing `vcvarsall.bat`. Then, run the script:

    ```bash
    vcvarsall.bat amd64
    ```

    *   `amd64` specifies the target architecture (64-bit). If you're on a 32-bit system, use `x86` instead.

*   **Test and Reinstall:** After running `vcvarsall.bat`, try running `pip install spacy` in the *same* command prompt window.  The environment variables set by `vcvarsall.bat` are only valid for that specific command prompt session.

**5. Consider Conda (Alternative to pip):**

If you're still having trouble with `pip`, consider using Conda, a package, dependency, and environment manager that's often more reliable for installing complex scientific computing packages on Windows.

*   **Install Conda:**  If you don't have Conda installed, download and install Anaconda or Miniconda from here: [https://docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html)

*   **Create an Environment (Recommended):**  Create a new Conda environment to avoid conflicts with your existing Python installation:

    ```bash
    conda create -n spacy_env python=3.9  # Or a compatible Python version
    conda activate spacy_env
    ```

*   **Install SpaCy with Conda:**

    ```bash
    conda install -c conda-forge spacy
    ```
*   **Install the Language Model:**
     ```bash
    python -m spacy download pt_core_news_sm
    ```

Conda often handles the underlying build tool dependencies more effectively than `pip` on Windows.

**Troubleshooting Steps if You Still Have Issues:**

*   **Check your Python version:** Ensure that you have a compatible Python version installed. The error message might provide clues if the Python version is the issue.  Spacy generally supports Python 3.7+.
*   **Update pip:** Make sure you have the latest version of pip: `python -m pip install --upgrade pip`
*   **Check your PATH:** Verify that the Python installation directory and the Scripts directory (where `pip` is located) are in your system's `PATH` environment variable.
*   **Firewall/Antivirus:** In rare cases, a firewall or antivirus program might interfere with the installation process. Temporarily disable them and try again.

By following these steps in order, you should be able to resolve the error and successfully install SpaCy and its dependencies. Remember to restart your computer after installing the Visual C++ Build Tools, and try each subsequent step in a new terminal window.

User: explique em portugues.
2025-04-14 16:34:08,126 [INFO] Resposta da IA (imagem): Claro, aqui est√° a explica√ß√£o detalhada em portugu√™s para resolver o erro de instala√ß√£o do SpaCy:

**O Problema:**

A mensagem de erro indica que o processo de constru√ß√£o do `numpy` (uma depend√™ncia do `spacy`) est√° falhando porque n√£o consegue encontrar um compilador adequado (como `icl`, `cl`, `cc`, `gcc`, etc.) no seu sistema. Isso geralmente acontece no Windows quando as ferramentas de constru√ß√£o necess√°rias para compilar pacotes Python n√£o est√£o instaladas.

**A Solu√ß√£o:**

Para resolver esse problema, voc√™ precisa instalar as ferramentas de constru√ß√£o da Microsoft Visual C++. Veja como:

**1. Instale as Ferramentas de Constru√ß√£o do Microsoft Visual C++:**

Esta √© a solu√ß√£o mais comum. Muitos pacotes Python (incluindo NumPy e alguns outros) dependem de c√≥digo C/C++ que precisa ser compilado durante a instala√ß√£o.

*   **Baixe as Ferramentas de Constru√ß√£o:** Acesse o seguinte site da Microsoft:
    [https://visualstudio.microsoft.com/visual-cpp-build-tools/](https://visualstudio.microsoft.com/visual-cpp-build-tools/)

*   **Selecione o Instalador:** Procure por "Build Tools for Visual Studio" e baixe o instalador.

*   **Execute o Instalador:**
    *   Quando o instalador abrir, certifique-se de selecionar a carga de trabalho "Ferramentas de constru√ß√£o C++". Isso inclui o compilador C++, bibliotecas e outras ferramentas necess√°rias. √â importante selecionar esta op√ß√£o.

    *   Voc√™ tamb√©m pode querer selecionar "SDK do Windows 10" (ou uma vers√£o mais recente, se aplic√°vel) se j√° n√£o estiver selecionado.

*   **Conclua a Instala√ß√£o:** Siga as instru√ß√µes para concluir a instala√ß√£o. Isso pode demorar algum tempo.

**2. Reinicie o Seu Computador (Importante):**

Ap√≥s instalar as Ferramentas de Constru√ß√£o do Visual C++, *reinicie o seu computador*. Isso garante que o sistema detecte as altera√ß√µes nas vari√°veis de ambiente que o instalador das Ferramentas de Constru√ß√£o faz. N√£o reiniciar √© uma causa comum de problemas persistentes.

**3. Tente Instalar o SpaCy Novamente:**

Abra uma nova janela de terminal (ou reinicie a sua existente) e tente instalar o SpaCy novamente:

```bash
pip install spacy
```

Desta vez, a instala√ß√£o deve prosseguir com sucesso porque o processo de constru√ß√£o do `numpy` ser√° capaz de encontrar o compilador C++ necess√°rio.

**4. Lidar com Poss√≠veis Problemas de Ambiente (Se o Acima N√£o Funcionar):**

√Äs vezes, mesmo ap√≥s instalar as Ferramentas de Constru√ß√£o, as vari√°veis de ambiente necess√°rias n√£o s√£o configuradas corretamente. Voc√™ pode tentar configurar manualmente as vari√°veis de ambiente:

*   **Encontre `vcvarsall.bat`:** O arquivo `vcvarsall.bat` √© um script em lote que configura as vari√°veis de ambiente para o compilador Visual C++. Geralmente est√° localizado em um diret√≥rio como este (ajuste para a sua vers√£o do Visual Studio):
    `C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Auxiliary\Build`
    ou
    `C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Auxiliary\Build`

*   **Execute `vcvarsall.bat`:** Abra um prompt de comando (n√£o PowerShell) e navegue at√© o diret√≥rio que cont√©m `vcvarsall.bat`. Em seguida, execute o script:

    ```bash
    vcvarsall.bat amd64
    ```

    *   `amd64` especifica a arquitetura de destino (64-bit). Se voc√™ estiver em um sistema de 32-bit, use `x86` em vez disso.

*   **Teste e Reinstale:** Depois de executar `vcvarsall.bat`, tente executar `pip install spacy` na *mesma* janela do prompt de comando. As vari√°veis de ambiente definidas por `vcvarsall.bat` s√£o v√°lidas apenas para essa sess√£o espec√≠fica do prompt de comando.

**5. Considere Conda (Alternativa ao pip):**

Se voc√™ ainda estiver tendo problemas com o `pip`, considere usar o Conda, um gerenciador de pacotes, depend√™ncias e ambientes que √© frequentemente mais confi√°vel para instalar pacotes complexos de computa√ß√£o cient√≠fica no Windows.

*   **Instale o Conda:** Se voc√™ n√£o tiver o Conda instalado, baixe e instale o Anaconda ou Miniconda daqui: [https://docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html)

*   **Crie um Ambiente (Recomendado):** Crie um novo ambiente Conda para evitar conflitos com a sua instala√ß√£o Python existente:

    ```bash
    conda create -n spacy_env python=3.9  # Ou uma vers√£o Python compat√≠vel
    conda activate spacy_env
    ```

*   **Instale o SpaCy com Conda:**
    ```bash
    conda install -c conda-forge spacy
    ```
    *   **Instale o Language Model:**
        ```bash
    python -m spacy download pt_core_news_sm
    ```

O Conda geralmente lida com as depend√™ncias subjacentes das ferramentas de constru√ß√£o de forma mais eficaz do que o `pip` no Windows.

**Passos de Solu√ß√£o de Problemas Adicionais Se Voc√™ Ainda Tiver Problemas:**

*   **Verifique a sua vers√£o do Python:** Certifique-se de que voc√™ tem uma vers√£o compat√≠vel do Python instalada. A mensagem de erro pode fornecer pistas se a vers√£o do Python √© o problema. Spacy geralmente suporta Python 3.7+.
*   **Atualize o pip:** Certifique-se de ter a vers√£o mais recente do pip: `python -m pip install --upgrade pip`
*   **Verifique o seu PATH:** Verifique se o diret√≥rio de instala√ß√£o do Python e o diret√≥rio Scripts (onde o `pip` est√° localizado) est√£o na vari√°vel de ambiente `PATH` do seu sistema.
*   **Firewall/Antiv√≠rus:** Em casos raros, um firewall ou programa antiv√≠rus pode interferir no processo de instala√ß√£o. Desative-os temporariamente e tente novamente.

Seguindo estes passos em ordem, voc√™ dever√° conseguir resolver o erro e instalar o SpaCy e suas depend√™ncias com sucesso. Lembre-se de reiniciar o seu computador ap√≥s instalar as Ferramentas de Constru√ß√£o do Visual C++, e tente cada passo subsequente numa nova janela de terminal.
2025-04-14 16:34:08,274 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:34:08,276 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:34:08,277 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:34:08,279 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:34:08,281 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:34:08,282 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:34:08,284 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:34:08,286 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:34:08,288 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:34:08,289 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:34:08,291 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:34:08,293 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:34:08,295 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:34:08,297 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:34:08,299 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:34:08,301 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:34:08,303 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:34:08,304 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:34:08,307 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:34:08,309 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:34:08,311 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:34:08,313 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:34:08,315 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:34:08,317 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:34:08,320 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:34:08,322 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:34:08,324 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:34:08,326 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:34:08,328 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:34:08,331 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:34:08,333 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:34:08,335 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:34:08,338 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:34:08,340 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:34:08,342 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:34:08,345 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:34:08,347 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:34:08,349 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:34:08,351 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:34:08,352 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:34:08,354 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:34:08,356 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:34:08,358 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:34:08,361 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:34:08,363 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:34:08,365 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:34:08,367 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:34:08,368 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:34:08,370 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:34:08,371 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:43:41,553 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:43:41,555 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:43:41,556 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:43:41,557 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:43:41,558 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:43:41,560 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:43:41,561 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:43:41,562 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:43:41,563 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:43:41,564 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:43:41,566 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:43:41,567 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:43:41,568 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:43:42,114 [ERROR] API Key n√£o encontrada nas vari√°veis de ambiente
2025-04-14 16:45:16,582 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:45:16,584 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:45:16,585 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:45:16,586 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:45:16,587 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:45:16,588 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:45:16,589 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:45:16,590 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:45:16,591 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:45:16,592 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:45:16,593 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:45:16,594 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:45:16,596 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:45:16,601 [INFO] Modelo Gemini 'gemini-2.0-flash-exp' inicializado com sucesso.
2025-04-14 16:45:33,246 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:45:33,248 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:45:33,250 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:45:33,251 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:45:33,253 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:45:33,254 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:45:33,256 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:45:33,258 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:45:33,259 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:45:33,260 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:45:33,262 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:45:33,264 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:45:33,265 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:45:33,377 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:45:33,378 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:45:33,380 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:45:33,382 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:45:33,383 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:45:33,384 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:45:33,386 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:45:33,387 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:45:33,389 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:45:33,390 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:45:33,392 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:45:33,394 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:45:33,396 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:45:33,398 [INFO] Enviando para IA - Prompt (sem imagem): Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: Resuma essa aplica√ß√£o 
2025-04-14 16:45:41,104 [INFO] Resposta da IA (texto): Esta aplica√ß√£o Streamlit oferece um chatbot inteligente que interage com o usu√°rio por meio de texto e imagens, utilizando o modelo Gemini da Google. Aqui est√° um resumo das principais funcionalidades e componentes:

**Funcionalidades Principais:**

*   **Chatbot Multimodal:** Permite conversas textuais e an√°lise de imagens. O usu√°rio pode enviar mensagens de texto e, opcionalmente, anexar imagens para obter informa√ß√µes ou an√°lises espec√≠ficas sobre elas.
*   **Gera√ß√£o de Imagens:** Integra um recurso para gerar imagens a partir de prompts de texto usando o Gemini.
*   **Hist√≥rico de Conversa:** Mant√©m um hist√≥rico das mensagens trocadas, limitado a um n√∫mero m√°ximo para evitar sobrecarga.
*   **Integra√ß√£o com √Årea de Transfer√™ncia:** Possibilita colar imagens diretamente da √°rea de transfer√™ncia, facilitando o envio de capturas de tela ou outras imagens copiadas.
*   **Upload de Imagens:** Permite o envio de imagens a partir do sistema de arquivos do usu√°rio.
*   **Limpeza de Imagens:** Oferece bot√µes para limpar imagens carregadas ou coladas, permitindo ao usu√°rio come√ßar com uma nova imagem sem recarregar a p√°gina.
*   **Contexto Din√¢mico:** O chatbot inclui o c√≥digo fonte da pr√≥pria aplica√ß√£o como contexto, permitindo perguntas sobre a estrutura e o funcionamento interno do aplicativo.
*   **Rate Limiter:** Implementa um sistema de rate limiting para controlar o n√∫mero de requisi√ß√µes √† API do Gemini, evitando sobrecarga e garantindo um uso justo dos recursos.

**Componentes e Arquitetura:**

*   **Streamlit:** A interface do usu√°rio √© constru√≠da com Streamlit, uma biblioteca Python para cria√ß√£o r√°pida de aplicativos web interativos.
*   **Gemini API:** Utiliza a API do Gemini, modelo de linguagem da Google, para gerar respostas inteligentes com base no texto e nas imagens fornecidas.
*   **Handlers:** A intera√ß√£o com o Gemini √© encapsulada em um `GeminiHandler`, que abstrai a complexidade da API e gerencia o rate limiting.
*   **Configura√ß√£o:** As configura√ß√µes da aplica√ß√£o, como caminhos de arquivos, limites de taxa e prompts, s√£o centralizadas em um m√≥dulo de configura√ß√£o (`config.py`).
*   **Logs:**  Utiliza um sistema de logs para registrar informa√ß√µes sobre o funcionamento da aplica√ß√£o, erros e intera√ß√µes com a API.
*   **Estados de Sess√£o:** O `session_state` do Streamlit √© utilizado para armazenar e gerenciar o estado da aplica√ß√£o, como hist√≥rico de mensagens, imagens carregadas e status de processamento.
*   **Servi√ßos:** A aplica√ß√£o √© organizada em servi√ßos, como:
    *   `gpt_services.py`: Lida com a intera√ß√£o direta com a API do Gemini.
    *   `document_service.py`: Respons√°vel pela cria√ß√£o e formata√ß√£o de documentos (Word).
    *   `markdown_service.py`: Respons√°vel pela cria√ß√£o de documentos Markdown.
    *   `image_processor.py`: Processa imagens, extrai informa√ß√µes e atualiza o hist√≥rico.
    *   `search_files.py`:  L√™ o c√≥digo fonte da aplica√ß√£o para fornecer contexto ao chatbot.

**Fluxo de Opera√ß√£o:**

1.  **Entrada do Usu√°rio:** O usu√°rio interage com a aplica√ß√£o atrav√©s da interface Streamlit, enviando mensagens de texto e, opcionalmente, carregando ou colando imagens.
2.  **Processamento da Mensagem:** A aplica√ß√£o verifica se h√° uma imagem anexada. Se houver, prepara a imagem para envio √† API do Gemini.
3.  **Gera√ß√£o da Resposta:** O `GeminiHandler` envia o prompt de texto e a imagem (se houver) para a API do Gemini.
4.  **Exibi√ß√£o da Resposta:** A resposta gerada pelo Gemini √© exibida na interface do usu√°rio, juntamente com a imagem (se houver).
5.  **Hist√≥rico:** As mensagens do usu√°rio e as respostas do Gemini s√£o adicionadas ao hist√≥rico da conversa.

**Em resumo, esta aplica√ß√£o oferece um chatbot vers√°til e inteligente, capaz de interagir com o usu√°rio por meio de texto e imagens, utilizando o poder da API do Gemini e uma arquitetura bem organizada.**

2025-04-14 16:45:41,223 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:45:41,224 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:45:41,226 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:45:41,227 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:45:41,228 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:45:41,229 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:45:41,231 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:45:41,233 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:45:41,234 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:45:41,235 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:45:41,236 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:45:41,237 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:45:41,239 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:50:08,376 [INFO] Enviando para IA - Prompt (sem imagem): Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



User: Resuma essa aplica√ß√£o 
Assistant: Esta aplica√ß√£o Streamlit oferece um chatbot inteligente que interage com o usu√°rio por meio de texto e imagens, utilizando o modelo Gemini da Google. Aqui est√° um resumo das principais funcionalidades e componentes:

**Funcionalidades Principais:**

*   **Chatbot Multimodal:** Permite conversas textuais e an√°lise de imagens. O usu√°rio pode enviar mensagens de texto e, opcionalmente, anexar imagens para obter informa√ß√µes ou an√°lises espec√≠ficas sobre elas.
*   **Gera√ß√£o de Imagens:** Integra um recurso para gerar imagens a partir de prompts de texto usando o Gemini.
*   **Hist√≥rico de Conversa:** Mant√©m um hist√≥rico das mensagens trocadas, limitado a um n√∫mero m√°ximo para evitar sobrecarga.
*   **Integra√ß√£o com √Årea de Transfer√™ncia:** Possibilita colar imagens diretamente da √°rea de transfer√™ncia, facilitando o envio de capturas de tela ou outras imagens copiadas.
*   **Upload de Imagens:** Permite o envio de imagens a partir do sistema de arquivos do usu√°rio.
*   **Limpeza de Imagens:** Oferece bot√µes para limpar imagens carregadas ou coladas, permitindo ao usu√°rio come√ßar com uma nova imagem sem recarregar a p√°gina.
*   **Contexto Din√¢mico:** O chatbot inclui o c√≥digo fonte da pr√≥pria aplica√ß√£o como contexto, permitindo perguntas sobre a estrutura e o funcionamento interno do aplicativo.
*   **Rate Limiter:** Implementa um sistema de rate limiting para controlar o n√∫mero de requisi√ß√µes √† API do Gemini, evitando sobrecarga e garantindo um uso justo dos recursos.

**Componentes e Arquitetura:**

*   **Streamlit:** A interface do usu√°rio √© constru√≠da com Streamlit, uma biblioteca Python para cria√ß√£o r√°pida de aplicativos web interativos.
*   **Gemini API:** Utiliza a API do Gemini, modelo de linguagem da Google, para gerar respostas inteligentes com base no texto e nas imagens fornecidas.
*   **Handlers:** A intera√ß√£o com o Gemini √© encapsulada em um `GeminiHandler`, que abstrai a complexidade da API e gerencia o rate limiting.
*   **Configura√ß√£o:** As configura√ß√µes da aplica√ß√£o, como caminhos de arquivos, limites de taxa e prompts, s√£o centralizadas em um m√≥dulo de configura√ß√£o (`config.py`).
*   **Logs:**  Utiliza um sistema de logs para registrar informa√ß√µes sobre o funcionamento da aplica√ß√£o, erros e intera√ß√µes com a API.
*   **Estados de Sess√£o:** O `session_state` do Streamlit √© utilizado para armazenar e gerenciar o estado da aplica√ß√£o, como hist√≥rico de mensagens, imagens carregadas e status de processamento.
*   **Servi√ßos:** A aplica√ß√£o √© organizada em servi√ßos, como:
    *   `gpt_services.py`: Lida com a intera√ß√£o direta com a API do Gemini.
    *   `document_service.py`: Respons√°vel pela cria√ß√£o e formata√ß√£o de documentos (Word).
    *   `markdown_service.py`: Respons√°vel pela cria√ß√£o de documentos Markdown.
    *   `image_processor.py`: Processa imagens, extrai informa√ß√µes e atualiza o hist√≥rico.
    *   `search_files.py`:  L√™ o c√≥digo fonte da aplica√ß√£o para fornecer contexto ao chatbot.

**Fluxo de Opera√ß√£o:**

1.  **Entrada do Usu√°rio:** O usu√°rio interage com a aplica√ß√£o atrav√©s da interface Streamlit, enviando mensagens de texto e, opcionalmente, carregando ou colando imagens.
2.  **Processamento da Mensagem:** A aplica√ß√£o verifica se h√° uma imagem anexada. Se houver, prepara a imagem para envio √† API do Gemini.
3.  **Gera√ß√£o da Resposta:** O `GeminiHandler` envia o prompt de texto e a imagem (se houver) para a API do Gemini.
4.  **Exibi√ß√£o da Resposta:** A resposta gerada pelo Gemini √© exibida na interface do usu√°rio, juntamente com a imagem (se houver).
5.  **Hist√≥rico:** As mensagens do usu√°rio e as respostas do Gemini s√£o adicionadas ao hist√≥rico da conversa.

**Em resumo, esta aplica√ß√£o oferece um chatbot vers√°til e inteligente, capaz de interagir com o usu√°rio por meio de texto e imagens, utilizando o poder da API do Gemini e uma arquitetura bem organizada.**

User: leia a aplica√ß√£o.
2025-04-14 16:50:11,720 [INFO] Resposta da IA (texto): Para ler a aplica√ß√£o, preciso ter acesso ao c√≥digo fonte. Como sou um modelo de linguagem, n√£o consigo acessar diretamente arquivos no seu computador ou em um reposit√≥rio online.

Para que eu possa "ler" a aplica√ß√£o e te ajudar melhor, voc√™ pode:

1.  **Compartilhar o c√≥digo:** Cole o conte√∫do dos principais arquivos da aplica√ß√£o aqui, um por vez. Comece pelos arquivos que voc√™ considera mais importantes para entender a l√≥gica geral, como `app.py` (o arquivo principal do Streamlit), `gpt_services.py`, `config.py` e qualquer outro arquivo central na arquitetura.
2.  **Fazer perguntas espec√≠ficas:** Se voc√™ tem d√∫vidas sobre partes espec√≠ficas do c√≥digo, como o funcionamento de uma fun√ß√£o ou a finalidade de um determinado m√≥dulo, pode me perguntar diretamente.

Ao me fornecer o c√≥digo ou fazer perguntas direcionadas, posso analisar a aplica√ß√£o e te auxiliar com:

*   **Explica√ß√£o do c√≥digo:** Descrever o que cada parte do c√≥digo faz.
*   **Identifica√ß√£o de problemas:** Ajudar a encontrar erros ou √°reas que podem ser otimizadas.
*   **Sugest√µes de melhorias:** Propor mudan√ßas no c√≥digo para torn√°-lo mais leg√≠vel, eficiente ou robusto.
*   **Responder a perguntas sobre o funcionamento:** Explicar como diferentes partes da aplica√ß√£o interagem entre si.

Por favor, forne√ßa o c√≥digo ou fa√ßa perguntas espec√≠ficas para que eu possa te ajudar a entender a aplica√ß√£o.

2025-04-14 16:51:21,514 [INFO] Enviando para IA - Prompt (sem imagem): Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



User: Resuma essa aplica√ß√£o 
Assistant: Esta aplica√ß√£o Streamlit oferece um chatbot inteligente que interage com o usu√°rio por meio de texto e imagens, utilizando o modelo Gemini da Google. Aqui est√° um resumo das principais funcionalidades e componentes:

**Funcionalidades Principais:**

*   **Chatbot Multimodal:** Permite conversas textuais e an√°lise de imagens. O usu√°rio pode enviar mensagens de texto e, opcionalmente, anexar imagens para obter informa√ß√µes ou an√°lises espec√≠ficas sobre elas.
*   **Gera√ß√£o de Imagens:** Integra um recurso para gerar imagens a partir de prompts de texto usando o Gemini.
*   **Hist√≥rico de Conversa:** Mant√©m um hist√≥rico das mensagens trocadas, limitado a um n√∫mero m√°ximo para evitar sobrecarga.
*   **Integra√ß√£o com √Årea de Transfer√™ncia:** Possibilita colar imagens diretamente da √°rea de transfer√™ncia, facilitando o envio de capturas de tela ou outras imagens copiadas.
*   **Upload de Imagens:** Permite o envio de imagens a partir do sistema de arquivos do usu√°rio.
*   **Limpeza de Imagens:** Oferece bot√µes para limpar imagens carregadas ou coladas, permitindo ao usu√°rio come√ßar com uma nova imagem sem recarregar a p√°gina.
*   **Contexto Din√¢mico:** O chatbot inclui o c√≥digo fonte da pr√≥pria aplica√ß√£o como contexto, permitindo perguntas sobre a estrutura e o funcionamento interno do aplicativo.
*   **Rate Limiter:** Implementa um sistema de rate limiting para controlar o n√∫mero de requisi√ß√µes √† API do Gemini, evitando sobrecarga e garantindo um uso justo dos recursos.

**Componentes e Arquitetura:**

*   **Streamlit:** A interface do usu√°rio √© constru√≠da com Streamlit, uma biblioteca Python para cria√ß√£o r√°pida de aplicativos web interativos.
*   **Gemini API:** Utiliza a API do Gemini, modelo de linguagem da Google, para gerar respostas inteligentes com base no texto e nas imagens fornecidas.
*   **Handlers:** A intera√ß√£o com o Gemini √© encapsulada em um `GeminiHandler`, que abstrai a complexidade da API e gerencia o rate limiting.
*   **Configura√ß√£o:** As configura√ß√µes da aplica√ß√£o, como caminhos de arquivos, limites de taxa e prompts, s√£o centralizadas em um m√≥dulo de configura√ß√£o (`config.py`).
*   **Logs:**  Utiliza um sistema de logs para registrar informa√ß√µes sobre o funcionamento da aplica√ß√£o, erros e intera√ß√µes com a API.
*   **Estados de Sess√£o:** O `session_state` do Streamlit √© utilizado para armazenar e gerenciar o estado da aplica√ß√£o, como hist√≥rico de mensagens, imagens carregadas e status de processamento.
*   **Servi√ßos:** A aplica√ß√£o √© organizada em servi√ßos, como:
    *   `gpt_services.py`: Lida com a intera√ß√£o direta com a API do Gemini.
    *   `document_service.py`: Respons√°vel pela cria√ß√£o e formata√ß√£o de documentos (Word).
    *   `markdown_service.py`: Respons√°vel pela cria√ß√£o de documentos Markdown.
    *   `image_processor.py`: Processa imagens, extrai informa√ß√µes e atualiza o hist√≥rico.
    *   `search_files.py`:  L√™ o c√≥digo fonte da aplica√ß√£o para fornecer contexto ao chatbot.

**Fluxo de Opera√ß√£o:**

1.  **Entrada do Usu√°rio:** O usu√°rio interage com a aplica√ß√£o atrav√©s da interface Streamlit, enviando mensagens de texto e, opcionalmente, carregando ou colando imagens.
2.  **Processamento da Mensagem:** A aplica√ß√£o verifica se h√° uma imagem anexada. Se houver, prepara a imagem para envio √† API do Gemini.
3.  **Gera√ß√£o da Resposta:** O `GeminiHandler` envia o prompt de texto e a imagem (se houver) para a API do Gemini.
4.  **Exibi√ß√£o da Resposta:** A resposta gerada pelo Gemini √© exibida na interface do usu√°rio, juntamente com a imagem (se houver).
5.  **Hist√≥rico:** As mensagens do usu√°rio e as respostas do Gemini s√£o adicionadas ao hist√≥rico da conversa.

**Em resumo, esta aplica√ß√£o oferece um chatbot vers√°til e inteligente, capaz de interagir com o usu√°rio por meio de texto e imagens, utilizando o poder da API do Gemini e uma arquitetura bem organizada.**

User: leia a aplica√ß√£o.
Assistant: Para ler a aplica√ß√£o, preciso ter acesso ao c√≥digo fonte. Como sou um modelo de linguagem, n√£o consigo acessar diretamente arquivos no seu computador ou em um reposit√≥rio online.

Para que eu possa "ler" a aplica√ß√£o e te ajudar melhor, voc√™ pode:

1.  **Compartilhar o c√≥digo:** Cole o conte√∫do dos principais arquivos da aplica√ß√£o aqui, um por vez. Comece pelos arquivos que voc√™ considera mais importantes para entender a l√≥gica geral, como `app.py` (o arquivo principal do Streamlit), `gpt_services.py`, `config.py` e qualquer outro arquivo central na arquitetura.
2.  **Fazer perguntas espec√≠ficas:** Se voc√™ tem d√∫vidas sobre partes espec√≠ficas do c√≥digo, como o funcionamento de uma fun√ß√£o ou a finalidade de um determinado m√≥dulo, pode me perguntar diretamente.

Ao me fornecer o c√≥digo ou fazer perguntas direcionadas, posso analisar a aplica√ß√£o e te auxiliar com:

*   **Explica√ß√£o do c√≥digo:** Descrever o que cada parte do c√≥digo faz.
*   **Identifica√ß√£o de problemas:** Ajudar a encontrar erros ou √°reas que podem ser otimizadas.
*   **Sugest√µes de melhorias:** Propor mudan√ßas no c√≥digo para torn√°-lo mais leg√≠vel, eficiente ou robusto.
*   **Responder a perguntas sobre o funcionamento:** Explicar como diferentes partes da aplica√ß√£o interagem entre si.

Por favor, forne√ßa o c√≥digo ou fa√ßa perguntas espec√≠ficas para que eu possa te ajudar a entender a aplica√ß√£o.

User: import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py', '*.java')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

Esse c√≥digo pode ser usado para ler arquivos python e java tbm ?


2025-04-14 16:51:28,660 [INFO] Resposta da IA (texto): Sim, o c√≥digo que voc√™ forneceu pode ser usado para ler arquivos Python e Java, mas com algumas ressalvas e poss√≠veis ajustes:

**Pontos Positivos:**

*   **Flexibilidade do `glob`:** A fun√ß√£o `glob.glob(padrao_busca, recursive=True)` √© poderosa e permite buscar arquivos recursivamente em subdiret√≥rios.  O `recursive=True` √© essencial para percorrer toda a estrutura de diret√≥rios.
*   **Tratamento de erros:** O bloco `try...except` dentro do loop √© importante para lidar com poss√≠veis erros de leitura de arquivos (por exemplo, permiss√µes insuficientes ou arquivos corrompidos) e evitar que a fun√ß√£o pare de funcionar.  O logging dos erros √© excelente para depura√ß√£o.
*   **Caminho Relativo:** Usar `os.path.relpath` para obter o caminho relativo ao diret√≥rio `src_dir` √© uma boa pr√°tica, pois torna o output mais leg√≠vel e independente da localiza√ß√£o absoluta do diret√≥rio.
*   **Logging:** A inclus√£o de logging para informar sobre o sucesso ou falha na leitura dos arquivos √© crucial para monitorar o processo e identificar problemas.

**Poss√≠veis Melhorias e Considera√ß√µes:**

*   **Padr√£o de busca (`padrao_busca`):** Atualmente, o padr√£o de busca √©  `os.path.join(src_dir.as_posix(), '**', '*.py', '*.java')`.  O problema √© que `glob` n√£o aceita multiplas exten√ß√µes como essa `'*.py', '*.java'` juntas. Ent√£o o correto seria:

    ```python
    padrao_busca_py = os.path.join(src_dir.as_posix(), '**', '*.py')
    padrao_busca_java = os.path.join(src_dir.as_posix(), '**', '*.java')

    arquivos_py = glob.glob(padrao_busca_py, recursive=True)
    arquivos_java = glob.glob(padrao_busca_java, recursive=True)

    arquivos = arquivos_py + arquivos_java
    ```

*   **Manipula√ß√£o do caminho do arquivo:** `src_dir.as_posix()`  j√° retorna o caminho no formato string (POSIX), ent√£o n√£o precisa usar `os.path.join` passando para string novamente.

*   **Codifica√ß√£o:** Especificar `encoding='utf-8'` ao abrir os arquivos √© uma excelente pr√°tica para garantir que caracteres especiais sejam lidos corretamente.

**C√≥digo Revisado:**

Aqui est√° uma vers√£o revisada do c√≥digo, incorporando as melhorias sugeridas:

```python
import os
import glob
from pathlib import Path
from config.config import Config
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python_e_java() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py e .java a partir de src/"""
    src_dir = Config.BASE_DIR

    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    # Busca arquivos .py e .java separadamente
    padrao_busca_py = os.path.join(src_dir, '**', '*.py')
    padrao_busca_java = os.path.join(src_dir, '**', '*.java')

    arquivos_py = glob.glob(padrao_busca_py, recursive=True)
    arquivos_java = glob.glob(padrao_busca_java, recursive=True)

    arquivos = arquivos_py + arquivos_java
    arquivos.sort() # Ordena a lista completa

    for arquivo in arquivos:
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")
            continue

    return conteudo_total
```

**Como usar:**

1.  **Certifique-se de ter a biblioteca `glob` instalada.** Ela geralmente vem com a instala√ß√£o padr√£o do Python.
2.  **Substitua `Config.BASE_DIR` pelo caminho correto para o seu diret√≥rio "src".**  √â importante que `Config.BASE_DIR` seja um objeto `Path` do m√≥dulo `pathlib`.
3.  **Execute a fun√ß√£o `ler_todos_arquivos_python_e_java()`**.  A fun√ß√£o retornar√° uma string contendo o conte√∫do de todos os arquivos .py e .java encontrados em "src" e seus subdiret√≥rios, separados por cabe√ßalhos contendo o caminho relativo de cada arquivo.

Essa vers√£o revisada √© mais robusta e segura, garantindo que o c√≥digo explore os arquivos .java e .py da melhor forma poss√≠vel.

2025-04-14 16:54:23,434 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\API_CommandAI\assets\20250414165423_clipboard_20250414165400.png, Prompt: Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



User: import os
import glob
from pathlib import Path
from config.config import Config
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python_e_java() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py e .java a partir de src/"""
    src_dir = Config.BASE_DIR

    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    # Busca arquivos .py e .java separadamente
    padrao_busca_py = os.path.join(src_dir, '**', '*.py')
    padrao_busca_java = os.path.join(src_dir, '**', '*.java')

    arquivos_py = glob.glob(padrao_busca_py, recursive=True)
    arquivos_java = glob.glob(padrao_busca_java, recursive=True)

    arquivos = arquivos_py + arquivos_java
    arquivos.sort() # Ordena a lista completa

    for arquivo in arquivos:
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")
            continue

    return conteudo_total
2025-04-14 16:54:29,910 [INFO] Resposta da IA (imagem): Okay, I see the error message:

**ImportError: cannot import name 'ler_todos_arquivos_python' from 'services.search_files'**

This error indicates that your Python code in `chat_streamlit.py` is trying to import a function or class named `ler_todos_arquivos_python` from the module `services.search_files`, but that name doesn't exist in that module.

Based on your prompt, the function `ler_todos_arquivos_python_e_java` exists but has a different name.

Here are the most likely causes and solutions:

1.  **Misspelled Name:** You might have misspelled the name when importing. Double-check that `ler_todos_arquivos_python` is exactly how it's defined in `services/search_files.py`. This is often the case with long or similar-sounding names.

2.  **Incorrect Function Name:** Your code now has a function named `ler_todos_arquivos_python_e_java` to read files of python and java extensions. You must to change the function call for this new function or rename the function to `ler_todos_arquivos_python`.

3.  **Function Not Defined or Misnamed:** The function `ler_todos_arquivos_python` might not be defined within the `services/search_files.py` file at all, or it might have a slightly different name there. Look at the source code of `services/search_files.py` to confirm the actual name of the function you want to import.

4.  **Incorrect Module Path:** You might be trying to import from the wrong module altogether. Double-check that the `services/search_files.py` is the correct location of the file containing the function.

5.  **Module Not Properly Saved:** If you've made changes to `services/search_files.py`, make sure you've saved the file after making the changes. Sometimes the interpreter loads an older version.

To Fix This Error:

1.  **Examine `services/search_files.py`:** Open the `services/search_files.py` file and carefully check the following:

    *   Confirm that a function or class named either `ler_todos_arquivos_python` or `ler_todos_arquivos_python_e_java` exists. If it doesn't exist, rename the function or class to be called `ler_todos_arquivos_python`
    *   Ensure there are no typos in the name's definition.
    *   Verify that the function is actually defined (not commented out or within an `if False` block).

2.  **Update the Import Statement:** Modify your import statement in `chat_streamlit.py` to match the actual name of the function.  For example:

    ```python
    from services.search_files import ler_todos_arquivos_python_e_java  # Use the correct name
    ```

3.  **Double-Check Paths:** Ensure that your Python environment is correctly finding the `services/search_files.py` file. If your project has a complex directory structure, you might need to adjust the Python path or use relative imports more carefully.

**Example (assuming `ler_todos_arquivos_python_e_java` is the correct name):**

In `chat_streamlit.py`, change:

```python
from services.search_files import ler_todos_arquivos_python
```

To:

```python
from services.search_files import ler_todos_arquivos_python_e_java
```

Then, in the code where you are using the function, make sure that you are calling it by its correct name.

By carefully checking the name and location of the function you're trying to import, you should be able to resolve this `ImportError`.

2025-04-14 16:54:56,793 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:54:56,794 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:54:56,795 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:54:56,797 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:54:56,798 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:54:56,799 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:54:56,801 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:54:56,802 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:54:56,804 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:54:56,805 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:54:56,807 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:54:56,808 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:54:56,810 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:54:56,814 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 16:54:56,822 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 16:54:56,829 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 16:54:56,833 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 16:54:56,838 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 16:54:56,842 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 16:54:56,849 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 16:54:56,854 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 16:54:56,859 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 16:54:56,866 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 16:54:56,871 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 16:54:56,876 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 16:54:56,881 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 16:54:56,887 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 16:54:56,891 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 16:54:56,897 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 16:54:56,902 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 16:54:56,908 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 16:54:56,913 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 16:54:56,917 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 16:54:56,923 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 16:54:56,928 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 16:54:56,933 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 16:55:07,745 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:55:07,746 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:55:07,747 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:55:07,749 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:55:07,750 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:55:07,752 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:55:07,754 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:55:07,756 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:55:07,757 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:55:07,759 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:55:07,760 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:55:07,761 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:55:07,763 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:55:07,764 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 16:55:07,766 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 16:55:07,767 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 16:55:07,769 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 16:55:07,770 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 16:55:07,773 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 16:55:07,775 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 16:55:07,777 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 16:55:07,778 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 16:55:07,779 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 16:55:07,780 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 16:55:07,781 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 16:55:07,782 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 16:55:07,783 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 16:55:07,784 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 16:55:07,785 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 16:55:07,786 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 16:55:07,788 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 16:55:07,790 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 16:55:07,792 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 16:55:07,793 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 16:55:07,794 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 16:55:07,795 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 16:55:07,925 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:55:07,926 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:55:07,928 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:55:07,929 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:55:07,931 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:55:07,932 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:55:07,934 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:55:07,936 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:55:07,937 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:55:07,939 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:55:07,940 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:55:07,942 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:55:07,943 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:55:07,944 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 16:55:07,945 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 16:55:07,946 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 16:55:07,948 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 16:55:07,949 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 16:55:07,950 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 16:55:07,952 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 16:55:07,964 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 16:55:07,966 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 16:55:07,969 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 16:55:07,971 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 16:55:07,973 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 16:55:07,975 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 16:55:07,976 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 16:55:07,978 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 16:55:07,980 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 16:55:07,981 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 16:55:07,982 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 16:55:07,983 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 16:55:07,985 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 16:55:07,986 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 16:55:07,987 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 16:55:07,988 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 16:55:08,000 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\API_CommandAI\assets\20250414165507_clipboard_20250414165400.png, Prompt: Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py e .java a partir de src/"""
    src_dir = Config.BASE_DIR

    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    # Busca arquivos .py e .java separadamente
    padrao_busca_py = os.path.join(src_dir, '**', '*.py')
    padrao_busca_java = os.path.join(src_dir, '**', '*.java')

    arquivos_py = glob.glob(padrao_busca_py, recursive=True)
    arquivos_java = glob.glob(padrao_busca_java, recursive=True)

    arquivos = arquivos_py + arquivos_java
    arquivos.sort() # Ordena a lista completa

    for arquivo in arquivos:
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# main\java\com\commandAI\commandAI\CommandAiApplication.java

package com.commandAI.commandAI;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class CommandAiApplication {

	public static void main(String[] args) {
		SpringApplication.run(CommandAiApplication.class, args);
	}

}


# main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java

package com.commandAI.commandAI.modules.AIComunication.controller;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import com.commandAI.commandAI.modules.AIComunication.model.dto.GTPCommunicationRequestDTO;
import com.commandAI.commandAI.modules.AIComunication.service.IGPTCommunicationService;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.RequiredArgsConstructor;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.Arrays;
import java.util.List;

@RequiredArgsConstructor
@RestController
@RequestMapping("/api/question_answers")
public class GPTCommunicationController {

    private final IGPTCommunicationService service;

    @PostMapping("/save")
    public ResponseEntity<GPTCommunication> saveCommunication(@RequestBody GTPCommunicationRequestDTO dto) {
        try {
            GPTCommunication savedCommunication = service.saveCommunication(dto);
            return new ResponseEntity<>(savedCommunication, HttpStatus.CREATED);
        } catch (Exception e) {
            // Log error and return an appropriate response
            System.err.println("Erro ao salvar comunica√ß√£o: " + e.getMessage());
            return new ResponseEntity<>(HttpStatus.INTERNAL_SERVER_ERROR);
        }
    }

    @PostMapping("/similar")
    public ResponseEntity<List<GTPCommunicationRequestDTO>> getSimilarCommunications(@RequestBody JsonNode requestBody) {
        try {
            // Validar se √© um array
            if (!requestBody.isArray()) {
                throw new IllegalArgumentException("Esperado um array");
            }

            // Converter JsonNode para array de float
            float[] embedding = new ObjectMapper().convertValue(requestBody, float[].class);

            // Log para verificar o embedding recebido
            System.out.println("Embedding recebido: " + Arrays.toString(embedding));

            // Chamar o servi√ßo com o array de float
            List<GTPCommunicationRequestDTO> similarCommunications = service.findSimilarCommunications(embedding);
            return ResponseEntity.ok(similarCommunications);
        } catch (IllegalArgumentException e) {
            System.err.println("Erro de valida√ß√£o: " + e.getMessage());
            return ResponseEntity.status(HttpStatus.BAD_REQUEST).body(null);
        } catch (Exception e) {
            // Log error and return an appropriate response
            System.err.println("Erro ao processar a solicita√ß√£o: " + e.getMessage());
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(null);
        }
    }
}


# main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java

package com.commandAI.commandAI.modules.AIComunication.model;

import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import jakarta.persistence.*;
import lombok.Data;

import java.time.LocalDateTime;
@Data
@Entity
@JsonIgnoreProperties(ignoreUnknown = true)
@Table(name = "gpt_communication")
public class GPTCommunication {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(name = "question", nullable = false, columnDefinition = "TEXT")
    private String question;

    @Column(name = "answer", nullable = false, columnDefinition = "TEXT")
    private String answer;

    @Column(name = "question_embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] questionEmbedding;

    @Column(name = "answer_embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] answerEmbedding;

    @Column(name = "created_at", nullable = false)
    private LocalDateTime createdAt;

}

# main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java

package com.commandAI.commandAI.modules.AIComunication.model.dto;

import com.fasterxml.jackson.annotation.JsonProperty;

public record GTPCommunicationRequestDTO(
        String question,
        float[] questionEmbedding,
        String answer,
        float[] answerEmbedding
) {}



# main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java

package com.commandAI.commandAI.modules.AIComunication.repository;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

@Repository
public interface IGPTCommunicationRepository extends JpaRepository<GPTCommunication, Long> {

}

# main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java

package com.commandAI.commandAI.modules.AIComunication.service;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import com.commandAI.commandAI.modules.AIComunication.model.dto.GTPCommunicationRequestDTO;

import java.util.List;

public interface IGPTCommunicationService {
    GPTCommunication saveCommunication(GTPCommunicationRequestDTO dto);
    List<GTPCommunicationRequestDTO> findSimilarCommunications(float[] embedding);
}

# main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java

package com.commandAI.commandAI.modules.AIComunication.service.impl;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import com.commandAI.commandAI.modules.AIComunication.model.dto.GTPCommunicationRequestDTO;
import com.commandAI.commandAI.modules.AIComunication.repository.IGPTCommunicationRepository;
import com.commandAI.commandAI.modules.AIComunication.service.IGPTCommunicationService;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;

import java.time.LocalDateTime;
import java.util.Arrays;
import java.util.List;
import java.util.stream.Collectors;

@RequiredArgsConstructor
@Service
public class GPTCommunicationServiceImpl implements IGPTCommunicationService {

    private final IGPTCommunicationRepository repository;

    @Override
    public GPTCommunication saveCommunication(GTPCommunicationRequestDTO dto) {
        GPTCommunication communication = new GPTCommunication();
        communication.setQuestion(dto.question());
        communication.setQuestionEmbedding(dto.questionEmbedding());
        communication.setAnswer(dto.answer());
        communication.setAnswerEmbedding(dto.answerEmbedding());
        communication.setCreatedAt(LocalDateTime.now());
        return repository.save(communication);
    }

    @Override
    public List<GTPCommunicationRequestDTO> findSimilarCommunications(float[] embedding) {
        List<GPTCommunication> allCommunications = repository.findAll();

        return allCommunications.stream()
                .filter(comm -> isSimilar(embedding, comm.getQuestionEmbedding()))
                .map(comm -> new GTPCommunicationRequestDTO(
                        comm.getQuestion(),
                        comm.getQuestionEmbedding(),
                        comm.getAnswer(),
                        comm.getAnswerEmbedding()))
                .collect(Collectors.toList());
    }

    private boolean isSimilar(float[] embedding1, float[] embedding2) {
        // Verificar se os embeddings t√™m o mesmo tamanho
        if (embedding1.length != embedding2.length) {
            System.err.println("Os embeddings t√™m tamanhos diferentes.");
            return false;
        }

        System.out.println("Embedding1: " + Arrays.toString(embedding1));
        System.out.println("Embedding2: " + Arrays.toString(embedding2));

        double dotProduct = 0.0;
        double normA = 0.0;
        double normB = 0.0;
        for (int i = 0; i < embedding1.length; i++) {
            dotProduct += embedding1[i] * embedding2[i];
            normA += Math.pow(embedding1[i], 2);
            normB += Math.pow(embedding2[i], 2);
        }
        return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB)) > 0.8; // Ajuste o limiar conforme necess√°rio
    }
}


# main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java

package com.commandAI.commandAI.modules.context.controller;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.context.service.IContextService;
import com.commandAI.commandAI.modules.context.model.context.Context;
import lombok.RequiredArgsConstructor;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.validation.annotation.Validated;
import org.springframework.web.bind.annotation.*;

import java.util.List;
import java.util.Map;

@RequiredArgsConstructor
@RestController
@Validated
@ResponseBody
@RequestMapping("/api/contexts")
public class ContextController {

    @Autowired
    private final IContextService contextService;

    @PostMapping("/save")
    public ResponseEntity<Map<String, Object>> saveContext(@RequestBody ContextDTO data) {
        Context newContext = contextService.saveContext(data);
        Map<String, Object> response = Map.of("result", newContext);
        return ResponseEntity.status(HttpStatus.CREATED).body(response);
    }


    @GetMapping("/last")
    public ResponseEntity<ContextDTO> getLastContext() {
        ContextDTO lastContext = contextService.getLastContext();
        return ResponseEntity.ok(lastContext);
    }

    @GetMapping("/all")
    public ResponseEntity<Map<String, Object>> getAllContexts() {
        List<Context> contexts = contextService.findAllContext();
        return ResponseEntity.ok(Map.of("contexts", contexts));
    }
}


# main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java

package com.commandAI.commandAI.modules.context.controller.hendler;

import com.commandAI.commandAI.modules.context.service.validation.ContextNotFoundException;
import lombok.Data;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.MissingServletRequestParameterException;
import org.springframework.web.bind.annotation.ControllerAdvice;
import org.springframework.web.bind.annotation.ExceptionHandler;
import org.springframework.web.context.request.WebRequest;

import java.time.LocalDateTime;
import java.util.HashMap;
import java.util.Map;

@Data
@ControllerAdvice
public class GlobalExceptionHandler {

    @ExceptionHandler(ContextNotFoundException.class)
    public ResponseEntity<Object> handleContextNotFoundException(ContextNotFoundException ex, WebRequest request) {
        Map<String, Object> body = new HashMap<>();
        body.put("timestamp", LocalDateTime.now());
        body.put("message", ex.getMessage());

        return new ResponseEntity<>(body, HttpStatus.NOT_FOUND);
    }
    @ExceptionHandler(MissingServletRequestParameterException.class)
    public ResponseEntity<Object> handleMissingServletRequestParameterException(MissingServletRequestParameterException ex, WebRequest request) {
        Map<String, Object> body = new HashMap<>();
        body.put("timestamp", LocalDateTime.now());
        body.put("message", "Par√¢metro de requisi√ß√£o ausente: " + ex.getParameterName());

        return new ResponseEntity<>(body, HttpStatus.BAD_REQUEST);
    }

    // Outros handlers de exce√ß√£o podem ser adicionados aqui
}

# main\java\com\commandAI\commandAI\modules\context\model\context\Context.java

    package com.commandAI.commandAI.modules.context.model.context;
    import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
    import jakarta.persistence.*;
    import lombok.Data;

    import java.time.LocalDateTime;
    @Data
    @Table(name = "tb_context")
    @Entity
    @JsonIgnoreProperties(ignoreUnknown = true)
    public class Context {

        @Id
        @GeneratedValue(strategy = GenerationType.IDENTITY)
        private Long id;

        @Column(name = "context", nullable = false, columnDefinition = "TEXT")
        private String context;

        @Column(name = "created_at", nullable = false)
        private LocalDateTime createdAt;

        @PrePersist
        protected void onCreate() {
            createdAt = LocalDateTime.now();
        }
    }



# main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java

package com.commandAI.commandAI.modules.context.model.dto;

import com.commandAI.commandAI.modules.context.model.context.Context;
import java.time.LocalDateTime;
public record ContextDTO(
        Long id,
        String context,
        LocalDateTime createdAt
) {
    public static ContextDTO fromEntity(Context context) {
        return new ContextDTO(context.getId(), context.getContext(), context.getCreatedAt());
    }
}


# main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java

package com.commandAI.commandAI.modules.context.repository;

import com.commandAI.commandAI.modules.context.model.context.Context;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import java.util.Optional;

@Repository
public interface IContextRepository extends JpaRepository<Context, Long> {
    Optional<Context> findTopByOrderByCreatedAtDesc();
}


# main\java\com\commandAI\commandAI\modules\context\service\IContextService.java

package com.commandAI.commandAI.modules.context.service;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;

import java.util.List;

public interface IContextService {
    Context saveContext(ContextDTO data);
    ContextDTO getLastContext();
    List<Context> findAllContext();
}


# main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java

package com.commandAI.commandAI.modules.context.service.impl;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.repository.IContextRepository;
import com.commandAI.commandAI.modules.context.service.IContextService;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.util.List;

@RequiredArgsConstructor
@Service
public class ContextServiceImpl implements IContextService {

    private final IContextRepository contextRepository;

    @Override
    @Transactional
    public Context saveContext(ContextDTO data) {
        Context context = new Context();
        context.setContext(data.context());
        context = contextRepository.save(context);
        return context;
    }

    @Override
    @Transactional
    public ContextDTO getLastContext() {
        Context lastContext = contextRepository.findTopByOrderByCreatedAtDesc()
                .orElseThrow(() -> new RuntimeException("No contexts found"));
        return ContextDTO.fromEntity(lastContext);
    }
    @Override
    @Transactional
    public List<Context> findAllContext() {
        return contextRepository.findAll();
    }
}


# main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java

package com.commandAI.commandAI.modules.context.service.validation;

public class ContextNotFoundException extends RuntimeException {
    public ContextNotFoundException(String message) {
        super(message);
    }
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java

package com.commandAI.commandAI.modules.embeddingTranscription.controller;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.service.IServiceTranscriptionEmbedded;
import lombok.RequiredArgsConstructor;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.List;

@RestController
@RequestMapping("/api/meetings")    
@RequiredArgsConstructor
public class TranscriptionController {

    private final IServiceTranscriptionEmbedded service;

    @PostMapping("/transcriptions")
    public ResponseEntity<Void> addTranscription(@RequestBody TranscriptionEmbeddedDTO transcriptionDTO) {
        service.saveTranscription(transcriptionDTO.transcriptionText(), transcriptionDTO.embedding());
        return ResponseEntity.status(HttpStatus.CREATED).build();
    }

    @GetMapping("/transcriptions")
    public List<TranscriptionEmbeddedDTO> getAllTranscriptions() {
        return service.getAllTranscriptions();
    }

    @GetMapping("/transcriptions/{id}")
    public ResponseEntity<TranscriptionEmbeddedDTO> getTranscriptionById(@PathVariable Long id) {
        TranscriptionEmbeddedDTO transcription = service.getTranscriptionById(id);
        if (transcription == null) {
            return ResponseEntity.status(HttpStatus.NOT_FOUND).build();
        }
        return ResponseEntity.ok(transcription);
    }

    @GetMapping("/transcriptions/similar")
    public List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(@RequestParam float[] queryEmbedding) {
        return service.findSimilarTranscriptions(queryEmbedding);
    }
    @GetMapping("/last")
    public ResponseEntity<TranscriptionEmbeddedDTO> getLastTranscription() {
        TranscriptionEmbeddedDTO lastTranscription = service.getLastTranscription();
        return ResponseEntity.ok(lastTranscription);
    }
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java

package com.commandAI.commandAI.modules.embeddingTranscription.model;

import jakarta.persistence.*;
import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import lombok.Data;

import java.time.LocalDateTime;

@Data
@JsonIgnoreProperties(ignoreUnknown = true)
@Entity
@Table(name = "transcriptionEmbedded")
public class TranscriptionEmbedded {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    @Column(name = "meeting_id", nullable = false)
    private Long meetingId;

    @Column(name = "transcription_text", nullable = false, columnDefinition = "TEXT")
    private String transcriptionText;

    @Column(name = "embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] embedding;

    @Column(name = "created_at", nullable = false)
    private LocalDateTime createdAt;

    @PrePersist
    protected void onCreate() {
        createdAt = LocalDateTime.now();
    }

}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java

package com.commandAI.commandAI.modules.embeddingTranscription.model.dto;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;

import java.time.LocalDateTime;

public record TranscriptionEmbeddedDTO(
        Long meetingId,
        String transcriptionText,
        float[] embedding,
        LocalDateTime createdAt
) {
    public static TranscriptionEmbeddedDTO fromEntity(TranscriptionEmbedded transcriptionEmbedded) {
        return new TranscriptionEmbeddedDTO(transcriptionEmbedded.getMeetingId(), transcriptionEmbedded.getTranscriptionText(), transcriptionEmbedded.getEmbedding(), transcriptionEmbedded.getCreatedAt());
    }
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java

package com.commandAI.commandAI.modules.embeddingTranscription.repository;

import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import java.util.Optional;

@Repository
public interface ITranscriptionEmbeddedRepository extends JpaRepository<TranscriptionEmbedded, Long> {
    Optional<TranscriptionEmbedded> findTopByOrderByCreatedAtDesc();

}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java

package com.commandAI.commandAI.modules.embeddingTranscription.service;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;

import java.util.List;

public interface IServiceTranscriptionEmbedded {
    void saveTranscription(String transcriptionText, float[] embedding);
    List<TranscriptionEmbeddedDTO> getAllTranscriptions();
    TranscriptionEmbeddedDTO getTranscriptionById(Long id);
    List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(float[] queryEmbedding);
    TranscriptionEmbeddedDTO getLastTranscription();
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java

package com.commandAI.commandAI.modules.embeddingTranscription.service.impl;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.repository.ITranscriptionEmbeddedRepository;
import com.commandAI.commandAI.modules.embeddingTranscription.service.IServiceTranscriptionEmbedded;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.util.List;
import java.util.stream.Collectors;

@RequiredArgsConstructor
@Service
public class ServiceTranscriptionEmbeddedImpl implements IServiceTranscriptionEmbedded {

    private final ITranscriptionEmbeddedRepository transcriptionEmbeddedRepository;

    @Override
    @Transactional
    public void saveTranscription(String transcriptionText, float[] embedding) {
        TranscriptionEmbedded transcription = new TranscriptionEmbedded();
        transcription.setTranscriptionText(transcriptionText);
        transcription.setEmbedding(embedding);
        System.out.println(transcription);
        transcriptionEmbeddedRepository.save(transcription);
    }

    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> getAllTranscriptions() {
        return transcriptionEmbeddedRepository.findAll().stream()
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }

    @Override
    @Transactional
    public TranscriptionEmbeddedDTO getTranscriptionById(Long id) {
        TranscriptionEmbedded transcription = transcriptionEmbeddedRepository.findById(id).orElse(null);
        if (transcription == null) {
            return null;
        }
        return new TranscriptionEmbeddedDTO(
                transcription.getMeetingId(),
                transcription.getTranscriptionText(),
                transcription.getEmbedding(),
                transcription.getCreatedAt());
    }

    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(float[] queryEmbedding) {
        List<TranscriptionEmbedded> allTranscriptions = transcriptionEmbeddedRepository.findAll();
        return allTranscriptions.stream()
                .sorted((t1, t2) -> Double.compare(
                        cosineSimilarity(queryEmbedding, t2.getEmbedding()),
                        cosineSimilarity(queryEmbedding, t1.getEmbedding())))
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }
    @Override
    @Transactional
    public TranscriptionEmbeddedDTO getLastTranscription() {
        TranscriptionEmbedded lastTranscription = transcriptionEmbeddedRepository.findTopByOrderByCreatedAtDesc()
                .orElseThrow(() -> new RuntimeException("No contexts found"));
        return TranscriptionEmbeddedDTO.fromEntity(lastTranscription);
    }
    private double cosineSimilarity(float[] vec1, float[] vec2) {
        double dotProduct = 0.0;
        double normA = 0.0;
        double normB = 0.0;
        for (int i = 0; i < vec1.length; i++) {
            dotProduct += vec1[i] * vec2[i];
            normA += Math.pow(vec1[i], 2);
            normB += Math.pow(vec2[i], 2);
        }
        return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
    }
}


# test\java\com\commandAI\commandAI\CommandAiApplicationTests.java

package com.commandAI.commandAI;

import org.junit.jupiter.api.Test;
import org.springframework.boot.test.context.SpringBootTest;

@SpringBootTest
class CommandAiApplicationTests {

	@Test
	void contextLoads() {
	}
}


# test\java\com\commandAI\commandAI\modelsTest\ContextTest.java

package com.commandAI.commandAI.modelsTest;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import com.commandAI.commandAI.modules.context.model.context.Context;
import java.time.LocalDateTime;

import static org.junit.jupiter.api.Assertions.*;

public class ContextTest {

    private Context context;

    @BeforeEach
    public void setUp() {
        context = new Context();
    }

    @Test
    public void testSetAndGetId() {
        Long id = 1L;
        context.setId(id);
        assertEquals(id, context.getId());
    }

    @Test
    public void testSetAndGetContext() {
        String testContext = "Sample context";
        context.setContext(testContext);
        assertEquals(testContext, context.getContext());
    }

    @Test
    public void testSetAndGetCreatedAt() {
        LocalDateTime now = LocalDateTime.now();
        context.setCreatedAt(now);
        assertEquals(now, context.getCreatedAt());
    }
}


User: import os
import glob
from pathlib import Path
from config.config import Config
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python_e_java() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py e .java a partir de src/"""
    src_dir = Config.BASE_DIR

    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    # Busca arquivos .py e .java separadamente
    padrao_busca_py = os.path.join(src_dir, '**', '*.py')
    padrao_busca_java = os.path.join(src_dir, '**', '*.java')

    arquivos_py = glob.glob(padrao_busca_py, recursive=True)
    arquivos_java = glob.glob(padrao_busca_java, recursive=True)

    arquivos = arquivos_py + arquivos_java
    arquivos.sort() # Ordena a lista completa

    for arquivo in arquivos:
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")
            continue

    return conteudo_total
Assistant: Okay, I see the error message:

**ImportError: cannot import name 'ler_todos_arquivos_python' from 'services.search_files'**

This error indicates that your Python code in `chat_streamlit.py` is trying to import a function or class named `ler_todos_arquivos_python` from the module `services.search_files`, but that name doesn't exist in that module.

Based on your prompt, the function `ler_todos_arquivos_python_e_java` exists but has a different name.

Here are the most likely causes and solutions:

1.  **Misspelled Name:** You might have misspelled the name when importing. Double-check that `ler_todos_arquivos_python` is exactly how it's defined in `services/search_files.py`. This is often the case with long or similar-sounding names.

2.  **Incorrect Function Name:** Your code now has a function named `ler_todos_arquivos_python_e_java` to read files of python and java extensions. You must to change the function call for this new function or rename the function to `ler_todos_arquivos_python`.

3.  **Function Not Defined or Misnamed:** The function `ler_todos_arquivos_python` might not be defined within the `services/search_files.py` file at all, or it might have a slightly different name there. Look at the source code of `services/search_files.py` to confirm the actual name of the function you want to import.

4.  **Incorrect Module Path:** You might be trying to import from the wrong module altogether. Double-check that the `services/search_files.py` is the correct location of the file containing the function.

5.  **Module Not Properly Saved:** If you've made changes to `services/search_files.py`, make sure you've saved the file after making the changes. Sometimes the interpreter loads an older version.

To Fix This Error:

1.  **Examine `services/search_files.py`:** Open the `services/search_files.py` file and carefully check the following:

    *   Confirm that a function or class named either `ler_todos_arquivos_python` or `ler_todos_arquivos_python_e_java` exists. If it doesn't exist, rename the function or class to be called `ler_todos_arquivos_python`
    *   Ensure there are no typos in the name's definition.
    *   Verify that the function is actually defined (not commented out or within an `if False` block).

2.  **Update the Import Statement:** Modify your import statement in `chat_streamlit.py` to match the actual name of the function.  For example:

    ```python
    from services.search_files import ler_todos_arquivos_python_e_java  # Use the correct name
    ```

3.  **Double-Check Paths:** Ensure that your Python environment is correctly finding the `services/search_files.py` file. If your project has a complex directory structure, you might need to adjust the Python path or use relative imports more carefully.

**Example (assuming `ler_todos_arquivos_python_e_java` is the correct name):**

In `chat_streamlit.py`, change:

```python
from services.search_files import ler_todos_arquivos_python
```

To:

```python
from services.search_files import ler_todos_arquivos_python_e_java
```

Then, in the code where you are using the function, make sure that you are calling it by its correct name.

By carefully checking the name and location of the function you're trying to import, you should be able to resolve this `ImportError`.

User: agora leu ?
2025-04-14 16:55:10,831 [INFO] Resposta da IA (imagem): Assistant: After making the changes discussed and re-running your Streamlit app, if the `ImportError` is gone and the code runs without that specific error, then yes, the code "read" the function successfully!

However, "reading" successfully just means that the import worked and the function is accessible. It *doesn't* guarantee that the function is working correctly or producing the intended results. You still need to check:

1.  **Functionality:** Is the `ler_todos_arquivos_python_e_java` function doing what you expect (reading all the Python and Java files and returning their combined content)? You may want to add print statements inside the function to confirm this.

2.  **Downstream Code:** Does the rest of your `chat_streamlit.py` code correctly handle the output of `ler_todos_arquivos_python_e_java`?

In short, successful import is the *first* step, but thorough testing is still required to ensure everything works as desired.

2025-04-14 16:55:10,962 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:55:10,964 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:55:10,965 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:55:10,967 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:55:10,968 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:55:10,970 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:55:10,971 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:55:10,973 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:55:10,974 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:55:10,976 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:55:10,977 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:55:10,978 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:55:10,979 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:55:10,980 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 16:55:10,981 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 16:55:10,982 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 16:55:10,983 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 16:55:10,984 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 16:55:10,985 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 16:55:10,986 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 16:55:10,988 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 16:55:10,989 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 16:55:10,990 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 16:55:10,991 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 16:55:10,992 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 16:55:10,993 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 16:55:10,994 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 16:55:10,995 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 16:55:10,997 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 16:55:10,998 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 16:55:10,999 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 16:55:11,000 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 16:55:11,001 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 16:55:11,002 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 16:55:11,003 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 16:55:11,005 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 16:56:34,224 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:56:34,225 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:56:34,226 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:56:34,227 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:56:34,228 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:56:34,229 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:56:34,231 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:56:34,233 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:56:34,234 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:56:34,235 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:56:34,237 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:56:34,238 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:56:34,239 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:56:34,240 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 16:56:34,241 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 16:56:34,243 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 16:56:34,244 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 16:56:34,245 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 16:56:34,247 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 16:56:34,248 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 16:56:34,249 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 16:56:34,250 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 16:56:34,252 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 16:56:34,253 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 16:56:34,254 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 16:56:34,256 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 16:56:34,257 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 16:56:34,258 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 16:56:34,259 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 16:56:34,261 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 16:56:34,262 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 16:56:34,264 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 16:56:34,265 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 16:56:34,267 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 16:56:34,268 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 16:56:34,270 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 16:56:44,600 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:56:44,601 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:56:44,602 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:56:44,604 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:56:44,605 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:56:44,606 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:56:44,608 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:56:44,609 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:56:44,611 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:56:44,612 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:56:44,613 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:56:44,615 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:56:44,616 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:56:44,618 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 16:56:44,620 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 16:56:44,623 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 16:56:44,624 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 16:56:44,626 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 16:56:44,628 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 16:56:44,629 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 16:56:44,632 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 16:56:44,635 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 16:56:44,637 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 16:56:44,638 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 16:56:44,639 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 16:56:44,641 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 16:56:44,642 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 16:56:44,644 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 16:56:44,645 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 16:56:44,647 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 16:56:44,649 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 16:56:44,651 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 16:56:44,653 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 16:56:44,654 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 16:56:44,655 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 16:56:44,656 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 16:56:44,780 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:56:44,781 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:56:44,783 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:56:44,785 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:56:44,787 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:56:44,789 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:56:44,790 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:56:44,791 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:56:44,792 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:56:44,794 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:56:44,795 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:56:44,796 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:56:44,798 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:56:44,799 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 16:56:44,801 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 16:56:44,803 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 16:56:44,804 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 16:56:44,806 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 16:56:44,808 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 16:56:44,809 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 16:56:44,810 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 16:56:44,812 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 16:56:44,813 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 16:56:44,814 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 16:56:44,817 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 16:56:44,818 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 16:56:44,820 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 16:56:44,821 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 16:56:44,822 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 16:56:44,823 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 16:56:44,824 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 16:56:44,825 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 16:56:44,826 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 16:56:44,828 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 16:56:44,829 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 16:56:44,830 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 16:56:44,834 [INFO] Enviando para IA - Prompt (sem imagem): Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python_e_java

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python_e_java()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python_e_java() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py e .java a partir de src/"""
    src_dir = Config.BASE_DIR

    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    # Busca arquivos .py e .java separadamente
    padrao_busca_py = os.path.join(src_dir, '**', '*.py')
    padrao_busca_java = os.path.join(src_dir, '**', '*.java')

    arquivos_py = glob.glob(padrao_busca_py, recursive=True)
    arquivos_java = glob.glob(padrao_busca_java, recursive=True)

    arquivos = arquivos_py + arquivos_java
    arquivos.sort() # Ordena a lista completa

    for arquivo in arquivos:
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# main\java\com\commandAI\commandAI\CommandAiApplication.java

package com.commandAI.commandAI;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class CommandAiApplication {

	public static void main(String[] args) {
		SpringApplication.run(CommandAiApplication.class, args);
	}

}


# main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java

package com.commandAI.commandAI.modules.AIComunication.controller;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import com.commandAI.commandAI.modules.AIComunication.model.dto.GTPCommunicationRequestDTO;
import com.commandAI.commandAI.modules.AIComunication.service.IGPTCommunicationService;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.RequiredArgsConstructor;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.Arrays;
import java.util.List;

@RequiredArgsConstructor
@RestController
@RequestMapping("/api/question_answers")
public class GPTCommunicationController {

    private final IGPTCommunicationService service;

    @PostMapping("/save")
    public ResponseEntity<GPTCommunication> saveCommunication(@RequestBody GTPCommunicationRequestDTO dto) {
        try {
            GPTCommunication savedCommunication = service.saveCommunication(dto);
            return new ResponseEntity<>(savedCommunication, HttpStatus.CREATED);
        } catch (Exception e) {
            // Log error and return an appropriate response
            System.err.println("Erro ao salvar comunica√ß√£o: " + e.getMessage());
            return new ResponseEntity<>(HttpStatus.INTERNAL_SERVER_ERROR);
        }
    }

    @PostMapping("/similar")
    public ResponseEntity<List<GTPCommunicationRequestDTO>> getSimilarCommunications(@RequestBody JsonNode requestBody) {
        try {
            // Validar se √© um array
            if (!requestBody.isArray()) {
                throw new IllegalArgumentException("Esperado um array");
            }

            // Converter JsonNode para array de float
            float[] embedding = new ObjectMapper().convertValue(requestBody, float[].class);

            // Log para verificar o embedding recebido
            System.out.println("Embedding recebido: " + Arrays.toString(embedding));

            // Chamar o servi√ßo com o array de float
            List<GTPCommunicationRequestDTO> similarCommunications = service.findSimilarCommunications(embedding);
            return ResponseEntity.ok(similarCommunications);
        } catch (IllegalArgumentException e) {
            System.err.println("Erro de valida√ß√£o: " + e.getMessage());
            return ResponseEntity.status(HttpStatus.BAD_REQUEST).body(null);
        } catch (Exception e) {
            // Log error and return an appropriate response
            System.err.println("Erro ao processar a solicita√ß√£o: " + e.getMessage());
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(null);
        }
    }
}


# main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java

package com.commandAI.commandAI.modules.AIComunication.model;

import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import jakarta.persistence.*;
import lombok.Data;

import java.time.LocalDateTime;
@Data
@Entity
@JsonIgnoreProperties(ignoreUnknown = true)
@Table(name = "gpt_communication")
public class GPTCommunication {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(name = "question", nullable = false, columnDefinition = "TEXT")
    private String question;

    @Column(name = "answer", nullable = false, columnDefinition = "TEXT")
    private String answer;

    @Column(name = "question_embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] questionEmbedding;

    @Column(name = "answer_embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] answerEmbedding;

    @Column(name = "created_at", nullable = false)
    private LocalDateTime createdAt;

}

# main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java

package com.commandAI.commandAI.modules.AIComunication.model.dto;

import com.fasterxml.jackson.annotation.JsonProperty;

public record GTPCommunicationRequestDTO(
        String question,
        float[] questionEmbedding,
        String answer,
        float[] answerEmbedding
) {}



# main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java

package com.commandAI.commandAI.modules.AIComunication.repository;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

@Repository
public interface IGPTCommunicationRepository extends JpaRepository<GPTCommunication, Long> {

}

# main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java

package com.commandAI.commandAI.modules.AIComunication.service;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import com.commandAI.commandAI.modules.AIComunication.model.dto.GTPCommunicationRequestDTO;

import java.util.List;

public interface IGPTCommunicationService {
    GPTCommunication saveCommunication(GTPCommunicationRequestDTO dto);
    List<GTPCommunicationRequestDTO> findSimilarCommunications(float[] embedding);
}

# main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java

package com.commandAI.commandAI.modules.AIComunication.service.impl;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import com.commandAI.commandAI.modules.AIComunication.model.dto.GTPCommunicationRequestDTO;
import com.commandAI.commandAI.modules.AIComunication.repository.IGPTCommunicationRepository;
import com.commandAI.commandAI.modules.AIComunication.service.IGPTCommunicationService;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;

import java.time.LocalDateTime;
import java.util.Arrays;
import java.util.List;
import java.util.stream.Collectors;

@RequiredArgsConstructor
@Service
public class GPTCommunicationServiceImpl implements IGPTCommunicationService {

    private final IGPTCommunicationRepository repository;

    @Override
    public GPTCommunication saveCommunication(GTPCommunicationRequestDTO dto) {
        GPTCommunication communication = new GPTCommunication();
        communication.setQuestion(dto.question());
        communication.setQuestionEmbedding(dto.questionEmbedding());
        communication.setAnswer(dto.answer());
        communication.setAnswerEmbedding(dto.answerEmbedding());
        communication.setCreatedAt(LocalDateTime.now());
        return repository.save(communication);
    }

    @Override
    public List<GTPCommunicationRequestDTO> findSimilarCommunications(float[] embedding) {
        List<GPTCommunication> allCommunications = repository.findAll();

        return allCommunications.stream()
                .filter(comm -> isSimilar(embedding, comm.getQuestionEmbedding()))
                .map(comm -> new GTPCommunicationRequestDTO(
                        comm.getQuestion(),
                        comm.getQuestionEmbedding(),
                        comm.getAnswer(),
                        comm.getAnswerEmbedding()))
                .collect(Collectors.toList());
    }

    private boolean isSimilar(float[] embedding1, float[] embedding2) {
        // Verificar se os embeddings t√™m o mesmo tamanho
        if (embedding1.length != embedding2.length) {
            System.err.println("Os embeddings t√™m tamanhos diferentes.");
            return false;
        }

        System.out.println("Embedding1: " + Arrays.toString(embedding1));
        System.out.println("Embedding2: " + Arrays.toString(embedding2));

        double dotProduct = 0.0;
        double normA = 0.0;
        double normB = 0.0;
        for (int i = 0; i < embedding1.length; i++) {
            dotProduct += embedding1[i] * embedding2[i];
            normA += Math.pow(embedding1[i], 2);
            normB += Math.pow(embedding2[i], 2);
        }
        return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB)) > 0.8; // Ajuste o limiar conforme necess√°rio
    }
}


# main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java

package com.commandAI.commandAI.modules.context.controller;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.context.service.IContextService;
import com.commandAI.commandAI.modules.context.model.context.Context;
import lombok.RequiredArgsConstructor;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.validation.annotation.Validated;
import org.springframework.web.bind.annotation.*;

import java.util.List;
import java.util.Map;

@RequiredArgsConstructor
@RestController
@Validated
@ResponseBody
@RequestMapping("/api/contexts")
public class ContextController {

    @Autowired
    private final IContextService contextService;

    @PostMapping("/save")
    public ResponseEntity<Map<String, Object>> saveContext(@RequestBody ContextDTO data) {
        Context newContext = contextService.saveContext(data);
        Map<String, Object> response = Map.of("result", newContext);
        return ResponseEntity.status(HttpStatus.CREATED).body(response);
    }


    @GetMapping("/last")
    public ResponseEntity<ContextDTO> getLastContext() {
        ContextDTO lastContext = contextService.getLastContext();
        return ResponseEntity.ok(lastContext);
    }

    @GetMapping("/all")
    public ResponseEntity<Map<String, Object>> getAllContexts() {
        List<Context> contexts = contextService.findAllContext();
        return ResponseEntity.ok(Map.of("contexts", contexts));
    }
}


# main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java

package com.commandAI.commandAI.modules.context.controller.hendler;

import com.commandAI.commandAI.modules.context.service.validation.ContextNotFoundException;
import lombok.Data;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.MissingServletRequestParameterException;
import org.springframework.web.bind.annotation.ControllerAdvice;
import org.springframework.web.bind.annotation.ExceptionHandler;
import org.springframework.web.context.request.WebRequest;

import java.time.LocalDateTime;
import java.util.HashMap;
import java.util.Map;

@Data
@ControllerAdvice
public class GlobalExceptionHandler {

    @ExceptionHandler(ContextNotFoundException.class)
    public ResponseEntity<Object> handleContextNotFoundException(ContextNotFoundException ex, WebRequest request) {
        Map<String, Object> body = new HashMap<>();
        body.put("timestamp", LocalDateTime.now());
        body.put("message", ex.getMessage());

        return new ResponseEntity<>(body, HttpStatus.NOT_FOUND);
    }
    @ExceptionHandler(MissingServletRequestParameterException.class)
    public ResponseEntity<Object> handleMissingServletRequestParameterException(MissingServletRequestParameterException ex, WebRequest request) {
        Map<String, Object> body = new HashMap<>();
        body.put("timestamp", LocalDateTime.now());
        body.put("message", "Par√¢metro de requisi√ß√£o ausente: " + ex.getParameterName());

        return new ResponseEntity<>(body, HttpStatus.BAD_REQUEST);
    }

    // Outros handlers de exce√ß√£o podem ser adicionados aqui
}

# main\java\com\commandAI\commandAI\modules\context\model\context\Context.java

    package com.commandAI.commandAI.modules.context.model.context;
    import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
    import jakarta.persistence.*;
    import lombok.Data;

    import java.time.LocalDateTime;
    @Data
    @Table(name = "tb_context")
    @Entity
    @JsonIgnoreProperties(ignoreUnknown = true)
    public class Context {

        @Id
        @GeneratedValue(strategy = GenerationType.IDENTITY)
        private Long id;

        @Column(name = "context", nullable = false, columnDefinition = "TEXT")
        private String context;

        @Column(name = "created_at", nullable = false)
        private LocalDateTime createdAt;

        @PrePersist
        protected void onCreate() {
            createdAt = LocalDateTime.now();
        }
    }



# main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java

package com.commandAI.commandAI.modules.context.model.dto;

import com.commandAI.commandAI.modules.context.model.context.Context;
import java.time.LocalDateTime;
public record ContextDTO(
        Long id,
        String context,
        LocalDateTime createdAt
) {
    public static ContextDTO fromEntity(Context context) {
        return new ContextDTO(context.getId(), context.getContext(), context.getCreatedAt());
    }
}


# main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java

package com.commandAI.commandAI.modules.context.repository;

import com.commandAI.commandAI.modules.context.model.context.Context;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import java.util.Optional;

@Repository
public interface IContextRepository extends JpaRepository<Context, Long> {
    Optional<Context> findTopByOrderByCreatedAtDesc();
}


# main\java\com\commandAI\commandAI\modules\context\service\IContextService.java

package com.commandAI.commandAI.modules.context.service;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;

import java.util.List;

public interface IContextService {
    Context saveContext(ContextDTO data);
    ContextDTO getLastContext();
    List<Context> findAllContext();
}


# main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java

package com.commandAI.commandAI.modules.context.service.impl;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.repository.IContextRepository;
import com.commandAI.commandAI.modules.context.service.IContextService;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.util.List;

@RequiredArgsConstructor
@Service
public class ContextServiceImpl implements IContextService {

    private final IContextRepository contextRepository;

    @Override
    @Transactional
    public Context saveContext(ContextDTO data) {
        Context context = new Context();
        context.setContext(data.context());
        context = contextRepository.save(context);
        return context;
    }

    @Override
    @Transactional
    public ContextDTO getLastContext() {
        Context lastContext = contextRepository.findTopByOrderByCreatedAtDesc()
                .orElseThrow(() -> new RuntimeException("No contexts found"));
        return ContextDTO.fromEntity(lastContext);
    }
    @Override
    @Transactional
    public List<Context> findAllContext() {
        return contextRepository.findAll();
    }
}


# main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java

package com.commandAI.commandAI.modules.context.service.validation;

public class ContextNotFoundException extends RuntimeException {
    public ContextNotFoundException(String message) {
        super(message);
    }
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java

package com.commandAI.commandAI.modules.embeddingTranscription.controller;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.service.IServiceTranscriptionEmbedded;
import lombok.RequiredArgsConstructor;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.List;

@RestController
@RequestMapping("/api/meetings")    
@RequiredArgsConstructor
public class TranscriptionController {

    private final IServiceTranscriptionEmbedded service;

    @PostMapping("/transcriptions")
    public ResponseEntity<Void> addTranscription(@RequestBody TranscriptionEmbeddedDTO transcriptionDTO) {
        service.saveTranscription(transcriptionDTO.transcriptionText(), transcriptionDTO.embedding());
        return ResponseEntity.status(HttpStatus.CREATED).build();
    }

    @GetMapping("/transcriptions")
    public List<TranscriptionEmbeddedDTO> getAllTranscriptions() {
        return service.getAllTranscriptions();
    }

    @GetMapping("/transcriptions/{id}")
    public ResponseEntity<TranscriptionEmbeddedDTO> getTranscriptionById(@PathVariable Long id) {
        TranscriptionEmbeddedDTO transcription = service.getTranscriptionById(id);
        if (transcription == null) {
            return ResponseEntity.status(HttpStatus.NOT_FOUND).build();
        }
        return ResponseEntity.ok(transcription);
    }

    @GetMapping("/transcriptions/similar")
    public List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(@RequestParam float[] queryEmbedding) {
        return service.findSimilarTranscriptions(queryEmbedding);
    }
    @GetMapping("/last")
    public ResponseEntity<TranscriptionEmbeddedDTO> getLastTranscription() {
        TranscriptionEmbeddedDTO lastTranscription = service.getLastTranscription();
        return ResponseEntity.ok(lastTranscription);
    }
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java

package com.commandAI.commandAI.modules.embeddingTranscription.model;

import jakarta.persistence.*;
import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import lombok.Data;

import java.time.LocalDateTime;

@Data
@JsonIgnoreProperties(ignoreUnknown = true)
@Entity
@Table(name = "transcriptionEmbedded")
public class TranscriptionEmbedded {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    @Column(name = "meeting_id", nullable = false)
    private Long meetingId;

    @Column(name = "transcription_text", nullable = false, columnDefinition = "TEXT")
    private String transcriptionText;

    @Column(name = "embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] embedding;

    @Column(name = "created_at", nullable = false)
    private LocalDateTime createdAt;

    @PrePersist
    protected void onCreate() {
        createdAt = LocalDateTime.now();
    }

}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java

package com.commandAI.commandAI.modules.embeddingTranscription.model.dto;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;

import java.time.LocalDateTime;

public record TranscriptionEmbeddedDTO(
        Long meetingId,
        String transcriptionText,
        float[] embedding,
        LocalDateTime createdAt
) {
    public static TranscriptionEmbeddedDTO fromEntity(TranscriptionEmbedded transcriptionEmbedded) {
        return new TranscriptionEmbeddedDTO(transcriptionEmbedded.getMeetingId(), transcriptionEmbedded.getTranscriptionText(), transcriptionEmbedded.getEmbedding(), transcriptionEmbedded.getCreatedAt());
    }
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java

package com.commandAI.commandAI.modules.embeddingTranscription.repository;

import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import java.util.Optional;

@Repository
public interface ITranscriptionEmbeddedRepository extends JpaRepository<TranscriptionEmbedded, Long> {
    Optional<TranscriptionEmbedded> findTopByOrderByCreatedAtDesc();

}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java

package com.commandAI.commandAI.modules.embeddingTranscription.service;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;

import java.util.List;

public interface IServiceTranscriptionEmbedded {
    void saveTranscription(String transcriptionText, float[] embedding);
    List<TranscriptionEmbeddedDTO> getAllTranscriptions();
    TranscriptionEmbeddedDTO getTranscriptionById(Long id);
    List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(float[] queryEmbedding);
    TranscriptionEmbeddedDTO getLastTranscription();
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java

package com.commandAI.commandAI.modules.embeddingTranscription.service.impl;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.repository.ITranscriptionEmbeddedRepository;
import com.commandAI.commandAI.modules.embeddingTranscription.service.IServiceTranscriptionEmbedded;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.util.List;
import java.util.stream.Collectors;

@RequiredArgsConstructor
@Service
public class ServiceTranscriptionEmbeddedImpl implements IServiceTranscriptionEmbedded {

    private final ITranscriptionEmbeddedRepository transcriptionEmbeddedRepository;

    @Override
    @Transactional
    public void saveTranscription(String transcriptionText, float[] embedding) {
        TranscriptionEmbedded transcription = new TranscriptionEmbedded();
        transcription.setTranscriptionText(transcriptionText);
        transcription.setEmbedding(embedding);
        System.out.println(transcription);
        transcriptionEmbeddedRepository.save(transcription);
    }

    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> getAllTranscriptions() {
        return transcriptionEmbeddedRepository.findAll().stream()
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }

    @Override
    @Transactional
    public TranscriptionEmbeddedDTO getTranscriptionById(Long id) {
        TranscriptionEmbedded transcription = transcriptionEmbeddedRepository.findById(id).orElse(null);
        if (transcription == null) {
            return null;
        }
        return new TranscriptionEmbeddedDTO(
                transcription.getMeetingId(),
                transcription.getTranscriptionText(),
                transcription.getEmbedding(),
                transcription.getCreatedAt());
    }

    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(float[] queryEmbedding) {
        List<TranscriptionEmbedded> allTranscriptions = transcriptionEmbeddedRepository.findAll();
        return allTranscriptions.stream()
                .sorted((t1, t2) -> Double.compare(
                        cosineSimilarity(queryEmbedding, t2.getEmbedding()),
                        cosineSimilarity(queryEmbedding, t1.getEmbedding())))
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }
    @Override
    @Transactional
    public TranscriptionEmbeddedDTO getLastTranscription() {
        TranscriptionEmbedded lastTranscription = transcriptionEmbeddedRepository.findTopByOrderByCreatedAtDesc()
                .orElseThrow(() -> new RuntimeException("No contexts found"));
        return TranscriptionEmbeddedDTO.fromEntity(lastTranscription);
    }
    private double cosineSimilarity(float[] vec1, float[] vec2) {
        double dotProduct = 0.0;
        double normA = 0.0;
        double normB = 0.0;
        for (int i = 0; i < vec1.length; i++) {
            dotProduct += vec1[i] * vec2[i];
            normA += Math.pow(vec1[i], 2);
            normB += Math.pow(vec2[i], 2);
        }
        return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
    }
}


# test\java\com\commandAI\commandAI\CommandAiApplicationTests.java

package com.commandAI.commandAI;

import org.junit.jupiter.api.Test;
import org.springframework.boot.test.context.SpringBootTest;

@SpringBootTest
class CommandAiApplicationTests {

	@Test
	void contextLoads() {
	}
}


# test\java\com\commandAI\commandAI\modelsTest\ContextTest.java

package com.commandAI.commandAI.modelsTest;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import com.commandAI.commandAI.modules.context.model.context.Context;
import java.time.LocalDateTime;

import static org.junit.jupiter.api.Assertions.*;

public class ContextTest {

    private Context context;

    @BeforeEach
    public void setUp() {
        context = new Context();
    }

    @Test
    public void testSetAndGetId() {
        Long id = 1L;
        context.setId(id);
        assertEquals(id, context.getId());
    }

    @Test
    public void testSetAndGetContext() {
        String testContext = "Sample context";
        context.setContext(testContext);
        assertEquals(testContext, context.getContext());
    }

    @Test
    public void testSetAndGetCreatedAt() {
        LocalDateTime now = LocalDateTime.now();
        context.setCreatedAt(now);
        assertEquals(now, context.getCreatedAt());
    }
}


User: resuma a minha aplica√ß√£o JAVA
2025-04-14 16:56:50,559 [INFO] Resposta da IA (texto): Com certeza! Sua aplica√ß√£o Java, "CommandAI", √© um projeto Spring Boot que visa facilitar a comunica√ß√£o com intelig√™ncia artificial (como o GPT), armazenando contextos e transcri√ß√µes de forma inteligente. Aqui est√° um resumo dos principais componentes:

**Componentes Principais:**

*   **M√≥dulos:**
    *   **AIComunication:** Lida com a comunica√ß√£o com a IA, incluindo salvar as perguntas e respostas, gerar embeddings para o texto e encontrar comunica√ß√µes similares com base nos embeddings.
    *   **Context:** Permite salvar e recuperar o contexto da conversa, mantendo o hist√≥rico das intera√ß√µes.
    *   **EmbeddingTranscription:** Gerencia as transcri√ß√µes das reuni√µes com os respectivos "embeddings" (representa√ß√µes vetoriais) para facilitar a busca de transcri√ß√µes similares.

*   **Entidades (Modelos):**

    *   `GPTCommunication`: Representa uma intera√ß√£o com a IA, armazenando a pergunta, a resposta e os "embeddings" vetoriais correspondentes.
    *   `Context`:  Representa um contexto de conversa.
    *   `TranscriptionEmbedded`: Representa uma transcri√ß√£o de reuni√£o com o seu embedding.

*   **Repositories:**
    *   Interfaces que estendem `JpaRepository` para interagir com o banco de dados para as entidades: `GPTCommunication`, `Context`, e `TranscriptionEmbedded`.

*   **Servi√ßos:**
    *   Implementa√ß√µes das interfaces de servi√ßo (ex: `GPTCommunicationServiceImpl`, `ContextServiceImpl`, `ServiceTranscriptionEmbeddedImpl`) que cont√™m a l√≥gica de neg√≥cios para salvar, recuperar e encontrar informa√ß√µes relacionadas √† comunica√ß√£o com a IA, contextos e transcri√ß√µes.

*   **Controladores (Controllers):**

    *   `GPTCommunicationController`:  Exp√µe endpoints REST para salvar as intera√ß√µes com a IA (`/api/question_answers`) e buscar intera√ß√µes similares.
    *   `ContextController`:  Gerencia os contextos de conversa, permitindo salvar, recuperar o √∫ltimo contexto e todos os contextos (`/api/contexts`).
    *   `TranscriptionController`:  Lida com as transcri√ß√µes de reuni√µes, permitindo adicionar, recuperar, buscar transcri√ß√µes similares e o √∫ltimo contexto (`/api/meetings`).

*   **DTOs (Data Transfer Objects):**

    *   `GTPCommunicationRequestDTO`, `ContextDTO`, `TranscriptionEmbeddedDTO`: Usados para transferir dados entre a aplica√ß√£o e o mundo externo (ex: requisi√ß√µes e respostas da API).

**Funcionalidades:**

*   **Armazenamento de intera√ß√µes com a IA:** Salva as perguntas e respostas trocadas com a IA, junto com os "embeddings" vetoriais do texto.
*   **Busca de intera√ß√µes similares:** Encontra intera√ß√µes semelhantes com base nos "embeddings" vetoriais, permitindo reutilizar informa√ß√µes e contextos.
*   **Gerenciamento de contexto:** Permite salvar e recuperar o contexto da conversa, garantindo que a IA tenha o hist√≥rico necess√°rio para responder adequadamente.
*   **Transcri√ß√µes de reuni√µes:** Permite salvar as transcri√ß√µes de reuni√µes e associ√°-las a "embeddings" vetoriais para facilitar a busca de reuni√µes relevantes.
*   **APIs REST:** Exp√µe APIs REST para facilitar a integra√ß√£o com outras aplica√ß√µes e servi√ßos.

**Em resumo:** A aplica√ß√£o CommandAI √© uma solu√ß√£o completa para gerenciar a comunica√ß√£o com a IA, armazenando contextos e transcri√ß√µes de forma inteligente, permitindo a busca de informa√ß√µes relevantes e a reutiliza√ß√£o de conhecimento.
2025-04-14 16:56:50,728 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:56:50,729 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:56:50,731 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:56:50,732 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:56:50,734 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:56:50,735 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:56:50,736 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:56:50,738 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:56:50,739 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:56:50,740 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:56:50,741 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:56:50,743 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:56:50,744 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:56:50,746 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 16:56:50,748 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 16:56:50,749 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 16:56:50,750 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 16:56:50,751 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 16:56:50,752 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 16:56:50,754 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 16:56:50,755 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 16:56:50,756 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 16:56:50,757 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 16:56:50,758 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 16:56:50,759 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 16:56:50,761 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 16:56:50,763 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 16:56:50,764 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 16:56:50,766 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 16:56:50,767 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 16:56:50,769 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 16:56:50,770 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 16:56:50,771 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 16:56:50,772 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 16:56:50,773 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 16:56:50,775 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 17:02:11,357 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 17:02:11,358 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 17:02:11,359 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 17:02:11,361 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 17:02:11,362 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 17:02:11,363 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 17:02:11,365 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 17:02:11,367 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 17:02:11,368 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 17:02:11,369 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 17:02:11,371 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 17:02:11,372 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 17:02:11,374 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 17:02:11,375 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 17:02:11,377 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 17:02:11,378 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 17:02:11,379 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 17:02:11,381 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 17:02:11,383 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 17:02:11,386 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 17:02:11,389 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 17:02:11,390 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 17:02:11,391 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 17:02:11,392 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 17:02:11,393 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 17:02:11,395 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 17:02:11,396 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 17:02:11,397 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 17:02:11,400 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 17:02:11,401 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 17:02:11,403 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 17:02:11,404 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 17:02:11,405 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 17:02:11,407 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 17:02:11,408 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 17:02:11,409 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 17:02:11,533 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 17:02:11,534 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 17:02:11,537 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 17:02:11,538 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 17:02:11,539 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 17:02:11,541 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 17:02:11,542 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 17:02:11,543 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 17:02:11,545 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 17:02:11,546 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 17:02:11,547 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 17:02:11,549 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 17:02:11,551 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 17:02:11,552 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 17:02:11,555 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 17:02:11,556 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 17:02:11,558 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 17:02:11,560 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 17:02:11,561 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 17:02:11,562 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 17:02:11,564 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 17:02:11,565 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 17:02:11,567 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 17:02:11,570 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 17:02:11,572 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 17:02:11,573 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 17:02:11,575 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 17:02:11,576 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 17:02:11,577 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 17:02:11,578 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 17:02:11,579 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 17:02:11,580 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 17:02:11,581 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 17:02:11,583 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 17:02:11,584 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 17:02:11,585 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 17:02:11,587 [INFO] Enviando para IA - Prompt (sem imagem): Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python_e_java

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python_e_java()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python_e_java() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py e .java a partir de src/"""
    src_dir = Config.BASE_DIR

    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    # Busca arquivos .py e .java separadamente
    padrao_busca_py = os.path.join(src_dir, '**', '*.py')
    padrao_busca_java = os.path.join(src_dir, '**', '*.java')

    arquivos_py = glob.glob(padrao_busca_py, recursive=True)
    arquivos_java = glob.glob(padrao_busca_java, recursive=True)

    arquivos = arquivos_py + arquivos_java
    arquivos.sort() # Ordena a lista completa

    for arquivo in arquivos:
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# main\java\com\commandAI\commandAI\CommandAiApplication.java

package com.commandAI.commandAI;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class CommandAiApplication {

	public static void main(String[] args) {
		SpringApplication.run(CommandAiApplication.class, args);
	}

}


# main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java

package com.commandAI.commandAI.modules.AIComunication.controller;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import com.commandAI.commandAI.modules.AIComunication.model.dto.GTPCommunicationRequestDTO;
import com.commandAI.commandAI.modules.AIComunication.service.IGPTCommunicationService;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.RequiredArgsConstructor;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.Arrays;
import java.util.List;

@RequiredArgsConstructor
@RestController
@RequestMapping("/api/question_answers")
public class GPTCommunicationController {

    private final IGPTCommunicationService service;

    @PostMapping("/save")
    public ResponseEntity<GPTCommunication> saveCommunication(@RequestBody GTPCommunicationRequestDTO dto) {
        try {
            GPTCommunication savedCommunication = service.saveCommunication(dto);
            return new ResponseEntity<>(savedCommunication, HttpStatus.CREATED);
        } catch (Exception e) {
            // Log error and return an appropriate response
            System.err.println("Erro ao salvar comunica√ß√£o: " + e.getMessage());
            return new ResponseEntity<>(HttpStatus.INTERNAL_SERVER_ERROR);
        }
    }

    @PostMapping("/similar")
    public ResponseEntity<List<GTPCommunicationRequestDTO>> getSimilarCommunications(@RequestBody JsonNode requestBody) {
        try {
            // Validar se √© um array
            if (!requestBody.isArray()) {
                throw new IllegalArgumentException("Esperado um array");
            }

            // Converter JsonNode para array de float
            float[] embedding = new ObjectMapper().convertValue(requestBody, float[].class);

            // Log para verificar o embedding recebido
            System.out.println("Embedding recebido: " + Arrays.toString(embedding));

            // Chamar o servi√ßo com o array de float
            List<GTPCommunicationRequestDTO> similarCommunications = service.findSimilarCommunications(embedding);
            return ResponseEntity.ok(similarCommunications);
        } catch (IllegalArgumentException e) {
            System.err.println("Erro de valida√ß√£o: " + e.getMessage());
            return ResponseEntity.status(HttpStatus.BAD_REQUEST).body(null);
        } catch (Exception e) {
            // Log error and return an appropriate response
            System.err.println("Erro ao processar a solicita√ß√£o: " + e.getMessage());
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(null);
        }
    }
}


# main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java

package com.commandAI.commandAI.modules.AIComunication.model;

import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import jakarta.persistence.*;
import lombok.Data;

import java.time.LocalDateTime;
@Data
@Entity
@JsonIgnoreProperties(ignoreUnknown = true)
@Table(name = "gpt_communication")
public class GPTCommunication {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(name = "question", nullable = false, columnDefinition = "TEXT")
    private String question;

    @Column(name = "answer", nullable = false, columnDefinition = "TEXT")
    private String answer;

    @Column(name = "question_embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] questionEmbedding;

    @Column(name = "answer_embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] answerEmbedding;

    @Column(name = "created_at", nullable = false)
    private LocalDateTime createdAt;

}

# main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java

package com.commandAI.commandAI.modules.AIComunication.model.dto;

import com.fasterxml.jackson.annotation.JsonProperty;

public record GTPCommunicationRequestDTO(
        String question,
        float[] questionEmbedding,
        String answer,
        float[] answerEmbedding
) {}



# main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java

package com.commandAI.commandAI.modules.AIComunication.repository;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

@Repository
public interface IGPTCommunicationRepository extends JpaRepository<GPTCommunication, Long> {

}

# main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java

package com.commandAI.commandAI.modules.AIComunication.service;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import com.commandAI.commandAI.modules.AIComunication.model.dto.GTPCommunicationRequestDTO;

import java.util.List;

public interface IGPTCommunicationService {
    GPTCommunication saveCommunication(GTPCommunicationRequestDTO dto);
    List<GTPCommunicationRequestDTO> findSimilarCommunications(float[] embedding);
}

# main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java

package com.commandAI.commandAI.modules.AIComunication.service.impl;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import com.commandAI.commandAI.modules.AIComunication.model.dto.GTPCommunicationRequestDTO;
import com.commandAI.commandAI.modules.AIComunication.repository.IGPTCommunicationRepository;
import com.commandAI.commandAI.modules.AIComunication.service.IGPTCommunicationService;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;

import java.time.LocalDateTime;
import java.util.Arrays;
import java.util.List;
import java.util.stream.Collectors;

@RequiredArgsConstructor
@Service
public class GPTCommunicationServiceImpl implements IGPTCommunicationService {

    private final IGPTCommunicationRepository repository;

    @Override
    public GPTCommunication saveCommunication(GTPCommunicationRequestDTO dto) {
        GPTCommunication communication = new GPTCommunication();
        communication.setQuestion(dto.question());
        communication.setQuestionEmbedding(dto.questionEmbedding());
        communication.setAnswer(dto.answer());
        communication.setAnswerEmbedding(dto.answerEmbedding());
        communication.setCreatedAt(LocalDateTime.now());
        return repository.save(communication);
    }

    @Override
    public List<GTPCommunicationRequestDTO> findSimilarCommunications(float[] embedding) {
        List<GPTCommunication> allCommunications = repository.findAll();

        return allCommunications.stream()
                .filter(comm -> isSimilar(embedding, comm.getQuestionEmbedding()))
                .map(comm -> new GTPCommunicationRequestDTO(
                        comm.getQuestion(),
                        comm.getQuestionEmbedding(),
                        comm.getAnswer(),
                        comm.getAnswerEmbedding()))
                .collect(Collectors.toList());
    }

    private boolean isSimilar(float[] embedding1, float[] embedding2) {
        // Verificar se os embeddings t√™m o mesmo tamanho
        if (embedding1.length != embedding2.length) {
            System.err.println("Os embeddings t√™m tamanhos diferentes.");
            return false;
        }

        System.out.println("Embedding1: " + Arrays.toString(embedding1));
        System.out.println("Embedding2: " + Arrays.toString(embedding2));

        double dotProduct = 0.0;
        double normA = 0.0;
        double normB = 0.0;
        for (int i = 0; i < embedding1.length; i++) {
            dotProduct += embedding1[i] * embedding2[i];
            normA += Math.pow(embedding1[i], 2);
            normB += Math.pow(embedding2[i], 2);
        }
        return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB)) > 0.8; // Ajuste o limiar conforme necess√°rio
    }
}


# main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java

package com.commandAI.commandAI.modules.context.controller;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.context.service.IContextService;
import com.commandAI.commandAI.modules.context.model.context.Context;
import lombok.RequiredArgsConstructor;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.validation.annotation.Validated;
import org.springframework.web.bind.annotation.*;

import java.util.List;
import java.util.Map;

@RequiredArgsConstructor
@RestController
@Validated
@ResponseBody
@RequestMapping("/api/contexts")
public class ContextController {

    @Autowired
    private final IContextService contextService;

    @PostMapping("/save")
    public ResponseEntity<Map<String, Object>> saveContext(@RequestBody ContextDTO data) {
        Context newContext = contextService.saveContext(data);
        Map<String, Object> response = Map.of("result", newContext);
        return ResponseEntity.status(HttpStatus.CREATED).body(response);
    }


    @GetMapping("/last")
    public ResponseEntity<ContextDTO> getLastContext() {
        ContextDTO lastContext = contextService.getLastContext();
        return ResponseEntity.ok(lastContext);
    }

    @GetMapping("/all")
    public ResponseEntity<Map<String, Object>> getAllContexts() {
        List<Context> contexts = contextService.findAllContext();
        return ResponseEntity.ok(Map.of("contexts", contexts));
    }
}


# main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java

package com.commandAI.commandAI.modules.context.controller.hendler;

import com.commandAI.commandAI.modules.context.service.validation.ContextNotFoundException;
import lombok.Data;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.MissingServletRequestParameterException;
import org.springframework.web.bind.annotation.ControllerAdvice;
import org.springframework.web.bind.annotation.ExceptionHandler;
import org.springframework.web.context.request.WebRequest;

import java.time.LocalDateTime;
import java.util.HashMap;
import java.util.Map;

@Data
@ControllerAdvice
public class GlobalExceptionHandler {

    @ExceptionHandler(ContextNotFoundException.class)
    public ResponseEntity<Object> handleContextNotFoundException(ContextNotFoundException ex, WebRequest request) {
        Map<String, Object> body = new HashMap<>();
        body.put("timestamp", LocalDateTime.now());
        body.put("message", ex.getMessage());

        return new ResponseEntity<>(body, HttpStatus.NOT_FOUND);
    }
    @ExceptionHandler(MissingServletRequestParameterException.class)
    public ResponseEntity<Object> handleMissingServletRequestParameterException(MissingServletRequestParameterException ex, WebRequest request) {
        Map<String, Object> body = new HashMap<>();
        body.put("timestamp", LocalDateTime.now());
        body.put("message", "Par√¢metro de requisi√ß√£o ausente: " + ex.getParameterName());

        return new ResponseEntity<>(body, HttpStatus.BAD_REQUEST);
    }

    // Outros handlers de exce√ß√£o podem ser adicionados aqui
}

# main\java\com\commandAI\commandAI\modules\context\model\context\Context.java

    package com.commandAI.commandAI.modules.context.model.context;
    import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
    import jakarta.persistence.*;
    import lombok.Data;

    import java.time.LocalDateTime;
    @Data
    @Table(name = "tb_context")
    @Entity
    @JsonIgnoreProperties(ignoreUnknown = true)
    public class Context {

        @Id
        @GeneratedValue(strategy = GenerationType.IDENTITY)
        private Long id;

        @Column(name = "context", nullable = false, columnDefinition = "TEXT")
        private String context;

        @Column(name = "created_at", nullable = false)
        private LocalDateTime createdAt;

        @PrePersist
        protected void onCreate() {
            createdAt = LocalDateTime.now();
        }
    }



# main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java

package com.commandAI.commandAI.modules.context.model.dto;

import com.commandAI.commandAI.modules.context.model.context.Context;
import java.time.LocalDateTime;
public record ContextDTO(
        Long id,
        String context,
        LocalDateTime createdAt
) {
    public static ContextDTO fromEntity(Context context) {
        return new ContextDTO(context.getId(), context.getContext(), context.getCreatedAt());
    }
}


# main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java

package com.commandAI.commandAI.modules.context.repository;

import com.commandAI.commandAI.modules.context.model.context.Context;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import java.util.Optional;

@Repository
public interface IContextRepository extends JpaRepository<Context, Long> {
    Optional<Context> findTopByOrderByCreatedAtDesc();
}


# main\java\com\commandAI\commandAI\modules\context\service\IContextService.java

package com.commandAI.commandAI.modules.context.service;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;

import java.util.List;

public interface IContextService {
    Context saveContext(ContextDTO data);
    ContextDTO getLastContext();
    List<Context> findAllContext();
}


# main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java

package com.commandAI.commandAI.modules.context.service.impl;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.repository.IContextRepository;
import com.commandAI.commandAI.modules.context.service.IContextService;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.util.List;

@RequiredArgsConstructor
@Service
public class ContextServiceImpl implements IContextService {

    private final IContextRepository contextRepository;

    @Override
    @Transactional
    public Context saveContext(ContextDTO data) {
        Context context = new Context();
        context.setContext(data.context());
        context = contextRepository.save(context);
        return context;
    }

    @Override
    @Transactional
    public ContextDTO getLastContext() {
        Context lastContext = contextRepository.findTopByOrderByCreatedAtDesc()
                .orElseThrow(() -> new RuntimeException("No contexts found"));
        return ContextDTO.fromEntity(lastContext);
    }
    @Override
    @Transactional
    public List<Context> findAllContext() {
        return contextRepository.findAll();
    }
}


# main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java

package com.commandAI.commandAI.modules.context.service.validation;

public class ContextNotFoundException extends RuntimeException {
    public ContextNotFoundException(String message) {
        super(message);
    }
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java

package com.commandAI.commandAI.modules.embeddingTranscription.controller;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.service.IServiceTranscriptionEmbedded;
import lombok.RequiredArgsConstructor;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.List;

@RestController
@RequestMapping("/api/meetings")    
@RequiredArgsConstructor
public class TranscriptionController {

    private final IServiceTranscriptionEmbedded service;

    @PostMapping("/transcriptions")
    public ResponseEntity<Void> addTranscription(@RequestBody TranscriptionEmbeddedDTO transcriptionDTO) {
        service.saveTranscription(transcriptionDTO.transcriptionText(), transcriptionDTO.embedding());
        return ResponseEntity.status(HttpStatus.CREATED).build();
    }

    @GetMapping("/transcriptions")
    public List<TranscriptionEmbeddedDTO> getAllTranscriptions() {
        return service.getAllTranscriptions();
    }

    @GetMapping("/transcriptions/{id}")
    public ResponseEntity<TranscriptionEmbeddedDTO> getTranscriptionById(@PathVariable Long id) {
        TranscriptionEmbeddedDTO transcription = service.getTranscriptionById(id);
        if (transcription == null) {
            return ResponseEntity.status(HttpStatus.NOT_FOUND).build();
        }
        return ResponseEntity.ok(transcription);
    }

    @GetMapping("/transcriptions/similar")
    public List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(@RequestParam float[] queryEmbedding) {
        return service.findSimilarTranscriptions(queryEmbedding);
    }
    @GetMapping("/last")
    public ResponseEntity<TranscriptionEmbeddedDTO> getLastTranscription() {
        TranscriptionEmbeddedDTO lastTranscription = service.getLastTranscription();
        return ResponseEntity.ok(lastTranscription);
    }
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java

package com.commandAI.commandAI.modules.embeddingTranscription.model;

import jakarta.persistence.*;
import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import lombok.Data;

import java.time.LocalDateTime;

@Data
@JsonIgnoreProperties(ignoreUnknown = true)
@Entity
@Table(name = "transcriptionEmbedded")
public class TranscriptionEmbedded {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    @Column(name = "meeting_id", nullable = false)
    private Long meetingId;

    @Column(name = "transcription_text", nullable = false, columnDefinition = "TEXT")
    private String transcriptionText;

    @Column(name = "embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] embedding;

    @Column(name = "created_at", nullable = false)
    private LocalDateTime createdAt;

    @PrePersist
    protected void onCreate() {
        createdAt = LocalDateTime.now();
    }

}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java

package com.commandAI.commandAI.modules.embeddingTranscription.model.dto;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;

import java.time.LocalDateTime;

public record TranscriptionEmbeddedDTO(
        Long meetingId,
        String transcriptionText,
        float[] embedding,
        LocalDateTime createdAt
) {
    public static TranscriptionEmbeddedDTO fromEntity(TranscriptionEmbedded transcriptionEmbedded) {
        return new TranscriptionEmbeddedDTO(transcriptionEmbedded.getMeetingId(), transcriptionEmbedded.getTranscriptionText(), transcriptionEmbedded.getEmbedding(), transcriptionEmbedded.getCreatedAt());
    }
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java

package com.commandAI.commandAI.modules.embeddingTranscription.repository;

import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import java.util.Optional;

@Repository
public interface ITranscriptionEmbeddedRepository extends JpaRepository<TranscriptionEmbedded, Long> {
    Optional<TranscriptionEmbedded> findTopByOrderByCreatedAtDesc();

}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java

package com.commandAI.commandAI.modules.embeddingTranscription.service;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;

import java.util.List;

public interface IServiceTranscriptionEmbedded {
    void saveTranscription(String transcriptionText, float[] embedding);
    List<TranscriptionEmbeddedDTO> getAllTranscriptions();
    TranscriptionEmbeddedDTO getTranscriptionById(Long id);
    List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(float[] queryEmbedding);
    TranscriptionEmbeddedDTO getLastTranscription();
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java

package com.commandAI.commandAI.modules.embeddingTranscription.service.impl;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.repository.ITranscriptionEmbeddedRepository;
import com.commandAI.commandAI.modules.embeddingTranscription.service.IServiceTranscriptionEmbedded;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.util.List;
import java.util.stream.Collectors;

@RequiredArgsConstructor
@Service
public class ServiceTranscriptionEmbeddedImpl implements IServiceTranscriptionEmbedded {

    private final ITranscriptionEmbeddedRepository transcriptionEmbeddedRepository;

    @Override
    @Transactional
    public void saveTranscription(String transcriptionText, float[] embedding) {
        TranscriptionEmbedded transcription = new TranscriptionEmbedded();
        transcription.setTranscriptionText(transcriptionText);
        transcription.setEmbedding(embedding);
        System.out.println(transcription);
        transcriptionEmbeddedRepository.save(transcription);
    }

    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> getAllTranscriptions() {
        return transcriptionEmbeddedRepository.findAll().stream()
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }

    @Override
    @Transactional
    public TranscriptionEmbeddedDTO getTranscriptionById(Long id) {
        TranscriptionEmbedded transcription = transcriptionEmbeddedRepository.findById(id).orElse(null);
        if (transcription == null) {
            return null;
        }
        return new TranscriptionEmbeddedDTO(
                transcription.getMeetingId(),
                transcription.getTranscriptionText(),
                transcription.getEmbedding(),
                transcription.getCreatedAt());
    }

    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(float[] queryEmbedding) {
        List<TranscriptionEmbedded> allTranscriptions = transcriptionEmbeddedRepository.findAll();
        return allTranscriptions.stream()
                .sorted((t1, t2) -> Double.compare(
                        cosineSimilarity(queryEmbedding, t2.getEmbedding()),
                        cosineSimilarity(queryEmbedding, t1.getEmbedding())))
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }
    @Override
    @Transactional
    public TranscriptionEmbeddedDTO getLastTranscription() {
        TranscriptionEmbedded lastTranscription = transcriptionEmbeddedRepository.findTopByOrderByCreatedAtDesc()
                .orElseThrow(() -> new RuntimeException("No contexts found"));
        return TranscriptionEmbeddedDTO.fromEntity(lastTranscription);
    }
    private double cosineSimilarity(float[] vec1, float[] vec2) {
        double dotProduct = 0.0;
        double normA = 0.0;
        double normB = 0.0;
        for (int i = 0; i < vec1.length; i++) {
            dotProduct += vec1[i] * vec2[i];
            normA += Math.pow(vec1[i], 2);
            normB += Math.pow(vec2[i], 2);
        }
        return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
    }
}


# test\java\com\commandAI\commandAI\CommandAiApplicationTests.java

package com.commandAI.commandAI;

import org.junit.jupiter.api.Test;
import org.springframework.boot.test.context.SpringBootTest;

@SpringBootTest
class CommandAiApplicationTests {

	@Test
	void contextLoads() {
	}
}


# test\java\com\commandAI\commandAI\modelsTest\ContextTest.java

package com.commandAI.commandAI.modelsTest;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import com.commandAI.commandAI.modules.context.model.context.Context;
import java.time.LocalDateTime;

import static org.junit.jupiter.api.Assertions.*;

public class ContextTest {

    private Context context;

    @BeforeEach
    public void setUp() {
        context = new Context();
    }

    @Test
    public void testSetAndGetId() {
        Long id = 1L;
        context.setId(id);
        assertEquals(id, context.getId());
    }

    @Test
    public void testSetAndGetContext() {
        String testContext = "Sample context";
        context.setContext(testContext);
        assertEquals(testContext, context.getContext());
    }

    @Test
    public void testSetAndGetCreatedAt() {
        LocalDateTime now = LocalDateTime.now();
        context.setCreatedAt(now);
        assertEquals(now, context.getCreatedAt());
    }
}


User: resuma a minha aplica√ß√£o JAVA
Assistant: Com certeza! Sua aplica√ß√£o Java, "CommandAI", √© um projeto Spring Boot que visa facilitar a comunica√ß√£o com intelig√™ncia artificial (como o GPT), armazenando contextos e transcri√ß√µes de forma inteligente. Aqui est√° um resumo dos principais componentes:

**Componentes Principais:**

*   **M√≥dulos:**
    *   **AIComunication:** Lida com a comunica√ß√£o com a IA, incluindo salvar as perguntas e respostas, gerar embeddings para o texto e encontrar comunica√ß√µes similares com base nos embeddings.
    *   **Context:** Permite salvar e recuperar o contexto da conversa, mantendo o hist√≥rico das intera√ß√µes.
    *   **EmbeddingTranscription:** Gerencia as transcri√ß√µes das reuni√µes com os respectivos "embeddings" (representa√ß√µes vetoriais) para facilitar a busca de transcri√ß√µes similares.

*   **Entidades (Modelos):**

    *   `GPTCommunication`: Representa uma intera√ß√£o com a IA, armazenando a pergunta, a resposta e os "embeddings" vetoriais correspondentes.
    *   `Context`:  Representa um contexto de conversa.
    *   `TranscriptionEmbedded`: Representa uma transcri√ß√£o de reuni√£o com o seu embedding.

*   **Repositories:**
    *   Interfaces que estendem `JpaRepository` para interagir com o banco de dados para as entidades: `GPTCommunication`, `Context`, e `TranscriptionEmbedded`.

*   **Servi√ßos:**
    *   Implementa√ß√µes das interfaces de servi√ßo (ex: `GPTCommunicationServiceImpl`, `ContextServiceImpl`, `ServiceTranscriptionEmbeddedImpl`) que cont√™m a l√≥gica de neg√≥cios para salvar, recuperar e encontrar informa√ß√µes relacionadas √† comunica√ß√£o com a IA, contextos e transcri√ß√µes.

*   **Controladores (Controllers):**

    *   `GPTCommunicationController`:  Exp√µe endpoints REST para salvar as intera√ß√µes com a IA (`/api/question_answers`) e buscar intera√ß√µes similares.
    *   `ContextController`:  Gerencia os contextos de conversa, permitindo salvar, recuperar o √∫ltimo contexto e todos os contextos (`/api/contexts`).
    *   `TranscriptionController`:  Lida com as transcri√ß√µes de reuni√µes, permitindo adicionar, recuperar, buscar transcri√ß√µes similares e o √∫ltimo contexto (`/api/meetings`).

*   **DTOs (Data Transfer Objects):**

    *   `GTPCommunicationRequestDTO`, `ContextDTO`, `TranscriptionEmbeddedDTO`: Usados para transferir dados entre a aplica√ß√£o e o mundo externo (ex: requisi√ß√µes e respostas da API).

**Funcionalidades:**

*   **Armazenamento de intera√ß√µes com a IA:** Salva as perguntas e respostas trocadas com a IA, junto com os "embeddings" vetoriais do texto.
*   **Busca de intera√ß√µes similares:** Encontra intera√ß√µes semelhantes com base nos "embeddings" vetoriais, permitindo reutilizar informa√ß√µes e contextos.
*   **Gerenciamento de contexto:** Permite salvar e recuperar o contexto da conversa, garantindo que a IA tenha o hist√≥rico necess√°rio para responder adequadamente.
*   **Transcri√ß√µes de reuni√µes:** Permite salvar as transcri√ß√µes de reuni√µes e associ√°-las a "embeddings" vetoriais para facilitar a busca de reuni√µes relevantes.
*   **APIs REST:** Exp√µe APIs REST para facilitar a integra√ß√£o com outras aplica√ß√µes e servi√ßos.

**Em resumo:** A aplica√ß√£o CommandAI √© uma solu√ß√£o completa para gerenciar a comunica√ß√£o com a IA, armazenando contextos e transcri√ß√µes de forma inteligente, permitindo a busca de informa√ß√µes relevantes e a reutiliza√ß√£o de conhecimento.
User: Estou implementando boas pr√°ticas de desenvolvimento JAVA ?
2025-04-14 17:02:19,584 [INFO] Resposta da IA (texto): Assistant: Sim, com base na estrutura e nos componentes da sua aplica√ß√£o CommandAI, parece que voc√™ est√° seguindo muitas boas pr√°ticas de desenvolvimento Java e Spring Boot. Aqui est√° uma an√°lise detalhada:

**Boas Pr√°ticas Implementadas:**

*   **Arquitetura em Camadas:** A aplica√ß√£o est√° bem organizada em camadas distintas (Controladores, Servi√ßos, Reposit√≥rios, Modelos), o que facilita a manuten√ß√£o, o teste e a evolu√ß√£o do sistema.
*   **Uso do Spring Boot:** O uso do Spring Boot simplifica a configura√ß√£o e o desenvolvimento, fornecendo "auto-configura√ß√£o", "starter dependencies" e outras ferramentas que agilizam o processo.
*   **Inje√ß√£o de Depend√™ncia (DI):** O uso de `@Autowired` e `@RequiredArgsConstructor` para inje√ß√£o de depend√™ncia promove o baixo acoplamento e facilita o teste unit√°rio.
*   **DTOs (Data Transfer Objects):** O uso de DTOs para transferir dados entre as camadas e expor dados na API √© uma pr√°tica recomendada para desacoplar os modelos de dados internos da API.
*   **RESTful APIs:** A cria√ß√£o de APIs RESTful com controladores (Controllers) bem definidos e endpoints claros (ex: `/api/question_answers`, `/api/contexts`, `/api/meetings`) facilita a integra√ß√£o com outros sistemas.
*   **Valida√ß√£o:** O uso de `@Validated` nos controladores indica a preocupa√ß√£o com a valida√ß√£o dos dados de entrada.
*   **Tratamento de Exce√ß√µes:** A implementa√ß√£o de um `GlobalExceptionHandler` para lidar com exce√ß√µes de forma centralizada √© uma boa pr√°tica para fornecer respostas de erro consistentes e informativas.
*   **Log:** O uso de `System.err.println` para log (enquanto n√£o ideal) indica uma preocupa√ß√£o com o registro de eventos e erros para facilitar a depura√ß√£o e o monitoramento.
*   **JPA e Spring Data JPA:** O uso de JPA (Java Persistence API) com Spring Data JPA simplifica o acesso ao banco de dados, fornecendo uma abstra√ß√£o sobre a camada de persist√™ncia.
*   **Embeddings:** A utiliza√ß√£o de Embeddings √© uma √≥tima estrat√©gia para similaridade sem√¢ntica.
*   **Testes:** A presen√ßa da classe `CommandAiApplicationTests` e `ContextTest` indica que voc√™ est√° escrevendo testes para sua aplica√ß√£o, o que √© fundamental para garantir a qualidade e a confiabilidade do c√≥digo.
*   **Uso de Records:** A utiliza√ß√£o de Records √© excelente para DTOs.

**Sugest√µes de Melhorias:**

*   **Log:** Substitua `System.err.println` por um framework de log mais robusto, como SLF4J com Logback ou Log4j2. Isso permite configurar diferentes n√≠veis de log, direcionar os logs para arquivos ou outros destinos, e adicionar informa√ß√µes contextuais aos logs.
*   **Tratamento de Exce√ß√µes:**
    *   Crie classes de exce√ß√£o personalizadas para representar diferentes tipos de erros na sua aplica√ß√£o (ex: `AICommunicationException`, `ContextException`, `TranscriptionException`).
    *   Use c√≥digos de status HTTP apropriados para cada tipo de erro.
    *   Forne√ßa mensagens de erro claras e informativas para o cliente.
*   **Seguran√ßa:** Implemente mecanismos de autentica√ß√£o e autoriza√ß√£o para proteger suas APIs. Spring Security √© uma √≥tima op√ß√£o para isso.
*   **Documenta√ß√£o:** Use ferramentas como o Swagger (OpenAPI) para gerar automaticamente a documenta√ß√£o da sua API REST.
*   **Internacionaliza√ß√£o (i18n):** Se sua aplica√ß√£o precisar suportar diferentes idiomas, considere implementar a internacionaliza√ß√£o.
*   **Monitoramento:** Integre ferramentas de monitoramento para coletar m√©tricas sobre o desempenho da sua aplica√ß√£o (ex: tempo de resposta da API, uso de recursos, etc.).
*   **Documenta√ß√£o Javadoc:** Adicione coment√°rios Javadoc para documentar suas classes, m√©todos e par√¢metros. Isso facilita a compreens√£o do c√≥digo e a gera√ß√£o de documenta√ß√£o autom√°tica.
*   **Cobertura de Testes:** Busque uma alta cobertura de testes unit√°rios e de integra√ß√£o.

**Em resumo:** Sua aplica√ß√£o CommandAI parece estar bem estruturada e seguindo muitas boas pr√°ticas de desenvolvimento Java e Spring Boot. Ao implementar as sugest√µes de melhorias acima, voc√™ pode aumentar ainda mais a qualidade, a manutenibilidade e a seguran√ßa do seu sistema.

2025-04-14 17:02:19,703 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 17:02:19,704 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 17:02:19,706 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 17:02:19,707 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 17:02:19,708 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 17:02:19,710 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 17:02:19,711 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 17:02:19,712 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 17:02:19,714 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 17:02:19,715 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 17:02:19,717 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 17:02:19,718 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 17:02:19,720 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 17:02:19,721 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 17:02:19,722 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 17:02:19,724 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 17:02:19,725 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 17:02:19,726 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 17:02:19,728 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 17:02:19,729 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 17:02:19,731 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 17:02:19,732 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 17:02:19,734 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 17:02:19,735 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 17:02:19,736 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 17:02:19,737 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 17:02:19,739 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 17:02:19,740 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 17:02:19,741 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 17:02:19,742 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 17:02:19,743 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 17:02:19,745 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 17:02:19,747 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 17:02:19,749 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 17:02:19,750 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 17:02:19,752 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 19:47:26,357 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 19:47:26,368 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 19:47:26,373 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 19:47:26,379 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 19:47:26,385 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 19:47:26,392 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 19:47:26,401 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 19:47:26,407 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 19:47:26,415 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 19:47:26,422 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 19:47:26,428 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 19:47:26,433 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 19:47:26,438 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 19:47:26,443 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 19:47:26,450 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 19:47:26,456 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 19:47:26,461 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 19:47:26,467 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 19:47:26,472 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 19:47:26,480 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 19:47:26,486 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 19:47:26,494 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 19:47:26,503 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 19:47:26,508 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 19:47:26,513 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 19:47:26,518 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 19:47:26,524 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 19:47:26,528 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 19:47:26,533 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 19:47:26,539 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 19:47:26,544 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 19:47:26,550 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 19:47:26,556 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 19:47:26,564 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 19:47:26,569 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 19:47:26,575 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 19:47:27,083 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 19:47:27,085 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 19:47:27,086 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 19:47:27,088 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 19:47:27,090 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 19:47:27,091 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 19:47:27,094 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 19:47:27,095 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 19:47:27,097 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 19:47:27,099 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 19:47:27,101 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 19:47:27,103 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 19:47:27,106 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 19:47:27,107 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 19:47:27,108 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 19:47:27,110 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 19:47:27,113 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 19:47:27,114 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 19:47:27,115 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 19:47:27,116 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 19:47:27,118 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 19:47:27,119 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 19:47:27,121 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 19:47:27,122 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 19:47:27,124 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 19:47:27,125 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 19:47:27,127 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 19:47:27,128 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 19:47:27,130 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 19:47:27,131 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 19:47:27,132 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 19:47:27,134 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 19:47:27,135 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 19:47:27,137 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 19:47:27,139 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 19:47:27,141 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 19:47:27,145 [INFO] Enviando para IA - Prompt (sem imagem): Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python_e_java

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python_e_java()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python_e_java() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py e .java a partir de src/"""
    src_dir = Config.BASE_DIR

    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    # Busca arquivos .py e .java separadamente
    padrao_busca_py = os.path.join(src_dir, '**', '*.py')
    padrao_busca_java = os.path.join(src_dir, '**', '*.java')

    arquivos_py = glob.glob(padrao_busca_py, recursive=True)
    arquivos_java = glob.glob(padrao_busca_java, recursive=True)

    arquivos = arquivos_py + arquivos_java
    arquivos.sort() # Ordena a lista completa

    for arquivo in arquivos:
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# main\java\com\commandAI\commandAI\CommandAiApplication.java

package com.commandAI.commandAI;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class CommandAiApplication {

	public static void main(String[] args) {
		SpringApplication.run(CommandAiApplication.class, args);
	}

}


# main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java

package com.commandAI.commandAI.modules.AIComunication.controller;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import com.commandAI.commandAI.modules.AIComunication.model.dto.GTPCommunicationRequestDTO;
import com.commandAI.commandAI.modules.AIComunication.service.IGPTCommunicationService;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.RequiredArgsConstructor;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.Arrays;
import java.util.List;

@RequiredArgsConstructor
@RestController
@RequestMapping("/api/question_answers")
public class GPTCommunicationController {

    private final IGPTCommunicationService service;

    @PostMapping("/save")
    public ResponseEntity<GPTCommunication> saveCommunication(@RequestBody GTPCommunicationRequestDTO dto) {
        try {
            GPTCommunication savedCommunication = service.saveCommunication(dto);
            return new ResponseEntity<>(savedCommunication, HttpStatus.CREATED);
        } catch (Exception e) {
            // Log error and return an appropriate response
            System.err.println("Erro ao salvar comunica√ß√£o: " + e.getMessage());
            return new ResponseEntity<>(HttpStatus.INTERNAL_SERVER_ERROR);
        }
    }

    @PostMapping("/similar")
    public ResponseEntity<List<GTPCommunicationRequestDTO>> getSimilarCommunications(@RequestBody JsonNode requestBody) {
        try {
            // Validar se √© um array
            if (!requestBody.isArray()) {
                throw new IllegalArgumentException("Esperado um array");
            }

            // Converter JsonNode para array de float
            float[] embedding = new ObjectMapper().convertValue(requestBody, float[].class);

            // Log para verificar o embedding recebido
            System.out.println("Embedding recebido: " + Arrays.toString(embedding));

            // Chamar o servi√ßo com o array de float
            List<GTPCommunicationRequestDTO> similarCommunications = service.findSimilarCommunications(embedding);
            return ResponseEntity.ok(similarCommunications);
        } catch (IllegalArgumentException e) {
            System.err.println("Erro de valida√ß√£o: " + e.getMessage());
            return ResponseEntity.status(HttpStatus.BAD_REQUEST).body(null);
        } catch (Exception e) {
            // Log error and return an appropriate response
            System.err.println("Erro ao processar a solicita√ß√£o: " + e.getMessage());
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(null);
        }
    }
}


# main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java

package com.commandAI.commandAI.modules.AIComunication.model;

import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import jakarta.persistence.*;
import lombok.Data;

import java.time.LocalDateTime;
@Data
@Entity
@JsonIgnoreProperties(ignoreUnknown = true)
@Table(name = "gpt_communication")
public class GPTCommunication {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(name = "question", nullable = false, columnDefinition = "TEXT")
    private String question;

    @Column(name = "answer", nullable = false, columnDefinition = "TEXT")
    private String answer;

    @Column(name = "question_embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] questionEmbedding;

    @Column(name = "answer_embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] answerEmbedding;

    @Column(name = "created_at", nullable = false)
    private LocalDateTime createdAt;

}

# main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java

package com.commandAI.commandAI.modules.AIComunication.model.dto;

import com.fasterxml.jackson.annotation.JsonProperty;

public record GTPCommunicationRequestDTO(
        String question,
        float[] questionEmbedding,
        String answer,
        float[] answerEmbedding
) {}



# main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java

package com.commandAI.commandAI.modules.AIComunication.repository;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

@Repository
public interface IGPTCommunicationRepository extends JpaRepository<GPTCommunication, Long> {

}

# main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java

package com.commandAI.commandAI.modules.AIComunication.service;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import com.commandAI.commandAI.modules.AIComunication.model.dto.GTPCommunicationRequestDTO;

import java.util.List;

public interface IGPTCommunicationService {
    GPTCommunication saveCommunication(GTPCommunicationRequestDTO dto);
    List<GTPCommunicationRequestDTO> findSimilarCommunications(float[] embedding);
}

# main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java

package com.commandAI.commandAI.modules.AIComunication.service.impl;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import com.commandAI.commandAI.modules.AIComunication.model.dto.GTPCommunicationRequestDTO;
import com.commandAI.commandAI.modules.AIComunication.repository.IGPTCommunicationRepository;
import com.commandAI.commandAI.modules.AIComunication.service.IGPTCommunicationService;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;

import java.time.LocalDateTime;
import java.util.Arrays;
import java.util.List;
import java.util.stream.Collectors;

@RequiredArgsConstructor
@Service
public class GPTCommunicationServiceImpl implements IGPTCommunicationService {

    private final IGPTCommunicationRepository repository;

    @Override
    public GPTCommunication saveCommunication(GTPCommunicationRequestDTO dto) {
        GPTCommunication communication = new GPTCommunication();
        communication.setQuestion(dto.question());
        communication.setQuestionEmbedding(dto.questionEmbedding());
        communication.setAnswer(dto.answer());
        communication.setAnswerEmbedding(dto.answerEmbedding());
        communication.setCreatedAt(LocalDateTime.now());
        return repository.save(communication);
    }

    @Override
    public List<GTPCommunicationRequestDTO> findSimilarCommunications(float[] embedding) {
        List<GPTCommunication> allCommunications = repository.findAll();

        return allCommunications.stream()
                .filter(comm -> isSimilar(embedding, comm.getQuestionEmbedding()))
                .map(comm -> new GTPCommunicationRequestDTO(
                        comm.getQuestion(),
                        comm.getQuestionEmbedding(),
                        comm.getAnswer(),
                        comm.getAnswerEmbedding()))
                .collect(Collectors.toList());
    }

    private boolean isSimilar(float[] embedding1, float[] embedding2) {
        // Verificar se os embeddings t√™m o mesmo tamanho
        if (embedding1.length != embedding2.length) {
            System.err.println("Os embeddings t√™m tamanhos diferentes.");
            return false;
        }

        System.out.println("Embedding1: " + Arrays.toString(embedding1));
        System.out.println("Embedding2: " + Arrays.toString(embedding2));

        double dotProduct = 0.0;
        double normA = 0.0;
        double normB = 0.0;
        for (int i = 0; i < embedding1.length; i++) {
            dotProduct += embedding1[i] * embedding2[i];
            normA += Math.pow(embedding1[i], 2);
            normB += Math.pow(embedding2[i], 2);
        }
        return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB)) > 0.8; // Ajuste o limiar conforme necess√°rio
    }
}


# main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java

package com.commandAI.commandAI.modules.context.controller;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.context.service.IContextService;
import com.commandAI.commandAI.modules.context.model.context.Context;
import lombok.RequiredArgsConstructor;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.validation.annotation.Validated;
import org.springframework.web.bind.annotation.*;

import java.util.List;
import java.util.Map;

@RequiredArgsConstructor
@RestController
@Validated
@ResponseBody
@RequestMapping("/api/contexts")
public class ContextController {

    @Autowired
    private final IContextService contextService;

    @PostMapping("/save")
    public ResponseEntity<Map<String, Object>> saveContext(@RequestBody ContextDTO data) {
        Context newContext = contextService.saveContext(data);
        Map<String, Object> response = Map.of("result", newContext);
        return ResponseEntity.status(HttpStatus.CREATED).body(response);
    }


    @GetMapping("/last")
    public ResponseEntity<ContextDTO> getLastContext() {
        ContextDTO lastContext = contextService.getLastContext();
        return ResponseEntity.ok(lastContext);
    }

    @GetMapping("/all")
    public ResponseEntity<Map<String, Object>> getAllContexts() {
        List<Context> contexts = contextService.findAllContext();
        return ResponseEntity.ok(Map.of("contexts", contexts));
    }
}


# main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java

package com.commandAI.commandAI.modules.context.controller.hendler;

import com.commandAI.commandAI.modules.context.service.validation.ContextNotFoundException;
import lombok.Data;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.MissingServletRequestParameterException;
import org.springframework.web.bind.annotation.ControllerAdvice;
import org.springframework.web.bind.annotation.ExceptionHandler;
import org.springframework.web.context.request.WebRequest;

import java.time.LocalDateTime;
import java.util.HashMap;
import java.util.Map;

@Data
@ControllerAdvice
public class GlobalExceptionHandler {

    @ExceptionHandler(ContextNotFoundException.class)
    public ResponseEntity<Object> handleContextNotFoundException(ContextNotFoundException ex, WebRequest request) {
        Map<String, Object> body = new HashMap<>();
        body.put("timestamp", LocalDateTime.now());
        body.put("message", ex.getMessage());

        return new ResponseEntity<>(body, HttpStatus.NOT_FOUND);
    }
    @ExceptionHandler(MissingServletRequestParameterException.class)
    public ResponseEntity<Object> handleMissingServletRequestParameterException(MissingServletRequestParameterException ex, WebRequest request) {
        Map<String, Object> body = new HashMap<>();
        body.put("timestamp", LocalDateTime.now());
        body.put("message", "Par√¢metro de requisi√ß√£o ausente: " + ex.getParameterName());

        return new ResponseEntity<>(body, HttpStatus.BAD_REQUEST);
    }

    // Outros handlers de exce√ß√£o podem ser adicionados aqui
}

# main\java\com\commandAI\commandAI\modules\context\model\context\Context.java

    package com.commandAI.commandAI.modules.context.model.context;
    import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
    import jakarta.persistence.*;
    import lombok.Data;

    import java.time.LocalDateTime;
    @Data
    @Table(name = "tb_context")
    @Entity
    @JsonIgnoreProperties(ignoreUnknown = true)
    public class Context {

        @Id
        @GeneratedValue(strategy = GenerationType.IDENTITY)
        private Long id;

        @Column(name = "context", nullable = false, columnDefinition = "TEXT")
        private String context;

        @Column(name = "created_at", nullable = false)
        private LocalDateTime createdAt;

        @PrePersist
        protected void onCreate() {
            createdAt = LocalDateTime.now();
        }
    }



# main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java

package com.commandAI.commandAI.modules.context.model.dto;

import com.commandAI.commandAI.modules.context.model.context.Context;
import java.time.LocalDateTime;
public record ContextDTO(
        Long id,
        String context,
        LocalDateTime createdAt
) {
    public static ContextDTO fromEntity(Context context) {
        return new ContextDTO(context.getId(), context.getContext(), context.getCreatedAt());
    }
}


# main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java

package com.commandAI.commandAI.modules.context.repository;

import com.commandAI.commandAI.modules.context.model.context.Context;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import java.util.Optional;

@Repository
public interface IContextRepository extends JpaRepository<Context, Long> {
    Optional<Context> findTopByOrderByCreatedAtDesc();
}


# main\java\com\commandAI\commandAI\modules\context\service\IContextService.java

package com.commandAI.commandAI.modules.context.service;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;

import java.util.List;

public interface IContextService {
    Context saveContext(ContextDTO data);
    ContextDTO getLastContext();
    List<Context> findAllContext();
}


# main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java

package com.commandAI.commandAI.modules.context.service.impl;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.repository.IContextRepository;
import com.commandAI.commandAI.modules.context.service.IContextService;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.util.List;

@RequiredArgsConstructor
@Service
public class ContextServiceImpl implements IContextService {

    private final IContextRepository contextRepository;

    @Override
    @Transactional
    public Context saveContext(ContextDTO data) {
        Context context = new Context();
        context.setContext(data.context());
        context = contextRepository.save(context);
        return context;
    }

    @Override
    @Transactional
    public ContextDTO getLastContext() {
        Context lastContext = contextRepository.findTopByOrderByCreatedAtDesc()
                .orElseThrow(() -> new RuntimeException("No contexts found"));
        return ContextDTO.fromEntity(lastContext);
    }
    @Override
    @Transactional
    public List<Context> findAllContext() {
        return contextRepository.findAll();
    }
}


# main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java

package com.commandAI.commandAI.modules.context.service.validation;

public class ContextNotFoundException extends RuntimeException {
    public ContextNotFoundException(String message) {
        super(message);
    }
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java

package com.commandAI.commandAI.modules.embeddingTranscription.controller;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.service.IServiceTranscriptionEmbedded;
import lombok.RequiredArgsConstructor;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.List;

@RestController
@RequestMapping("/api/meetings")    
@RequiredArgsConstructor
public class TranscriptionController {

    private final IServiceTranscriptionEmbedded service;

    @PostMapping("/transcriptions")
    public ResponseEntity<Void> addTranscription(@RequestBody TranscriptionEmbeddedDTO transcriptionDTO) {
        service.saveTranscription(transcriptionDTO.transcriptionText(), transcriptionDTO.embedding());
        return ResponseEntity.status(HttpStatus.CREATED).build();
    }

    @GetMapping("/transcriptions")
    public List<TranscriptionEmbeddedDTO> getAllTranscriptions() {
        return service.getAllTranscriptions();
    }

    @GetMapping("/transcriptions/{id}")
    public ResponseEntity<TranscriptionEmbeddedDTO> getTranscriptionById(@PathVariable Long id) {
        TranscriptionEmbeddedDTO transcription = service.getTranscriptionById(id);
        if (transcription == null) {
            return ResponseEntity.status(HttpStatus.NOT_FOUND).build();
        }
        return ResponseEntity.ok(transcription);
    }

    @GetMapping("/transcriptions/similar")
    public List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(@RequestParam float[] queryEmbedding) {
        return service.findSimilarTranscriptions(queryEmbedding);
    }
    @GetMapping("/last")
    public ResponseEntity<TranscriptionEmbeddedDTO> getLastTranscription() {
        TranscriptionEmbeddedDTO lastTranscription = service.getLastTranscription();
        return ResponseEntity.ok(lastTranscription);
    }
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java

package com.commandAI.commandAI.modules.embeddingTranscription.model;

import jakarta.persistence.*;
import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import lombok.Data;

import java.time.LocalDateTime;

@Data
@JsonIgnoreProperties(ignoreUnknown = true)
@Entity
@Table(name = "transcriptionEmbedded")
public class TranscriptionEmbedded {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    @Column(name = "meeting_id", nullable = false)
    private Long meetingId;

    @Column(name = "transcription_text", nullable = false, columnDefinition = "TEXT")
    private String transcriptionText;

    @Column(name = "embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] embedding;

    @Column(name = "created_at", nullable = false)
    private LocalDateTime createdAt;

    @PrePersist
    protected void onCreate() {
        createdAt = LocalDateTime.now();
    }

}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java

package com.commandAI.commandAI.modules.embeddingTranscription.model.dto;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;

import java.time.LocalDateTime;

public record TranscriptionEmbeddedDTO(
        Long meetingId,
        String transcriptionText,
        float[] embedding,
        LocalDateTime createdAt
) {
    public static TranscriptionEmbeddedDTO fromEntity(TranscriptionEmbedded transcriptionEmbedded) {
        return new TranscriptionEmbeddedDTO(transcriptionEmbedded.getMeetingId(), transcriptionEmbedded.getTranscriptionText(), transcriptionEmbedded.getEmbedding(), transcriptionEmbedded.getCreatedAt());
    }
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java

package com.commandAI.commandAI.modules.embeddingTranscription.repository;

import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import java.util.Optional;

@Repository
public interface ITranscriptionEmbeddedRepository extends JpaRepository<TranscriptionEmbedded, Long> {
    Optional<TranscriptionEmbedded> findTopByOrderByCreatedAtDesc();

}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java

package com.commandAI.commandAI.modules.embeddingTranscription.service;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;

import java.util.List;

public interface IServiceTranscriptionEmbedded {
    void saveTranscription(String transcriptionText, float[] embedding);
    List<TranscriptionEmbeddedDTO> getAllTranscriptions();
    TranscriptionEmbeddedDTO getTranscriptionById(Long id);
    List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(float[] queryEmbedding);
    TranscriptionEmbeddedDTO getLastTranscription();
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java

package com.commandAI.commandAI.modules.embeddingTranscription.service.impl;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.repository.ITranscriptionEmbeddedRepository;
import com.commandAI.commandAI.modules.embeddingTranscription.service.IServiceTranscriptionEmbedded;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.util.List;
import java.util.stream.Collectors;

@RequiredArgsConstructor
@Service
public class ServiceTranscriptionEmbeddedImpl implements IServiceTranscriptionEmbedded {

    private final ITranscriptionEmbeddedRepository transcriptionEmbeddedRepository;

    @Override
    @Transactional
    public void saveTranscription(String transcriptionText, float[] embedding) {
        TranscriptionEmbedded transcription = new TranscriptionEmbedded();
        transcription.setTranscriptionText(transcriptionText);
        transcription.setEmbedding(embedding);
        System.out.println(transcription);
        transcriptionEmbeddedRepository.save(transcription);
    }

    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> getAllTranscriptions() {
        return transcriptionEmbeddedRepository.findAll().stream()
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }

    @Override
    @Transactional
    public TranscriptionEmbeddedDTO getTranscriptionById(Long id) {
        TranscriptionEmbedded transcription = transcriptionEmbeddedRepository.findById(id).orElse(null);
        if (transcription == null) {
            return null;
        }
        return new TranscriptionEmbeddedDTO(
                transcription.getMeetingId(),
                transcription.getTranscriptionText(),
                transcription.getEmbedding(),
                transcription.getCreatedAt());
    }

    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(float[] queryEmbedding) {
        List<TranscriptionEmbedded> allTranscriptions = transcriptionEmbeddedRepository.findAll();
        return allTranscriptions.stream()
                .sorted((t1, t2) -> Double.compare(
                        cosineSimilarity(queryEmbedding, t2.getEmbedding()),
                        cosineSimilarity(queryEmbedding, t1.getEmbedding())))
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }
    @Override
    @Transactional
    public TranscriptionEmbeddedDTO getLastTranscription() {
        TranscriptionEmbedded lastTranscription = transcriptionEmbeddedRepository.findTopByOrderByCreatedAtDesc()
                .orElseThrow(() -> new RuntimeException("No contexts found"));
        return TranscriptionEmbeddedDTO.fromEntity(lastTranscription);
    }
    private double cosineSimilarity(float[] vec1, float[] vec2) {
        double dotProduct = 0.0;
        double normA = 0.0;
        double normB = 0.0;
        for (int i = 0; i < vec1.length; i++) {
            dotProduct += vec1[i] * vec2[i];
            normA += Math.pow(vec1[i], 2);
            normB += Math.pow(vec2[i], 2);
        }
        return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
    }
}


# test\java\com\commandAI\commandAI\CommandAiApplicationTests.java

package com.commandAI.commandAI;

import org.junit.jupiter.api.Test;
import org.springframework.boot.test.context.SpringBootTest;

@SpringBootTest
class CommandAiApplicationTests {

	@Test
	void contextLoads() {
	}
}


# test\java\com\commandAI\commandAI\modelsTest\ContextTest.java

package com.commandAI.commandAI.modelsTest;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import com.commandAI.commandAI.modules.context.model.context.Context;
import java.time.LocalDateTime;

import static org.junit.jupiter.api.Assertions.*;

public class ContextTest {

    private Context context;

    @BeforeEach
    public void setUp() {
        context = new Context();
    }

    @Test
    public void testSetAndGetId() {
        Long id = 1L;
        context.setId(id);
        assertEquals(id, context.getId());
    }

    @Test
    public void testSetAndGetContext() {
        String testContext = "Sample context";
        context.setContext(testContext);
        assertEquals(testContext, context.getContext());
    }

    @Test
    public void testSetAndGetCreatedAt() {
        LocalDateTime now = LocalDateTime.now();
        context.setCreatedAt(now);
        assertEquals(now, context.getCreatedAt());
    }
}


User: resuma a minha aplica√ß√£o JAVA
Assistant: Com certeza! Sua aplica√ß√£o Java, "CommandAI", √© um projeto Spring Boot que visa facilitar a comunica√ß√£o com intelig√™ncia artificial (como o GPT), armazenando contextos e transcri√ß√µes de forma inteligente. Aqui est√° um resumo dos principais componentes:

**Componentes Principais:**

*   **M√≥dulos:**
    *   **AIComunication:** Lida com a comunica√ß√£o com a IA, incluindo salvar as perguntas e respostas, gerar embeddings para o texto e encontrar comunica√ß√µes similares com base nos embeddings.
    *   **Context:** Permite salvar e recuperar o contexto da conversa, mantendo o hist√≥rico das intera√ß√µes.
    *   **EmbeddingTranscription:** Gerencia as transcri√ß√µes das reuni√µes com os respectivos "embeddings" (representa√ß√µes vetoriais) para facilitar a busca de transcri√ß√µes similares.

*   **Entidades (Modelos):**

    *   `GPTCommunication`: Representa uma intera√ß√£o com a IA, armazenando a pergunta, a resposta e os "embeddings" vetoriais correspondentes.
    *   `Context`:  Representa um contexto de conversa.
    *   `TranscriptionEmbedded`: Representa uma transcri√ß√£o de reuni√£o com o seu embedding.

*   **Repositories:**
    *   Interfaces que estendem `JpaRepository` para interagir com o banco de dados para as entidades: `GPTCommunication`, `Context`, e `TranscriptionEmbedded`.

*   **Servi√ßos:**
    *   Implementa√ß√µes das interfaces de servi√ßo (ex: `GPTCommunicationServiceImpl`, `ContextServiceImpl`, `ServiceTranscriptionEmbeddedImpl`) que cont√™m a l√≥gica de neg√≥cios para salvar, recuperar e encontrar informa√ß√µes relacionadas √† comunica√ß√£o com a IA, contextos e transcri√ß√µes.

*   **Controladores (Controllers):**

    *   `GPTCommunicationController`:  Exp√µe endpoints REST para salvar as intera√ß√µes com a IA (`/api/question_answers`) e buscar intera√ß√µes similares.
    *   `ContextController`:  Gerencia os contextos de conversa, permitindo salvar, recuperar o √∫ltimo contexto e todos os contextos (`/api/contexts`).
    *   `TranscriptionController`:  Lida com as transcri√ß√µes de reuni√µes, permitindo adicionar, recuperar, buscar transcri√ß√µes similares e o √∫ltimo contexto (`/api/meetings`).

*   **DTOs (Data Transfer Objects):**

    *   `GTPCommunicationRequestDTO`, `ContextDTO`, `TranscriptionEmbeddedDTO`: Usados para transferir dados entre a aplica√ß√£o e o mundo externo (ex: requisi√ß√µes e respostas da API).

**Funcionalidades:**

*   **Armazenamento de intera√ß√µes com a IA:** Salva as perguntas e respostas trocadas com a IA, junto com os "embeddings" vetoriais do texto.
*   **Busca de intera√ß√µes similares:** Encontra intera√ß√µes semelhantes com base nos "embeddings" vetoriais, permitindo reutilizar informa√ß√µes e contextos.
*   **Gerenciamento de contexto:** Permite salvar e recuperar o contexto da conversa, garantindo que a IA tenha o hist√≥rico necess√°rio para responder adequadamente.
*   **Transcri√ß√µes de reuni√µes:** Permite salvar as transcri√ß√µes de reuni√µes e associ√°-las a "embeddings" vetoriais para facilitar a busca de reuni√µes relevantes.
*   **APIs REST:** Exp√µe APIs REST para facilitar a integra√ß√£o com outras aplica√ß√µes e servi√ßos.

**Em resumo:** A aplica√ß√£o CommandAI √© uma solu√ß√£o completa para gerenciar a comunica√ß√£o com a IA, armazenando contextos e transcri√ß√µes de forma inteligente, permitindo a busca de informa√ß√µes relevantes e a reutiliza√ß√£o de conhecimento.
User: Estou implementando boas pr√°ticas de desenvolvimento JAVA ?
Assistant: Assistant: Sim, com base na estrutura e nos componentes da sua aplica√ß√£o CommandAI, parece que voc√™ est√° seguindo muitas boas pr√°ticas de desenvolvimento Java e Spring Boot. Aqui est√° uma an√°lise detalhada:

**Boas Pr√°ticas Implementadas:**

*   **Arquitetura em Camadas:** A aplica√ß√£o est√° bem organizada em camadas distintas (Controladores, Servi√ßos, Reposit√≥rios, Modelos), o que facilita a manuten√ß√£o, o teste e a evolu√ß√£o do sistema.
*   **Uso do Spring Boot:** O uso do Spring Boot simplifica a configura√ß√£o e o desenvolvimento, fornecendo "auto-configura√ß√£o", "starter dependencies" e outras ferramentas que agilizam o processo.
*   **Inje√ß√£o de Depend√™ncia (DI):** O uso de `@Autowired` e `@RequiredArgsConstructor` para inje√ß√£o de depend√™ncia promove o baixo acoplamento e facilita o teste unit√°rio.
*   **DTOs (Data Transfer Objects):** O uso de DTOs para transferir dados entre as camadas e expor dados na API √© uma pr√°tica recomendada para desacoplar os modelos de dados internos da API.
*   **RESTful APIs:** A cria√ß√£o de APIs RESTful com controladores (Controllers) bem definidos e endpoints claros (ex: `/api/question_answers`, `/api/contexts`, `/api/meetings`) facilita a integra√ß√£o com outros sistemas.
*   **Valida√ß√£o:** O uso de `@Validated` nos controladores indica a preocupa√ß√£o com a valida√ß√£o dos dados de entrada.
*   **Tratamento de Exce√ß√µes:** A implementa√ß√£o de um `GlobalExceptionHandler` para lidar com exce√ß√µes de forma centralizada √© uma boa pr√°tica para fornecer respostas de erro consistentes e informativas.
*   **Log:** O uso de `System.err.println` para log (enquanto n√£o ideal) indica uma preocupa√ß√£o com o registro de eventos e erros para facilitar a depura√ß√£o e o monitoramento.
*   **JPA e Spring Data JPA:** O uso de JPA (Java Persistence API) com Spring Data JPA simplifica o acesso ao banco de dados, fornecendo uma abstra√ß√£o sobre a camada de persist√™ncia.
*   **Embeddings:** A utiliza√ß√£o de Embeddings √© uma √≥tima estrat√©gia para similaridade sem√¢ntica.
*   **Testes:** A presen√ßa da classe `CommandAiApplicationTests` e `ContextTest` indica que voc√™ est√° escrevendo testes para sua aplica√ß√£o, o que √© fundamental para garantir a qualidade e a confiabilidade do c√≥digo.
*   **Uso de Records:** A utiliza√ß√£o de Records √© excelente para DTOs.

**Sugest√µes de Melhorias:**

*   **Log:** Substitua `System.err.println` por um framework de log mais robusto, como SLF4J com Logback ou Log4j2. Isso permite configurar diferentes n√≠veis de log, direcionar os logs para arquivos ou outros destinos, e adicionar informa√ß√µes contextuais aos logs.
*   **Tratamento de Exce√ß√µes:**
    *   Crie classes de exce√ß√£o personalizadas para representar diferentes tipos de erros na sua aplica√ß√£o (ex: `AICommunicationException`, `ContextException`, `TranscriptionException`).
    *   Use c√≥digos de status HTTP apropriados para cada tipo de erro.
    *   Forne√ßa mensagens de erro claras e informativas para o cliente.
*   **Seguran√ßa:** Implemente mecanismos de autentica√ß√£o e autoriza√ß√£o para proteger suas APIs. Spring Security √© uma √≥tima op√ß√£o para isso.
*   **Documenta√ß√£o:** Use ferramentas como o Swagger (OpenAPI) para gerar automaticamente a documenta√ß√£o da sua API REST.
*   **Internacionaliza√ß√£o (i18n):** Se sua aplica√ß√£o precisar suportar diferentes idiomas, considere implementar a internacionaliza√ß√£o.
*   **Monitoramento:** Integre ferramentas de monitoramento para coletar m√©tricas sobre o desempenho da sua aplica√ß√£o (ex: tempo de resposta da API, uso de recursos, etc.).
*   **Documenta√ß√£o Javadoc:** Adicione coment√°rios Javadoc para documentar suas classes, m√©todos e par√¢metros. Isso facilita a compreens√£o do c√≥digo e a gera√ß√£o de documenta√ß√£o autom√°tica.
*   **Cobertura de Testes:** Busque uma alta cobertura de testes unit√°rios e de integra√ß√£o.

**Em resumo:** Sua aplica√ß√£o CommandAI parece estar bem estruturada e seguindo muitas boas pr√°ticas de desenvolvimento Java e Spring Boot. Ao implementar as sugest√µes de melhorias acima, voc√™ pode aumentar ainda mais a qualidade, a manutenibilidade e a seguran√ßa do seu sistema.

User: Preciso implementar a l√≥gica para que um end point possa buscar reuni√µes com palavras chaves. Preciso implementar a l√≥gica de salvar as reuni√µes em embeddings ?
2025-04-14 19:47:44,439 [INFO] Resposta da IA (texto): Com certeza! Aqui est√° um guia de como implementar a l√≥gica para buscar reuni√µes por palavras-chave usando "embeddings" (representa√ß√µes vetoriais) e como salvar as reuni√µes em embeddings:

**1. Atualize a Entidade `TranscriptionEmbedded`:**

*   Adicione um campo para as palavras-chave (opcional, mas recomendado para buscas mais r√°pidas):

```java
package com.commandAI.commandAI.modules.embeddingTranscription.model;

import jakarta.persistence.*;
import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import lombok.Data;

import java.time.LocalDateTime;

@Data
@JsonIgnoreProperties(ignoreUnknown = true)
@Entity
@Table(name = "transcriptionEmbedded")
public class TranscriptionEmbedded {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    @Column(name = "meeting_id", nullable = false)
    private Long meetingId;

    @Column(name = "transcription_text", nullable = false, columnDefinition = "TEXT")
    private String transcriptionText;

    @Column(name = "embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] embedding;

    @Column(name = "keywords", columnDefinition = "TEXT") // Novo campo
    private String keywords;

    @Column(name = "created_at", nullable = false)
    private LocalDateTime createdAt;

    @PrePersist
    protected void onCreate() {
        createdAt = LocalDateTime.now();
    }

}
```

**2. Atualize o DTO `TranscriptionEmbeddedDTO`:**

*   Adicione o campo `keywords` ao DTO:

```java
package com.commandAI.commandAI.modules.embeddingTranscription.model.dto;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;

import java.time.LocalDateTime;

public record TranscriptionEmbeddedDTO(
        Long meetingId,
        String transcriptionText,
        float[] embedding,
        String keywords, // Novo campo
        LocalDateTime createdAt
) {
    public static TranscriptionEmbeddedDTO fromEntity(TranscriptionEmbedded transcriptionEmbedded) {
        return new TranscriptionEmbeddedDTO(transcriptionEmbedded.getMeetingId(), transcriptionEmbedded.getTranscriptionText(), transcriptionEmbedded.getEmbedding(), transcriptionEmbedded.getKeywords(), transcriptionEmbedded.getCreatedAt());
    }
}
```

**3. Atualize o Servi√ßo `IServiceTranscriptionEmbedded`:**

*   Adicione um m√©todo para buscar transcri√ß√µes por palavras-chave:

```java
package com.commandAI.commandAI.modules.embeddingTranscription.service;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;

import java.util.List;

public interface IServiceTranscriptionEmbedded {
    void saveTranscription(String transcriptionText, float[] embedding, String keywords); // Modifiquei
    List<TranscriptionEmbeddedDTO> getAllTranscriptions();
    TranscriptionEmbeddedDTO getTranscriptionById(Long id);
    List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(float[] queryEmbedding);
    TranscriptionEmbeddedDTO getLastTranscription();
    List<TranscriptionEmbeddedDTO> findByKeywords(String keywords);  // Novo m√©todo
}
```

**4. Atualize a Implementa√ß√£o do Servi√ßo `ServiceTranscriptionEmbeddedImpl`:**

```java
package com.commandAI.commandAI.modules.embeddingTranscription.service.impl;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.repository.ITranscriptionEmbeddedRepository;
import com.commandAI.commandAI.modules.embeddingTranscription.service.IServiceTranscriptionEmbedded;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.util.Arrays;
import java.util.List;
import java.util.stream.Collectors;

@RequiredArgsConstructor
@Service
public class ServiceTranscriptionEmbeddedImpl implements IServiceTranscriptionEmbedded {

    private final ITranscriptionEmbeddedRepository transcriptionEmbeddedRepository;

    @Override
    @Transactional
    public void saveTranscription(String transcriptionText, float[] embedding, String keywords) {  // Modifiquei
        TranscriptionEmbedded transcription = new TranscriptionEmbedded();
        transcription.setTranscriptionText(transcriptionText);
        transcription.setEmbedding(embedding);
        transcription.setKeywords(keywords); // Adicionei
        System.out.println(transcription);
        transcriptionEmbeddedRepository.save(transcription);
    }

    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> getAllTranscriptions() {
        return transcriptionEmbeddedRepository.findAll().stream()
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                        transcription.getKeywords(), // Adicionei
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }

    @Override
    @Transactional
    public TranscriptionEmbeddedDTO getTranscriptionById(Long id) {
        TranscriptionEmbedded transcription = transcriptionEmbeddedRepository.findById(id).orElse(null);
        if (transcription == null) {
            return null;
        }
        return new TranscriptionEmbeddedDTO(
                transcription.getMeetingId(),
                transcription.getTranscriptionText(),
                transcription.getEmbedding(),
                transcription.getKeywords(), // Adicionei
                transcription.getCreatedAt());
    }

    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(float[] queryEmbedding) {
        List<TranscriptionEmbedded> allTranscriptions = transcriptionEmbeddedRepository.findAll();
        return allTranscriptions.stream()
                .sorted((t1, t2) -> Double.compare(
                        cosineSimilarity(queryEmbedding, t2.getEmbedding()),
                        cosineSimilarity(queryEmbedding, t1.getEmbedding())))
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                         transcription.getKeywords(), // Adicionei
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }

    @Override
    @Transactional
    public TranscriptionEmbeddedDTO getLastTranscription() {
        TranscriptionEmbedded lastTranscription = transcriptionEmbeddedRepository.findTopByOrderByCreatedAtDesc()
                .orElseThrow(() -> new RuntimeException("No contexts found"));
        return TranscriptionEmbeddedDTO.fromEntity(lastTranscription);
    }

    private double cosineSimilarity(float[] vec1, float[] vec2) {
        double dotProduct = 0.0;
        double normA = 0.0;
        double normB = 0.0;
        for (int i = 0; i < vec1.length; i++) {
            dotProduct += vec1[i] * vec2[i];
            normA += Math.pow(vec1[i], 2);
            normB += Math.pow(vec2[i], 2);
        }
        return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
    }

    // Implementa√ß√£o para buscar por palavras-chave
    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> findByKeywords(String keywords) {
        //String[] keywordsArray = keywords.toLowerCase().split("\\s+");  // Divide as palavras-chave
        return transcriptionEmbeddedRepository.findAll().stream()
        .filter(transcription -> transcription.getTranscriptionText().toLowerCase().contains(keywords.toLowerCase()))
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                        transcription.getKeywords(), // Adicionei
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }
}
```

**5. Atualize o Reposit√≥rio `ITranscriptionEmbeddedRepository`:**

*   Voc√™ n√£o precisa modificar o reposit√≥rio, pois estamos filtrando com JAVA. Caso queira fazer uma query no banco de dados:

```java
package com.commandAI.commandAI.modules.embeddingTranscription.repository;

import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import java.util.List;
import java.util.Optional;

@Repository
public interface ITranscriptionEmbeddedRepository extends JpaRepository<TranscriptionEmbedded, Long> {
    Optional<TranscriptionEmbedded> findTopByOrderByCreatedAtDesc();
    List<TranscriptionEmbedded> findByKeywordsContainingIgnoreCase(String keywords); // Novo m√©todo
}
```

**6. Atualize o Controlador `TranscriptionController`:**

*   Adicione o novo endpoint para buscar por palavras-chave:

```java
package com.commandAI.commandAI.modules.embeddingTranscription.controller;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.service.IServiceTranscriptionEmbedded;
import lombok.RequiredArgsConstructor;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.List;

@RestController
@RequestMapping("/api/meetings")    
@RequiredArgsConstructor
public class TranscriptionController {

    private final IServiceTranscriptionEmbedded service;

    @PostMapping("/transcriptions")
    public ResponseEntity<Void> addTranscription(@RequestBody TranscriptionEmbeddedDTO transcriptionDTO) {
        service.saveTranscription(transcriptionDTO.transcriptionText(), transcriptionDTO.embedding(), transcriptionDTO.keywords()); // Modifiquei
        return ResponseEntity.status(HttpStatus.CREATED).build();
    }

    @GetMapping("/transcriptions")
    public List<TranscriptionEmbeddedDTO> getAllTranscriptions() {
        return service.getAllTranscriptions();
    }

    @GetMapping("/transcriptions/{id}")
    public ResponseEntity<TranscriptionEmbeddedDTO> getTranscriptionById(@PathVariable Long id) {
        TranscriptionEmbeddedDTO transcription = service.getTranscriptionById(id);
        if (transcription == null) {
            return ResponseEntity.status(HttpStatus.NOT_FOUND).build();
        }
        return ResponseEntity.ok(transcription);
    }

    @GetMapping("/transcriptions/similar")
    public List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(@RequestParam float[] queryEmbedding) {
        return service.findSimilarTranscriptions(queryEmbedding);
    }
    @GetMapping("/last")
    public ResponseEntity<TranscriptionEmbeddedDTO> getLastTranscription() {
        TranscriptionEmbeddedDTO lastTranscription = service.getLastTranscription();
        return ResponseEntity.ok(lastTranscription);
    }

    @GetMapping("/transcriptions/search") // Novo endpoint
    public List<TranscriptionEmbeddedDTO> searchTranscriptions(@RequestParam String keywords) {
        return service.findByKeywords(keywords);
    }
}
```

**7. L√≥gica para Salvar Reuni√µes com Embeddings e Palavras-Chave:**

*   **Obtenha a Transcri√ß√£o:**  Primeiro, voc√™ precisa obter a transcri√ß√£o da reuni√£o (texto).
*   **Gere o Embedding:** Use um modelo de linguagem (como o Gemini) para gerar o "embedding" vetorial da transcri√ß√£o.
*   **Extraia Palavras-Chave:** Use um algoritmo de extra√ß√£o de palavras-chave para identificar os termos mais relevantes na transcri√ß√£o.
*   **Salve no Banco de Dados:** Salve a transcri√ß√£o, o embedding e as palavras-chave no banco de dados.

**Exemplo de como salvar (adaptado para o seu c√≥digo):**

```java
// Supondo que voc√™ tenha a transcri√ß√£o, o embedding e as palavras-chave
String transcriptionText = "Transcri√ß√£o da reuni√£o...";
float[] embedding = // ... seu embedding gerado ...
String keywords = "palavras-chave extra√≠das...";

// Crie um DTO
TranscriptionEmbeddedDTO transcriptionDTO = new TranscriptionEmbeddedDTO(
    null, // ID ser√° gerado automaticamente
    transcriptionText,
    embedding,
    keywords,
    null // Data ser√° gerada automaticamente
);

// Salve a transcri√ß√£o
service.saveTranscription(transcriptionDTO.transcriptionText(), transcriptionDTO.embedding(), transcriptionDTO.keywords());
```

**Pontos Importantes:**

*   **Extra√ß√£o de Palavras-Chave:** Voc√™ pode usar bibliotecas como Apache OpenNLP, Stanford CoreNLP ou at√© mesmo APIs online para extrair palavras-chave.
*   **Gera√ß√£o de Embeddings:**  Use a API do Gemini para gerar embeddings para cada frase.
*   **Banco de Dados:** Certifique-se de que o tipo de dados `FLOAT8[]` √© suportado pelo seu banco de dados.
*   **Performance:** Se voc√™ tiver muitas transcri√ß√µes, considere usar t√©cnicas de indexa√ß√£o para acelerar as buscas.

Este guia fornece um ponto de partida para implementar a busca de reuni√µes por palavras-chave e o armazenamento de reuni√µes com embeddings. Adapte o c√≥digo e as t√©cnicas √†s suas necessidades espec√≠ficas.
2025-04-14 19:47:44,588 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 19:47:44,590 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 19:47:44,592 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 19:47:44,594 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 19:47:44,598 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 19:47:44,600 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 19:47:44,603 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 19:47:44,605 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 19:47:44,607 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 19:47:44,610 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 19:47:44,612 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 19:47:44,614 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 19:47:44,616 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 19:47:44,618 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 19:47:44,620 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 19:47:44,622 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 19:47:44,624 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 19:47:44,626 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 19:47:44,628 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 19:47:44,631 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 19:47:44,633 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 19:47:44,636 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 19:47:44,638 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 19:47:44,641 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 19:47:44,644 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 19:47:44,647 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 19:47:44,649 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 19:47:44,650 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 19:47:44,652 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 19:47:44,654 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 19:47:44,657 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 19:47:44,659 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 19:47:44,661 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 19:47:44,663 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 19:47:44,665 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 19:47:44,668 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 22:39:31,765 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 22:39:31,774 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 22:39:31,777 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 22:39:31,779 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 22:39:31,782 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 22:39:31,784 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 22:39:31,787 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 22:39:31,789 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 22:39:31,791 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 22:39:31,793 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 22:39:31,794 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 22:39:31,798 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 22:39:31,799 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 22:39:31,801 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 22:39:31,803 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 22:39:31,805 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 22:39:31,806 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 22:39:31,808 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 22:39:31,809 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 22:39:31,810 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 22:39:31,812 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 22:39:31,815 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 22:39:31,817 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 22:39:31,819 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 22:39:31,820 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 22:39:31,822 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 22:39:31,824 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 22:39:31,825 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 22:39:31,828 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 22:39:31,832 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 22:39:31,833 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 22:39:31,835 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 22:39:31,837 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 22:39:31,839 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 22:39:31,841 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 22:39:31,843 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 22:39:32,347 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 22:39:32,350 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 22:39:32,352 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 22:39:32,353 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 22:39:32,355 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 22:39:32,356 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 22:39:32,357 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 22:39:32,359 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 22:39:32,360 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 22:39:32,362 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 22:39:32,363 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 22:39:32,365 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 22:39:32,366 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 22:39:32,367 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 22:39:32,369 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 22:39:32,370 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 22:39:32,372 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 22:39:32,373 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 22:39:32,374 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 22:39:32,375 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 22:39:32,376 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 22:39:32,377 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 22:39:32,378 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 22:39:32,380 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 22:39:32,382 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 22:39:32,384 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 22:39:32,386 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 22:39:32,387 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 22:39:32,389 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 22:39:32,391 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 22:39:32,393 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 22:39:32,394 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 22:39:32,395 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 22:39:32,398 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 22:39:32,400 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 22:39:32,401 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 22:39:46,239 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 22:39:46,240 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 22:39:46,241 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 22:39:46,242 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 22:39:46,244 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 22:39:46,245 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 22:39:46,246 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 22:39:46,247 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 22:39:46,248 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 22:39:46,250 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 22:39:46,251 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 22:39:46,252 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 22:39:46,254 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 22:39:46,255 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 22:39:46,256 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 22:39:46,258 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 22:39:46,260 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 22:39:46,261 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 22:39:46,263 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 22:39:46,264 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 22:39:46,266 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 22:39:46,268 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 22:39:46,270 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 22:39:46,271 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 22:39:46,272 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 22:39:46,273 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 22:39:46,274 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 22:39:46,274 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 22:39:46,275 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 22:39:46,276 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 22:39:46,277 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 22:39:46,278 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 22:39:46,279 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 22:39:46,281 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 22:39:46,284 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 22:39:46,285 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 22:39:46,397 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 22:39:46,399 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 22:39:46,400 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 22:39:46,401 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 22:39:46,402 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 22:39:46,404 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 22:39:46,406 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 22:39:46,408 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 22:39:46,410 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 22:39:46,412 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 22:39:46,413 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 22:39:46,415 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 22:39:46,417 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 22:39:46,418 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 22:39:46,419 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 22:39:46,421 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 22:39:46,422 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 22:39:46,424 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 22:39:46,425 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 22:39:46,426 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 22:39:46,427 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 22:39:46,429 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 22:39:46,431 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 22:39:46,432 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 22:39:46,433 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 22:39:46,435 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 22:39:46,436 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 22:39:46,437 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 22:39:46,438 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 22:39:46,440 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 22:39:46,441 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 22:39:46,442 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 22:39:46,443 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 22:39:46,444 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 22:39:46,446 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 22:39:46,447 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 22:39:46,461 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\API_CommandAI\assets\20250414223946_clipboard_20250414223931.png, Prompt: Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python_e_java

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python_e_java()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python_e_java() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py e .java a partir de src/"""
    src_dir = Config.BASE_DIR

    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    # Busca arquivos .py e .java separadamente
    padrao_busca_py = os.path.join(src_dir, '**', '*.py')
    padrao_busca_java = os.path.join(src_dir, '**', '*.java')

    arquivos_py = glob.glob(padrao_busca_py, recursive=True)
    arquivos_java = glob.glob(padrao_busca_java, recursive=True)

    arquivos = arquivos_py + arquivos_java
    arquivos.sort() # Ordena a lista completa

    for arquivo in arquivos:
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# main\java\com\commandAI\commandAI\CommandAiApplication.java

package com.commandAI.commandAI;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class CommandAiApplication {

	public static void main(String[] args) {
		SpringApplication.run(CommandAiApplication.class, args);
	}

}


# main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java

package com.commandAI.commandAI.modules.AIComunication.controller;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import com.commandAI.commandAI.modules.AIComunication.model.dto.GTPCommunicationRequestDTO;
import com.commandAI.commandAI.modules.AIComunication.service.IGPTCommunicationService;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.RequiredArgsConstructor;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.Arrays;
import java.util.List;

@RequiredArgsConstructor
@RestController
@RequestMapping("/api/question_answers")
public class GPTCommunicationController {

    private final IGPTCommunicationService service;

    @PostMapping("/save")
    public ResponseEntity<GPTCommunication> saveCommunication(@RequestBody GTPCommunicationRequestDTO dto) {
        try {
            GPTCommunication savedCommunication = service.saveCommunication(dto);
            return new ResponseEntity<>(savedCommunication, HttpStatus.CREATED);
        } catch (Exception e) {
            // Log error and return an appropriate response
            System.err.println("Erro ao salvar comunica√ß√£o: " + e.getMessage());
            return new ResponseEntity<>(HttpStatus.INTERNAL_SERVER_ERROR);
        }
    }

    @PostMapping("/similar")
    public ResponseEntity<List<GTPCommunicationRequestDTO>> getSimilarCommunications(@RequestBody JsonNode requestBody) {
        try {
            // Validar se √© um array
            if (!requestBody.isArray()) {
                throw new IllegalArgumentException("Esperado um array");
            }

            // Converter JsonNode para array de float
            float[] embedding = new ObjectMapper().convertValue(requestBody, float[].class);

            // Log para verificar o embedding recebido
            System.out.println("Embedding recebido: " + Arrays.toString(embedding));

            // Chamar o servi√ßo com o array de float
            List<GTPCommunicationRequestDTO> similarCommunications = service.findSimilarCommunications(embedding);
            return ResponseEntity.ok(similarCommunications);
        } catch (IllegalArgumentException e) {
            System.err.println("Erro de valida√ß√£o: " + e.getMessage());
            return ResponseEntity.status(HttpStatus.BAD_REQUEST).body(null);
        } catch (Exception e) {
            // Log error and return an appropriate response
            System.err.println("Erro ao processar a solicita√ß√£o: " + e.getMessage());
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(null);
        }
    }
}


# main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java

package com.commandAI.commandAI.modules.AIComunication.model;

import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import jakarta.persistence.*;
import lombok.Data;

import java.time.LocalDateTime;
@Data
@Entity
@JsonIgnoreProperties(ignoreUnknown = true)
@Table(name = "gpt_communication")
public class GPTCommunication {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(name = "question", nullable = false, columnDefinition = "TEXT")
    private String question;

    @Column(name = "answer", nullable = false, columnDefinition = "TEXT")
    private String answer;

    @Column(name = "question_embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] questionEmbedding;

    @Column(name = "answer_embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] answerEmbedding;

    @Column(name = "created_at", nullable = false)
    private LocalDateTime createdAt;

}

# main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java

package com.commandAI.commandAI.modules.AIComunication.model.dto;

import com.fasterxml.jackson.annotation.JsonProperty;

public record GTPCommunicationRequestDTO(
        String question,
        float[] questionEmbedding,
        String answer,
        float[] answerEmbedding
) {}



# main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java

package com.commandAI.commandAI.modules.AIComunication.repository;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

@Repository
public interface IGPTCommunicationRepository extends JpaRepository<GPTCommunication, Long> {

}

# main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java

package com.commandAI.commandAI.modules.AIComunication.service;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import com.commandAI.commandAI.modules.AIComunication.model.dto.GTPCommunicationRequestDTO;

import java.util.List;

public interface IGPTCommunicationService {
    GPTCommunication saveCommunication(GTPCommunicationRequestDTO dto);
    List<GTPCommunicationRequestDTO> findSimilarCommunications(float[] embedding);
}

# main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java

package com.commandAI.commandAI.modules.AIComunication.service.impl;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import com.commandAI.commandAI.modules.AIComunication.model.dto.GTPCommunicationRequestDTO;
import com.commandAI.commandAI.modules.AIComunication.repository.IGPTCommunicationRepository;
import com.commandAI.commandAI.modules.AIComunication.service.IGPTCommunicationService;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;

import java.time.LocalDateTime;
import java.util.Arrays;
import java.util.List;
import java.util.stream.Collectors;

@RequiredArgsConstructor
@Service
public class GPTCommunicationServiceImpl implements IGPTCommunicationService {

    private final IGPTCommunicationRepository repository;

    @Override
    public GPTCommunication saveCommunication(GTPCommunicationRequestDTO dto) {
        GPTCommunication communication = new GPTCommunication();
        communication.setQuestion(dto.question());
        communication.setQuestionEmbedding(dto.questionEmbedding());
        communication.setAnswer(dto.answer());
        communication.setAnswerEmbedding(dto.answerEmbedding());
        communication.setCreatedAt(LocalDateTime.now());
        return repository.save(communication);
    }

    @Override
    public List<GTPCommunicationRequestDTO> findSimilarCommunications(float[] embedding) {
        List<GPTCommunication> allCommunications = repository.findAll();

        return allCommunications.stream()
                .filter(comm -> isSimilar(embedding, comm.getQuestionEmbedding()))
                .map(comm -> new GTPCommunicationRequestDTO(
                        comm.getQuestion(),
                        comm.getQuestionEmbedding(),
                        comm.getAnswer(),
                        comm.getAnswerEmbedding()))
                .collect(Collectors.toList());
    }

    private boolean isSimilar(float[] embedding1, float[] embedding2) {
        // Verificar se os embeddings t√™m o mesmo tamanho
        if (embedding1.length != embedding2.length) {
            System.err.println("Os embeddings t√™m tamanhos diferentes.");
            return false;
        }

        System.out.println("Embedding1: " + Arrays.toString(embedding1));
        System.out.println("Embedding2: " + Arrays.toString(embedding2));

        double dotProduct = 0.0;
        double normA = 0.0;
        double normB = 0.0;
        for (int i = 0; i < embedding1.length; i++) {
            dotProduct += embedding1[i] * embedding2[i];
            normA += Math.pow(embedding1[i], 2);
            normB += Math.pow(embedding2[i], 2);
        }
        return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB)) > 0.8; // Ajuste o limiar conforme necess√°rio
    }
}


# main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java

package com.commandAI.commandAI.modules.context.controller;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.context.service.IContextService;
import com.commandAI.commandAI.modules.context.model.context.Context;
import lombok.RequiredArgsConstructor;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.validation.annotation.Validated;
import org.springframework.web.bind.annotation.*;

import java.util.List;
import java.util.Map;

@RequiredArgsConstructor
@RestController
@Validated
@ResponseBody
@RequestMapping("/api/contexts")
public class ContextController {

    @Autowired
    private final IContextService contextService;

    @PostMapping("/save")
    public ResponseEntity<Map<String, Object>> saveContext(@RequestBody ContextDTO data) {
        Context newContext = contextService.saveContext(data);
        Map<String, Object> response = Map.of("result", newContext);
        return ResponseEntity.status(HttpStatus.CREATED).body(response);
    }


    @GetMapping("/last")
    public ResponseEntity<ContextDTO> getLastContext() {
        ContextDTO lastContext = contextService.getLastContext();
        return ResponseEntity.ok(lastContext);
    }

    @GetMapping("/all")
    public ResponseEntity<Map<String, Object>> getAllContexts() {
        List<Context> contexts = contextService.findAllContext();
        return ResponseEntity.ok(Map.of("contexts", contexts));
    }
}


# main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java

package com.commandAI.commandAI.modules.context.controller.hendler;

import com.commandAI.commandAI.modules.context.service.validation.ContextNotFoundException;
import lombok.Data;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.MissingServletRequestParameterException;
import org.springframework.web.bind.annotation.ControllerAdvice;
import org.springframework.web.bind.annotation.ExceptionHandler;
import org.springframework.web.context.request.WebRequest;

import java.time.LocalDateTime;
import java.util.HashMap;
import java.util.Map;

@Data
@ControllerAdvice
public class GlobalExceptionHandler {

    @ExceptionHandler(ContextNotFoundException.class)
    public ResponseEntity<Object> handleContextNotFoundException(ContextNotFoundException ex, WebRequest request) {
        Map<String, Object> body = new HashMap<>();
        body.put("timestamp", LocalDateTime.now());
        body.put("message", ex.getMessage());

        return new ResponseEntity<>(body, HttpStatus.NOT_FOUND);
    }
    @ExceptionHandler(MissingServletRequestParameterException.class)
    public ResponseEntity<Object> handleMissingServletRequestParameterException(MissingServletRequestParameterException ex, WebRequest request) {
        Map<String, Object> body = new HashMap<>();
        body.put("timestamp", LocalDateTime.now());
        body.put("message", "Par√¢metro de requisi√ß√£o ausente: " + ex.getParameterName());

        return new ResponseEntity<>(body, HttpStatus.BAD_REQUEST);
    }

    // Outros handlers de exce√ß√£o podem ser adicionados aqui
}

# main\java\com\commandAI\commandAI\modules\context\model\context\Context.java

    package com.commandAI.commandAI.modules.context.model.context;
    import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
    import jakarta.persistence.*;
    import lombok.Data;

    import java.time.LocalDateTime;
    @Data
    @Table(name = "tb_context")
    @Entity
    @JsonIgnoreProperties(ignoreUnknown = true)
    public class Context {

        @Id
        @GeneratedValue(strategy = GenerationType.IDENTITY)
        private Long id;

        @Column(name = "context", nullable = false, columnDefinition = "TEXT")
        private String context;

        @Column(name = "created_at", nullable = false)
        private LocalDateTime createdAt;

        @PrePersist
        protected void onCreate() {
            createdAt = LocalDateTime.now();
        }
    }



# main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java

package com.commandAI.commandAI.modules.context.model.dto;

import com.commandAI.commandAI.modules.context.model.context.Context;
import java.time.LocalDateTime;
public record ContextDTO(
        Long id,
        String context,
        LocalDateTime createdAt
) {
    public static ContextDTO fromEntity(Context context) {
        return new ContextDTO(context.getId(), context.getContext(), context.getCreatedAt());
    }
}


# main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java

package com.commandAI.commandAI.modules.context.repository;

import com.commandAI.commandAI.modules.context.model.context.Context;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import java.util.Optional;

@Repository
public interface IContextRepository extends JpaRepository<Context, Long> {
    Optional<Context> findTopByOrderByCreatedAtDesc();
}


# main\java\com\commandAI\commandAI\modules\context\service\IContextService.java

package com.commandAI.commandAI.modules.context.service;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;

import java.util.List;

public interface IContextService {
    Context saveContext(ContextDTO data);
    ContextDTO getLastContext();
    List<Context> findAllContext();
}


# main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java

package com.commandAI.commandAI.modules.context.service.impl;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.repository.IContextRepository;
import com.commandAI.commandAI.modules.context.service.IContextService;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.util.List;

@RequiredArgsConstructor
@Service
public class ContextServiceImpl implements IContextService {

    private final IContextRepository contextRepository;

    @Override
    @Transactional
    public Context saveContext(ContextDTO data) {
        Context context = new Context();
        context.setContext(data.context());
        context = contextRepository.save(context);
        return context;
    }

    @Override
    @Transactional
    public ContextDTO getLastContext() {
        Context lastContext = contextRepository.findTopByOrderByCreatedAtDesc()
                .orElseThrow(() -> new RuntimeException("No contexts found"));
        return ContextDTO.fromEntity(lastContext);
    }
    @Override
    @Transactional
    public List<Context> findAllContext() {
        return contextRepository.findAll();
    }
}


# main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java

package com.commandAI.commandAI.modules.context.service.validation;

public class ContextNotFoundException extends RuntimeException {
    public ContextNotFoundException(String message) {
        super(message);
    }
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java

package com.commandAI.commandAI.modules.embeddingTranscription.controller;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.service.IServiceTranscriptionEmbedded;
import lombok.RequiredArgsConstructor;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.List;

@RestController
@RequestMapping("/api/meetings")    
@RequiredArgsConstructor
public class TranscriptionController {

    private final IServiceTranscriptionEmbedded service;

    @PostMapping("/transcriptions")
    public ResponseEntity<Void> addTranscription(@RequestBody TranscriptionEmbeddedDTO transcriptionDTO) {
        service.saveTranscription(transcriptionDTO.transcriptionText(), transcriptionDTO.embedding());
        return ResponseEntity.status(HttpStatus.CREATED).build();
    }

    @GetMapping("/transcriptions")
    public List<TranscriptionEmbeddedDTO> getAllTranscriptions() {
        return service.getAllTranscriptions();
    }

    @GetMapping("/transcriptions/{id}")
    public ResponseEntity<TranscriptionEmbeddedDTO> getTranscriptionById(@PathVariable Long id) {
        TranscriptionEmbeddedDTO transcription = service.getTranscriptionById(id);
        if (transcription == null) {
            return ResponseEntity.status(HttpStatus.NOT_FOUND).build();
        }
        return ResponseEntity.ok(transcription);
    }

    @GetMapping("/transcriptions/similar")
    public List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(@RequestParam float[] queryEmbedding) {
        return service.findSimilarTranscriptions(queryEmbedding);
    }
    @GetMapping("/last")
    public ResponseEntity<TranscriptionEmbeddedDTO> getLastTranscription() {
        TranscriptionEmbeddedDTO lastTranscription = service.getLastTranscription();
        return ResponseEntity.ok(lastTranscription);
    }
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java

package com.commandAI.commandAI.modules.embeddingTranscription.model;

import jakarta.persistence.*;
import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import lombok.Data;

import java.time.LocalDateTime;

@Data
@JsonIgnoreProperties(ignoreUnknown = true)
@Entity
@Table(name = "transcriptionEmbedded")
public class TranscriptionEmbedded {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    @Column(name = "meeting_id", nullable = false)
    private Long meetingId;

    @Column(name = "transcription_text", nullable = false, columnDefinition = "TEXT")
    private String transcriptionText;

    @Column(name = "embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] embedding;

    @Column(name = "created_at", nullable = false)
    private LocalDateTime createdAt;

    @PrePersist
    protected void onCreate() {
        createdAt = LocalDateTime.now();
    }

}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java

package com.commandAI.commandAI.modules.embeddingTranscription.model.dto;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;

import java.time.LocalDateTime;

public record TranscriptionEmbeddedDTO(
        Long meetingId,
        String transcriptionText,
        float[] embedding,
        LocalDateTime createdAt
) {
    public static TranscriptionEmbeddedDTO fromEntity(TranscriptionEmbedded transcriptionEmbedded) {
        return new TranscriptionEmbeddedDTO(transcriptionEmbedded.getMeetingId(), transcriptionEmbedded.getTranscriptionText(), transcriptionEmbedded.getEmbedding(), transcriptionEmbedded.getCreatedAt());
    }
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java

package com.commandAI.commandAI.modules.embeddingTranscription.repository;

import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import java.util.Optional;

@Repository
public interface ITranscriptionEmbeddedRepository extends JpaRepository<TranscriptionEmbedded, Long> {
    Optional<TranscriptionEmbedded> findTopByOrderByCreatedAtDesc();

}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java

package com.commandAI.commandAI.modules.embeddingTranscription.service;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;

import java.util.List;

public interface IServiceTranscriptionEmbedded {
    void saveTranscription(String transcriptionText, float[] embedding);
    List<TranscriptionEmbeddedDTO> getAllTranscriptions();
    TranscriptionEmbeddedDTO getTranscriptionById(Long id);
    List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(float[] queryEmbedding);
    TranscriptionEmbeddedDTO getLastTranscription();
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java

package com.commandAI.commandAI.modules.embeddingTranscription.service.impl;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.repository.ITranscriptionEmbeddedRepository;
import com.commandAI.commandAI.modules.embeddingTranscription.service.IServiceTranscriptionEmbedded;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.util.List;
import java.util.stream.Collectors;

@RequiredArgsConstructor
@Service
public class ServiceTranscriptionEmbeddedImpl implements IServiceTranscriptionEmbedded {

    private final ITranscriptionEmbeddedRepository transcriptionEmbeddedRepository;

    @Override
    @Transactional
    public void saveTranscription(String transcriptionText, float[] embedding) {
        TranscriptionEmbedded transcription = new TranscriptionEmbedded();
        transcription.setTranscriptionText(transcriptionText);
        transcription.setEmbedding(embedding);
        System.out.println(transcription);
        transcriptionEmbeddedRepository.save(transcription);
    }

    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> getAllTranscriptions() {
        return transcriptionEmbeddedRepository.findAll().stream()
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }

    @Override
    @Transactional
    public TranscriptionEmbeddedDTO getTranscriptionById(Long id) {
        TranscriptionEmbedded transcription = transcriptionEmbeddedRepository.findById(id).orElse(null);
        if (transcription == null) {
            return null;
        }
        return new TranscriptionEmbeddedDTO(
                transcription.getMeetingId(),
                transcription.getTranscriptionText(),
                transcription.getEmbedding(),
                transcription.getCreatedAt());
    }

    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(float[] queryEmbedding) {
        List<TranscriptionEmbedded> allTranscriptions = transcriptionEmbeddedRepository.findAll();
        return allTranscriptions.stream()
                .sorted((t1, t2) -> Double.compare(
                        cosineSimilarity(queryEmbedding, t2.getEmbedding()),
                        cosineSimilarity(queryEmbedding, t1.getEmbedding())))
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }
    @Override
    @Transactional
    public TranscriptionEmbeddedDTO getLastTranscription() {
        TranscriptionEmbedded lastTranscription = transcriptionEmbeddedRepository.findTopByOrderByCreatedAtDesc()
                .orElseThrow(() -> new RuntimeException("No contexts found"));
        return TranscriptionEmbeddedDTO.fromEntity(lastTranscription);
    }
    private double cosineSimilarity(float[] vec1, float[] vec2) {
        double dotProduct = 0.0;
        double normA = 0.0;
        double normB = 0.0;
        for (int i = 0; i < vec1.length; i++) {
            dotProduct += vec1[i] * vec2[i];
            normA += Math.pow(vec1[i], 2);
            normB += Math.pow(vec2[i], 2);
        }
        return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
    }
}


# test\java\com\commandAI\commandAI\CommandAiApplicationTests.java

package com.commandAI.commandAI;

import org.junit.jupiter.api.Test;
import org.springframework.boot.test.context.SpringBootTest;

@SpringBootTest
class CommandAiApplicationTests {

	@Test
	void contextLoads() {
	}
}


# test\java\com\commandAI\commandAI\modelsTest\ContextTest.java

package com.commandAI.commandAI.modelsTest;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import com.commandAI.commandAI.modules.context.model.context.Context;
import java.time.LocalDateTime;

import static org.junit.jupiter.api.Assertions.*;

public class ContextTest {

    private Context context;

    @BeforeEach
    public void setUp() {
        context = new Context();
    }

    @Test
    public void testSetAndGetId() {
        Long id = 1L;
        context.setId(id);
        assertEquals(id, context.getId());
    }

    @Test
    public void testSetAndGetContext() {
        String testContext = "Sample context";
        context.setContext(testContext);
        assertEquals(testContext, context.getContext());
    }

    @Test
    public void testSetAndGetCreatedAt() {
        LocalDateTime now = LocalDateTime.now();
        context.setCreatedAt(now);
        assertEquals(now, context.getCreatedAt());
    }
}


User: resuma a minha aplica√ß√£o JAVA
Assistant: Com certeza! Sua aplica√ß√£o Java, "CommandAI", √© um projeto Spring Boot que visa facilitar a comunica√ß√£o com intelig√™ncia artificial (como o GPT), armazenando contextos e transcri√ß√µes de forma inteligente. Aqui est√° um resumo dos principais componentes:

**Componentes Principais:**

*   **M√≥dulos:**
    *   **AIComunication:** Lida com a comunica√ß√£o com a IA, incluindo salvar as perguntas e respostas, gerar embeddings para o texto e encontrar comunica√ß√µes similares com base nos embeddings.
    *   **Context:** Permite salvar e recuperar o contexto da conversa, mantendo o hist√≥rico das intera√ß√µes.
    *   **EmbeddingTranscription:** Gerencia as transcri√ß√µes das reuni√µes com os respectivos "embeddings" (representa√ß√µes vetoriais) para facilitar a busca de transcri√ß√µes similares.

*   **Entidades (Modelos):**

    *   `GPTCommunication`: Representa uma intera√ß√£o com a IA, armazenando a pergunta, a resposta e os "embeddings" vetoriais correspondentes.
    *   `Context`:  Representa um contexto de conversa.
    *   `TranscriptionEmbedded`: Representa uma transcri√ß√£o de reuni√£o com o seu embedding.

*   **Repositories:**
    *   Interfaces que estendem `JpaRepository` para interagir com o banco de dados para as entidades: `GPTCommunication`, `Context`, e `TranscriptionEmbedded`.

*   **Servi√ßos:**
    *   Implementa√ß√µes das interfaces de servi√ßo (ex: `GPTCommunicationServiceImpl`, `ContextServiceImpl`, `ServiceTranscriptionEmbeddedImpl`) que cont√™m a l√≥gica de neg√≥cios para salvar, recuperar e encontrar informa√ß√µes relacionadas √† comunica√ß√£o com a IA, contextos e transcri√ß√µes.

*   **Controladores (Controllers):**

    *   `GPTCommunicationController`:  Exp√µe endpoints REST para salvar as intera√ß√µes com a IA (`/api/question_answers`) e buscar intera√ß√µes similares.
    *   `ContextController`:  Gerencia os contextos de conversa, permitindo salvar, recuperar o √∫ltimo contexto e todos os contextos (`/api/contexts`).
    *   `TranscriptionController`:  Lida com as transcri√ß√µes de reuni√µes, permitindo adicionar, recuperar, buscar transcri√ß√µes similares e o √∫ltimo contexto (`/api/meetings`).

*   **DTOs (Data Transfer Objects):**

    *   `GTPCommunicationRequestDTO`, `ContextDTO`, `TranscriptionEmbeddedDTO`: Usados para transferir dados entre a aplica√ß√£o e o mundo externo (ex: requisi√ß√µes e respostas da API).

**Funcionalidades:**

*   **Armazenamento de intera√ß√µes com a IA:** Salva as perguntas e respostas trocadas com a IA, junto com os "embeddings" vetoriais do texto.
*   **Busca de intera√ß√µes similares:** Encontra intera√ß√µes semelhantes com base nos "embeddings" vetoriais, permitindo reutilizar informa√ß√µes e contextos.
*   **Gerenciamento de contexto:** Permite salvar e recuperar o contexto da conversa, garantindo que a IA tenha o hist√≥rico necess√°rio para responder adequadamente.
*   **Transcri√ß√µes de reuni√µes:** Permite salvar as transcri√ß√µes de reuni√µes e associ√°-las a "embeddings" vetoriais para facilitar a busca de reuni√µes relevantes.
*   **APIs REST:** Exp√µe APIs REST para facilitar a integra√ß√£o com outras aplica√ß√µes e servi√ßos.

**Em resumo:** A aplica√ß√£o CommandAI √© uma solu√ß√£o completa para gerenciar a comunica√ß√£o com a IA, armazenando contextos e transcri√ß√µes de forma inteligente, permitindo a busca de informa√ß√µes relevantes e a reutiliza√ß√£o de conhecimento.
User: Estou implementando boas pr√°ticas de desenvolvimento JAVA ?
Assistant: Assistant: Sim, com base na estrutura e nos componentes da sua aplica√ß√£o CommandAI, parece que voc√™ est√° seguindo muitas boas pr√°ticas de desenvolvimento Java e Spring Boot. Aqui est√° uma an√°lise detalhada:

**Boas Pr√°ticas Implementadas:**

*   **Arquitetura em Camadas:** A aplica√ß√£o est√° bem organizada em camadas distintas (Controladores, Servi√ßos, Reposit√≥rios, Modelos), o que facilita a manuten√ß√£o, o teste e a evolu√ß√£o do sistema.
*   **Uso do Spring Boot:** O uso do Spring Boot simplifica a configura√ß√£o e o desenvolvimento, fornecendo "auto-configura√ß√£o", "starter dependencies" e outras ferramentas que agilizam o processo.
*   **Inje√ß√£o de Depend√™ncia (DI):** O uso de `@Autowired` e `@RequiredArgsConstructor` para inje√ß√£o de depend√™ncia promove o baixo acoplamento e facilita o teste unit√°rio.
*   **DTOs (Data Transfer Objects):** O uso de DTOs para transferir dados entre as camadas e expor dados na API √© uma pr√°tica recomendada para desacoplar os modelos de dados internos da API.
*   **RESTful APIs:** A cria√ß√£o de APIs RESTful com controladores (Controllers) bem definidos e endpoints claros (ex: `/api/question_answers`, `/api/contexts`, `/api/meetings`) facilita a integra√ß√£o com outros sistemas.
*   **Valida√ß√£o:** O uso de `@Validated` nos controladores indica a preocupa√ß√£o com a valida√ß√£o dos dados de entrada.
*   **Tratamento de Exce√ß√µes:** A implementa√ß√£o de um `GlobalExceptionHandler` para lidar com exce√ß√µes de forma centralizada √© uma boa pr√°tica para fornecer respostas de erro consistentes e informativas.
*   **Log:** O uso de `System.err.println` para log (enquanto n√£o ideal) indica uma preocupa√ß√£o com o registro de eventos e erros para facilitar a depura√ß√£o e o monitoramento.
*   **JPA e Spring Data JPA:** O uso de JPA (Java Persistence API) com Spring Data JPA simplifica o acesso ao banco de dados, fornecendo uma abstra√ß√£o sobre a camada de persist√™ncia.
*   **Embeddings:** A utiliza√ß√£o de Embeddings √© uma √≥tima estrat√©gia para similaridade sem√¢ntica.
*   **Testes:** A presen√ßa da classe `CommandAiApplicationTests` e `ContextTest` indica que voc√™ est√° escrevendo testes para sua aplica√ß√£o, o que √© fundamental para garantir a qualidade e a confiabilidade do c√≥digo.
*   **Uso de Records:** A utiliza√ß√£o de Records √© excelente para DTOs.

**Sugest√µes de Melhorias:**

*   **Log:** Substitua `System.err.println` por um framework de log mais robusto, como SLF4J com Logback ou Log4j2. Isso permite configurar diferentes n√≠veis de log, direcionar os logs para arquivos ou outros destinos, e adicionar informa√ß√µes contextuais aos logs.
*   **Tratamento de Exce√ß√µes:**
    *   Crie classes de exce√ß√£o personalizadas para representar diferentes tipos de erros na sua aplica√ß√£o (ex: `AICommunicationException`, `ContextException`, `TranscriptionException`).
    *   Use c√≥digos de status HTTP apropriados para cada tipo de erro.
    *   Forne√ßa mensagens de erro claras e informativas para o cliente.
*   **Seguran√ßa:** Implemente mecanismos de autentica√ß√£o e autoriza√ß√£o para proteger suas APIs. Spring Security √© uma √≥tima op√ß√£o para isso.
*   **Documenta√ß√£o:** Use ferramentas como o Swagger (OpenAPI) para gerar automaticamente a documenta√ß√£o da sua API REST.
*   **Internacionaliza√ß√£o (i18n):** Se sua aplica√ß√£o precisar suportar diferentes idiomas, considere implementar a internacionaliza√ß√£o.
*   **Monitoramento:** Integre ferramentas de monitoramento para coletar m√©tricas sobre o desempenho da sua aplica√ß√£o (ex: tempo de resposta da API, uso de recursos, etc.).
*   **Documenta√ß√£o Javadoc:** Adicione coment√°rios Javadoc para documentar suas classes, m√©todos e par√¢metros. Isso facilita a compreens√£o do c√≥digo e a gera√ß√£o de documenta√ß√£o autom√°tica.
*   **Cobertura de Testes:** Busque uma alta cobertura de testes unit√°rios e de integra√ß√£o.

**Em resumo:** Sua aplica√ß√£o CommandAI parece estar bem estruturada e seguindo muitas boas pr√°ticas de desenvolvimento Java e Spring Boot. Ao implementar as sugest√µes de melhorias acima, voc√™ pode aumentar ainda mais a qualidade, a manutenibilidade e a seguran√ßa do seu sistema.

User: Preciso implementar a l√≥gica para que um end point possa buscar reuni√µes com palavras chaves. Preciso implementar a l√≥gica de salvar as reuni√µes em embeddings ?
Assistant: Com certeza! Aqui est√° um guia de como implementar a l√≥gica para buscar reuni√µes por palavras-chave usando "embeddings" (representa√ß√µes vetoriais) e como salvar as reuni√µes em embeddings:

**1. Atualize a Entidade `TranscriptionEmbedded`:**

*   Adicione um campo para as palavras-chave (opcional, mas recomendado para buscas mais r√°pidas):

```java
package com.commandAI.commandAI.modules.embeddingTranscription.model;

import jakarta.persistence.*;
import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import lombok.Data;

import java.time.LocalDateTime;

@Data
@JsonIgnoreProperties(ignoreUnknown = true)
@Entity
@Table(name = "transcriptionEmbedded")
public class TranscriptionEmbedded {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    @Column(name = "meeting_id", nullable = false)
    private Long meetingId;

    @Column(name = "transcription_text", nullable = false, columnDefinition = "TEXT")
    private String transcriptionText;

    @Column(name = "embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] embedding;

    @Column(name = "keywords", columnDefinition = "TEXT") // Novo campo
    private String keywords;

    @Column(name = "created_at", nullable = false)
    private LocalDateTime createdAt;

    @PrePersist
    protected void onCreate() {
        createdAt = LocalDateTime.now();
    }

}
```

**2. Atualize o DTO `TranscriptionEmbeddedDTO`:**

*   Adicione o campo `keywords` ao DTO:

```java
package com.commandAI.commandAI.modules.embeddingTranscription.model.dto;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;

import java.time.LocalDateTime;

public record TranscriptionEmbeddedDTO(
        Long meetingId,
        String transcriptionText,
        float[] embedding,
        String keywords, // Novo campo
        LocalDateTime createdAt
) {
    public static TranscriptionEmbeddedDTO fromEntity(TranscriptionEmbedded transcriptionEmbedded) {
        return new TranscriptionEmbeddedDTO(transcriptionEmbedded.getMeetingId(), transcriptionEmbedded.getTranscriptionText(), transcriptionEmbedded.getEmbedding(), transcriptionEmbedded.getKeywords(), transcriptionEmbedded.getCreatedAt());
    }
}
```

**3. Atualize o Servi√ßo `IServiceTranscriptionEmbedded`:**

*   Adicione um m√©todo para buscar transcri√ß√µes por palavras-chave:

```java
package com.commandAI.commandAI.modules.embeddingTranscription.service;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;

import java.util.List;

public interface IServiceTranscriptionEmbedded {
    void saveTranscription(String transcriptionText, float[] embedding, String keywords); // Modifiquei
    List<TranscriptionEmbeddedDTO> getAllTranscriptions();
    TranscriptionEmbeddedDTO getTranscriptionById(Long id);
    List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(float[] queryEmbedding);
    TranscriptionEmbeddedDTO getLastTranscription();
    List<TranscriptionEmbeddedDTO> findByKeywords(String keywords);  // Novo m√©todo
}
```

**4. Atualize a Implementa√ß√£o do Servi√ßo `ServiceTranscriptionEmbeddedImpl`:**

```java
package com.commandAI.commandAI.modules.embeddingTranscription.service.impl;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.repository.ITranscriptionEmbeddedRepository;
import com.commandAI.commandAI.modules.embeddingTranscription.service.IServiceTranscriptionEmbedded;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.util.Arrays;
import java.util.List;
import java.util.stream.Collectors;

@RequiredArgsConstructor
@Service
public class ServiceTranscriptionEmbeddedImpl implements IServiceTranscriptionEmbedded {

    private final ITranscriptionEmbeddedRepository transcriptionEmbeddedRepository;

    @Override
    @Transactional
    public void saveTranscription(String transcriptionText, float[] embedding, String keywords) {  // Modifiquei
        TranscriptionEmbedded transcription = new TranscriptionEmbedded();
        transcription.setTranscriptionText(transcriptionText);
        transcription.setEmbedding(embedding);
        transcription.setKeywords(keywords); // Adicionei
        System.out.println(transcription);
        transcriptionEmbeddedRepository.save(transcription);
    }

    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> getAllTranscriptions() {
        return transcriptionEmbeddedRepository.findAll().stream()
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                        transcription.getKeywords(), // Adicionei
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }

    @Override
    @Transactional
    public TranscriptionEmbeddedDTO getTranscriptionById(Long id) {
        TranscriptionEmbedded transcription = transcriptionEmbeddedRepository.findById(id).orElse(null);
        if (transcription == null) {
            return null;
        }
        return new TranscriptionEmbeddedDTO(
                transcription.getMeetingId(),
                transcription.getTranscriptionText(),
                transcription.getEmbedding(),
                transcription.getKeywords(), // Adicionei
                transcription.getCreatedAt());
    }

    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(float[] queryEmbedding) {
        List<TranscriptionEmbedded> allTranscriptions = transcriptionEmbeddedRepository.findAll();
        return allTranscriptions.stream()
                .sorted((t1, t2) -> Double.compare(
                        cosineSimilarity(queryEmbedding, t2.getEmbedding()),
                        cosineSimilarity(queryEmbedding, t1.getEmbedding())))
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                         transcription.getKeywords(), // Adicionei
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }

    @Override
    @Transactional
    public TranscriptionEmbeddedDTO getLastTranscription() {
        TranscriptionEmbedded lastTranscription = transcriptionEmbeddedRepository.findTopByOrderByCreatedAtDesc()
                .orElseThrow(() -> new RuntimeException("No contexts found"));
        return TranscriptionEmbeddedDTO.fromEntity(lastTranscription);
    }

    private double cosineSimilarity(float[] vec1, float[] vec2) {
        double dotProduct = 0.0;
        double normA = 0.0;
        double normB = 0.0;
        for (int i = 0; i < vec1.length; i++) {
            dotProduct += vec1[i] * vec2[i];
            normA += Math.pow(vec1[i], 2);
            normB += Math.pow(vec2[i], 2);
        }
        return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
    }

    // Implementa√ß√£o para buscar por palavras-chave
    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> findByKeywords(String keywords) {
        //String[] keywordsArray = keywords.toLowerCase().split("\\s+");  // Divide as palavras-chave
        return transcriptionEmbeddedRepository.findAll().stream()
        .filter(transcription -> transcription.getTranscriptionText().toLowerCase().contains(keywords.toLowerCase()))
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                        transcription.getKeywords(), // Adicionei
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }
}
```

**5. Atualize o Reposit√≥rio `ITranscriptionEmbeddedRepository`:**

*   Voc√™ n√£o precisa modificar o reposit√≥rio, pois estamos filtrando com JAVA. Caso queira fazer uma query no banco de dados:

```java
package com.commandAI.commandAI.modules.embeddingTranscription.repository;

import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import java.util.List;
import java.util.Optional;

@Repository
public interface ITranscriptionEmbeddedRepository extends JpaRepository<TranscriptionEmbedded, Long> {
    Optional<TranscriptionEmbedded> findTopByOrderByCreatedAtDesc();
    List<TranscriptionEmbedded> findByKeywordsContainingIgnoreCase(String keywords); // Novo m√©todo
}
```

**6. Atualize o Controlador `TranscriptionController`:**

*   Adicione o novo endpoint para buscar por palavras-chave:

```java
package com.commandAI.commandAI.modules.embeddingTranscription.controller;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.service.IServiceTranscriptionEmbedded;
import lombok.RequiredArgsConstructor;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.List;

@RestController
@RequestMapping("/api/meetings")    
@RequiredArgsConstructor
public class TranscriptionController {

    private final IServiceTranscriptionEmbedded service;

    @PostMapping("/transcriptions")
    public ResponseEntity<Void> addTranscription(@RequestBody TranscriptionEmbeddedDTO transcriptionDTO) {
        service.saveTranscription(transcriptionDTO.transcriptionText(), transcriptionDTO.embedding(), transcriptionDTO.keywords()); // Modifiquei
        return ResponseEntity.status(HttpStatus.CREATED).build();
    }

    @GetMapping("/transcriptions")
    public List<TranscriptionEmbeddedDTO> getAllTranscriptions() {
        return service.getAllTranscriptions();
    }

    @GetMapping("/transcriptions/{id}")
    public ResponseEntity<TranscriptionEmbeddedDTO> getTranscriptionById(@PathVariable Long id) {
        TranscriptionEmbeddedDTO transcription = service.getTranscriptionById(id);
        if (transcription == null) {
            return ResponseEntity.status(HttpStatus.NOT_FOUND).build();
        }
        return ResponseEntity.ok(transcription);
    }

    @GetMapping("/transcriptions/similar")
    public List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(@RequestParam float[] queryEmbedding) {
        return service.findSimilarTranscriptions(queryEmbedding);
    }
    @GetMapping("/last")
    public ResponseEntity<TranscriptionEmbeddedDTO> getLastTranscription() {
        TranscriptionEmbeddedDTO lastTranscription = service.getLastTranscription();
        return ResponseEntity.ok(lastTranscription);
    }

    @GetMapping("/transcriptions/search") // Novo endpoint
    public List<TranscriptionEmbeddedDTO> searchTranscriptions(@RequestParam String keywords) {
        return service.findByKeywords(keywords);
    }
}
```

**7. L√≥gica para Salvar Reuni√µes com Embeddings e Palavras-Chave:**

*   **Obtenha a Transcri√ß√£o:**  Primeiro, voc√™ precisa obter a transcri√ß√£o da reuni√£o (texto).
*   **Gere o Embedding:** Use um modelo de linguagem (como o Gemini) para gerar o "embedding" vetorial da transcri√ß√£o.
*   **Extraia Palavras-Chave:** Use um algoritmo de extra√ß√£o de palavras-chave para identificar os termos mais relevantes na transcri√ß√£o.
*   **Salve no Banco de Dados:** Salve a transcri√ß√£o, o embedding e as palavras-chave no banco de dados.

**Exemplo de como salvar (adaptado para o seu c√≥digo):**

```java
// Supondo que voc√™ tenha a transcri√ß√£o, o embedding e as palavras-chave
String transcriptionText = "Transcri√ß√£o da reuni√£o...";
float[] embedding = // ... seu embedding gerado ...
String keywords = "palavras-chave extra√≠das...";

// Crie um DTO
TranscriptionEmbeddedDTO transcriptionDTO = new TranscriptionEmbeddedDTO(
    null, // ID ser√° gerado automaticamente
    transcriptionText,
    embedding,
    keywords,
    null // Data ser√° gerada automaticamente
);

// Salve a transcri√ß√£o
service.saveTranscription(transcriptionDTO.transcriptionText(), transcriptionDTO.embedding(), transcriptionDTO.keywords());
```

**Pontos Importantes:**

*   **Extra√ß√£o de Palavras-Chave:** Voc√™ pode usar bibliotecas como Apache OpenNLP, Stanford CoreNLP ou at√© mesmo APIs online para extrair palavras-chave.
*   **Gera√ß√£o de Embeddings:**  Use a API do Gemini para gerar embeddings para cada frase.
*   **Banco de Dados:** Certifique-se de que o tipo de dados `FLOAT8[]` √© suportado pelo seu banco de dados.
*   **Performance:** Se voc√™ tiver muitas transcri√ß√µes, considere usar t√©cnicas de indexa√ß√£o para acelerar as buscas.

Este guia fornece um ponto de partida para implementar a busca de reuni√µes por palavras-chave e o armazenamento de reuni√µes com embeddings. Adapte o c√≥digo e as t√©cnicas √†s suas necessidades espec√≠ficas.
User: porque n√£o consegui fazer esse push ? veja imagem ?
2025-04-14 22:39:55,531 [INFO] Resposta da IA (imagem): Com base na sa√≠da do terminal que voc√™ forneceu, o push falhou devido a um erro de RPC (Remote Procedure Call) com um c√≥digo de status HTTP 408, que significa "Request Timeout". Adicionalmente, a mensagem "fatal: the remote end hung up unexpectedly" sugere que a conex√£o com o servidor remoto foi interrompida inesperadamente.

**Poss√≠veis causas e solu√ß√µes:**

1.  **Tamanho Grande do Push:**

    *   **Causa:** O tamanho dos objetos que voc√™ est√° tentando enviar (1.93 GiB) √© relativamente grande. Pode ser que o servidor Git remoto tenha um limite de tempo para o recebimento de dados, e o seu push est√° excedendo esse limite.
    *   **Solu√ß√£o:**
        *   **Aumente o buffer do Git:** Tente aumentar o tamanho do buffer HTTP do Git. Isso pode ajudar a lidar com grandes quantidades de dados.

            ```bash
            git config http.postBuffer 524288000 # 500MB
            git config http.maxRequestBuffer 100M
            ```

        *   **Divida o Push:** Se poss√≠vel, tente dividir o push em partes menores. Isso pode ser feito commitando menos altera√ß√µes de cada vez ou usando `git push --all`.
        *   **Garanta uma boa conex√£o de rede:** Se a sua conex√£o for lenta ou inst√°vel, isso pode causar timeouts.
        *   **Limpe seu reposit√≥rio:** Remova arquivos grandes desnecess√°rios do seu reposit√≥rio, se houver.

2.  **Tempo Limite de Conex√£o:**

    *   **Causa:** O servidor remoto pode estar fechando a conex√£o devido a um tempo limite configurado.
    *   **Solu√ß√£o:**
        *   **Aumente o Tempo Limite HTTP:** Aumente o tempo limite HTTP no Git para permitir que a opera√ß√£o seja conclu√≠da.

            ```bash
            git config --global http.timeout 300
            ```

            Isso define o tempo limite para 300 segundos (5 minutos). Ajuste conforme necess√°rio.
        *   **Verifique a estabilidade da sua conex√£o com a internet.**

3.  **Problemas no Servidor Remoto:**

    *   **Causa:** Pode haver um problema tempor√°rio no servidor Git remoto (GitHub, GitLab, Bitbucket, etc.).
    *   **Solu√ß√£o:**
        *   **Tente Novamente Mais Tarde:** Espere um pouco e tente o push novamente. O problema pode ser resolvido por si s√≥.
        *   **Verifique o Status do Servi√ßo:** Se voc√™ estiver usando um servi√ßo como GitHub ou GitLab, verifique a p√°gina de status do servi√ßo para ver se h√° algum problema conhecido.
        *   **Entre em Contato com o Suporte:** Se o problema persistir, entre em contato com o suporte do servi√ßo Git remoto.

4.  **Problemas de Configura√ß√£o de Proxy:**

    *   **Causa:** Se voc√™ estiver usando um proxy, pode haver um problema com a configura√ß√£o do proxy que est√° causando o erro.
    *   **Solu√ß√£o:**
        *   **Verifique as Configura√ß√µes de Proxy:** Verifique se as configura√ß√µes de proxy est√£o corretas.
        *   **Teste Sem o Proxy:** Tente fazer o push sem usar o proxy para ver se o problema √© com o proxy.

5.  **Problemas de Firewall ou Antiv√≠rus:**

    *   **Causa:** Um firewall ou antiv√≠rus pode estar bloqueando a conex√£o Git.
    *   **Solu√ß√£o:**
        *   **Verifique as Configura√ß√µes do Firewall/Antiv√≠rus:** Verifique as configura√ß√µes do seu firewall ou antiv√≠rus para garantir que o Git n√£o est√° sendo bloqueado.

**Passos para Diagnosticar e Resolver o Problema:**

1.  **Verifique sua Conex√£o com a Internet:** Certifique-se de que sua conex√£o com a internet est√° funcionando corretamente e √© est√°vel.
2.  **Tente um Push Menor:** Fa√ßa um pequeno commit e tente enviar esse commit para ver se o problema √© com o tamanho do push.
3.  **Aumente o Tempo Limite HTTP:** Execute o comando `git config --global http.timeout 300` para aumentar o tempo limite.
4.  **Aumente o Buffer HTTP:** Execute o comando `git config http.postBuffer 524288000` para aumentar o buffer HTTP.
5.  **Tente Novamente Mais Tarde:** Se as etapas acima n√£o resolverem o problema, espere um pouco e tente novamente.
6.  **Verifique o Status do Servi√ßo:** Se estiver usando um servi√ßo de hospedagem Git, verifique a p√°gina de status do servi√ßo.
7.  **Entre em Contato com o Suporte:** Se o problema persistir, entre em contato com o suporte do servi√ßo Git remoto.

Espero que isso ajude a resolver o problema!

2025-04-14 22:39:55,647 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 22:39:55,649 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 22:39:55,650 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 22:39:55,651 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 22:39:55,652 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 22:39:55,654 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 22:39:55,655 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 22:39:55,656 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 22:39:55,657 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 22:39:55,663 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 22:39:55,664 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 22:39:55,666 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 22:39:55,667 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 22:39:55,668 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 22:39:55,670 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 22:39:55,671 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 22:39:55,672 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 22:39:55,673 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 22:39:55,675 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 22:39:55,676 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 22:39:55,677 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 22:39:55,678 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 22:39:55,680 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 22:39:55,681 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 22:39:55,682 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 22:39:55,684 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 22:39:55,686 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 22:39:55,689 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 22:39:55,690 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 22:39:55,691 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 22:39:55,693 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 22:39:55,694 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 22:39:55,695 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 22:39:55,696 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 22:39:55,698 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 22:39:55,700 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
