2025-04-14 13:54:00,532 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 13:54:00,535 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 13:54:00,536 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 13:54:00,537 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 13:54:00,540 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 13:54:00,543 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 13:54:00,545 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 13:54:00,548 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 13:54:00,550 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 13:54:00,552 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 13:54:00,555 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 13:54:00,557 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 13:54:00,561 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 13:54:00,563 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 13:54:00,565 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 13:54:00,567 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 13:54:00,569 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 13:54:00,570 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 13:54:00,572 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 13:54:00,574 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 13:54:00,576 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 13:54:00,578 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 13:54:00,580 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 13:54:00,582 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 13:54:00,584 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-14 13:54:00,586 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 13:54:00,587 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 13:54:00,589 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 13:54:00,592 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 13:54:00,594 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 13:54:00,596 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 13:54:00,598 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 13:54:00,600 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 13:54:00,602 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 13:54:00,604 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 13:54:00,606 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 13:54:00,608 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 13:54:00,610 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 13:54:00,612 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 13:54:00,614 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 13:54:00,616 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 13:54:00,618 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 13:54:00,619 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 13:54:00,622 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 13:54:00,624 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 13:54:00,627 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 13:54:00,629 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 13:54:00,631 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 13:54:00,633 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 13:54:00,634 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 13:54:00,637 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 13:54:00,639 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 13:54:00,640 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 13:54:00,644 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 13:54:01,563 [INFO] Modelo Gemini 'gemini-2.0-flash-exp' inicializado com sucesso.
2025-04-14 13:59:36,598 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 13:59:36,600 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 13:59:36,602 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 13:59:36,603 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 13:59:36,605 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 13:59:36,607 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 13:59:36,610 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 13:59:36,614 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 13:59:36,618 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 13:59:36,620 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 13:59:36,622 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 13:59:36,624 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 13:59:36,628 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 13:59:36,630 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 13:59:36,632 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 13:59:36,634 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 13:59:36,636 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 13:59:36,638 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 13:59:36,641 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 13:59:36,644 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 13:59:36,647 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 13:59:36,648 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 13:59:36,650 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 13:59:36,652 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 13:59:36,654 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-14 13:59:36,656 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 13:59:36,658 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 13:59:36,660 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 13:59:36,662 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 13:59:36,664 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 13:59:36,665 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 13:59:36,668 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 13:59:36,669 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 13:59:36,671 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 13:59:36,673 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 13:59:36,675 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 13:59:36,677 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 13:59:36,679 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 13:59:36,681 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 13:59:36,683 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 13:59:36,685 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 13:59:36,687 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 13:59:36,689 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 13:59:36,691 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 13:59:36,693 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 13:59:36,695 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 13:59:36,697 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 13:59:36,698 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 13:59:36,701 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 13:59:36,702 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 13:59:36,704 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 13:59:36,705 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 13:59:36,707 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 13:59:36,708 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 13:59:36,821 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 13:59:36,822 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 13:59:36,824 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 13:59:36,826 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 13:59:36,828 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 13:59:36,831 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 13:59:36,832 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 13:59:36,835 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 13:59:36,838 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 13:59:36,839 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 13:59:36,841 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 13:59:36,845 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 13:59:36,847 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 13:59:36,849 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 13:59:36,851 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 13:59:36,853 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 13:59:36,855 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 13:59:36,856 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 13:59:36,859 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 13:59:36,862 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 13:59:36,864 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 13:59:36,866 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 13:59:36,867 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 13:59:36,878 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 13:59:36,881 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-14 13:59:36,883 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 13:59:36,885 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 13:59:36,887 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 13:59:36,888 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 13:59:36,890 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 13:59:36,891 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 13:59:36,893 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 13:59:36,896 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 13:59:36,898 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 13:59:36,899 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 13:59:36,901 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 13:59:36,902 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 13:59:36,904 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 13:59:36,906 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 13:59:36,908 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 13:59:36,910 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 13:59:36,913 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 13:59:36,914 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 13:59:36,916 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 13:59:36,918 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 13:59:36,921 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 13:59:36,923 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 13:59:36,926 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 13:59:36,928 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 13:59:36,931 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 13:59:36,933 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 13:59:36,936 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 13:59:36,938 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 13:59:36,940 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 13:59:36,945 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diretório atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Definição dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Criação dos diretórios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configuração do layout do Streamlit
st.set_page_config(
    page_title="Análise de Ações",
    layout="wide",
    page_icon="📈"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscilações

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Período', 'Oscilação'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("📊 Dashboard de Análise de Ações")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma ação", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"📌 Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("🔍 Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descrição', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa ação.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("💰 Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("📉 Oscilações")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("📈 Tabela Geral de Ações")
        st.dataframe(self.acoes)

# Execução
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para análise fundamentalista de ações da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from config.common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"Não foram encontrados arquivos .txt no diretório {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conteúdo do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conteúdo HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conteúdo HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se está correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configuração de vídeos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configuração de áudios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcrições e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diretório
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diretório.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do último arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conteúdo do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimensões do embedding:", embedding.shape)

# Caminho do diretório de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o último embedding
load_and_print_embedding(embedding_directory)


# test_whisper.py

from transformers import pipeline
from common_paths import CommonPaths

commmom_paths = CommonPaths()

# Função para ler a transcrição de um arquivo
def read_transcription(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()
    return lines

# Função para converter a pontuação em estrelas
def score_to_stars(score):
    if score >= 0.75:
        return 5
    elif score >= 0.55:
        return 4
    elif score >= 0.35:
        return 3
    elif score >= 0.15:
        return 2
    else:
        return 1

# Inicializar o pipeline de análise de sentimento
sentiment_pipeline = pipeline("sentiment-analysis", model="neuralmind/bert-base-portuguese-cased")

# Caminho para o arquivo de transcrição
file_path = commmom_paths.TRANSCRIPTION_OUTPUT_PATH / 'feedback.txt'

# Ler a transcrição
lines = read_transcription(file_path)

# Analisar sentimento para cada feedback
for line in lines:
    sentiment = sentiment_pipeline(line.strip())
    score = sentiment[0]['score']
    stars = score_to_stars(score)
    
    # Ajuste manual para feedbacks com aspectos positivos e negativos
    if any(word in line.lower() for word in ["mas", "porém", "no entanto"]):
        stars = max(1, stars - 1)  # Reduz uma estrela se houver contradição
    
    # Ajuste adicional baseado em palavras-chave
    positive_keywords = ["excelente", "ótimo", "perfeito", "rápido", "eficiente", "satisfeito", "clara", "eficaz", "bom trabalho"]
    negative_keywords = ["ruim", "péssimo", "horrível", "poderia", "não resolveu", "insatisfatória", "confusa", "inadequado", "lento", "não houve seguimento", "não atendeu", "não funcionou", "demorado"]
    
    positive_count = sum(word in line.lower() for word in positive_keywords)
    negative_count = sum(word in line.lower() for word in negative_keywords)
    
    # Ajuste de estrelas baseado na contagem de palavras-chave
    if positive_count > negative_count:
        stars = min(5, stars + 1)
    elif negative_count > positive_count:
        stars = max(1, stars - 1)
    
    print(f"Texto: {line.strip()}")
    print(f"Sentimento: {stars} estrelas")
    print(f"Pontuação: {score:.2f}")
    print('---')


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configuração de transcrições e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diretório onde está procurando as transcrições
        print(f"Diretório de entrada das transcrições: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcrição no diretório de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcrição encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo não encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcrição do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} está vazio.")
                    return

            # Gerar o embedding da transcrição
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcrição e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de áudio
    with sr.Microphone() as source:
        print("Diga algo em português...")

        while True:
            try:
                # Escuta o áudio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {texto_portugues}")

                # Traduz o texto para o inglês
                traducao = translator.translate(texto_portugues)
                print(f"Tradução para o inglês: {traducao}")

            except sr.UnknownValueError:
                print("Não foi possível entender o áudio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usuário")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configurações de áudio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de áudio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em português...")

try:
    audio_buffer = []

    while True:
        # Lê dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o áudio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o áudio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradução
            print(f"Tradução para o inglês: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usuário")

    # Fecha o stream de áudio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O vídeo {video_path} não contém áudio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de vídeo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de vídeo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de vídeo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Conversão de vídeo para áudio concluída!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configurações da API
handler = genai('gemini-1.5-flash')

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Função para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conteúdo com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares únicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcrição obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' não é uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuniões: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de reuniões: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|tópicos da|pontos (relevantes|principais)|análise)\b.*\b(reunião|última (reunião|conversa|sessão))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como análise de reunião.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padrão.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que são muito similares ao comando atual para evitar redundância.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padrão: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # Métodos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de análise de reunião.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conteúdo usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inválida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar variáveis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conteúdo com base no prompt, contexto e reunião"""
        try:
            # Supondo que a API espera um dicionário com os parâmetros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisição para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar variáveis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Criação da conclusão do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que você acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["faça um resumo da última reunião.", "tópicos da última reunião", "resuma a última reunião", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Você é responsável por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - Não acrescentar título nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informações acima e forneça uma solução detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Você é responsável analisar com detalhes a reunião de [str_meeting], e fornecer uma longa estória sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precisão da resposta.
            - Não acrescente título nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e faça uma pesquisa online para uma resposta mais precisa das questões de [str_text]
        - Não acrescente título nas respostas.
        
        ------
        [context]: Regras Básicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("Áudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum áudio detectado.")
                return None
            except sr.UnknownValueError:
                print("Não foi possível entender o áudio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configurações da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicionário
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta não está no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API: {e}")
        return None

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: Gere um README.md para minha aplicação atual.
2025-04-14 13:59:48,158 [INFO] Resposta da IA (texto): Com base nos arquivos fornecidos, aqui está um esboço de um arquivo `README.md` para sua aplicação:

```markdown
# Visão Geral da Aplicação

Esta aplicação é uma plataforma versátil de IA projetada para realizar diversas tarefas, incluindo:

*   **Processamento de Áudio e Vídeo:** Conversão de vídeos para áudio e transcrição de áudio para texto.
*   **Análise de Imagens:** Análise de imagens usando modelos de IA e geração de resumos.
*   **Assistente de Voz Inteligente:** Interface de assistente de voz para interação com a aplicação, capaz de responder a perguntas, realizar pesquisas online e analisar sentimentos.
*   **Geração de Embeddings:** Criação de embeddings de texto para tarefas de similaridade semântica e armazenamento de informações.
*   **Análise de Ações da B3:** Ferramentas para análise fundamentalista de ações da bolsa brasileira.

## Estrutura do Projeto

A aplicação é organizada nos seguintes diretórios principais:

*   `app_config`: Contém configurações de nível de aplicativo e utilitários para gerenciamento de diretórios.
*   `audio_to_text`: Módulos para conversão de áudio para texto usando o Whisper.
*   `chat_app`: Código para a interface de chat Streamlit, incluindo processamento de imagem e integração com os modelos Gemini.
*   `common_paths`: Define caminhos comuns para diretórios de entrada e saída de arquivos.
*   `fundamentus_api`: Implementa funcionalidades para análise de ações da B3.
*   `ia_generator.py`: Script para gerar conteúdo HTML a partir de transcrições usando uma API externa.
*   `main.py`: Ponto de entrada principal para executar várias tarefas de processamento.
*   `send_embeddings_database`: Configurações e scripts para enviar embeddings para um banco de dados.
*   `services`: Contém vários serviços como processamento de imagens, serviços de documentos (Word, Markdown) e comunicação com modelos de IA (Gemini).
*   `text_to_embedding`: Módulos para gerar embeddings de texto usando o SentenceTransformer.
*   `transcriptions`: Configurações relacionadas às transcrições de áudio e envio para bancos de dados.
*   `translate`: Scripts para tradução de voz e texto, incluindo tradução em tempo real.
*   `video_to_audio`: Módulos para extrair áudio de arquivos de vídeo.
*   `voice_assistent`: Implementação do assistente de voz inteligente, incluindo interpretação de comandos, interação com modelos de IA e gerenciamento de contexto.

## Pré-requisitos

Antes de executar a aplicação, certifique-se de ter instalado o seguinte:

*   **Python:** Python 3.6 ou superior
*   **Bibliotecas:** As dependências podem ser instaladas usando `pip`:
    ```bash
    pip install -r requirements.txt
    ```
    _Nota: Um arquivo `requirements.txt` completo e atualizado deve ser criado para garantir a instalação correta de todas as dependências._

*   **Variáveis de Ambiente:**
    *   `API_KEY_GEMINI`: Necessária para acessar os modelos Gemini.
    *   `GROQ_API_KEY`: Necessária para acessar a API Groq (se usada).

## Configuração

1.  **Clone o Repositório:**
    ```bash
    git clone <repository_url>
    cd <application_directory>
    ```

2.  **Instale as Dependências:**
    ```bash
    pip install -r requirements.txt
    ```

3.  **Configure as Variáveis de Ambiente:**
    *   Crie um arquivo `.env` no diretório raiz.
    *   Adicione as chaves de API necessárias:
        ```
        API_KEY_GEMINI=your_gemini_api_key
        GROQ_API_KEY=your_groq_api_key
        ```

## Execução

A aplicação pode ser executada através do script `main.py` para processamento de áudio/vídeo e geração de embeddings:

```bash
python main.py
```

Para executar a interface de chat Streamlit:

```bash
streamlit run chat_app/chat_streamlit.py
```

Para iniciar o assistente de voz:

```bash
python voice_assistent/main.py
```

## Uso

### Chat Streamlit

1.  Execute o aplicativo Streamlit.
2.  Interaja com o chatbot via interface web, inserindo texto ou enviando imagens.
3.  Use a barra lateral para configurar parâmetros ou limpar o histórico da conversa.

### Assistente de Voz

1.  Execute o script do assistente de voz.
2.  Siga as instruções para usar comandos de voz ou de texto para interagir com o assistente.
3.  O assistente pode responder a perguntas, realizar pesquisas online, analisar o sentimento dos feedbacks e muito mais.

## Arquivos de Configuração

*   `chat_app/config/config.py`: Contém os caminhos de arquivos e configurações gerais para a aplicação de chat.
*   `voice_assistent/config.py`: Configurações para o assistente de voz, incluindo chaves de API e parâmetros.
*   `app_config/app_config.py`: Configurações básicas para a aplicação como os path
*   `text_to_embedding/texto_to_embedding.py`: Configuração do modelo de embedding
## Estrutura de Diretórios

```
.
├── app_config/
├── audio_to_text/
├── chat_app/
├── common_paths/
├── fundamentus_api/
├── ia_generator.py
├── main.py
├── send_embeddings_database/
├── services/
├── text_to_embedding/
├── transcriptions/
├── translate/
├── video_to_audio/
├── voice_assistent/
├── test_whisper.py
└── README.md
```

## Contribuição

Contribuições são bem-vindas! Sinta-se à vontade para enviar pull requests para melhorar a aplicação.

## Licença

Este projeto é licenciado sob a [MIT License](LICENSE).
```

**Notas:**

*   **`requirements.txt`**: Certifique-se de gerar um arquivo `requirements.txt` preciso usando o comando `pip freeze > requirements.txt` no seu ambiente para que outras pessoas possam instalar as dependências corretamente.
*   **Variáveis de Ambiente**: Enfatize a importância de proteger as chaves de API e nunca commitá-las diretamente no código.
*   **Instruções de Execução**: Inclua instruções detalhadas sobre como executar cada parte da aplicação.
*   **Estrutura de Diretórios**: Forneça uma visão geral clara da estrutura de diretórios para facilitar a navegação e a compreensão do projeto.
*   **Licença**: Adicione um arquivo `LICENSE` ao seu repositório e especifique a licença no README.

Este esqueleto deve fornecer um bom ponto de partida para o seu `README.md`. Lembre-se de adaptá-lo às necessidades e detalhes específicos do seu projeto!
2025-04-14 13:59:48,271 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 13:59:48,272 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 13:59:48,274 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 13:59:48,276 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 13:59:48,278 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 13:59:48,280 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 13:59:48,282 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 13:59:48,284 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 13:59:48,286 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 13:59:48,288 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 13:59:48,290 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 13:59:48,291 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 13:59:48,293 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 13:59:48,295 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 13:59:48,296 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 13:59:48,298 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 13:59:48,300 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 13:59:48,301 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 13:59:48,303 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 13:59:48,304 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 13:59:48,305 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 13:59:48,307 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 13:59:48,308 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 13:59:48,309 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 13:59:48,312 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-14 13:59:48,314 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 13:59:48,315 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 13:59:48,317 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 13:59:48,318 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 13:59:48,320 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 13:59:48,321 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 13:59:48,322 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 13:59:48,324 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 13:59:48,325 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 13:59:48,327 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 13:59:48,329 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 13:59:48,331 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 13:59:48,332 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 13:59:48,333 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 13:59:48,335 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 13:59:48,337 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 13:59:48,338 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 13:59:48,340 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 13:59:48,342 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 13:59:48,343 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 13:59:48,345 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 13:59:48,347 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 13:59:48,348 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 13:59:48,349 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 13:59:48,351 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 13:59:48,352 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 13:59:48,354 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 13:59:48,355 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 13:59:48,356 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:03:43,913 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:03:43,915 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:03:43,917 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:03:43,919 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:03:43,921 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:03:43,923 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:03:43,926 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:03:43,929 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:03:43,932 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:03:43,935 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:03:43,938 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:03:43,940 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:03:43,942 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:03:43,943 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:03:43,945 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:03:43,946 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:03:43,948 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:03:43,949 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:03:43,951 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:03:43,954 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:03:43,956 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:03:43,958 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:03:43,959 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:03:43,961 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:03:43,963 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-14 14:03:43,965 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:03:43,967 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:03:43,970 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:03:43,973 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:03:43,975 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:03:43,977 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:03:43,979 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:03:43,981 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:03:43,983 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:03:43,987 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:03:43,989 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:03:43,991 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:03:43,993 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:03:43,995 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:03:43,998 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:03:44,000 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:03:44,003 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:03:44,005 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:03:44,007 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:03:44,010 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:03:44,013 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:03:44,015 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:03:44,019 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:03:44,022 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:03:44,025 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:03:44,027 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:03:44,030 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:03:44,032 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:03:44,035 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:03:44,152 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:03:44,154 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:03:44,156 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:03:44,158 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:03:44,161 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:03:44,162 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:03:44,165 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:03:44,168 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:03:44,170 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:03:44,172 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:03:44,174 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:03:44,176 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:03:44,177 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:03:44,179 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:03:44,181 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:03:44,183 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:03:44,185 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:03:44,187 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:03:44,189 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:03:44,190 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:03:44,192 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:03:44,193 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:03:44,195 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:03:44,197 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:03:44,199 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-14 14:03:44,200 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:03:44,203 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:03:44,205 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:03:44,206 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:03:44,208 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:03:44,210 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:03:44,211 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:03:44,213 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:03:44,215 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:03:44,217 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:03:44,230 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:03:44,232 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:03:44,237 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:03:44,239 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:03:44,240 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:03:44,242 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:03:44,243 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:03:44,244 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:03:44,246 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:03:44,248 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:03:44,250 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:03:44,252 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:03:44,255 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:03:44,256 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:03:44,258 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:03:44,260 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:03:44,262 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:03:44,264 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:03:44,265 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:03:44,268 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diretório atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Definição dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Criação dos diretórios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configuração do layout do Streamlit
st.set_page_config(
    page_title="Análise de Ações",
    layout="wide",
    page_icon="📈"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscilações

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Período', 'Oscilação'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("📊 Dashboard de Análise de Ações")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma ação", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"📌 Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("🔍 Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descrição', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa ação.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("💰 Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("📉 Oscilações")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("📈 Tabela Geral de Ações")
        st.dataframe(self.acoes)

# Execução
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para análise fundamentalista de ações da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from config.common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"Não foram encontrados arquivos .txt no diretório {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conteúdo do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conteúdo HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conteúdo HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se está correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configuração de vídeos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configuração de áudios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcrições e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diretório
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diretório.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do último arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conteúdo do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimensões do embedding:", embedding.shape)

# Caminho do diretório de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o último embedding
load_and_print_embedding(embedding_directory)


# test_whisper.py

from transformers import pipeline
from common_paths import CommonPaths

commmom_paths = CommonPaths()

# Função para ler a transcrição de um arquivo
def read_transcription(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()
    return lines

# Função para converter a pontuação em estrelas
def score_to_stars(score):
    if score >= 0.75:
        return 5
    elif score >= 0.55:
        return 4
    elif score >= 0.35:
        return 3
    elif score >= 0.15:
        return 2
    else:
        return 1

# Inicializar o pipeline de análise de sentimento
sentiment_pipeline = pipeline("sentiment-analysis", model="neuralmind/bert-base-portuguese-cased")

# Caminho para o arquivo de transcrição
file_path = commmom_paths.TRANSCRIPTION_OUTPUT_PATH / 'feedback.txt'

# Ler a transcrição
lines = read_transcription(file_path)

# Analisar sentimento para cada feedback
for line in lines:
    sentiment = sentiment_pipeline(line.strip())
    score = sentiment[0]['score']
    stars = score_to_stars(score)
    
    # Ajuste manual para feedbacks com aspectos positivos e negativos
    if any(word in line.lower() for word in ["mas", "porém", "no entanto"]):
        stars = max(1, stars - 1)  # Reduz uma estrela se houver contradição
    
    # Ajuste adicional baseado em palavras-chave
    positive_keywords = ["excelente", "ótimo", "perfeito", "rápido", "eficiente", "satisfeito", "clara", "eficaz", "bom trabalho"]
    negative_keywords = ["ruim", "péssimo", "horrível", "poderia", "não resolveu", "insatisfatória", "confusa", "inadequado", "lento", "não houve seguimento", "não atendeu", "não funcionou", "demorado"]
    
    positive_count = sum(word in line.lower() for word in positive_keywords)
    negative_count = sum(word in line.lower() for word in negative_keywords)
    
    # Ajuste de estrelas baseado na contagem de palavras-chave
    if positive_count > negative_count:
        stars = min(5, stars + 1)
    elif negative_count > positive_count:
        stars = max(1, stars - 1)
    
    print(f"Texto: {line.strip()}")
    print(f"Sentimento: {stars} estrelas")
    print(f"Pontuação: {score:.2f}")
    print('---')


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configuração de transcrições e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diretório onde está procurando as transcrições
        print(f"Diretório de entrada das transcrições: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcrição no diretório de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcrição encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo não encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcrição do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} está vazio.")
                    return

            # Gerar o embedding da transcrição
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcrição e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de áudio
    with sr.Microphone() as source:
        print("Diga algo em português...")

        while True:
            try:
                # Escuta o áudio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {texto_portugues}")

                # Traduz o texto para o inglês
                traducao = translator.translate(texto_portugues)
                print(f"Tradução para o inglês: {traducao}")

            except sr.UnknownValueError:
                print("Não foi possível entender o áudio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usuário")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configurações de áudio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de áudio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em português...")

try:
    audio_buffer = []

    while True:
        # Lê dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o áudio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o áudio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradução
            print(f"Tradução para o inglês: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usuário")

    # Fecha o stream de áudio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O vídeo {video_path} não contém áudio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de vídeo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de vídeo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de vídeo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Conversão de vídeo para áudio concluída!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configurações da API
handler = genai('gemini-1.5-flash')

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Função para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conteúdo com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares únicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcrição obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' não é uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuniões: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de reuniões: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|tópicos da|pontos (relevantes|principais)|análise)\b.*\b(reunião|última (reunião|conversa|sessão))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como análise de reunião.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padrão.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que são muito similares ao comando atual para evitar redundância.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padrão: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # Métodos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de análise de reunião.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conteúdo usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inválida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar variáveis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conteúdo com base no prompt, contexto e reunião"""
        try:
            # Supondo que a API espera um dicionário com os parâmetros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisição para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar variáveis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Criação da conclusão do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que você acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["faça um resumo da última reunião.", "tópicos da última reunião", "resuma a última reunião", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Você é responsável por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - Não acrescentar título nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informações acima e forneça uma solução detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Você é responsável analisar com detalhes a reunião de [str_meeting], e fornecer uma longa estória sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precisão da resposta.
            - Não acrescente título nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e faça uma pesquisa online para uma resposta mais precisa das questões de [str_text]
        - Não acrescente título nas respostas.
        
        ------
        [context]: Regras Básicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("Áudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum áudio detectado.")
                return None
            except sr.UnknownValueError:
                print("Não foi possível entender o áudio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configurações da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicionário
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta não está no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API: {e}")
        return None

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: Gere um README.md para minha aplicação atual.
Assistant: Com base nos arquivos fornecidos, aqui está um esboço de um arquivo `README.md` para sua aplicação:

```markdown
# Visão Geral da Aplicação

Esta aplicação é uma plataforma versátil de IA projetada para realizar diversas tarefas, incluindo:

*   **Processamento de Áudio e Vídeo:** Conversão de vídeos para áudio e transcrição de áudio para texto.
*   **Análise de Imagens:** Análise de imagens usando modelos de IA e geração de resumos.
*   **Assistente de Voz Inteligente:** Interface de assistente de voz para interação com a aplicação, capaz de responder a perguntas, realizar pesquisas online e analisar sentimentos.
*   **Geração de Embeddings:** Criação de embeddings de texto para tarefas de similaridade semântica e armazenamento de informações.
*   **Análise de Ações da B3:** Ferramentas para análise fundamentalista de ações da bolsa brasileira.

## Estrutura do Projeto

A aplicação é organizada nos seguintes diretórios principais:

*   `app_config`: Contém configurações de nível de aplicativo e utilitários para gerenciamento de diretórios.
*   `audio_to_text`: Módulos para conversão de áudio para texto usando o Whisper.
*   `chat_app`: Código para a interface de chat Streamlit, incluindo processamento de imagem e integração com os modelos Gemini.
*   `common_paths`: Define caminhos comuns para diretórios de entrada e saída de arquivos.
*   `fundamentus_api`: Implementa funcionalidades para análise de ações da B3.
*   `ia_generator.py`: Script para gerar conteúdo HTML a partir de transcrições usando uma API externa.
*   `main.py`: Ponto de entrada principal para executar várias tarefas de processamento.
*   `send_embeddings_database`: Configurações e scripts para enviar embeddings para um banco de dados.
*   `services`: Contém vários serviços como processamento de imagens, serviços de documentos (Word, Markdown) e comunicação com modelos de IA (Gemini).
*   `text_to_embedding`: Módulos para gerar embeddings de texto usando o SentenceTransformer.
*   `transcriptions`: Configurações relacionadas às transcrições de áudio e envio para bancos de dados.
*   `translate`: Scripts para tradução de voz e texto, incluindo tradução em tempo real.
*   `video_to_audio`: Módulos para extrair áudio de arquivos de vídeo.
*   `voice_assistent`: Implementação do assistente de voz inteligente, incluindo interpretação de comandos, interação com modelos de IA e gerenciamento de contexto.

## Pré-requisitos

Antes de executar a aplicação, certifique-se de ter instalado o seguinte:

*   **Python:** Python 3.6 ou superior
*   **Bibliotecas:** As dependências podem ser instaladas usando `pip`:
    ```bash
    pip install -r requirements.txt
    ```
    _Nota: Um arquivo `requirements.txt` completo e atualizado deve ser criado para garantir a instalação correta de todas as dependências._

*   **Variáveis de Ambiente:**
    *   `API_KEY_GEMINI`: Necessária para acessar os modelos Gemini.
    *   `GROQ_API_KEY`: Necessária para acessar a API Groq (se usada).

## Configuração

1.  **Clone o Repositório:**
    ```bash
    git clone <repository_url>
    cd <application_directory>
    ```

2.  **Instale as Dependências:**
    ```bash
    pip install -r requirements.txt
    ```

3.  **Configure as Variáveis de Ambiente:**
    *   Crie um arquivo `.env` no diretório raiz.
    *   Adicione as chaves de API necessárias:
        ```
        API_KEY_GEMINI=your_gemini_api_key
        GROQ_API_KEY=your_groq_api_key
        ```

## Execução

A aplicação pode ser executada através do script `main.py` para processamento de áudio/vídeo e geração de embeddings:

```bash
python main.py
```

Para executar a interface de chat Streamlit:

```bash
streamlit run chat_app/chat_streamlit.py
```

Para iniciar o assistente de voz:

```bash
python voice_assistent/main.py
```

## Uso

### Chat Streamlit

1.  Execute o aplicativo Streamlit.
2.  Interaja com o chatbot via interface web, inserindo texto ou enviando imagens.
3.  Use a barra lateral para configurar parâmetros ou limpar o histórico da conversa.

### Assistente de Voz

1.  Execute o script do assistente de voz.
2.  Siga as instruções para usar comandos de voz ou de texto para interagir com o assistente.
3.  O assistente pode responder a perguntas, realizar pesquisas online, analisar o sentimento dos feedbacks e muito mais.

## Arquivos de Configuração

*   `chat_app/config/config.py`: Contém os caminhos de arquivos e configurações gerais para a aplicação de chat.
*   `voice_assistent/config.py`: Configurações para o assistente de voz, incluindo chaves de API e parâmetros.
*   `app_config/app_config.py`: Configurações básicas para a aplicação como os path
*   `text_to_embedding/texto_to_embedding.py`: Configuração do modelo de embedding
## Estrutura de Diretórios

```
.
├── app_config/
├── audio_to_text/
├── chat_app/
├── common_paths/
├── fundamentus_api/
├── ia_generator.py
├── main.py
├── send_embeddings_database/
├── services/
├── text_to_embedding/
├── transcriptions/
├── translate/
├── video_to_audio/
├── voice_assistent/
├── test_whisper.py
└── README.md
```

## Contribuição

Contribuições são bem-vindas! Sinta-se à vontade para enviar pull requests para melhorar a aplicação.

## Licença

Este projeto é licenciado sob a [MIT License](LICENSE).
```

**Notas:**

*   **`requirements.txt`**: Certifique-se de gerar um arquivo `requirements.txt` preciso usando o comando `pip freeze > requirements.txt` no seu ambiente para que outras pessoas possam instalar as dependências corretamente.
*   **Variáveis de Ambiente**: Enfatize a importância de proteger as chaves de API e nunca commitá-las diretamente no código.
*   **Instruções de Execução**: Inclua instruções detalhadas sobre como executar cada parte da aplicação.
*   **Estrutura de Diretórios**: Forneça uma visão geral clara da estrutura de diretórios para facilitar a navegação e a compreensão do projeto.
*   **Licença**: Adicione um arquivo `LICENSE` ao seu repositório e especifique a licença no README.

Este esqueleto deve fornecer um bom ponto de partida para o seu `README.md`. Lembre-se de adaptá-lo às necessidades e detalhes específicos do seu projeto!
User: Aplique SOLID e POO na aplicação
2025-04-14 14:03:55,459 [INFO] Resposta da IA (texto): A aplicação parece ter vários componentes, cada um com suas próprias responsabilidades. Aqui está uma análise de como os princípios SOLID e POO podem ser aplicados para melhorar a estrutura e a manutenção do código.

### Princípios SOLID

1.  **Single Responsibility Principle (SRP)**

    *   **Problema:** Algumas classes podem estar acumulando muitas responsabilidades. Por exemplo, a classe `ImageProcessor` em `services/image_processor.py` lida tanto com o processamento de imagens quanto com a interação com o modelo GPT, o serviço de documentos e o histórico.
    *   **Solução:** Dividir a classe `ImageProcessor` em classes menores, cada uma com uma única responsabilidade.

        *   Criar uma classe `ImageAnalyzer` que lide especificamente com a análise da imagem usando o modelo GPT.
        *   Criar uma classe `DocumentUpdater` que lide com a atualização do documento (Word, Markdown).
        *   Criar uma classe `HistoryManager` para gerenciar o histórico de análises.

2.  **Open/Closed Principle (OCP)**

    *   **Problema:** Se você precisar adicionar um novo tipo de análise de imagem (por exemplo, análise de sentimento), pode ser necessário modificar a classe `ImageProcessor`.
    *   **Solução:** Projetar interfaces ou classes abstratas para definir contratos para as diferentes análises de imagem.

        *   Criar uma interface `ImageAnalysisStrategy` com um método `analyze(image_path)`.
        *   Implementar classes concretas para cada tipo de análise (por exemplo, `ObjectDetectionAnalysis`, `SentimentAnalysis`).
        *   A classe `ImageAnalyzer` receberia uma `ImageAnalysisStrategy` via construtor e delegaria a análise para essa estratégia.

3.  **Liskov Substitution Principle (LSP)**

    *   **Problema:** Se você tem uma classe base `AppConfig` e classes derivadas como `AudioConfig` e `VideoConfig`, você deve garantir que as classes derivadas possam ser usadas no lugar da classe base sem quebrar o programa.
    *   **Solução:** Garantir que todas as classes derivadas de `AppConfig` implementem os métodos da classe base de forma consistente.

        *   Verificar se o método `create_directories` em `AudioConfig` e `VideoConfig` funciona como esperado quando chamado com instâncias de `AppConfig`.

4.  **Interface Segregation Principle (ISP)**

    *   **Problema:** Se uma interface tem muitos métodos que uma classe não precisa implementar, essa classe é forçada a implementar métodos que não usa.
    *   **Solução:** Dividir interfaces grandes em interfaces menores e mais específicas.

        *   Se você tem uma interface `TextToSpeechInterface` com métodos como `speak()` e `pause()`, mas uma classe só precisa de `speak()`, criar uma interface `Speakable` com apenas o método `speak()`.

5.  **Dependency Inversion Principle (DIP)**

    *   **Problema:** Classes de alto nível (por exemplo, `MainApp`) dependem de classes de baixo nível (por exemplo, `VoiceCommandHandler`, `TextCommandHandler`).
    *   **Solução:** Classes de alto nível não devem depender de classes de baixo nível. Ambas devem depender de abstrações.

        *   Criar interfaces para os command handlers (por exemplo, `CommandHandlerInterface`) e fazer com que `MainApp` dependa dessa interface em vez de depender diretamente das classes `VoiceCommandHandler` e `TextCommandHandler`.

### POO (Programação Orientada a Objetos)

*   **Encapsulamento:**
    *   **Problema:** Alguns dados podem não estar devidamente encapsulados, resultando em acesso direto a atributos.
    *   **Solução:** Use propriedades (`@property`) para controlar o acesso aos atributos.
*   **Herança:**
    *   **Problema:** Pode haver uso excessivo ou inadequado de herança, o que pode levar a classes infladas e difíceis de manter.
    *   **Solução:** Prefira composição em vez de herança sempre que possível.
*   **Polimorfismo:**
    *   **Problema:** Falta de polimorfismo pode resultar em código repetitivo e difícil de estender.
    *   **Solução:** Use interfaces e classes abstratas para definir contratos e implementar polimorfismo.

### Aplicando as Mudanças

1.  **Refatoração do `ImageProcessor`:**

```python
# services/image_analyzer.py
class ImageAnalyzer:
    def __init__(self, analysis_strategy):
        self.analysis_strategy = analysis_strategy

    def analyze(self, image_path):
        return self.analysis_strategy.analyze(image_path)

# services/document_updater.py
class DocumentUpdater:
    def __init__(self, document_service, markdown_service):
        self.document_service = document_service
        self.markdown_service = markdown_service

    def update_documents(self, image_name, summary):
        self.document_service.add_image_summary(image_name, summary)
        self.markdown_service.add_image_summary(image_name, summary)
        self.document_service.save_document()
        self.markdown_service.save_markdown()

# services/history_manager.py
class HistoryManager:
    def __init__(self):
        self.history = []

    def update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})

    def get_history(self):
        return self.history

# services/image_processing_strategy.py
from abc import ABC, abstractmethod

class ImageAnalysisStrategy(ABC):
    @abstractmethod
    def analyze(self, image_path):
        pass

# services/gpt_analysis_strategy.py
class GPTAnalysisStrategy(ImageAnalysisStrategy):
    def __init__(self, gpt_handler, prompt):
        self.gpt_handler = gpt_handler
        self.prompt = prompt

    def analyze(self, image_path):
        # Your GPT analysis logic here
        return self.gpt_handler.generate_content_from_image(image_path, self.prompt)

# services/image_processor.py
from services.image_analyzer import ImageAnalyzer
from services.document_updater import DocumentUpdater
from services.history_manager import HistoryManager
from services.gpt_analysis_strategy import GPTAnalysisStrategy

class ImageProcessor:
    def __init__(self, gpt_handler, document_service, markdown_service):
        self.gpt_handler = gpt_handler
        self.document_updater = DocumentUpdater(document_service, markdown_service)
        self.history_manager = HistoryManager()
        self.prompt = self._load_prompt()

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                # Use a strategy for analysis
                analysis_strategy = GPTAnalysisStrategy(self.gpt_handler, self.prompt)
                image_analyzer = ImageAnalyzer(analysis_strategy)

                # Now analyze
                summary = image_analyzer.analyze(os.path.join(Config.ASSETS_DIR, image_name))
                self.document_updater.update_documents(image_name, summary)
                self.history_manager.update_history(image_name, summary)

                time.sleep(4)
                logger.info("Preparando a próxima análise...")

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)
```

### Conclusão

Aplicar os princípios SOLID e POO pode resultar em um código mais modular, testável e fácil de manter. A refatoração envolveria a decomposição de classes grandes em unidades menores e mais coesas, o uso de interfaces para definir contratos claros e a aplicação de injeção de dependência para reduzir o acoplamento.
2025-04-14 14:03:55,563 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:03:55,565 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:03:55,567 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:03:55,570 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:03:55,572 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:03:55,573 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:03:55,575 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:03:55,577 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:03:55,579 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:03:55,580 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:03:55,582 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:03:55,584 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:03:55,587 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:03:55,589 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:03:55,590 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:03:55,592 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:03:55,594 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:03:55,595 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:03:55,596 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:03:55,597 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:03:55,599 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:03:55,600 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:03:55,602 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:03:55,604 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:03:55,606 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-14 14:03:55,608 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:03:55,609 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:03:55,610 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:03:55,612 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:03:55,613 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:03:55,614 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:03:55,615 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:03:55,617 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:03:55,618 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:03:55,620 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:03:55,622 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:03:55,624 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:03:55,625 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:03:55,626 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:03:55,628 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:03:55,629 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:03:55,630 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:03:55,631 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:03:55,633 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:03:55,634 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:03:55,635 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:03:55,637 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:03:55,639 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:03:55,641 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:03:55,642 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:03:55,643 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:03:55,644 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:03:55,645 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:03:55,646 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:14:28,795 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:14:28,797 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:14:28,800 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:14:28,802 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:14:28,804 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:14:28,805 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:14:28,807 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:14:28,808 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:14:28,809 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:14:28,811 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:14:28,813 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:14:28,815 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:14:28,816 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:14:28,817 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:14:28,819 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:14:28,821 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:14:28,823 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:14:28,825 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:14:28,826 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:14:28,828 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:14:28,829 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:14:28,830 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:14:28,831 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:14:28,833 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:14:28,834 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:14:28,836 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:14:28,838 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:14:28,840 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:14:28,844 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:14:28,845 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:14:28,847 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:14:28,849 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:14:28,852 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:14:28,854 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:14:28,855 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:14:28,858 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:14:28,859 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:14:28,860 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:14:28,862 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:14:28,863 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:14:28,865 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:14:28,866 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:14:28,868 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:14:28,870 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:14:28,873 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:14:28,876 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:14:28,877 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:14:28,879 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:14:28,880 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:14:28,881 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:14:28,883 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:14:28,885 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:14:28,887 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:14:29,021 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:14:29,023 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:14:29,025 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:14:29,026 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:14:29,028 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:14:29,030 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:14:29,032 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:14:29,035 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:14:29,038 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:14:29,040 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:14:29,041 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:14:29,043 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:14:29,044 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:14:29,046 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:14:29,048 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:14:29,049 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:14:29,051 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:14:29,053 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:14:29,054 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:14:29,056 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:14:29,058 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:14:29,059 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:14:29,061 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:14:29,062 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:14:29,064 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:14:29,066 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:14:29,067 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:14:29,073 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:14:29,078 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:14:29,080 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:14:29,082 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:14:29,085 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:14:29,087 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:14:29,089 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:14:29,091 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:14:29,092 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:14:29,094 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:14:29,095 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:14:29,096 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:14:29,097 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:14:29,099 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:14:29,100 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:14:29,103 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:14:29,106 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:14:29,108 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:14:29,109 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:14:29,111 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:14:29,113 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:14:29,114 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:14:29,115 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:14:29,117 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:14:29,119 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:14:29,120 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:14:29,124 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diretório atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Definição dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Criação dos diretórios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configuração do layout do Streamlit
st.set_page_config(
    page_title="Análise de Ações",
    layout="wide",
    page_icon="📈"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscilações

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Período', 'Oscilação'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("📊 Dashboard de Análise de Ações")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma ação", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"📌 Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("🔍 Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descrição', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa ação.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("💰 Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("📉 Oscilações")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("📈 Tabela Geral de Ações")
        st.dataframe(self.acoes)

# Execução
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para análise fundamentalista de ações da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from config.common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"Não foram encontrados arquivos .txt no diretório {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conteúdo do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conteúdo HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conteúdo HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se está correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configuração de vídeos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configuração de áudios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcrições e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diretório
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diretório.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do último arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conteúdo do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimensões do embedding:", embedding.shape)

# Caminho do diretório de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o último embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configuração de transcrições e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diretório onde está procurando as transcrições
        print(f"Diretório de entrada das transcrições: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcrição no diretório de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcrição encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo não encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcrição do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} está vazio.")
                    return

            # Gerar o embedding da transcrição
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcrição e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de áudio
    with sr.Microphone() as source:
        print("Diga algo em português...")

        while True:
            try:
                # Escuta o áudio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {texto_portugues}")

                # Traduz o texto para o inglês
                traducao = translator.translate(texto_portugues)
                print(f"Tradução para o inglês: {traducao}")

            except sr.UnknownValueError:
                print("Não foi possível entender o áudio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usuário")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configurações de áudio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de áudio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em português...")

try:
    audio_buffer = []

    while True:
        # Lê dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o áudio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o áudio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradução
            print(f"Tradução para o inglês: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usuário")

    # Fecha o stream de áudio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O vídeo {video_path} não contém áudio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de vídeo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de vídeo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de vídeo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Conversão de vídeo para áudio concluída!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configurações da API
handler = genai('gemini-1.5-flash')

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Função para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conteúdo com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares únicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcrição obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' não é uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuniões: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de reuniões: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|tópicos da|pontos (relevantes|principais)|análise)\b.*\b(reunião|última (reunião|conversa|sessão))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como análise de reunião.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padrão.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que são muito similares ao comando atual para evitar redundância.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padrão: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # Métodos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de análise de reunião.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conteúdo usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inválida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar variáveis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conteúdo com base no prompt, contexto e reunião"""
        try:
            # Supondo que a API espera um dicionário com os parâmetros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisição para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar variáveis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Criação da conclusão do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que você acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["faça um resumo da última reunião.", "tópicos da última reunião", "resuma a última reunião", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Você é responsável por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - Não acrescentar título nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informações acima e forneça uma solução detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Você é responsável analisar com detalhes a reunião de [str_meeting], e fornecer uma longa estória sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precisão da resposta.
            - Não acrescente título nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e faça uma pesquisa online para uma resposta mais precisa das questões de [str_text]
        - Não acrescente título nas respostas.
        
        ------
        [context]: Regras Básicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("Áudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum áudio detectado.")
                return None
            except sr.UnknownValueError:
                print("Não foi possível entender o áudio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configurações da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicionário
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta não está no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API: {e}")
        return None

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: Gere um README.md para minha aplicação atual.
Assistant: Com base nos arquivos fornecidos, aqui está um esboço de um arquivo `README.md` para sua aplicação:

```markdown
# Visão Geral da Aplicação

Esta aplicação é uma plataforma versátil de IA projetada para realizar diversas tarefas, incluindo:

*   **Processamento de Áudio e Vídeo:** Conversão de vídeos para áudio e transcrição de áudio para texto.
*   **Análise de Imagens:** Análise de imagens usando modelos de IA e geração de resumos.
*   **Assistente de Voz Inteligente:** Interface de assistente de voz para interação com a aplicação, capaz de responder a perguntas, realizar pesquisas online e analisar sentimentos.
*   **Geração de Embeddings:** Criação de embeddings de texto para tarefas de similaridade semântica e armazenamento de informações.
*   **Análise de Ações da B3:** Ferramentas para análise fundamentalista de ações da bolsa brasileira.

## Estrutura do Projeto

A aplicação é organizada nos seguintes diretórios principais:

*   `app_config`: Contém configurações de nível de aplicativo e utilitários para gerenciamento de diretórios.
*   `audio_to_text`: Módulos para conversão de áudio para texto usando o Whisper.
*   `chat_app`: Código para a interface de chat Streamlit, incluindo processamento de imagem e integração com os modelos Gemini.
*   `common_paths`: Define caminhos comuns para diretórios de entrada e saída de arquivos.
*   `fundamentus_api`: Implementa funcionalidades para análise de ações da B3.
*   `ia_generator.py`: Script para gerar conteúdo HTML a partir de transcrições usando uma API externa.
*   `main.py`: Ponto de entrada principal para executar várias tarefas de processamento.
*   `send_embeddings_database`: Configurações e scripts para enviar embeddings para um banco de dados.
*   `services`: Contém vários serviços como processamento de imagens, serviços de documentos (Word, Markdown) e comunicação com modelos de IA (Gemini).
*   `text_to_embedding`: Módulos para gerar embeddings de texto usando o SentenceTransformer.
*   `transcriptions`: Configurações relacionadas às transcrições de áudio e envio para bancos de dados.
*   `translate`: Scripts para tradução de voz e texto, incluindo tradução em tempo real.
*   `video_to_audio`: Módulos para extrair áudio de arquivos de vídeo.
*   `voice_assistent`: Implementação do assistente de voz inteligente, incluindo interpretação de comandos, interação com modelos de IA e gerenciamento de contexto.

## Pré-requisitos

Antes de executar a aplicação, certifique-se de ter instalado o seguinte:

*   **Python:** Python 3.6 ou superior
*   **Bibliotecas:** As dependências podem ser instaladas usando `pip`:
    ```bash
    pip install -r requirements.txt
    ```
    _Nota: Um arquivo `requirements.txt` completo e atualizado deve ser criado para garantir a instalação correta de todas as dependências._

*   **Variáveis de Ambiente:**
    *   `API_KEY_GEMINI`: Necessária para acessar os modelos Gemini.
    *   `GROQ_API_KEY`: Necessária para acessar a API Groq (se usada).

## Configuração

1.  **Clone o Repositório:**
    ```bash
    git clone <repository_url>
    cd <application_directory>
    ```

2.  **Instale as Dependências:**
    ```bash
    pip install -r requirements.txt
    ```

3.  **Configure as Variáveis de Ambiente:**
    *   Crie um arquivo `.env` no diretório raiz.
    *   Adicione as chaves de API necessárias:
        ```
        API_KEY_GEMINI=your_gemini_api_key
        GROQ_API_KEY=your_groq_api_key
        ```

## Execução

A aplicação pode ser executada através do script `main.py` para processamento de áudio/vídeo e geração de embeddings:

```bash
python main.py
```

Para executar a interface de chat Streamlit:

```bash
streamlit run chat_app/chat_streamlit.py
```

Para iniciar o assistente de voz:

```bash
python voice_assistent/main.py
```

## Uso

### Chat Streamlit

1.  Execute o aplicativo Streamlit.
2.  Interaja com o chatbot via interface web, inserindo texto ou enviando imagens.
3.  Use a barra lateral para configurar parâmetros ou limpar o histórico da conversa.

### Assistente de Voz

1.  Execute o script do assistente de voz.
2.  Siga as instruções para usar comandos de voz ou de texto para interagir com o assistente.
3.  O assistente pode responder a perguntas, realizar pesquisas online, analisar o sentimento dos feedbacks e muito mais.

## Arquivos de Configuração

*   `chat_app/config/config.py`: Contém os caminhos de arquivos e configurações gerais para a aplicação de chat.
*   `voice_assistent/config.py`: Configurações para o assistente de voz, incluindo chaves de API e parâmetros.
*   `app_config/app_config.py`: Configurações básicas para a aplicação como os path
*   `text_to_embedding/texto_to_embedding.py`: Configuração do modelo de embedding
## Estrutura de Diretórios

```
.
├── app_config/
├── audio_to_text/
├── chat_app/
├── common_paths/
├── fundamentus_api/
├── ia_generator.py
├── main.py
├── send_embeddings_database/
├── services/
├── text_to_embedding/
├── transcriptions/
├── translate/
├── video_to_audio/
├── voice_assistent/
├── test_whisper.py
└── README.md
```

## Contribuição

Contribuições são bem-vindas! Sinta-se à vontade para enviar pull requests para melhorar a aplicação.

## Licença

Este projeto é licenciado sob a [MIT License](LICENSE).
```

**Notas:**

*   **`requirements.txt`**: Certifique-se de gerar um arquivo `requirements.txt` preciso usando o comando `pip freeze > requirements.txt` no seu ambiente para que outras pessoas possam instalar as dependências corretamente.
*   **Variáveis de Ambiente**: Enfatize a importância de proteger as chaves de API e nunca commitá-las diretamente no código.
*   **Instruções de Execução**: Inclua instruções detalhadas sobre como executar cada parte da aplicação.
*   **Estrutura de Diretórios**: Forneça uma visão geral clara da estrutura de diretórios para facilitar a navegação e a compreensão do projeto.
*   **Licença**: Adicione um arquivo `LICENSE` ao seu repositório e especifique a licença no README.

Este esqueleto deve fornecer um bom ponto de partida para o seu `README.md`. Lembre-se de adaptá-lo às necessidades e detalhes específicos do seu projeto!
User: Aplique SOLID e POO na aplicação
Assistant: A aplicação parece ter vários componentes, cada um com suas próprias responsabilidades. Aqui está uma análise de como os princípios SOLID e POO podem ser aplicados para melhorar a estrutura e a manutenção do código.

### Princípios SOLID

1.  **Single Responsibility Principle (SRP)**

    *   **Problema:** Algumas classes podem estar acumulando muitas responsabilidades. Por exemplo, a classe `ImageProcessor` em `services/image_processor.py` lida tanto com o processamento de imagens quanto com a interação com o modelo GPT, o serviço de documentos e o histórico.
    *   **Solução:** Dividir a classe `ImageProcessor` em classes menores, cada uma com uma única responsabilidade.

        *   Criar uma classe `ImageAnalyzer` que lide especificamente com a análise da imagem usando o modelo GPT.
        *   Criar uma classe `DocumentUpdater` que lide com a atualização do documento (Word, Markdown).
        *   Criar uma classe `HistoryManager` para gerenciar o histórico de análises.

2.  **Open/Closed Principle (OCP)**

    *   **Problema:** Se você precisar adicionar um novo tipo de análise de imagem (por exemplo, análise de sentimento), pode ser necessário modificar a classe `ImageProcessor`.
    *   **Solução:** Projetar interfaces ou classes abstratas para definir contratos para as diferentes análises de imagem.

        *   Criar uma interface `ImageAnalysisStrategy` com um método `analyze(image_path)`.
        *   Implementar classes concretas para cada tipo de análise (por exemplo, `ObjectDetectionAnalysis`, `SentimentAnalysis`).
        *   A classe `ImageAnalyzer` receberia uma `ImageAnalysisStrategy` via construtor e delegaria a análise para essa estratégia.

3.  **Liskov Substitution Principle (LSP)**

    *   **Problema:** Se você tem uma classe base `AppConfig` e classes derivadas como `AudioConfig` e `VideoConfig`, você deve garantir que as classes derivadas possam ser usadas no lugar da classe base sem quebrar o programa.
    *   **Solução:** Garantir que todas as classes derivadas de `AppConfig` implementem os métodos da classe base de forma consistente.

        *   Verificar se o método `create_directories` em `AudioConfig` e `VideoConfig` funciona como esperado quando chamado com instâncias de `AppConfig`.

4.  **Interface Segregation Principle (ISP)**

    *   **Problema:** Se uma interface tem muitos métodos que uma classe não precisa implementar, essa classe é forçada a implementar métodos que não usa.
    *   **Solução:** Dividir interfaces grandes em interfaces menores e mais específicas.

        *   Se você tem uma interface `TextToSpeechInterface` com métodos como `speak()` e `pause()`, mas uma classe só precisa de `speak()`, criar uma interface `Speakable` com apenas o método `speak()`.

5.  **Dependency Inversion Principle (DIP)**

    *   **Problema:** Classes de alto nível (por exemplo, `MainApp`) dependem de classes de baixo nível (por exemplo, `VoiceCommandHandler`, `TextCommandHandler`).
    *   **Solução:** Classes de alto nível não devem depender de classes de baixo nível. Ambas devem depender de abstrações.

        *   Criar interfaces para os command handlers (por exemplo, `CommandHandlerInterface`) e fazer com que `MainApp` dependa dessa interface em vez de depender diretamente das classes `VoiceCommandHandler` e `TextCommandHandler`.

### POO (Programação Orientada a Objetos)

*   **Encapsulamento:**
    *   **Problema:** Alguns dados podem não estar devidamente encapsulados, resultando em acesso direto a atributos.
    *   **Solução:** Use propriedades (`@property`) para controlar o acesso aos atributos.
*   **Herança:**
    *   **Problema:** Pode haver uso excessivo ou inadequado de herança, o que pode levar a classes infladas e difíceis de manter.
    *   **Solução:** Prefira composição em vez de herança sempre que possível.
*   **Polimorfismo:**
    *   **Problema:** Falta de polimorfismo pode resultar em código repetitivo e difícil de estender.
    *   **Solução:** Use interfaces e classes abstratas para definir contratos e implementar polimorfismo.

### Aplicando as Mudanças

1.  **Refatoração do `ImageProcessor`:**

```python
# services/image_analyzer.py
class ImageAnalyzer:
    def __init__(self, analysis_strategy):
        self.analysis_strategy = analysis_strategy

    def analyze(self, image_path):
        return self.analysis_strategy.analyze(image_path)

# services/document_updater.py
class DocumentUpdater:
    def __init__(self, document_service, markdown_service):
        self.document_service = document_service
        self.markdown_service = markdown_service

    def update_documents(self, image_name, summary):
        self.document_service.add_image_summary(image_name, summary)
        self.markdown_service.add_image_summary(image_name, summary)
        self.document_service.save_document()
        self.markdown_service.save_markdown()

# services/history_manager.py
class HistoryManager:
    def __init__(self):
        self.history = []

    def update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})

    def get_history(self):
        return self.history

# services/image_processing_strategy.py
from abc import ABC, abstractmethod

class ImageAnalysisStrategy(ABC):
    @abstractmethod
    def analyze(self, image_path):
        pass

# services/gpt_analysis_strategy.py
class GPTAnalysisStrategy(ImageAnalysisStrategy):
    def __init__(self, gpt_handler, prompt):
        self.gpt_handler = gpt_handler
        self.prompt = prompt

    def analyze(self, image_path):
        # Your GPT analysis logic here
        return self.gpt_handler.generate_content_from_image(image_path, self.prompt)

# services/image_processor.py
from services.image_analyzer import ImageAnalyzer
from services.document_updater import DocumentUpdater
from services.history_manager import HistoryManager
from services.gpt_analysis_strategy import GPTAnalysisStrategy

class ImageProcessor:
    def __init__(self, gpt_handler, document_service, markdown_service):
        self.gpt_handler = gpt_handler
        self.document_updater = DocumentUpdater(document_service, markdown_service)
        self.history_manager = HistoryManager()
        self.prompt = self._load_prompt()

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                # Use a strategy for analysis
                analysis_strategy = GPTAnalysisStrategy(self.gpt_handler, self.prompt)
                image_analyzer = ImageAnalyzer(analysis_strategy)

                # Now analyze
                summary = image_analyzer.analyze(os.path.join(Config.ASSETS_DIR, image_name))
                self.document_updater.update_documents(image_name, summary)
                self.history_manager.update_history(image_name, summary)

                time.sleep(4)
                logger.info("Preparando a próxima análise...")

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)
```

### Conclusão

Aplicar os princípios SOLID e POO pode resultar em um código mais modular, testável e fácil de manter. A refatoração envolveria a decomposição de classes grandes em unidades menores e mais coesas, o uso de interfaces para definir contratos claros e a aplicação de injeção de dependência para reduzir o acoplamento.
User: Faça uma limpeza no código em tudo que é desnecessário e que não estou usando.
2025-04-14 14:14:38,276 [INFO] Resposta da IA (texto): Ok, vou analisar o código e identificar partes que parecem desnecessárias ou não utilizadas. Aqui está uma lista de possíveis remoções e otimizações:

### Remoções Potenciais

1.  **Arquivos e Funções Não Utilizados:**

    *   `fundamentus_api/setup.py`: Este é um arquivo de configuração para o pacote `fundamentalvision`. Se você não planeja distribuir este pacote, o arquivo `setup.py` não é necessário.

    *   `translate/translator_to_english.py`: Este arquivo usa `speech_recognition` e `translate`. Se você não está usando ativamente a tradução em tempo real por voz, pode remover este arquivo.

    *   `voice_assistent/class_voice_assistent/conversation_history.py`: Se este arquivo estiver vazio e não estiver sendo usado, pode ser removido.

    *   `voice_assistent/template.py`: Este script parece ser uma versão anterior ou um modelo. Se as funcionalidades foram integradas em `voice_assistent/assistent.py` ou `voice_assistent/class_voice_assistent/main.py`, pode ser removido.

    *   `ia_generator.py`: Se a geração de conteúdo HTML não for usada regularmente, pode ser removida.

    *   `send_embeddings_database/verify_last_enbedding.py`: Se não for usado para verificar os embeddings, pode ser removido.
2.  **Código Comentado:**

    *   Remova todos os blocos de código comentados que não são mais relevantes.

3.  **Importações Desnecessárias:**

    *   `chat_app/chat_streamlit.py`: Verifique todas as importações para garantir que cada uma seja utilizada.

4.  **Logs e Prints:**

    *   `chat_app/config/config.py`: Remova as linhas `print(f"Base Directory: {BASE_DIR}")` e `print(PROCESSED_DIR)`.
    *   `main.py`: Remova a linha `print(f"Root path: {root_path}")`.

### Otimizações Potenciais

1.  **Rate Limiting e Configurações:**

    *   `chat_app/config/config.py`: Se os limites de taxa (`CHAT_RATE_LIMIT`, `API_RATE_LIMIT`) não forem usados ou precisarem ser ajustados, revise essas configurações.

2.  **Estrutura de Diretórios:**

    *   Considere mover todos os arquivos de configuração (config.py) para um diretório `config` central para melhor organização.

3.  **Duplicação de Código:**

    *   `voice_assistent/class_voice_assistent/gpt_communication/gemini_api_client.py` e `voice_assistent/class_voice_assistent/api_client.py`: Essas classes tem código duplicado.

4.  **Verificação de Variáveis de Ambiente:**

    *   `services/gpt_services.py` e `services/image_services.py`: A verificação da variável de ambiente `API_KEY_GEMINI` é feita em ambos os arquivos. Mova essa verificação para um local centralizado (por exemplo, `app_config/app_config.py`).
    *   `voice_assistent/class_voice_assistent/gpt_communication/gemini_gpt.py` Também tem verificação do `API_KEY_GEMINI`.

### Revisão Detalhada de Arquivos

1.  **`main.py`:**

    *   Remova as linhas comentadas para `video_config` e `audio_config` se essas funcionalidades não forem mais necessárias.

2.  **`chat_app/chat_streamlit.py`:**

    *   Verifique as variáveis de estado da sessão (`st.session_state`) e remova aquelas que não são usadas.
    *   Revise a função `check_clipboard()` para garantir que ainda seja relevante.

3.  **`services/image_processor.py`:**

    *   Se o histórico de análises não for necessário, remova o código relacionado e a classe `HistoryManager`.
    *   Se a funcionalidade de carregar histórico não for usada, remova os métodos `_carregar_historico_json` e `_salvar_historico_json`.
    *   Remova a função `_carregar_historico_json` do escopo `_process_image`

4.  **`fundamentus_api/dados_b3.py`:**

    *   Se não pretende exibir o dashboard, remova a classe `Aplicacao` e a lógica de exibir o dashboard.

### Passos para Limpeza

1.  **Backup:** Faça um backup completo do seu código antes de começar.
2.  **Remova Arquivos Desnecessários:** Exclua os arquivos identificados como desnecessários.
3.  **Remova Código Comentado:** Limpe todos os blocos de código comentados irrelevantes.
4.  **Otimize as Importações:** Remova as importações não utilizadas em cada arquivo.
5.  **Simplifique as Condições:** Remova condições desnecessárias ou redundantes.
6.  **Teste:** Após cada grande mudança, execute testes para garantir que a aplicação continua funcionando corretamente.

Ao seguir esses passos, você pode limpar seu código, tornando-o mais fácil de manter e entender.

2025-04-14 14:14:38,409 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:14:38,411 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:14:38,413 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:14:38,415 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:14:38,418 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:14:38,420 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:14:38,422 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:14:38,423 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:14:38,425 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:14:38,427 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:14:38,428 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:14:38,429 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:14:38,430 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:14:38,431 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:14:38,432 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:14:38,433 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:14:38,434 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:14:38,436 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:14:38,438 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:14:38,439 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:14:38,440 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:14:38,441 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:14:38,442 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:14:38,443 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:14:38,445 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:14:38,446 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:14:38,447 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:14:38,448 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:14:38,449 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:14:38,450 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:14:38,452 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:14:38,453 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:14:38,455 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:14:38,457 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:14:38,458 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:14:38,459 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:14:38,459 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:14:38,460 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:14:38,462 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:14:38,463 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:14:38,464 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:14:38,465 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:14:38,466 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:14:38,467 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:14:38,469 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:14:38,470 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:14:38,472 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:14:38,473 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:14:38,474 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:14:38,475 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:14:38,476 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:14:38,477 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:14:38,478 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:26:03,997 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:26:03,999 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:26:04,000 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:26:04,001 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:26:04,003 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:26:04,005 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:26:04,006 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:26:04,007 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:26:04,008 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:26:04,009 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:26:04,010 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:26:04,013 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:26:04,014 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:26:04,015 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:26:04,016 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:26:04,017 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:26:04,018 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:26:04,019 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:26:04,020 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:26:04,021 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:26:04,022 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:26:04,023 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:26:04,025 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:26:04,026 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:26:04,027 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:26:04,028 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:26:04,029 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:26:04,030 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:26:04,031 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:26:04,032 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:26:04,033 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:26:04,034 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:26:04,035 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:26:04,036 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:26:04,037 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:26:04,039 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:26:04,040 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:26:04,041 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:26:04,043 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:26:04,044 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:26:04,045 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:26:04,046 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:26:04,048 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:26:04,050 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:26:04,052 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:26:04,053 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:26:04,055 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:26:04,056 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:26:04,058 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:26:04,059 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:26:04,060 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:26:04,063 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:26:04,065 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:26:04,649 [INFO] Modelo Gemini 'gemini-2.0-flash-exp' inicializado com sucesso.
2025-04-14 14:40:58,452 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:40:58,454 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:40:58,455 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:40:58,457 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:40:58,459 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:40:58,461 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:40:58,462 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:40:58,463 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:40:58,466 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:40:58,468 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:40:58,471 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:40:58,472 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:40:58,474 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:40:58,476 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:40:58,478 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:40:58,479 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:40:58,481 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:40:58,483 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:40:58,485 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:40:58,486 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:40:58,487 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:40:58,489 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:40:58,491 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:40:58,492 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:40:58,494 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:40:58,496 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:40:58,499 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:40:58,501 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:40:58,502 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:40:58,504 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:40:58,506 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:40:58,507 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:40:58,509 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:40:58,511 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:40:58,513 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:40:58,515 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:40:58,516 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:40:58,518 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:40:58,520 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:40:58,522 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:40:58,524 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:40:58,525 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:40:58,526 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:40:58,528 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:40:58,529 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:40:58,532 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:40:58,533 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:40:58,534 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:40:58,536 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:40:58,537 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:40:58,538 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:40:58,540 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:40:58,541 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:41:00,355 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:41:00,357 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:41:00,358 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:41:00,360 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:41:00,362 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:41:00,364 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:41:00,366 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:41:00,367 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:41:00,370 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:41:00,371 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:41:00,372 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:41:00,374 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:41:00,376 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:41:00,378 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:41:00,379 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:41:00,382 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:41:00,383 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:41:00,384 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:41:00,386 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:41:00,388 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:41:00,390 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:41:00,391 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:41:00,393 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:41:00,394 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:41:00,395 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:41:00,397 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:41:00,399 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:41:00,400 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:41:00,402 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:41:00,403 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:41:00,404 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:41:00,405 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:41:00,406 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:41:00,408 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:41:00,409 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:41:00,410 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:41:00,411 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:41:00,414 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:41:00,416 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:41:00,417 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:41:00,419 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:41:00,421 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:41:00,422 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:41:00,423 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:41:00,425 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:41:00,427 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:41:00,429 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:41:00,431 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:41:00,433 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:41:00,434 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:41:00,436 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:41:00,437 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:41:00,439 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:41:00,936 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:41:00,939 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:41:00,940 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:41:00,943 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:41:00,945 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:41:00,947 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:41:00,950 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:41:00,952 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:41:00,954 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:41:00,955 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:41:00,957 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:41:00,959 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:41:00,961 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:41:00,962 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:41:00,964 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:41:00,966 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:41:00,968 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:41:00,970 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:41:00,971 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:41:00,973 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:41:00,974 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:41:00,976 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:41:00,978 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:41:00,979 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:41:00,982 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:41:00,984 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:41:00,986 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:41:00,987 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:41:00,989 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:41:00,990 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:41:00,992 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:41:00,993 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:41:00,995 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:41:00,997 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:41:00,999 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:41:01,002 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:41:01,003 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:41:01,004 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:41:01,006 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:41:01,008 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:41:01,009 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:41:01,011 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:41:01,013 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:41:01,015 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:41:01,018 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:41:01,020 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:41:01,022 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:41:01,023 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:41:01,025 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:41:01,027 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:41:01,028 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:41:01,030 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:41:01,033 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:41:07,325 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:41:07,326 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:41:07,328 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:41:07,329 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:41:07,332 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:41:07,335 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:41:07,337 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:41:07,339 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:41:07,340 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:41:07,342 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:41:07,344 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:41:07,345 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:41:07,347 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:41:07,348 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:41:07,351 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:41:07,354 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:41:07,355 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:41:07,356 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:41:07,357 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:41:07,358 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:41:07,359 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:41:07,361 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:41:07,362 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:41:07,363 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:41:07,365 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:41:07,366 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:41:07,368 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:41:07,369 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:41:07,370 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:41:07,371 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:41:07,372 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:41:07,374 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:41:07,375 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:41:07,377 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:41:07,378 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:41:07,380 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:41:07,381 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:41:07,383 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:41:07,385 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:41:07,387 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:41:07,388 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:41:07,390 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:41:07,392 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:41:07,393 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:41:07,395 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:41:07,397 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:41:07,399 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:41:07,401 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:41:07,403 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:41:07,404 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:41:07,406 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:41:07,408 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:41:07,409 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:41:07,536 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:41:07,538 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:41:07,539 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:41:07,542 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:41:07,544 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:41:07,545 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:41:07,547 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:41:07,548 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:41:07,550 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:41:07,552 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:41:07,553 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:41:07,555 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:41:07,557 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:41:07,558 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:41:07,561 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:41:07,563 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:41:07,565 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:41:07,567 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:41:07,568 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:41:07,570 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:41:07,571 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:41:07,573 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:41:07,575 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:41:07,576 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:41:07,578 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:41:07,580 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:41:07,581 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:41:07,583 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:41:07,584 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:41:07,586 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:41:07,587 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:41:07,588 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:41:07,590 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:41:07,591 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:41:07,592 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:41:07,593 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:41:07,594 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:41:07,596 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:41:07,597 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:41:07,599 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:41:07,600 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:41:07,601 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:41:07,602 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:41:07,604 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:41:07,605 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:41:07,607 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:41:07,608 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:41:07,609 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:41:07,610 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:41:07,612 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:41:07,613 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:41:07,614 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:41:07,616 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:41:07,664 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\agents_ia\comandAI\assets\20250414144107_clipboard_20250414144100.png, Prompt: Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diretório atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Definição dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Criação dos diretórios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configuração do layout do Streamlit
st.set_page_config(
    page_title="Análise de Ações",
    layout="wide",
    page_icon="📈"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscilações

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Período', 'Oscilação'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("📊 Dashboard de Análise de Ações")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma ação", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"📌 Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("🔍 Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descrição', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa ação.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("💰 Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("📉 Oscilações")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("📈 Tabela Geral de Ações")
        st.dataframe(self.acoes)

# Execução
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para análise fundamentalista de ações da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"Não foram encontrados arquivos .txt no diretório {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conteúdo do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conteúdo HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conteúdo HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se está correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configuração de vídeos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configuração de áudios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcrições e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diretório
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diretório.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do último arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conteúdo do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimensões do embedding:", embedding.shape)

# Caminho do diretório de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o último embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configuração de transcrições e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diretório onde está procurando as transcrições
        print(f"Diretório de entrada das transcrições: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcrição no diretório de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcrição encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo não encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcrição do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} está vazio.")
                    return

            # Gerar o embedding da transcrição
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcrição e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de áudio
    with sr.Microphone() as source:
        print("Diga algo em português...")

        while True:
            try:
                # Escuta o áudio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {texto_portugues}")

                # Traduz o texto para o inglês
                traducao = translator.translate(texto_portugues)
                print(f"Tradução para o inglês: {traducao}")

            except sr.UnknownValueError:
                print("Não foi possível entender o áudio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usuário")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configurações de áudio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de áudio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em português...")

try:
    audio_buffer = []

    while True:
        # Lê dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o áudio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o áudio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradução
            print(f"Tradução para o inglês: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usuário")

    # Fecha o stream de áudio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O vídeo {video_path} não contém áudio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de vídeo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de vídeo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de vídeo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Conversão de vídeo para áudio concluída!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configurações da API
handler = genai('gemini-1.5-flash')

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Função para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conteúdo com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares únicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcrição obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' não é uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuniões: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de reuniões: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|tópicos da|pontos (relevantes|principais)|análise)\b.*\b(reunião|última (reunião|conversa|sessão))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como análise de reunião.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padrão.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que são muito similares ao comando atual para evitar redundância.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padrão: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # Métodos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de análise de reunião.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conteúdo usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inválida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar variáveis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conteúdo com base no prompt, contexto e reunião"""
        try:
            # Supondo que a API espera um dicionário com os parâmetros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisição para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar variáveis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Criação da conclusão do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que você acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["faça um resumo da última reunião.", "tópicos da última reunião", "resuma a última reunião", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Você é responsável por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - Não acrescentar título nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informações acima e forneça uma solução detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Você é responsável analisar com detalhes a reunião de [str_meeting], e fornecer uma longa estória sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precisão da resposta.
            - Não acrescente título nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e faça uma pesquisa online para uma resposta mais precisa das questões de [str_text]
        - Não acrescente título nas respostas.
        
        ------
        [context]: Regras Básicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("Áudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum áudio detectado.")
                return None
            except sr.UnknownValueError:
                print("Não foi possível entender o áudio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configurações da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicionário
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta não está no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API: {e}")
        return None

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: o que significa essa imagem ?
2025-04-14 14:41:13,183 [INFO] Resposta da IA (imagem): The image is a data visualization, specifically a bar graph, titled "What Kind of Data Do AI Chatbots Collect?" It compares the data collection practices of several AI chatbots: Gemini, Claude, CO PILOT, deepseek, CHAT GPT, perplexity, and Grok.

Here's a breakdown of the data categories and what they likely represent:

*   **Contact Info:** Email address, phone number, etc.

*   **Location:** Geographic location data (e.g., GPS coordinates, IP address).

*   **Contacts:** Information about your address book or social connections.

*   **User Content:**  The text, images, audio, and video you provide to the chatbot.

*   **History:** Records of your past interactions with the chatbot.

*   **Identifiers:** Unique IDs that can be used to identify you.

*   **Diagnostics:** Data about the chatbot's performance and any errors encountered.

*   **Usage Data:** How you use the chatbot (e.g., features used, frequency of use).

*   **Purchases:** Information about any transactions made through the chatbot.

*   **Other Data:** A catch-all category for data that doesn't fit into the other categories.

The numbers next to each chatbot indicate how many of these data points the chatbot collects. For example, Gemini collects 4 types of contact info, 2 location data points, etc. The last number indicates the total number of unique data points that each chatbot collects.

The source of the data is Surfshark, and it was compiled as of February 2025. The graphic is created by Visual Capitalist.
2025-04-14 14:41:13,416 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:41:13,418 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:41:13,421 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:41:13,424 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:41:13,428 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:41:13,430 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:41:13,434 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:41:13,437 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:41:13,440 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:41:13,443 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:41:13,446 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:41:13,449 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:41:13,452 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:41:13,455 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:41:13,458 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:41:13,461 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:41:13,463 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:41:13,466 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:41:13,471 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:41:13,474 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:41:13,476 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:41:13,478 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:41:13,480 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:41:13,482 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:41:13,484 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:41:13,487 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:41:13,489 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:41:13,491 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:41:13,493 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:41:13,495 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:41:13,497 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:41:13,500 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:41:13,501 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:41:13,502 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:41:13,505 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:41:13,507 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:41:13,508 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:41:13,510 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:41:13,513 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:41:13,515 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:41:13,517 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:41:13,519 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:41:13,521 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:41:13,523 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:41:13,525 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:41:13,527 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:41:13,529 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:41:13,531 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:41:13,533 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:41:13,534 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:41:13,536 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:41:13,538 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:41:13,539 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:41:37,661 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:41:37,662 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:41:37,664 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:41:37,666 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:41:37,668 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:41:37,670 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:41:37,671 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:41:37,673 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:41:37,675 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:41:37,677 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:41:37,678 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:41:37,680 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:41:37,682 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:41:37,683 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:41:37,685 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:41:37,686 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:41:37,688 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:41:37,689 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:41:37,691 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:41:37,692 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:41:37,694 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:41:37,696 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:41:37,697 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:41:37,699 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:41:37,701 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:41:37,703 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:41:37,706 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:41:37,707 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:41:37,709 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:41:37,711 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:41:37,712 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:41:37,713 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:41:37,714 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:41:37,715 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:41:37,717 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:41:37,718 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:41:37,720 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:41:37,722 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:41:37,723 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:41:37,724 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:41:37,726 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:41:37,727 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:41:37,728 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:41:37,730 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:41:37,731 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:41:37,732 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:41:37,733 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:41:37,735 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:41:37,737 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:41:37,738 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:41:37,739 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:41:37,741 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:41:37,743 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:41:37,866 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:41:37,868 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:41:37,870 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:41:37,873 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:41:37,874 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:41:37,876 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:41:37,877 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:41:37,879 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:41:37,881 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:41:37,883 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:41:37,885 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:41:37,887 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:41:37,889 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:41:37,891 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:41:37,893 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:41:37,895 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:41:37,897 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:41:37,898 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:41:37,899 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:41:37,901 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:41:37,903 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:41:37,905 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:41:37,906 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:41:37,908 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:41:37,910 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:41:37,911 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:41:37,913 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:41:37,915 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:41:37,917 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:41:37,918 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:41:37,920 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:41:37,922 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:41:37,924 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:41:37,926 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:41:37,927 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:41:37,928 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:41:37,929 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:41:37,931 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:41:37,932 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:41:37,933 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:41:37,935 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:41:37,937 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:41:37,938 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:41:37,940 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:41:37,941 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:41:37,943 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:41:37,945 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:41:37,947 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:41:37,948 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:41:37,949 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:41:37,952 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:41:37,953 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:41:37,955 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:41:38,010 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\agents_ia\comandAI\assets\20250414144137_clipboard_20250414144100.png, Prompt: Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diretório atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Definição dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Criação dos diretórios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configuração do layout do Streamlit
st.set_page_config(
    page_title="Análise de Ações",
    layout="wide",
    page_icon="📈"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscilações

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Período', 'Oscilação'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("📊 Dashboard de Análise de Ações")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma ação", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"📌 Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("🔍 Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descrição', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa ação.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("💰 Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("📉 Oscilações")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("📈 Tabela Geral de Ações")
        st.dataframe(self.acoes)

# Execução
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para análise fundamentalista de ações da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"Não foram encontrados arquivos .txt no diretório {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conteúdo do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conteúdo HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conteúdo HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se está correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configuração de vídeos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configuração de áudios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcrições e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diretório
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diretório.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do último arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conteúdo do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimensões do embedding:", embedding.shape)

# Caminho do diretório de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o último embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configuração de transcrições e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diretório onde está procurando as transcrições
        print(f"Diretório de entrada das transcrições: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcrição no diretório de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcrição encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo não encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcrição do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} está vazio.")
                    return

            # Gerar o embedding da transcrição
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcrição e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de áudio
    with sr.Microphone() as source:
        print("Diga algo em português...")

        while True:
            try:
                # Escuta o áudio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {texto_portugues}")

                # Traduz o texto para o inglês
                traducao = translator.translate(texto_portugues)
                print(f"Tradução para o inglês: {traducao}")

            except sr.UnknownValueError:
                print("Não foi possível entender o áudio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usuário")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configurações de áudio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de áudio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em português...")

try:
    audio_buffer = []

    while True:
        # Lê dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o áudio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o áudio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradução
            print(f"Tradução para o inglês: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usuário")

    # Fecha o stream de áudio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O vídeo {video_path} não contém áudio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de vídeo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de vídeo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de vídeo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Conversão de vídeo para áudio concluída!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configurações da API
handler = genai('gemini-1.5-flash')

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Função para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conteúdo com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares únicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcrição obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' não é uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuniões: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de reuniões: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|tópicos da|pontos (relevantes|principais)|análise)\b.*\b(reunião|última (reunião|conversa|sessão))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como análise de reunião.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padrão.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que são muito similares ao comando atual para evitar redundância.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padrão: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # Métodos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de análise de reunião.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conteúdo usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inválida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar variáveis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conteúdo com base no prompt, contexto e reunião"""
        try:
            # Supondo que a API espera um dicionário com os parâmetros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisição para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar variáveis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Criação da conclusão do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que você acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["faça um resumo da última reunião.", "tópicos da última reunião", "resuma a última reunião", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Você é responsável por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - Não acrescentar título nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informações acima e forneça uma solução detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Você é responsável analisar com detalhes a reunião de [str_meeting], e fornecer uma longa estória sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precisão da resposta.
            - Não acrescente título nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e faça uma pesquisa online para uma resposta mais precisa das questões de [str_text]
        - Não acrescente título nas respostas.
        
        ------
        [context]: Regras Básicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("Áudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum áudio detectado.")
                return None
            except sr.UnknownValueError:
                print("Não foi possível entender o áudio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configurações da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicionário
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta não está no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API: {e}")
        return None

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: o que significa essa imagem ?
Assistant: The image is a data visualization, specifically a bar graph, titled "What Kind of Data Do AI Chatbots Collect?" It compares the data collection practices of several AI chatbots: Gemini, Claude, CO PILOT, deepseek, CHAT GPT, perplexity, and Grok.

Here's a breakdown of the data categories and what they likely represent:

*   **Contact Info:** Email address, phone number, etc.

*   **Location:** Geographic location data (e.g., GPS coordinates, IP address).

*   **Contacts:** Information about your address book or social connections.

*   **User Content:**  The text, images, audio, and video you provide to the chatbot.

*   **History:** Records of your past interactions with the chatbot.

*   **Identifiers:** Unique IDs that can be used to identify you.

*   **Diagnostics:** Data about the chatbot's performance and any errors encountered.

*   **Usage Data:** How you use the chatbot (e.g., features used, frequency of use).

*   **Purchases:** Information about any transactions made through the chatbot.

*   **Other Data:** A catch-all category for data that doesn't fit into the other categories.

The numbers next to each chatbot indicate how many of these data points the chatbot collects. For example, Gemini collects 4 types of contact info, 2 location data points, etc. The last number indicates the total number of unique data points that each chatbot collects.

The source of the data is Surfshark, and it was compiled as of February 2025. The graphic is created by Visual Capitalist.
User: explique em portugues.
2025-04-14 14:41:42,123 [INFO] Resposta da IA (imagem): Assistant: A imagem é uma visualização de dados, especificamente um gráfico de barras, intitulado "Que Tipo de Dados os Chatbots de IA Coletam?". Ele compara as práticas de coleta de dados de vários chatbots de IA: Gemini, Claude, CO PILOT, deepseek, CHAT GPT, perplexity e Grok.

Aqui está uma análise das categorias de dados e o que elas provavelmente representam:

*   **Informações de Contato:** Endereço de e-mail, número de telefone, etc.

*   **Localização:** Dados de localização geográfica (por exemplo, coordenadas GPS, endereço IP).

*   **Contatos:** Informações sobre sua lista de endereços ou conexões sociais.

*   **Conteúdo do Usuário:** O texto, imagens, áudio e vídeo que você fornece ao chatbot.

*   **Histórico:** Registros de suas interações passadas com o chatbot.

*   **Identificadores:** IDs únicos que podem ser usados para identificá-lo.

*   **Diagnósticos:** Dados sobre o desempenho do chatbot e quaisquer erros encontrados.

*   **Dados de Uso:** Como você usa o chatbot (por exemplo, recursos usados, frequência de uso).

*   **Compras:** Informações sobre quaisquer transações feitas através do chatbot.

*   **Outros Dados:** Uma categoria genérica para dados que não se encaixam nas outras categorias.

Os números ao lado de cada chatbot indicam quantos desses pontos de dados o chatbot coleta. Por exemplo, o Gemini coleta 4 tipos de informações de contato, 2 pontos de dados de localização, etc. O último número indica o número total de pontos de dados exclusivos que cada chatbot coleta.

A fonte dos dados é a Surfshark, e foi compilada em fevereiro de 2025. O gráfico é criado pela Visual Capitalist.
2025-04-14 14:41:42,300 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:41:42,302 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:41:42,304 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:41:42,307 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:41:42,308 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:41:42,310 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:41:42,312 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:41:42,314 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:41:42,316 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:41:42,318 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:41:42,320 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:41:42,322 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:41:42,324 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:41:42,326 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:41:42,328 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:41:42,329 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:41:42,331 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:41:42,332 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:41:42,335 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:41:42,337 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:41:42,339 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:41:42,341 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:41:42,343 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:41:42,345 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:41:42,346 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:41:42,348 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:41:42,350 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:41:42,353 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:41:42,354 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:41:42,357 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:41:42,358 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:41:42,360 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:41:42,362 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:41:42,364 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:41:42,366 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:41:42,368 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:41:42,370 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:41:42,372 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:41:42,374 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:41:42,376 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:41:42,377 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:41:42,379 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:41:42,381 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:41:42,383 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:41:42,386 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:41:42,388 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:41:42,390 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:41:42,392 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:41:42,394 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:41:42,396 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:41:42,397 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:41:42,399 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:41:42,402 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:47:42,537 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:47:42,539 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:47:42,541 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:47:42,542 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:47:42,544 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:47:42,546 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:47:42,547 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:47:42,548 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:47:42,550 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:47:42,552 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:47:42,553 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:47:42,555 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:47:42,557 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:47:42,558 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:47:42,560 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:47:42,561 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:47:42,563 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:47:42,564 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:47:42,565 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:47:42,567 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:47:42,568 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:47:42,570 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:47:42,572 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:47:42,574 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:47:42,575 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:47:42,577 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:47:42,578 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:47:42,580 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:47:42,581 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:47:42,583 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:47:42,585 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:47:42,587 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:47:42,590 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:47:42,591 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:47:42,593 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:47:42,595 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:47:42,596 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:47:42,597 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:47:42,600 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:47:42,601 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:47:42,603 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:47:42,604 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:47:42,611 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:47:42,614 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:47:42,616 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:47:42,620 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:47:42,621 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:47:42,623 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:47:42,624 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:47:42,625 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:47:42,626 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:47:42,628 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:47:42,629 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:47:42,755 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:47:42,758 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:47:42,760 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:47:42,763 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:47:42,765 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:47:42,768 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:47:42,770 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:47:42,773 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:47:42,775 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:47:42,778 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:47:42,780 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:47:42,782 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:47:42,784 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:47:42,786 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:47:42,788 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:47:42,790 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:47:42,792 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:47:42,794 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:47:42,795 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:47:42,797 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:47:42,799 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:47:42,801 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:47:42,803 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:47:42,804 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:47:42,807 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:47:42,809 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:47:42,811 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:47:42,813 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:47:42,814 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:47:42,816 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:47:42,818 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:47:42,819 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:47:42,821 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:47:42,824 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:47:42,826 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:47:42,827 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:47:42,829 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:47:42,830 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:47:42,832 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:47:42,833 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:47:42,835 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:47:42,836 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:47:42,838 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:47:42,840 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:47:42,842 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:47:42,844 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:47:42,845 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:47:42,847 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:47:42,849 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:47:42,850 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:47:42,851 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:47:42,853 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:47:42,855 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:47:42,858 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diretório atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Definição dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Criação dos diretórios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configuração do layout do Streamlit
st.set_page_config(
    page_title="Análise de Ações",
    layout="wide",
    page_icon="📈"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscilações

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Período', 'Oscilação'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("📊 Dashboard de Análise de Ações")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma ação", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"📌 Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("🔍 Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descrição', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa ação.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("💰 Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("📉 Oscilações")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("📈 Tabela Geral de Ações")
        st.dataframe(self.acoes)

# Execução
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para análise fundamentalista de ações da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"Não foram encontrados arquivos .txt no diretório {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conteúdo do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conteúdo HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conteúdo HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se está correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configuração de vídeos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configuração de áudios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcrições e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diretório
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diretório.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do último arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conteúdo do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimensões do embedding:", embedding.shape)

# Caminho do diretório de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o último embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configuração de transcrições e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diretório onde está procurando as transcrições
        print(f"Diretório de entrada das transcrições: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcrição no diretório de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcrição encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo não encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcrição do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} está vazio.")
                    return

            # Gerar o embedding da transcrição
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcrição e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de áudio
    with sr.Microphone() as source:
        print("Diga algo em português...")

        while True:
            try:
                # Escuta o áudio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {texto_portugues}")

                # Traduz o texto para o inglês
                traducao = translator.translate(texto_portugues)
                print(f"Tradução para o inglês: {traducao}")

            except sr.UnknownValueError:
                print("Não foi possível entender o áudio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usuário")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configurações de áudio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de áudio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em português...")

try:
    audio_buffer = []

    while True:
        # Lê dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o áudio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o áudio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradução
            print(f"Tradução para o inglês: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usuário")

    # Fecha o stream de áudio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O vídeo {video_path} não contém áudio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de vídeo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de vídeo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de vídeo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Conversão de vídeo para áudio concluída!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configurações da API
handler = genai('gemini-1.5-flash')

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Função para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conteúdo com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares únicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcrição obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' não é uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuniões: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de reuniões: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|tópicos da|pontos (relevantes|principais)|análise)\b.*\b(reunião|última (reunião|conversa|sessão))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como análise de reunião.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padrão.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que são muito similares ao comando atual para evitar redundância.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padrão: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # Métodos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de análise de reunião.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conteúdo usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inválida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar variáveis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conteúdo com base no prompt, contexto e reunião"""
        try:
            # Supondo que a API espera um dicionário com os parâmetros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisição para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar variáveis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Criação da conclusão do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que você acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["faça um resumo da última reunião.", "tópicos da última reunião", "resuma a última reunião", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Você é responsável por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - Não acrescentar título nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informações acima e forneça uma solução detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Você é responsável analisar com detalhes a reunião de [str_meeting], e fornecer uma longa estória sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precisão da resposta.
            - Não acrescente título nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e faça uma pesquisa online para uma resposta mais precisa das questões de [str_text]
        - Não acrescente título nas respostas.
        
        ------
        [context]: Regras Básicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("Áudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum áudio detectado.")
                return None
            except sr.UnknownValueError:
                print("Não foi possível entender o áudio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configurações da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicionário
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta não está no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API: {e}")
        return None

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: quais IDES já possuem integração com alguma IA que consegue receber o seu código como contexto e fornecer assistencia no desenvolvimento.???

Atualmente só conheço o VScode com o copilot e o gemini integrados.
2025-04-14 14:47:44,682 [INFO] Resposta da IA (texto): Com base no código fornecido e nas suas instruções, compilei uma lista de IDEs (Ambientes de Desenvolvimento Integrados) que já possuem integração com alguma IA capaz de receber o seu código como contexto e fornecer assistência no desenvolvimento:

*   **VS Code:** Já possui integrações com o GitHub Copilot e o Gemini.

A pesquisa foi realizada utilizando o código fonte para identificar as ferramentas e frameworks mencionados, e, com base nisso, fornecer uma resposta precisa.
2025-04-14 14:47:44,796 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:47:44,798 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:47:44,800 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:47:44,802 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:47:44,803 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:47:44,805 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:47:44,806 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:47:44,808 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:47:44,809 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:47:44,811 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:47:44,812 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:47:44,813 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:47:44,814 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:47:44,816 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:47:44,817 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:47:44,818 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:47:44,820 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:47:44,821 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:47:44,823 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:47:44,825 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:47:44,826 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:47:44,827 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:47:44,829 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:47:44,830 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:47:44,832 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:47:44,833 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:47:44,834 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:47:44,836 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:47:44,837 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:47:44,838 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:47:44,842 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:47:44,843 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:47:44,845 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:47:44,846 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:47:44,848 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:47:44,850 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:47:44,852 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:47:44,853 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:47:44,857 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:47:44,859 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:47:44,861 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:47:44,863 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:47:44,865 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:47:44,867 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:47:44,868 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:47:44,870 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:47:44,873 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:47:44,875 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:47:44,877 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:47:44,880 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:47:44,882 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:47:44,883 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:47:44,886 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:50:37,655 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:50:37,657 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:50:37,659 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:50:37,660 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:50:37,662 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:50:37,664 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:50:37,666 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:50:37,667 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:50:37,669 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:50:37,671 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:50:37,672 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:50:37,675 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:50:37,678 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:50:37,679 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:50:37,681 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:50:37,683 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:50:37,694 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:50:37,696 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:50:37,700 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:50:37,701 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:50:37,702 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:50:37,704 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:50:37,705 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:50:37,707 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:50:37,709 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:50:37,710 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:50:37,711 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:50:37,713 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:50:37,714 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:50:37,715 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:50:37,716 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:50:37,718 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:50:37,719 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:50:37,720 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:50:37,722 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:50:37,724 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:50:37,726 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:50:37,727 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:50:37,728 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:50:37,730 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:50:37,731 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:50:37,732 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:50:37,734 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:50:37,735 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:50:37,737 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:50:37,738 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:50:37,740 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:50:37,742 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:50:37,743 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:50:37,744 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:50:37,745 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:50:37,746 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:50:37,747 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:50:37,894 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:50:37,896 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:50:37,898 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:50:37,900 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:50:37,903 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:50:37,906 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:50:37,908 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:50:37,910 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:50:37,911 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:50:37,913 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:50:37,914 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:50:37,916 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:50:37,918 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:50:37,920 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:50:37,922 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:50:37,924 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:50:37,926 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:50:37,927 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:50:37,929 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:50:37,932 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:50:37,936 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:50:37,938 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:50:37,941 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:50:37,943 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:50:37,945 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:50:37,946 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:50:37,948 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:50:37,950 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:50:37,951 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:50:37,954 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:50:37,956 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:50:37,958 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:50:37,960 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:50:37,962 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:50:37,964 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:50:37,965 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:50:37,966 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:50:37,967 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:50:37,969 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:50:37,971 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:50:37,972 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:50:37,974 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:50:37,977 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:50:37,979 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:50:37,981 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:50:37,982 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:50:37,984 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:50:37,986 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:50:37,988 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:50:37,992 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:50:37,997 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:50:38,000 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:50:38,002 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:50:52,681 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:50:52,683 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:50:52,684 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:50:52,686 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:50:52,688 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:50:52,690 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:50:52,693 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:50:52,695 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:50:52,697 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:50:52,699 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:50:52,700 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:50:52,702 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:50:52,704 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:50:52,705 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:50:52,707 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:50:52,710 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:50:52,711 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:50:52,712 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:50:52,714 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:50:52,715 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:50:52,718 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:50:52,720 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:50:52,722 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:50:52,725 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:50:52,727 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:50:52,730 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:50:52,733 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:50:52,735 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:50:52,736 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:50:52,737 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:50:52,738 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:50:52,739 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:50:52,740 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:50:52,742 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:50:52,743 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:50:52,745 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:50:52,745 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:50:52,746 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:50:52,748 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:50:52,749 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:50:52,750 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:50:52,751 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:50:52,752 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:50:52,753 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:50:52,754 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:50:52,755 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:50:52,757 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:50:52,758 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:50:52,759 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:50:52,760 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:50:52,761 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:50:52,763 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:50:52,764 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:50:52,863 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:50:52,865 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:50:52,866 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:50:52,868 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:50:52,870 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:50:52,871 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:50:52,873 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:50:52,875 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:50:52,876 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:50:52,877 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:50:52,879 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:50:52,881 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:50:52,882 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:50:52,884 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:50:52,885 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:50:52,887 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:50:52,888 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:50:52,889 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:50:52,890 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:50:52,892 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:50:52,893 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:50:52,895 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:50:52,896 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:50:52,897 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:50:52,899 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:50:52,900 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:50:52,902 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:50:52,903 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:50:52,904 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:50:52,905 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:50:52,906 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:50:52,907 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:50:52,909 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:50:52,910 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:50:52,912 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:50:52,913 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:50:52,913 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:50:52,915 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:50:52,916 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:50:52,917 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:50:52,919 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:50:52,920 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:50:52,921 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:50:52,922 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:50:52,923 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:50:52,925 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:50:52,926 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:50:52,928 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:50:52,929 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:50:52,930 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:50:52,931 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:50:52,932 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:50:52,933 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:50:52,952 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\agents_ia\comandAI\assets\20250414145052_clipboard_20250414145037.png, Prompt: Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diretório atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Definição dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Criação dos diretórios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configuração do layout do Streamlit
st.set_page_config(
    page_title="Análise de Ações",
    layout="wide",
    page_icon="📈"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscilações

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Período', 'Oscilação'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("📊 Dashboard de Análise de Ações")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma ação", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"📌 Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("🔍 Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descrição', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa ação.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("💰 Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("📉 Oscilações")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("📈 Tabela Geral de Ações")
        st.dataframe(self.acoes)

# Execução
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para análise fundamentalista de ações da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"Não foram encontrados arquivos .txt no diretório {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conteúdo do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conteúdo HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conteúdo HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se está correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configuração de vídeos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configuração de áudios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcrições e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diretório
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diretório.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do último arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conteúdo do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimensões do embedding:", embedding.shape)

# Caminho do diretório de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o último embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configuração de transcrições e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diretório onde está procurando as transcrições
        print(f"Diretório de entrada das transcrições: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcrição no diretório de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcrição encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo não encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcrição do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} está vazio.")
                    return

            # Gerar o embedding da transcrição
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcrição e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de áudio
    with sr.Microphone() as source:
        print("Diga algo em português...")

        while True:
            try:
                # Escuta o áudio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {texto_portugues}")

                # Traduz o texto para o inglês
                traducao = translator.translate(texto_portugues)
                print(f"Tradução para o inglês: {traducao}")

            except sr.UnknownValueError:
                print("Não foi possível entender o áudio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usuário")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configurações de áudio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de áudio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em português...")

try:
    audio_buffer = []

    while True:
        # Lê dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o áudio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o áudio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradução
            print(f"Tradução para o inglês: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usuário")

    # Fecha o stream de áudio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O vídeo {video_path} não contém áudio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de vídeo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de vídeo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de vídeo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Conversão de vídeo para áudio concluída!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configurações da API
handler = genai('gemini-1.5-flash')

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Função para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conteúdo com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares únicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcrição obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' não é uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuniões: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de reuniões: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|tópicos da|pontos (relevantes|principais)|análise)\b.*\b(reunião|última (reunião|conversa|sessão))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como análise de reunião.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padrão.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que são muito similares ao comando atual para evitar redundância.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padrão: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # Métodos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de análise de reunião.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conteúdo usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inválida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar variáveis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conteúdo com base no prompt, contexto e reunião"""
        try:
            # Supondo que a API espera um dicionário com os parâmetros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisição para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar variáveis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Criação da conclusão do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que você acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["faça um resumo da última reunião.", "tópicos da última reunião", "resuma a última reunião", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Você é responsável por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - Não acrescentar título nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informações acima e forneça uma solução detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Você é responsável analisar com detalhes a reunião de [str_meeting], e fornecer uma longa estória sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precisão da resposta.
            - Não acrescente título nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e faça uma pesquisa online para uma resposta mais precisa das questões de [str_text]
        - Não acrescente título nas respostas.
        
        ------
        [context]: Regras Básicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("Áudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum áudio detectado.")
                return None
            except sr.UnknownValueError:
                print("Não foi possível entender o áudio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configurações da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicionário
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta não está no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API: {e}")
        return None

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: quais IDES já possuem integração com alguma IA que consegue receber o seu código como contexto e fornecer assistencia no desenvolvimento.???

Atualmente só conheço o VScode com o copilot e o gemini integrados.
Assistant: Com base no código fornecido e nas suas instruções, compilei uma lista de IDEs (Ambientes de Desenvolvimento Integrados) que já possuem integração com alguma IA capaz de receber o seu código como contexto e fornecer assistência no desenvolvimento:

*   **VS Code:** Já possui integrações com o GitHub Copilot e o Gemini.

A pesquisa foi realizada utilizando o código fonte para identificar as ferramentas e frameworks mencionados, e, com base nisso, fornecer uma resposta precisa.
User: mesmo instalando o pacote não reconhece.
2025-04-14 14:50:58,844 [INFO] Resposta da IA (imagem): Com base no contexto do código fornecido, aqui estão alguns dos possíveis problemas e soluções que podem causar problemas de reconhecimento de pacotes:

1.  **Problema de ambiente:** Certifique-se de que o pacote google-generativeai esteja instalado no ambiente Python correto. Às vezes, pode ser instalado em um ambiente diferente do que está sendo usado.

    *   **Solução:** Ative o ambiente virtual apropriado e use `pip show google-generativeai` para verificar se o pacote está instalado naquele ambiente. Se não estiver, instale-o usando `pip install google-generativeai`.

2.  **Problema de cache:** Às vezes, o pip pode ter problemas de cache que impedem que ele reconheça pacotes recém-instalados.

    *   **Solução:** Tente atualizar o pip e limpar o cache do pip antes de instalar o pacote novamente. Use os seguintes comandos:

    ```bash
    pip install --upgrade pip
    pip cache purge
    pip install google-generativeai
    ```

3.  **Problema de caminho:** Às vezes, o Python pode não encontrar o pacote instalado porque o caminho de instalação do pacote não está no `sys.path`.

    *   **Solução:** Adicione o diretório do pacote ao `sys.path` em seu script Python ou bloco de anotações. Você pode encontrar o diretório de instalação do pacote usando `pip show google-generativeai -f`. Em seguida, adicione esse caminho ao `sys.path` em seu script:

    ```python
    import sys
    sys.path.append('/caminho/para/site-packages')  # Substitua com o caminho real
    import google.generativeai as genai
    ```

4.  **Problema de nome:** Às vezes, o nome do pacote no código não corresponde ao nome real do pacote instalado.

    *   **Solução:** Verifique se o nome do pacote está correto em sua instrução `import`. Nesse caso, ele deve ser `import google.generativeai as genai`.

5.  **Problema de versão:** Às vezes, os pacotes podem apresentar problemas de compatibilidade com outras versões de pacotes ou com a versão Python que você está usando.

    *   **Solução:** Considere usar uma versão diferente do pacote que seja compatível com sua configuração.

6.  **IDE ou problema de editor:** Às vezes, o IDE ou editor que você está usando pode não ser capaz de reconhecer pacotes recém-instalados.

    *   **Solução:** Reinicie o IDE ou editor ou recarregue o ambiente Python.
2025-04-14 14:50:58,934 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:50:58,935 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:50:58,937 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:50:58,939 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:50:58,940 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:50:58,942 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:50:58,943 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:50:58,944 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:50:58,945 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:50:58,946 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:50:58,948 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:50:58,949 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:50:58,951 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:50:58,952 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:50:58,953 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:50:58,954 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:50:58,955 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:50:58,956 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:50:58,957 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:50:58,958 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:50:58,959 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:50:58,961 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:50:58,962 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:50:58,963 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:50:58,964 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:50:58,965 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:50:58,967 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:50:58,968 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:50:58,969 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:50:58,970 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:50:58,971 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:50:58,972 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:50:58,973 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:50:58,974 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:50:58,975 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:50:58,976 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:50:58,977 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:50:58,978 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:50:58,979 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:50:58,980 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:50:58,981 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:50:58,983 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:50:58,984 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:50:58,985 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:50:58,986 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:50:58,987 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:50:58,988 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:50:58,989 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:50:58,991 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:50:58,992 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:50:58,993 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:50:58,994 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:50:58,995 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:53:45,341 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:53:45,344 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:53:45,345 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:53:45,347 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:53:45,348 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:53:45,350 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:53:45,351 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:53:45,352 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:53:45,353 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:53:45,355 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:53:45,356 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:53:45,357 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:53:45,360 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:53:45,361 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:53:45,363 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:53:45,364 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:53:45,366 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:53:45,367 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:53:45,368 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:53:45,370 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:53:45,371 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:53:45,382 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:53:45,384 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:53:45,386 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:53:45,391 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:53:45,394 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:53:45,397 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:53:45,398 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:53:45,400 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:53:45,401 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:53:45,402 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:53:45,403 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:53:45,404 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:53:45,406 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:53:45,407 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:53:45,409 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:53:45,410 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:53:45,412 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:53:45,413 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:53:45,414 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:53:45,415 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:53:45,416 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:53:45,417 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:53:45,418 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:53:45,419 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:53:45,420 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:53:45,421 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:53:45,422 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:53:45,425 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:53:45,426 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:53:45,427 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:53:45,428 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:53:45,429 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:53:45,535 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:53:45,537 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:53:45,538 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:53:45,540 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:53:45,543 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:53:45,544 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:53:45,546 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:53:45,548 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:53:45,550 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:53:45,551 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:53:45,553 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:53:45,554 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:53:45,556 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:53:45,559 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:53:45,561 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:53:45,562 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:53:45,564 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:53:45,566 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:53:45,567 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:53:45,568 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:53:45,569 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:53:45,571 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:53:45,572 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:53:45,574 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:53:45,575 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:53:45,577 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:53:45,579 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:53:45,581 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:53:45,583 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:53:45,585 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:53:45,587 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:53:45,588 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:53:45,589 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:53:45,591 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:53:45,593 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:53:45,595 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:53:45,596 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:53:45,597 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:53:45,599 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:53:45,600 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:53:45,602 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:53:45,603 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:53:45,604 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:53:45,606 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:53:45,608 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:53:45,609 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:53:45,611 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:53:45,612 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:53:45,614 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:53:45,615 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:53:45,616 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:53:45,617 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:53:45,618 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:53:45,636 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\agents_ia\comandAI\assets\20250414145345_clipboard_20250414145037.png, Prompt: Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diretório atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Definição dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Criação dos diretórios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configuração do layout do Streamlit
st.set_page_config(
    page_title="Análise de Ações",
    layout="wide",
    page_icon="📈"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscilações

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Período', 'Oscilação'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("📊 Dashboard de Análise de Ações")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma ação", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"📌 Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("🔍 Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descrição', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa ação.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("💰 Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("📉 Oscilações")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("📈 Tabela Geral de Ações")
        st.dataframe(self.acoes)

# Execução
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para análise fundamentalista de ações da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"Não foram encontrados arquivos .txt no diretório {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conteúdo do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conteúdo HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conteúdo HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se está correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configuração de vídeos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configuração de áudios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcrições e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diretório
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diretório.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do último arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conteúdo do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimensões do embedding:", embedding.shape)

# Caminho do diretório de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o último embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configuração de transcrições e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diretório onde está procurando as transcrições
        print(f"Diretório de entrada das transcrições: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcrição no diretório de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcrição encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo não encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcrição do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} está vazio.")
                    return

            # Gerar o embedding da transcrição
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcrição e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de áudio
    with sr.Microphone() as source:
        print("Diga algo em português...")

        while True:
            try:
                # Escuta o áudio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {texto_portugues}")

                # Traduz o texto para o inglês
                traducao = translator.translate(texto_portugues)
                print(f"Tradução para o inglês: {traducao}")

            except sr.UnknownValueError:
                print("Não foi possível entender o áudio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usuário")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configurações de áudio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de áudio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em português...")

try:
    audio_buffer = []

    while True:
        # Lê dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o áudio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o áudio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradução
            print(f"Tradução para o inglês: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usuário")

    # Fecha o stream de áudio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O vídeo {video_path} não contém áudio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de vídeo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de vídeo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de vídeo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Conversão de vídeo para áudio concluída!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configurações da API
handler = genai('gemini-1.5-flash')

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Função para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conteúdo com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares únicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcrição obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' não é uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuniões: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de reuniões: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|tópicos da|pontos (relevantes|principais)|análise)\b.*\b(reunião|última (reunião|conversa|sessão))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como análise de reunião.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padrão.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que são muito similares ao comando atual para evitar redundância.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padrão: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # Métodos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de análise de reunião.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conteúdo usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inválida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar variáveis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conteúdo com base no prompt, contexto e reunião"""
        try:
            # Supondo que a API espera um dicionário com os parâmetros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisição para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar variáveis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Criação da conclusão do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que você acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["faça um resumo da última reunião.", "tópicos da última reunião", "resuma a última reunião", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Você é responsável por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - Não acrescentar título nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informações acima e forneça uma solução detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Você é responsável analisar com detalhes a reunião de [str_meeting], e fornecer uma longa estória sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precisão da resposta.
            - Não acrescente título nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e faça uma pesquisa online para uma resposta mais precisa das questões de [str_text]
        - Não acrescente título nas respostas.
        
        ------
        [context]: Regras Básicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("Áudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum áudio detectado.")
                return None
            except sr.UnknownValueError:
                print("Não foi possível entender o áudio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configurações da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicionário
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta não está no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API: {e}")
        return None

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: quais IDES já possuem integração com alguma IA que consegue receber o seu código como contexto e fornecer assistencia no desenvolvimento.???

Atualmente só conheço o VScode com o copilot e o gemini integrados.
Assistant: Com base no código fornecido e nas suas instruções, compilei uma lista de IDEs (Ambientes de Desenvolvimento Integrados) que já possuem integração com alguma IA capaz de receber o seu código como contexto e fornecer assistência no desenvolvimento:

*   **VS Code:** Já possui integrações com o GitHub Copilot e o Gemini.

A pesquisa foi realizada utilizando o código fonte para identificar as ferramentas e frameworks mencionados, e, com base nisso, fornecer uma resposta precisa.
User: mesmo instalando o pacote não reconhece.
Assistant: Com base no contexto do código fornecido, aqui estão alguns dos possíveis problemas e soluções que podem causar problemas de reconhecimento de pacotes:

1.  **Problema de ambiente:** Certifique-se de que o pacote google-generativeai esteja instalado no ambiente Python correto. Às vezes, pode ser instalado em um ambiente diferente do que está sendo usado.

    *   **Solução:** Ative o ambiente virtual apropriado e use `pip show google-generativeai` para verificar se o pacote está instalado naquele ambiente. Se não estiver, instale-o usando `pip install google-generativeai`.

2.  **Problema de cache:** Às vezes, o pip pode ter problemas de cache que impedem que ele reconheça pacotes recém-instalados.

    *   **Solução:** Tente atualizar o pip e limpar o cache do pip antes de instalar o pacote novamente. Use os seguintes comandos:

    ```bash
    pip install --upgrade pip
    pip cache purge
    pip install google-generativeai
    ```

3.  **Problema de caminho:** Às vezes, o Python pode não encontrar o pacote instalado porque o caminho de instalação do pacote não está no `sys.path`.

    *   **Solução:** Adicione o diretório do pacote ao `sys.path` em seu script Python ou bloco de anotações. Você pode encontrar o diretório de instalação do pacote usando `pip show google-generativeai -f`. Em seguida, adicione esse caminho ao `sys.path` em seu script:

    ```python
    import sys
    sys.path.append('/caminho/para/site-packages')  # Substitua com o caminho real
    import google.generativeai as genai
    ```

4.  **Problema de nome:** Às vezes, o nome do pacote no código não corresponde ao nome real do pacote instalado.

    *   **Solução:** Verifique se o nome do pacote está correto em sua instrução `import`. Nesse caso, ele deve ser `import google.generativeai as genai`.

5.  **Problema de versão:** Às vezes, os pacotes podem apresentar problemas de compatibilidade com outras versões de pacotes ou com a versão Python que você está usando.

    *   **Solução:** Considere usar uma versão diferente do pacote que seja compatível com sua configuração.

6.  **IDE ou problema de editor:** Às vezes, o IDE ou editor que você está usando pode não ser capaz de reconhecer pacotes recém-instalados.

    *   **Solução:** Reinicie o IDE ou editor ou recarregue o ambiente Python.
User: parece que é algo com ambiente...porque outras aplicações estão funcionando com essa lib instalada.
2025-04-14 14:53:52,501 [INFO] Resposta da IA (imagem): Compreendo. Se outras aplicações estão funcionando corretamente com a `google-generativeai` instalada, o problema provavelmente reside no ambiente específico em que a aplicação atual está sendo executada. Aqui estão algumas áreas para investigar e possíveis soluções:

1.  **Ambiente Virtual:**
    *   **Verificação:** Certifique-se de que a aplicação esteja rodando dentro do ambiente virtual correto. Se você estiver utilizando um ambiente virtual (como `venv` ou `conda`), ative-o antes de executar a aplicação.

    ```bash
    # Para venv
    source venv/bin/activate  # ou venv\Scripts\activate no Windows

    # Para conda
    conda activate meu_ambiente
    ```
    *   **Confirmação:** Verifique se o ambiente virtual ativado é o mesmo onde `google-generativeai` está instalado. Utilize `pip show google-generativeai` para confirmar a instalação e o caminho do pacote.

2.  **Caminhos de Importação:**
    *   **Investigação:** Verifique se o caminho para o diretório do pacote está corretamente configurado no `sys.path`. Isso pode variar entre diferentes ambientes e instalações.
    *   **Solução:** Adicione o caminho correto ao `sys.path` no início do script. Para descobrir o caminho correto, execute o seguinte no ambiente onde a lib funciona:

    ```python
    import google.generativeai
    print(google.generativeai.__file__)
    ```
    Copie o diretório onde o arquivo `__init__.py` está localizado e adicione ao `sys.path` no seu script.

    ```python
    import sys
    sys.path.append('/caminho/para/o/site-packages')
    import google.generativeai as genai
    ```

3.  **Conflitos de Dependências:**
    *   **Investigação:** Pode haver conflitos entre as dependências de diferentes pacotes no seu ambiente.
    *   **Solução:** Tente criar um ambiente virtual limpo e instalar apenas as dependências necessárias para a sua aplicação atual.

    ```bash
    python -m venv novo_ambiente
    source novo_ambiente/bin/activate  # ou novo_ambiente\Scripts\activate no Windows
    pip install google-generativeai
    # Instale outros pacotes necessários para esta aplicação
    ```

4.  **Cache do Editor/IDE:**
    *   **Investigação:** O editor ou IDE pode estar usando um cache que não está atualizado com as alterações no ambiente.
    *   **Solução:** Reinicie o editor ou IDE, ou force a atualização do cache. No VS Code, isso pode ser feito através do comando "Developer: Reload Window".

5.  **Variáveis de Ambiente:**
    *   **Investigação:** Verifique se todas as variáveis de ambiente necessárias (como `API_KEY_GEMINI`) estão corretamente configuradas e acessíveis no ambiente em que a aplicação está rodando.
    *   **Solução:** Certifique-se de que as variáveis de ambiente estão definidas corretamente e recarregue o ambiente se necessário.

6.  **Permissões:**
    *   **Investigação:** Verifique se o usuário que está executando a aplicação tem permissões para acessar os arquivos e diretórios necessários.
    *   **Solução:** Ajuste as permissões conforme necessário para garantir que o usuário tenha acesso aos arquivos e diretórios requeridos.

7.  **Versões do Python:**
    *   **Investigação:** Confirme que a versão do Python em uso no ambiente é compatível com a `google-generativeai`.
    *   **Solução:** Verifique a versão do Python utilizada (`python --version`) e, se necessário, instale uma versão compatível com a biblioteca.

Com esses passos, você deve conseguir identificar e resolver o problema de ambiente que está impedindo a sua aplicação de reconhecer o pacote `google-generativeai`.
2025-04-14 14:53:52,604 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:53:52,607 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:53:52,609 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:53:52,611 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:53:52,613 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:53:52,615 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:53:52,616 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:53:52,618 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:53:52,619 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:53:52,621 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:53:52,622 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:53:52,624 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:53:52,625 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:53:52,627 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:53:52,628 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:53:52,629 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:53:52,630 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:53:52,631 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:53:52,633 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:53:52,634 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:53:52,635 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:53:52,636 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:53:52,637 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:53:52,639 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:53:52,640 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:53:52,641 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:53:52,642 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:53:52,644 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:53:52,646 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:53:52,647 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:53:52,648 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:53:52,650 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:53:52,651 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:53:52,653 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:53:52,654 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:53:52,656 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:53:52,657 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:53:52,659 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:53:52,661 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:53:52,663 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:53:52,665 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:53:52,667 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:53:52,668 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:53:52,670 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:53:52,671 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:53:52,672 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:53:52,673 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:53:52,674 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:53:52,675 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:53:52,676 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:53:52,677 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:53:52,678 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:53:52,679 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:55:13,777 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:55:13,779 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:55:13,781 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:55:13,783 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:55:13,785 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:55:13,787 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:55:13,788 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:55:13,790 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:55:13,791 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:55:13,793 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:55:13,795 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:55:13,796 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:55:13,798 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:55:13,800 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:55:13,802 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:55:13,803 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:55:13,804 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:55:13,806 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:55:13,807 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:55:13,809 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:55:13,811 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:55:13,813 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:55:13,816 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:55:13,818 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:55:13,820 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:55:13,823 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:55:13,825 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:55:13,827 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:55:13,829 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:55:13,831 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:55:13,832 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:55:13,834 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:55:13,836 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:55:13,839 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:55:13,840 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:55:13,842 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:55:13,843 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:55:13,844 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:55:13,846 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:55:13,847 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:55:13,848 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:55:13,850 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:55:13,851 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:55:13,854 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:55:13,855 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:55:13,856 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:55:13,858 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:55:13,859 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:55:13,861 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:55:13,862 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:55:13,863 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:55:13,865 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:55:13,867 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:55:14,028 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:55:14,029 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:55:14,031 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:55:14,033 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:55:14,035 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:55:14,037 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:55:14,040 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:55:14,043 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:55:14,045 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:55:14,047 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:55:14,049 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:55:14,051 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:55:14,053 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:55:14,055 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:55:14,057 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:55:14,060 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:55:14,061 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:55:14,063 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:55:14,065 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:55:14,066 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:55:14,068 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:55:14,080 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:55:14,082 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:55:14,084 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:55:14,090 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:55:14,092 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:55:14,093 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:55:14,095 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:55:14,096 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:55:14,098 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:55:14,099 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:55:14,100 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:55:14,104 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:55:14,106 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:55:14,107 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:55:14,109 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:55:14,110 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:55:14,111 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:55:14,113 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:55:14,114 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:55:14,115 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:55:14,116 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:55:14,118 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:55:14,121 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:55:14,123 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:55:14,124 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:55:14,125 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:55:14,127 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:55:14,128 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:55:14,130 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:55:14,131 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:55:14,132 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:55:14,134 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:55:23,711 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:55:23,713 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:55:23,714 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:55:23,716 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:55:23,718 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:55:23,719 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:55:23,721 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:55:23,722 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:55:23,724 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:55:23,726 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:55:23,727 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:55:23,729 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:55:23,730 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:55:23,732 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:55:23,734 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:55:23,735 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:55:23,736 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:55:23,737 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:55:23,739 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:55:23,740 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:55:23,741 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:55:23,743 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:55:23,744 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:55:23,748 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:55:23,750 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:55:23,752 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:55:23,754 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:55:23,757 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:55:23,759 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:55:23,762 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:55:23,765 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:55:23,766 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:55:23,768 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:55:23,771 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:55:23,773 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:55:23,774 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:55:23,776 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:55:23,778 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:55:23,780 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:55:23,781 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:55:23,783 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:55:23,784 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:55:23,786 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:55:23,787 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:55:23,788 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:55:23,790 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:55:23,792 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:55:23,793 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:55:23,794 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:55:23,795 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:55:23,797 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:55:23,798 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:55:23,799 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:55:23,921 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:55:23,923 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:55:23,925 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:55:23,927 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:55:23,929 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:55:23,931 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:55:23,932 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:55:23,934 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:55:23,936 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:55:23,940 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:55:23,942 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:55:23,943 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:55:23,945 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:55:23,947 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:55:23,949 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:55:23,951 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:55:23,954 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:55:23,955 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:55:23,957 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:55:23,958 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:55:23,960 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:55:23,962 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:55:23,964 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:55:23,966 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:55:23,968 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:55:23,970 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:55:23,973 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:55:23,975 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:55:23,976 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:55:23,978 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:55:23,980 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:55:23,981 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:55:23,983 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:55:23,985 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:55:23,988 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:55:23,990 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:55:23,991 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:55:24,003 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:55:24,006 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:55:24,008 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:55:24,011 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:55:24,013 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:55:24,014 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:55:24,015 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:55:24,017 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:55:24,019 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:55:24,021 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:55:24,022 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:55:24,023 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:55:24,025 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:55:24,026 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:55:24,027 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:55:24,028 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:55:24,048 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\agents_ia\comandAI\assets\20250414145524_clipboard_20250414145513.png, Prompt: Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diretório atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Definição dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Criação dos diretórios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configuração do layout do Streamlit
st.set_page_config(
    page_title="Análise de Ações",
    layout="wide",
    page_icon="📈"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscilações

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Período', 'Oscilação'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("📊 Dashboard de Análise de Ações")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma ação", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"📌 Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("🔍 Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descrição', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa ação.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("💰 Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("📉 Oscilações")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("📈 Tabela Geral de Ações")
        st.dataframe(self.acoes)

# Execução
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para análise fundamentalista de ações da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"Não foram encontrados arquivos .txt no diretório {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conteúdo do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conteúdo HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conteúdo HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se está correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configuração de vídeos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configuração de áudios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcrições e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diretório
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diretório.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do último arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conteúdo do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimensões do embedding:", embedding.shape)

# Caminho do diretório de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o último embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configuração de transcrições e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diretório onde está procurando as transcrições
        print(f"Diretório de entrada das transcrições: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcrição no diretório de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcrição encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo não encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcrição do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} está vazio.")
                    return

            # Gerar o embedding da transcrição
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcrição e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de áudio
    with sr.Microphone() as source:
        print("Diga algo em português...")

        while True:
            try:
                # Escuta o áudio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {texto_portugues}")

                # Traduz o texto para o inglês
                traducao = translator.translate(texto_portugues)
                print(f"Tradução para o inglês: {traducao}")

            except sr.UnknownValueError:
                print("Não foi possível entender o áudio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usuário")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configurações de áudio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de áudio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em português...")

try:
    audio_buffer = []

    while True:
        # Lê dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o áudio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o áudio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradução
            print(f"Tradução para o inglês: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usuário")

    # Fecha o stream de áudio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O vídeo {video_path} não contém áudio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de vídeo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de vídeo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de vídeo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Conversão de vídeo para áudio concluída!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configurações da API
handler = genai('gemini-1.5-flash')

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Função para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conteúdo com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares únicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcrição obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' não é uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuniões: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de reuniões: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|tópicos da|pontos (relevantes|principais)|análise)\b.*\b(reunião|última (reunião|conversa|sessão))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como análise de reunião.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padrão.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que são muito similares ao comando atual para evitar redundância.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padrão: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # Métodos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de análise de reunião.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conteúdo usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inválida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar variáveis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conteúdo com base no prompt, contexto e reunião"""
        try:
            # Supondo que a API espera um dicionário com os parâmetros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisição para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar variáveis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Criação da conclusão do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que você acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["faça um resumo da última reunião.", "tópicos da última reunião", "resuma a última reunião", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Você é responsável por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - Não acrescentar título nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informações acima e forneça uma solução detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Você é responsável analisar com detalhes a reunião de [str_meeting], e fornecer uma longa estória sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precisão da resposta.
            - Não acrescente título nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e faça uma pesquisa online para uma resposta mais precisa das questões de [str_text]
        - Não acrescente título nas respostas.
        
        ------
        [context]: Regras Básicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("Áudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum áudio detectado.")
                return None
            except sr.UnknownValueError:
                print("Não foi possível entender o áudio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configurações da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicionário
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta não está no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API: {e}")
        return None

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: quais IDES já possuem integração com alguma IA que consegue receber o seu código como contexto e fornecer assistencia no desenvolvimento.???

Atualmente só conheço o VScode com o copilot e o gemini integrados.
Assistant: Com base no código fornecido e nas suas instruções, compilei uma lista de IDEs (Ambientes de Desenvolvimento Integrados) que já possuem integração com alguma IA capaz de receber o seu código como contexto e fornecer assistência no desenvolvimento:

*   **VS Code:** Já possui integrações com o GitHub Copilot e o Gemini.

A pesquisa foi realizada utilizando o código fonte para identificar as ferramentas e frameworks mencionados, e, com base nisso, fornecer uma resposta precisa.
User: mesmo instalando o pacote não reconhece.
Assistant: Com base no contexto do código fornecido, aqui estão alguns dos possíveis problemas e soluções que podem causar problemas de reconhecimento de pacotes:

1.  **Problema de ambiente:** Certifique-se de que o pacote google-generativeai esteja instalado no ambiente Python correto. Às vezes, pode ser instalado em um ambiente diferente do que está sendo usado.

    *   **Solução:** Ative o ambiente virtual apropriado e use `pip show google-generativeai` para verificar se o pacote está instalado naquele ambiente. Se não estiver, instale-o usando `pip install google-generativeai`.

2.  **Problema de cache:** Às vezes, o pip pode ter problemas de cache que impedem que ele reconheça pacotes recém-instalados.

    *   **Solução:** Tente atualizar o pip e limpar o cache do pip antes de instalar o pacote novamente. Use os seguintes comandos:

    ```bash
    pip install --upgrade pip
    pip cache purge
    pip install google-generativeai
    ```

3.  **Problema de caminho:** Às vezes, o Python pode não encontrar o pacote instalado porque o caminho de instalação do pacote não está no `sys.path`.

    *   **Solução:** Adicione o diretório do pacote ao `sys.path` em seu script Python ou bloco de anotações. Você pode encontrar o diretório de instalação do pacote usando `pip show google-generativeai -f`. Em seguida, adicione esse caminho ao `sys.path` em seu script:

    ```python
    import sys
    sys.path.append('/caminho/para/site-packages')  # Substitua com o caminho real
    import google.generativeai as genai
    ```

4.  **Problema de nome:** Às vezes, o nome do pacote no código não corresponde ao nome real do pacote instalado.

    *   **Solução:** Verifique se o nome do pacote está correto em sua instrução `import`. Nesse caso, ele deve ser `import google.generativeai as genai`.

5.  **Problema de versão:** Às vezes, os pacotes podem apresentar problemas de compatibilidade com outras versões de pacotes ou com a versão Python que você está usando.

    *   **Solução:** Considere usar uma versão diferente do pacote que seja compatível com sua configuração.

6.  **IDE ou problema de editor:** Às vezes, o IDE ou editor que você está usando pode não ser capaz de reconhecer pacotes recém-instalados.

    *   **Solução:** Reinicie o IDE ou editor ou recarregue o ambiente Python.
User: parece que é algo com ambiente...porque outras aplicações estão funcionando com essa lib instalada.
Assistant: Compreendo. Se outras aplicações estão funcionando corretamente com a `google-generativeai` instalada, o problema provavelmente reside no ambiente específico em que a aplicação atual está sendo executada. Aqui estão algumas áreas para investigar e possíveis soluções:

1.  **Ambiente Virtual:**
    *   **Verificação:** Certifique-se de que a aplicação esteja rodando dentro do ambiente virtual correto. Se você estiver utilizando um ambiente virtual (como `venv` ou `conda`), ative-o antes de executar a aplicação.

    ```bash
    # Para venv
    source venv/bin/activate  # ou venv\Scripts\activate no Windows

    # Para conda
    conda activate meu_ambiente
    ```
    *   **Confirmação:** Verifique se o ambiente virtual ativado é o mesmo onde `google-generativeai` está instalado. Utilize `pip show google-generativeai` para confirmar a instalação e o caminho do pacote.

2.  **Caminhos de Importação:**
    *   **Investigação:** Verifique se o caminho para o diretório do pacote está corretamente configurado no `sys.path`. Isso pode variar entre diferentes ambientes e instalações.
    *   **Solução:** Adicione o caminho correto ao `sys.path` no início do script. Para descobrir o caminho correto, execute o seguinte no ambiente onde a lib funciona:

    ```python
    import google.generativeai
    print(google.generativeai.__file__)
    ```
    Copie o diretório onde o arquivo `__init__.py` está localizado e adicione ao `sys.path` no seu script.

    ```python
    import sys
    sys.path.append('/caminho/para/o/site-packages')
    import google.generativeai as genai
    ```

3.  **Conflitos de Dependências:**
    *   **Investigação:** Pode haver conflitos entre as dependências de diferentes pacotes no seu ambiente.
    *   **Solução:** Tente criar um ambiente virtual limpo e instalar apenas as dependências necessárias para a sua aplicação atual.

    ```bash
    python -m venv novo_ambiente
    source novo_ambiente/bin/activate  # ou novo_ambiente\Scripts\activate no Windows
    pip install google-generativeai
    # Instale outros pacotes necessários para esta aplicação
    ```

4.  **Cache do Editor/IDE:**
    *   **Investigação:** O editor ou IDE pode estar usando um cache que não está atualizado com as alterações no ambiente.
    *   **Solução:** Reinicie o editor ou IDE, ou force a atualização do cache. No VS Code, isso pode ser feito através do comando "Developer: Reload Window".

5.  **Variáveis de Ambiente:**
    *   **Investigação:** Verifique se todas as variáveis de ambiente necessárias (como `API_KEY_GEMINI`) estão corretamente configuradas e acessíveis no ambiente em que a aplicação está rodando.
    *   **Solução:** Certifique-se de que as variáveis de ambiente estão definidas corretamente e recarregue o ambiente se necessário.

6.  **Permissões:**
    *   **Investigação:** Verifique se o usuário que está executando a aplicação tem permissões para acessar os arquivos e diretórios necessários.
    *   **Solução:** Ajuste as permissões conforme necessário para garantir que o usuário tenha acesso aos arquivos e diretórios requeridos.

7.  **Versões do Python:**
    *   **Investigação:** Confirme que a versão do Python em uso no ambiente é compatível com a `google-generativeai`.
    *   **Solução:** Verifique a versão do Python utilizada (`python --version`) e, se necessário, instale uma versão compatível com a biblioteca.

Com esses passos, você deve conseguir identificar e resolver o problema de ambiente que está impedindo a sua aplicação de reconhecer o pacote `google-generativeai`.
User: ocorreu esse erro na imagem.
2025-04-14 14:55:27,906 [INFO] Resposta da IA (imagem): The image shows that the virtual environment is not activated.

Here's how to fix this:

1.  **Verify the virtual environment's `venv` directory:** The error message  `bash: venv/bin/activate: No such file or directory` indicates that the `activate` script is not found in `venv/bin/`. This usually happens if the virtual environment was not created properly or the `venv` directory was renamed or deleted.  You can see from the file explorer in the image that there is a `venv` directory in the project root.
2.  **Check the correct activation command:**  The command `source venv/bin/activate` is usually used on Linux/macOS systems. For Windows, the activation script is located at `venv\Scripts\activate`. Therefore, try running  `venv\Scripts\activate` in the terminal instead.
3.  **Recreate the virtual environment (if necessary):** If you've confirmed that the virtual environment is broken, delete the existing `venv` directory and create a new virtual environment using  `python -m venv venv`. After creating the new environment, try activating it again.

After resolving the issue with the virtual environment, install all dependencies: `pip install -r requirements.txt` if you have a requirements file or install them manually. Then, rerun the application.
2025-04-14 14:55:28,008 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:55:28,009 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:55:28,011 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:55:28,012 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:55:28,014 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:55:28,015 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:55:28,016 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:55:28,018 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:55:28,019 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:55:28,020 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:55:28,022 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:55:28,023 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:55:28,024 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:55:28,025 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:55:28,026 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:55:28,027 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:55:28,028 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:55:28,029 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:55:28,030 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:55:28,031 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:55:28,032 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:55:28,034 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:55:28,035 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:55:28,036 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:55:28,038 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:55:28,039 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:55:28,040 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:55:28,041 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:55:28,042 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:55:28,043 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:55:28,044 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:55:28,045 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:55:28,046 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:55:28,047 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:55:28,048 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:55:28,049 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:55:28,050 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:55:28,052 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:55:28,053 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:55:28,054 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:55:28,055 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:55:28,057 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:55:28,058 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:55:28,059 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:55:28,060 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:55:28,061 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:55:28,061 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:55:28,062 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:55:28,063 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:55:28,064 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:55:28,065 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:55:28,066 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:55:28,067 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:56:22,227 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:56:22,229 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:56:22,230 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:56:22,232 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:56:22,233 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:56:22,235 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:56:22,236 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:56:22,238 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:56:22,239 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:56:22,240 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:56:22,241 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:56:22,242 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:56:22,244 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:56:22,245 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:56:22,246 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:56:22,248 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:56:22,250 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:56:22,251 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:56:22,253 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:56:22,255 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:56:22,256 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:56:22,257 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:56:22,259 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:56:22,261 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:56:22,264 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:56:22,265 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:56:22,267 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:56:22,268 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:56:22,269 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:56:22,270 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:56:22,271 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:56:22,272 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:56:22,273 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:56:22,275 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:56:22,277 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:56:22,278 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:56:22,279 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:56:22,281 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:56:22,283 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:56:22,284 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:56:22,285 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:56:22,286 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:56:22,287 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:56:22,289 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:56:22,290 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:56:22,291 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:56:22,292 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:56:22,294 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:56:22,296 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:56:22,297 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:56:22,298 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:56:22,299 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:56:22,300 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:56:22,418 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:56:22,420 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:56:22,421 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:56:22,423 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:56:22,425 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:56:22,427 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:56:22,429 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:56:22,430 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:56:22,431 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:56:22,433 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:56:22,435 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:56:22,436 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:56:22,437 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:56:22,439 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:56:22,440 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:56:22,442 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:56:22,444 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:56:22,445 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:56:22,447 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:56:22,448 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:56:22,450 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:56:22,451 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:56:22,452 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:56:22,454 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:56:22,455 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:56:22,459 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:56:22,461 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:56:22,463 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:56:22,466 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:56:22,467 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:56:22,469 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:56:22,471 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:56:22,473 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:56:22,475 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:56:22,477 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:56:22,479 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:56:22,480 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:56:22,482 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:56:22,484 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:56:22,485 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:56:22,487 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:56:22,489 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:56:22,490 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:56:22,492 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:56:22,494 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:56:22,496 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:56:22,498 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:56:22,500 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:56:22,501 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:56:22,502 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:56:22,503 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:56:22,504 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:56:22,505 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:56:22,527 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\agents_ia\comandAI\assets\20250414145622_clipboard_20250414145513.png, Prompt: Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diretório atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Definição dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Criação dos diretórios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configuração do layout do Streamlit
st.set_page_config(
    page_title="Análise de Ações",
    layout="wide",
    page_icon="📈"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscilações

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Período', 'Oscilação'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("📊 Dashboard de Análise de Ações")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma ação", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"📌 Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("🔍 Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descrição', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa ação.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("💰 Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("📉 Oscilações")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("📈 Tabela Geral de Ações")
        st.dataframe(self.acoes)

# Execução
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para análise fundamentalista de ações da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"Não foram encontrados arquivos .txt no diretório {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conteúdo do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conteúdo HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conteúdo HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se está correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configuração de vídeos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configuração de áudios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcrições e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diretório
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diretório.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do último arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conteúdo do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimensões do embedding:", embedding.shape)

# Caminho do diretório de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o último embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configuração de transcrições e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diretório onde está procurando as transcrições
        print(f"Diretório de entrada das transcrições: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcrição no diretório de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcrição encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo não encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcrição do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} está vazio.")
                    return

            # Gerar o embedding da transcrição
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcrição e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de áudio
    with sr.Microphone() as source:
        print("Diga algo em português...")

        while True:
            try:
                # Escuta o áudio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {texto_portugues}")

                # Traduz o texto para o inglês
                traducao = translator.translate(texto_portugues)
                print(f"Tradução para o inglês: {traducao}")

            except sr.UnknownValueError:
                print("Não foi possível entender o áudio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usuário")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configurações de áudio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de áudio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em português...")

try:
    audio_buffer = []

    while True:
        # Lê dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o áudio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o áudio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradução
            print(f"Tradução para o inglês: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usuário")

    # Fecha o stream de áudio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O vídeo {video_path} não contém áudio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de vídeo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de vídeo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de vídeo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Conversão de vídeo para áudio concluída!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configurações da API
handler = genai('gemini-1.5-flash')

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Função para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conteúdo com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares únicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcrição obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' não é uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuniões: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de reuniões: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|tópicos da|pontos (relevantes|principais)|análise)\b.*\b(reunião|última (reunião|conversa|sessão))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como análise de reunião.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padrão.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que são muito similares ao comando atual para evitar redundância.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padrão: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # Métodos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de análise de reunião.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conteúdo usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inválida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar variáveis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conteúdo com base no prompt, contexto e reunião"""
        try:
            # Supondo que a API espera um dicionário com os parâmetros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisição para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar variáveis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Criação da conclusão do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que você acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["faça um resumo da última reunião.", "tópicos da última reunião", "resuma a última reunião", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Você é responsável por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - Não acrescentar título nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informações acima e forneça uma solução detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Você é responsável analisar com detalhes a reunião de [str_meeting], e fornecer uma longa estória sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precisão da resposta.
            - Não acrescente título nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e faça uma pesquisa online para uma resposta mais precisa das questões de [str_text]
        - Não acrescente título nas respostas.
        
        ------
        [context]: Regras Básicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("Áudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum áudio detectado.")
                return None
            except sr.UnknownValueError:
                print("Não foi possível entender o áudio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configurações da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicionário
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta não está no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API: {e}")
        return None

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: quais IDES já possuem integração com alguma IA que consegue receber o seu código como contexto e fornecer assistencia no desenvolvimento.???

Atualmente só conheço o VScode com o copilot e o gemini integrados.
Assistant: Com base no código fornecido e nas suas instruções, compilei uma lista de IDEs (Ambientes de Desenvolvimento Integrados) que já possuem integração com alguma IA capaz de receber o seu código como contexto e fornecer assistência no desenvolvimento:

*   **VS Code:** Já possui integrações com o GitHub Copilot e o Gemini.

A pesquisa foi realizada utilizando o código fonte para identificar as ferramentas e frameworks mencionados, e, com base nisso, fornecer uma resposta precisa.
User: mesmo instalando o pacote não reconhece.
Assistant: Com base no contexto do código fornecido, aqui estão alguns dos possíveis problemas e soluções que podem causar problemas de reconhecimento de pacotes:

1.  **Problema de ambiente:** Certifique-se de que o pacote google-generativeai esteja instalado no ambiente Python correto. Às vezes, pode ser instalado em um ambiente diferente do que está sendo usado.

    *   **Solução:** Ative o ambiente virtual apropriado e use `pip show google-generativeai` para verificar se o pacote está instalado naquele ambiente. Se não estiver, instale-o usando `pip install google-generativeai`.

2.  **Problema de cache:** Às vezes, o pip pode ter problemas de cache que impedem que ele reconheça pacotes recém-instalados.

    *   **Solução:** Tente atualizar o pip e limpar o cache do pip antes de instalar o pacote novamente. Use os seguintes comandos:

    ```bash
    pip install --upgrade pip
    pip cache purge
    pip install google-generativeai
    ```

3.  **Problema de caminho:** Às vezes, o Python pode não encontrar o pacote instalado porque o caminho de instalação do pacote não está no `sys.path`.

    *   **Solução:** Adicione o diretório do pacote ao `sys.path` em seu script Python ou bloco de anotações. Você pode encontrar o diretório de instalação do pacote usando `pip show google-generativeai -f`. Em seguida, adicione esse caminho ao `sys.path` em seu script:

    ```python
    import sys
    sys.path.append('/caminho/para/site-packages')  # Substitua com o caminho real
    import google.generativeai as genai
    ```

4.  **Problema de nome:** Às vezes, o nome do pacote no código não corresponde ao nome real do pacote instalado.

    *   **Solução:** Verifique se o nome do pacote está correto em sua instrução `import`. Nesse caso, ele deve ser `import google.generativeai as genai`.

5.  **Problema de versão:** Às vezes, os pacotes podem apresentar problemas de compatibilidade com outras versões de pacotes ou com a versão Python que você está usando.

    *   **Solução:** Considere usar uma versão diferente do pacote que seja compatível com sua configuração.

6.  **IDE ou problema de editor:** Às vezes, o IDE ou editor que você está usando pode não ser capaz de reconhecer pacotes recém-instalados.

    *   **Solução:** Reinicie o IDE ou editor ou recarregue o ambiente Python.
User: parece que é algo com ambiente...porque outras aplicações estão funcionando com essa lib instalada.
Assistant: Compreendo. Se outras aplicações estão funcionando corretamente com a `google-generativeai` instalada, o problema provavelmente reside no ambiente específico em que a aplicação atual está sendo executada. Aqui estão algumas áreas para investigar e possíveis soluções:

1.  **Ambiente Virtual:**
    *   **Verificação:** Certifique-se de que a aplicação esteja rodando dentro do ambiente virtual correto. Se você estiver utilizando um ambiente virtual (como `venv` ou `conda`), ative-o antes de executar a aplicação.

    ```bash
    # Para venv
    source venv/bin/activate  # ou venv\Scripts\activate no Windows

    # Para conda
    conda activate meu_ambiente
    ```
    *   **Confirmação:** Verifique se o ambiente virtual ativado é o mesmo onde `google-generativeai` está instalado. Utilize `pip show google-generativeai` para confirmar a instalação e o caminho do pacote.

2.  **Caminhos de Importação:**
    *   **Investigação:** Verifique se o caminho para o diretório do pacote está corretamente configurado no `sys.path`. Isso pode variar entre diferentes ambientes e instalações.
    *   **Solução:** Adicione o caminho correto ao `sys.path` no início do script. Para descobrir o caminho correto, execute o seguinte no ambiente onde a lib funciona:

    ```python
    import google.generativeai
    print(google.generativeai.__file__)
    ```
    Copie o diretório onde o arquivo `__init__.py` está localizado e adicione ao `sys.path` no seu script.

    ```python
    import sys
    sys.path.append('/caminho/para/o/site-packages')
    import google.generativeai as genai
    ```

3.  **Conflitos de Dependências:**
    *   **Investigação:** Pode haver conflitos entre as dependências de diferentes pacotes no seu ambiente.
    *   **Solução:** Tente criar um ambiente virtual limpo e instalar apenas as dependências necessárias para a sua aplicação atual.

    ```bash
    python -m venv novo_ambiente
    source novo_ambiente/bin/activate  # ou novo_ambiente\Scripts\activate no Windows
    pip install google-generativeai
    # Instale outros pacotes necessários para esta aplicação
    ```

4.  **Cache do Editor/IDE:**
    *   **Investigação:** O editor ou IDE pode estar usando um cache que não está atualizado com as alterações no ambiente.
    *   **Solução:** Reinicie o editor ou IDE, ou force a atualização do cache. No VS Code, isso pode ser feito através do comando "Developer: Reload Window".

5.  **Variáveis de Ambiente:**
    *   **Investigação:** Verifique se todas as variáveis de ambiente necessárias (como `API_KEY_GEMINI`) estão corretamente configuradas e acessíveis no ambiente em que a aplicação está rodando.
    *   **Solução:** Certifique-se de que as variáveis de ambiente estão definidas corretamente e recarregue o ambiente se necessário.

6.  **Permissões:**
    *   **Investigação:** Verifique se o usuário que está executando a aplicação tem permissões para acessar os arquivos e diretórios necessários.
    *   **Solução:** Ajuste as permissões conforme necessário para garantir que o usuário tenha acesso aos arquivos e diretórios requeridos.

7.  **Versões do Python:**
    *   **Investigação:** Confirme que a versão do Python em uso no ambiente é compatível com a `google-generativeai`.
    *   **Solução:** Verifique a versão do Python utilizada (`python --version`) e, se necessário, instale uma versão compatível com a biblioteca.

Com esses passos, você deve conseguir identificar e resolver o problema de ambiente que está impedindo a sua aplicação de reconhecer o pacote `google-generativeai`.
User: ocorreu esse erro na imagem.
Assistant: The image shows that the virtual environment is not activated.

Here's how to fix this:

1.  **Verify the virtual environment's `venv` directory:** The error message  `bash: venv/bin/activate: No such file or directory` indicates that the `activate` script is not found in `venv/bin/`. This usually happens if the virtual environment was not created properly or the `venv` directory was renamed or deleted.  You can see from the file explorer in the image that there is a `venv` directory in the project root.
2.  **Check the correct activation command:**  The command `source venv/bin/activate` is usually used on Linux/macOS systems. For Windows, the activation script is located at `venv\Scripts\activate`. Therefore, try running  `venv\Scripts\activate` in the terminal instead.
3.  **Recreate the virtual environment (if necessary):** If you've confirmed that the virtual environment is broken, delete the existing `venv` directory and create a new virtual environment using  `python -m venv venv`. After creating the new environment, try activating it again.

After resolving the issue with the virtual environment, install all dependencies: `pip install -r requirements.txt` if you have a requirements file or install them manually. Then, rerun the application.
User: estou usando windows... mas no terminal é o git bash
2025-04-14 14:56:29,119 [INFO] Resposta da IA (imagem): Okay, I see the problem. Even though you're on Windows, because you're using Git Bash as your terminal, you need to use the Linux-style activation command.  The fact that it's not working suggests that *either* the `venv` directory doesn't contain the `bin/activate` script (as the error message says) *or* there's some kind of permission or execution issue with the script itself in the Git Bash environment.

Here's a breakdown of the likely problem and solutions:

**The Problem:**

Git Bash (MinGW64 specifically) is a Unix-like environment on Windows. While it uses many Linux commands, it still operates within the Windows file system.  Therefore, even when using `source venv/bin/activate`, Git Bash is looking for a proper executable file inside that bin directory, but in your case, that doesn't exist in the proper format.

**Solutions:**

1.  **Use the `.` command as alias to `source`:**  In some Git Bash setups, `.` is explicitly aliased to `source`. Try using it:

    ```bash
    . venv/bin/activate
    ```

    This is a slight variation of the `source` command, but it might work in your particular Git Bash environment.
2.  **Use the correct script in Git Bash (if it exists):** Some virtual environment setups for Git Bash will create a script that is more appropriate for the shell.  Look within your `venv` directory; in some cases, you will see a file `venv/Scripts/activate`. If this file exists, it should be ran directly in Git Bash. However, if that's the case, the path would more likely be `source venv/Scripts/activate`. In this case, that is *not* what you are looking for. You can use the next solution for the `activate` script.
3.  **Execute the Activate.ps1 Script:** Git Bash can run PowerShell scripts directly. Run the PowerShell script activate.ps1 inside powershell. To do this, you can run `powershell -ExecutionPolicy Bypass -File ./venv/Scripts/Activate.ps1` in your terminal. This method is likely the most reliable method to run virtual environments on Git Bash in Windows.

**Troubleshooting steps to confirm the root cause:**

1.  **List the contents of `venv/bin`:**  In your Git Bash terminal, run the command `ls -l venv/bin`.  Examine the output.  You should see a file named `activate` (without a file extension).  If it's not there, that's your primary problem - the venv wasn't created properly.
2.  **Check permissions:** If the `activate` script *is* there, check its permissions. It should have execute permissions. The output of `ls -l venv/bin` will show you the permissions. If the file doesn't have an `x` in the permissions for the user, group, or others, the script won't be executable.  (Note:  Git Bash's permissions model is somewhat emulated and not always perfectly accurate, but it's worth checking.)

If the directory `venv/bin` does *not* contain the activate script, then you should delete the `venv` folder and re-create the virtual environment.

In summary, even if the most likely solution fails, use the troubleshooting steps to confirm that `venv/bin/activate` is an executable.
2025-04-14 14:56:29,217 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:56:29,219 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:56:29,220 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:56:29,222 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:56:29,223 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:56:29,225 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:56:29,226 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:56:29,228 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:56:29,229 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:56:29,231 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:56:29,233 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:56:29,269 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:56:29,286 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:56:29,288 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:56:29,290 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:56:29,292 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:56:29,294 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:56:29,295 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:56:29,297 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:56:29,298 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:56:29,312 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:56:29,313 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:56:29,314 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:56:29,315 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:56:29,316 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:56:29,317 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:56:29,319 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:56:29,320 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:56:29,321 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:56:29,322 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:56:29,324 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:56:29,325 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:56:29,326 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:56:29,327 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:56:29,329 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:56:29,330 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:56:29,331 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:56:29,332 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:56:29,333 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:56:29,334 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:56:29,335 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:56:29,337 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:56:29,338 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:56:29,339 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:56:29,341 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:56:29,342 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:56:29,344 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:56:29,345 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:56:29,347 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:56:29,348 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:56:29,349 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:56:29,351 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:56:29,352 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:57:16,265 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:57:16,267 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:57:16,269 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:57:16,271 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:57:16,273 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:57:16,275 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:57:16,277 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:57:16,278 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:57:16,281 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:57:16,282 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:57:16,284 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:57:16,285 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:57:16,287 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:57:16,289 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:57:16,291 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:57:16,292 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:57:16,294 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:57:16,295 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:57:16,297 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:57:16,299 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:57:16,301 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:57:16,303 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:57:16,304 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:57:16,306 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:57:16,309 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:57:16,310 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:57:16,312 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:57:16,313 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:57:16,314 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:57:16,316 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:57:16,317 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:57:16,318 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:57:16,320 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:57:16,321 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:57:16,322 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:57:16,324 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:57:16,325 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:57:16,326 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:57:16,327 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:57:16,329 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:57:16,330 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:57:16,332 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:57:16,333 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:57:16,335 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:57:16,337 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:57:16,338 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:57:16,340 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:57:16,341 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:57:16,342 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:57:16,343 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:57:16,344 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:57:16,345 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:57:16,346 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:57:16,440 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:57:16,441 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:57:16,443 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:57:16,444 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:57:16,447 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:57:16,450 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:57:16,452 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:57:16,454 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:57:16,456 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:57:16,457 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:57:16,459 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:57:16,460 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:57:16,462 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:57:16,463 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:57:16,466 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:57:16,467 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:57:16,469 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:57:16,470 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:57:16,472 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:57:16,473 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:57:16,478 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:57:16,484 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:57:16,486 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:57:16,488 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:57:16,490 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:57:16,491 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:57:16,495 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:57:16,497 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:57:16,499 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:57:16,500 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:57:16,502 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:57:16,503 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:57:16,505 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:57:16,506 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:57:16,508 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:57:16,509 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:57:16,510 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:57:16,511 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:57:16,512 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:57:16,513 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:57:16,515 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:57:16,517 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:57:16,520 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:57:16,521 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:57:16,522 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:57:16,524 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:57:16,525 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:57:16,526 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:57:16,527 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:57:16,528 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:57:16,529 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:57:16,530 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:57:16,532 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:57:21,763 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:57:21,764 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:57:21,766 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:57:21,769 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:57:21,772 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:57:21,774 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:57:21,775 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:57:21,777 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:57:21,779 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:57:21,780 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:57:21,783 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:57:21,784 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:57:21,786 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:57:21,787 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:57:21,789 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:57:21,790 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:57:21,801 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:57:21,803 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:57:21,805 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:57:21,810 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:57:21,812 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:57:21,815 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:57:21,818 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:57:21,819 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:57:21,820 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:57:21,821 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:57:21,822 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:57:21,824 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:57:21,825 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:57:21,826 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:57:21,827 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:57:21,828 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:57:21,829 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:57:21,830 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:57:21,832 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:57:21,834 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:57:21,835 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:57:21,836 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:57:21,837 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:57:21,838 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:57:21,839 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:57:21,840 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:57:21,841 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:57:21,843 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:57:21,844 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:57:21,845 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:57:21,846 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:57:21,847 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:57:21,849 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:57:21,851 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:57:21,852 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:57:21,853 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:57:21,854 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:57:22,004 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:57:22,006 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:57:22,007 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:57:22,009 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:57:22,011 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:57:22,013 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:57:22,014 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:57:22,016 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:57:22,018 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:57:22,020 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:57:22,021 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:57:22,022 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:57:22,024 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:57:22,026 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:57:22,028 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:57:22,029 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:57:22,030 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:57:22,032 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:57:22,034 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:57:22,035 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:57:22,037 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:57:22,038 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:57:22,041 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:57:22,042 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:57:22,044 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:57:22,045 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:57:22,047 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:57:22,049 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:57:22,050 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:57:22,052 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:57:22,054 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:57:22,056 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:57:22,058 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:57:22,060 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:57:22,061 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:57:22,062 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:57:22,063 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:57:22,064 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:57:22,066 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:57:22,068 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:57:22,069 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:57:22,070 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:57:22,071 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:57:22,072 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:57:22,074 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:57:22,075 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:57:22,076 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:57:22,077 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:57:22,078 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:57:22,079 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:57:22,080 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:57:22,081 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:57:22,083 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 14:57:22,090 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\agents_ia\comandAI\assets\20250414145722_clipboard_20250414145716.png, Prompt: Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diretório atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Definição dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Criação dos diretórios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configuração do layout do Streamlit
st.set_page_config(
    page_title="Análise de Ações",
    layout="wide",
    page_icon="📈"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscilações

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Período', 'Oscilação'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("📊 Dashboard de Análise de Ações")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma ação", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"📌 Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("🔍 Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descrição', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa ação.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("💰 Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("📉 Oscilações")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("📈 Tabela Geral de Ações")
        st.dataframe(self.acoes)

# Execução
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para análise fundamentalista de ações da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"Não foram encontrados arquivos .txt no diretório {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conteúdo do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conteúdo HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conteúdo HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se está correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configuração de vídeos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configuração de áudios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcrições e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diretório
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diretório.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do último arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conteúdo do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimensões do embedding:", embedding.shape)

# Caminho do diretório de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o último embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configuração de transcrições e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diretório onde está procurando as transcrições
        print(f"Diretório de entrada das transcrições: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcrição no diretório de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcrição encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo não encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcrição do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} está vazio.")
                    return

            # Gerar o embedding da transcrição
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcrição e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de áudio
    with sr.Microphone() as source:
        print("Diga algo em português...")

        while True:
            try:
                # Escuta o áudio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {texto_portugues}")

                # Traduz o texto para o inglês
                traducao = translator.translate(texto_portugues)
                print(f"Tradução para o inglês: {traducao}")

            except sr.UnknownValueError:
                print("Não foi possível entender o áudio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usuário")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configurações de áudio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de áudio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em português...")

try:
    audio_buffer = []

    while True:
        # Lê dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o áudio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o áudio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradução
            print(f"Tradução para o inglês: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usuário")

    # Fecha o stream de áudio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O vídeo {video_path} não contém áudio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de vídeo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de vídeo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de vídeo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Conversão de vídeo para áudio concluída!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configurações da API
handler = genai('gemini-1.5-flash')

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Função para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conteúdo com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares únicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcrição obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' não é uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuniões: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de reuniões: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|tópicos da|pontos (relevantes|principais)|análise)\b.*\b(reunião|última (reunião|conversa|sessão))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como análise de reunião.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padrão.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que são muito similares ao comando atual para evitar redundância.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padrão: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # Métodos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de análise de reunião.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conteúdo usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inválida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar variáveis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conteúdo com base no prompt, contexto e reunião"""
        try:
            # Supondo que a API espera um dicionário com os parâmetros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisição para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar variáveis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Criação da conclusão do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que você acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["faça um resumo da última reunião.", "tópicos da última reunião", "resuma a última reunião", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Você é responsável por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - Não acrescentar título nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informações acima e forneça uma solução detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Você é responsável analisar com detalhes a reunião de [str_meeting], e fornecer uma longa estória sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precisão da resposta.
            - Não acrescente título nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e faça uma pesquisa online para uma resposta mais precisa das questões de [str_text]
        - Não acrescente título nas respostas.
        
        ------
        [context]: Regras Básicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("Áudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum áudio detectado.")
                return None
            except sr.UnknownValueError:
                print("Não foi possível entender o áudio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configurações da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicionário
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta não está no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API: {e}")
        return None

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: quais IDES já possuem integração com alguma IA que consegue receber o seu código como contexto e fornecer assistencia no desenvolvimento.???

Atualmente só conheço o VScode com o copilot e o gemini integrados.
Assistant: Com base no código fornecido e nas suas instruções, compilei uma lista de IDEs (Ambientes de Desenvolvimento Integrados) que já possuem integração com alguma IA capaz de receber o seu código como contexto e fornecer assistência no desenvolvimento:

*   **VS Code:** Já possui integrações com o GitHub Copilot e o Gemini.

A pesquisa foi realizada utilizando o código fonte para identificar as ferramentas e frameworks mencionados, e, com base nisso, fornecer uma resposta precisa.
User: mesmo instalando o pacote não reconhece.
Assistant: Com base no contexto do código fornecido, aqui estão alguns dos possíveis problemas e soluções que podem causar problemas de reconhecimento de pacotes:

1.  **Problema de ambiente:** Certifique-se de que o pacote google-generativeai esteja instalado no ambiente Python correto. Às vezes, pode ser instalado em um ambiente diferente do que está sendo usado.

    *   **Solução:** Ative o ambiente virtual apropriado e use `pip show google-generativeai` para verificar se o pacote está instalado naquele ambiente. Se não estiver, instale-o usando `pip install google-generativeai`.

2.  **Problema de cache:** Às vezes, o pip pode ter problemas de cache que impedem que ele reconheça pacotes recém-instalados.

    *   **Solução:** Tente atualizar o pip e limpar o cache do pip antes de instalar o pacote novamente. Use os seguintes comandos:

    ```bash
    pip install --upgrade pip
    pip cache purge
    pip install google-generativeai
    ```

3.  **Problema de caminho:** Às vezes, o Python pode não encontrar o pacote instalado porque o caminho de instalação do pacote não está no `sys.path`.

    *   **Solução:** Adicione o diretório do pacote ao `sys.path` em seu script Python ou bloco de anotações. Você pode encontrar o diretório de instalação do pacote usando `pip show google-generativeai -f`. Em seguida, adicione esse caminho ao `sys.path` em seu script:

    ```python
    import sys
    sys.path.append('/caminho/para/site-packages')  # Substitua com o caminho real
    import google.generativeai as genai
    ```

4.  **Problema de nome:** Às vezes, o nome do pacote no código não corresponde ao nome real do pacote instalado.

    *   **Solução:** Verifique se o nome do pacote está correto em sua instrução `import`. Nesse caso, ele deve ser `import google.generativeai as genai`.

5.  **Problema de versão:** Às vezes, os pacotes podem apresentar problemas de compatibilidade com outras versões de pacotes ou com a versão Python que você está usando.

    *   **Solução:** Considere usar uma versão diferente do pacote que seja compatível com sua configuração.

6.  **IDE ou problema de editor:** Às vezes, o IDE ou editor que você está usando pode não ser capaz de reconhecer pacotes recém-instalados.

    *   **Solução:** Reinicie o IDE ou editor ou recarregue o ambiente Python.
User: parece que é algo com ambiente...porque outras aplicações estão funcionando com essa lib instalada.
Assistant: Compreendo. Se outras aplicações estão funcionando corretamente com a `google-generativeai` instalada, o problema provavelmente reside no ambiente específico em que a aplicação atual está sendo executada. Aqui estão algumas áreas para investigar e possíveis soluções:

1.  **Ambiente Virtual:**
    *   **Verificação:** Certifique-se de que a aplicação esteja rodando dentro do ambiente virtual correto. Se você estiver utilizando um ambiente virtual (como `venv` ou `conda`), ative-o antes de executar a aplicação.

    ```bash
    # Para venv
    source venv/bin/activate  # ou venv\Scripts\activate no Windows

    # Para conda
    conda activate meu_ambiente
    ```
    *   **Confirmação:** Verifique se o ambiente virtual ativado é o mesmo onde `google-generativeai` está instalado. Utilize `pip show google-generativeai` para confirmar a instalação e o caminho do pacote.

2.  **Caminhos de Importação:**
    *   **Investigação:** Verifique se o caminho para o diretório do pacote está corretamente configurado no `sys.path`. Isso pode variar entre diferentes ambientes e instalações.
    *   **Solução:** Adicione o caminho correto ao `sys.path` no início do script. Para descobrir o caminho correto, execute o seguinte no ambiente onde a lib funciona:

    ```python
    import google.generativeai
    print(google.generativeai.__file__)
    ```
    Copie o diretório onde o arquivo `__init__.py` está localizado e adicione ao `sys.path` no seu script.

    ```python
    import sys
    sys.path.append('/caminho/para/o/site-packages')
    import google.generativeai as genai
    ```

3.  **Conflitos de Dependências:**
    *   **Investigação:** Pode haver conflitos entre as dependências de diferentes pacotes no seu ambiente.
    *   **Solução:** Tente criar um ambiente virtual limpo e instalar apenas as dependências necessárias para a sua aplicação atual.

    ```bash
    python -m venv novo_ambiente
    source novo_ambiente/bin/activate  # ou novo_ambiente\Scripts\activate no Windows
    pip install google-generativeai
    # Instale outros pacotes necessários para esta aplicação
    ```

4.  **Cache do Editor/IDE:**
    *   **Investigação:** O editor ou IDE pode estar usando um cache que não está atualizado com as alterações no ambiente.
    *   **Solução:** Reinicie o editor ou IDE, ou force a atualização do cache. No VS Code, isso pode ser feito através do comando "Developer: Reload Window".

5.  **Variáveis de Ambiente:**
    *   **Investigação:** Verifique se todas as variáveis de ambiente necessárias (como `API_KEY_GEMINI`) estão corretamente configuradas e acessíveis no ambiente em que a aplicação está rodando.
    *   **Solução:** Certifique-se de que as variáveis de ambiente estão definidas corretamente e recarregue o ambiente se necessário.

6.  **Permissões:**
    *   **Investigação:** Verifique se o usuário que está executando a aplicação tem permissões para acessar os arquivos e diretórios necessários.
    *   **Solução:** Ajuste as permissões conforme necessário para garantir que o usuário tenha acesso aos arquivos e diretórios requeridos.

7.  **Versões do Python:**
    *   **Investigação:** Confirme que a versão do Python em uso no ambiente é compatível com a `google-generativeai`.
    *   **Solução:** Verifique a versão do Python utilizada (`python --version`) e, se necessário, instale uma versão compatível com a biblioteca.

Com esses passos, você deve conseguir identificar e resolver o problema de ambiente que está impedindo a sua aplicação de reconhecer o pacote `google-generativeai`.
User: ocorreu esse erro na imagem.
Assistant: The image shows that the virtual environment is not activated.

Here's how to fix this:

1.  **Verify the virtual environment's `venv` directory:** The error message  `bash: venv/bin/activate: No such file or directory` indicates that the `activate` script is not found in `venv/bin/`. This usually happens if the virtual environment was not created properly or the `venv` directory was renamed or deleted.  You can see from the file explorer in the image that there is a `venv` directory in the project root.
2.  **Check the correct activation command:**  The command `source venv/bin/activate` is usually used on Linux/macOS systems. For Windows, the activation script is located at `venv\Scripts\activate`. Therefore, try running  `venv\Scripts\activate` in the terminal instead.
3.  **Recreate the virtual environment (if necessary):** If you've confirmed that the virtual environment is broken, delete the existing `venv` directory and create a new virtual environment using  `python -m venv venv`. After creating the new environment, try activating it again.

After resolving the issue with the virtual environment, install all dependencies: `pip install -r requirements.txt` if you have a requirements file or install them manually. Then, rerun the application.
User: estou usando windows... mas no terminal é o git bash
Assistant: Okay, I see the problem. Even though you're on Windows, because you're using Git Bash as your terminal, you need to use the Linux-style activation command.  The fact that it's not working suggests that *either* the `venv` directory doesn't contain the `bin/activate` script (as the error message says) *or* there's some kind of permission or execution issue with the script itself in the Git Bash environment.

Here's a breakdown of the likely problem and solutions:

**The Problem:**

Git Bash (MinGW64 specifically) is a Unix-like environment on Windows. While it uses many Linux commands, it still operates within the Windows file system.  Therefore, even when using `source venv/bin/activate`, Git Bash is looking for a proper executable file inside that bin directory, but in your case, that doesn't exist in the proper format.

**Solutions:**

1.  **Use the `.` command as alias to `source`:**  In some Git Bash setups, `.` is explicitly aliased to `source`. Try using it:

    ```bash
    . venv/bin/activate
    ```

    This is a slight variation of the `source` command, but it might work in your particular Git Bash environment.
2.  **Use the correct script in Git Bash (if it exists):** Some virtual environment setups for Git Bash will create a script that is more appropriate for the shell.  Look within your `venv` directory; in some cases, you will see a file `venv/Scripts/activate`. If this file exists, it should be ran directly in Git Bash. However, if that's the case, the path would more likely be `source venv/Scripts/activate`. In this case, that is *not* what you are looking for. You can use the next solution for the `activate` script.
3.  **Execute the Activate.ps1 Script:** Git Bash can run PowerShell scripts directly. Run the PowerShell script activate.ps1 inside powershell. To do this, you can run `powershell -ExecutionPolicy Bypass -File ./venv/Scripts/Activate.ps1` in your terminal. This method is likely the most reliable method to run virtual environments on Git Bash in Windows.

**Troubleshooting steps to confirm the root cause:**

1.  **List the contents of `venv/bin`:**  In your Git Bash terminal, run the command `ls -l venv/bin`.  Examine the output.  You should see a file named `activate` (without a file extension).  If it's not there, that's your primary problem - the venv wasn't created properly.
2.  **Check permissions:** If the `activate` script *is* there, check its permissions. It should have execute permissions. The output of `ls -l venv/bin` will show you the permissions. If the file doesn't have an `x` in the permissions for the user, group, or others, the script won't be executable.  (Note:  Git Bash's permissions model is somewhat emulated and not always perfectly accurate, but it's worth checking.)

If the directory `venv/bin` does *not* contain the activate script, then you should delete the `venv` folder and re-create the virtual environment.

In summary, even if the most likely solution fails, use the troubleshooting steps to confirm that `venv/bin/activate` is an executable.
User: Agora deu esse erro.
2025-04-14 14:57:27,432 [INFO] Resposta da IA (imagem): Based on the screenshot provided, you're still facing an issue related to the virtual environment, but now it's a different error. The error is:

```
/c/Users/jfreis/Documents/agents_ia/comandAI/venv/bin/python: can't open file '/c/Users/jfreis/Documents/agents_ia/comandAI/main.py': [Errno 2] No such file or directory
```

Here's what this means and how to troubleshoot it:

**The Problem:**

The error "No such file or directory" clearly indicates that the Python interpreter within your activated virtual environment cannot find the `main.py` file you're trying to execute. This could be due to several reasons:

1.  **Incorrect working directory:** You might be running the command from a directory where `main.py` doesn't exist.

2.  **Incorrect file path:** The file path in your command (`/c/Users/jfreis/Documents/agents_ia/comandAI/main.py`) might be wrong.

3.  **File doesn't exist:** The `main.py` file may simply not exist at the specified location.

**Troubleshooting Steps:**

1.  **Verify file existence:**

    *   Double-check that `main.py` exists at the path `/c/Users/jfreis/Documents/agents_ia/comandAI/main.py`.  Use your file explorer to confirm this.
    *   Make sure you didn't accidentally rename or move the file.

2.  **Check working directory:**

    *   Run the command `pwd` (print working directory) in your Git Bash terminal.  This will tell you the current directory from which you're running the command.
    *   If the current directory isn't `/c/Users/jfreis/Documents/agents_ia/comandAI`, you need to change it. You can use the `cd` command to navigate to the correct directory:
    ```bash
    cd /c/Users/jfreis/Documents/agents_ia/comandAI
    ```

3.  **Check the command:**
If your pwd is on directory, check the command used to execute the file. If you executed the command like `python /c/Users/jfreis/Documents/agents_ia/comandAI/main.py`, you can simply run `python main.py`.

4.  **Double-check case sensitivity:**  Git Bash, being a Unix-like environment, is case-sensitive. Make sure the file name "main.py" matches the case exactly.

**Example:**

Let's say `pwd` shows that you're currently in `/c/Users/jfreis/`

*   You would run:

    ```bash
    cd Documents/agents_ia/comandAI
    python main.py
    ```

After you've checked the file path and working directory, you should be able to run your Python script without the "No such file or directory" error.

2025-04-14 14:57:27,519 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 14:57:27,521 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 14:57:27,522 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 14:57:27,524 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 14:57:27,525 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 14:57:27,527 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 14:57:27,529 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 14:57:27,530 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 14:57:27,531 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 14:57:27,533 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 14:57:27,534 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 14:57:27,535 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 14:57:27,536 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 14:57:27,537 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 14:57:27,538 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 14:57:27,539 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 14:57:27,541 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 14:57:27,541 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 14:57:27,542 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 14:57:27,543 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 14:57:27,544 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 14:57:27,546 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 14:57:27,547 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 14:57:27,548 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 14:57:27,549 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 14:57:27,550 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 14:57:27,551 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 14:57:27,552 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 14:57:27,553 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 14:57:27,554 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 14:57:27,556 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 14:57:27,557 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 14:57:27,558 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 14:57:27,559 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 14:57:27,560 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 14:57:27,561 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 14:57:27,562 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 14:57:27,563 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 14:57:27,564 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 14:57:27,565 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 14:57:27,567 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 14:57:27,568 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 14:57:27,569 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 14:57:27,570 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 14:57:27,571 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 14:57:27,573 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 14:57:27,574 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 14:57:27,575 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 14:57:27,576 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 14:57:27,577 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 14:57:27,578 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 14:57:27,579 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 14:57:27,580 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 15:12:41,428 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 15:12:41,430 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 15:12:41,432 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 15:12:41,434 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 15:12:41,435 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 15:12:41,437 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 15:12:41,438 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 15:12:41,440 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 15:12:41,441 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 15:12:41,444 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 15:12:41,445 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 15:12:41,447 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 15:12:41,449 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 15:12:41,450 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 15:12:41,452 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 15:12:41,453 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 15:12:41,455 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 15:12:41,456 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 15:12:41,458 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 15:12:41,460 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 15:12:41,462 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 15:12:41,463 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 15:12:41,465 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 15:12:41,466 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 15:12:41,467 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 15:12:41,469 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 15:12:41,470 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 15:12:41,471 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 15:12:41,473 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 15:12:41,474 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 15:12:41,477 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 15:12:41,478 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 15:12:41,480 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 15:12:41,481 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 15:12:41,483 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 15:12:41,486 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 15:12:41,488 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 15:12:41,489 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 15:12:41,491 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 15:12:41,492 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 15:12:41,494 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 15:12:41,495 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 15:12:41,497 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 15:12:41,498 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 15:12:41,499 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 15:12:41,500 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 15:12:41,501 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 15:12:41,502 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 15:12:41,503 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 15:12:41,504 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 15:12:41,505 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 15:12:41,506 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 15:12:41,508 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 15:12:41,654 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 15:12:41,656 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 15:12:41,658 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 15:12:41,659 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 15:12:41,661 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 15:12:41,663 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 15:12:41,665 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 15:12:41,666 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 15:12:41,667 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 15:12:41,669 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 15:12:41,670 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 15:12:41,672 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 15:12:41,673 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 15:12:41,674 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 15:12:41,676 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 15:12:41,678 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 15:12:41,679 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 15:12:41,680 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 15:12:41,682 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 15:12:41,684 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 15:12:41,687 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 15:12:41,690 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 15:12:41,693 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 15:12:41,694 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 15:12:41,696 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 15:12:41,697 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 15:12:41,699 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 15:12:41,701 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 15:12:41,703 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 15:12:41,705 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 15:12:41,707 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 15:12:41,708 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 15:12:41,710 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 15:12:41,712 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 15:12:41,713 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 15:12:41,715 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 15:12:41,716 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 15:12:41,718 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 15:12:41,719 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 15:12:41,720 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 15:12:41,721 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 15:12:41,722 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 15:12:41,723 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 15:12:41,724 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 15:12:41,725 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 15:12:41,727 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 15:12:41,728 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 15:12:41,729 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 15:12:41,730 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 15:12:41,732 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 15:12:41,733 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 15:12:41,734 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 15:12:41,734 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 15:12:41,742 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\agents_ia\comandAI\assets\20250414151241_clipboard_20250414145716.png, Prompt: Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diretório atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Definição dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Criação dos diretórios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configuração do layout do Streamlit
st.set_page_config(
    page_title="Análise de Ações",
    layout="wide",
    page_icon="📈"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscilações

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Período', 'Oscilação'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("📊 Dashboard de Análise de Ações")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma ação", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"📌 Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("🔍 Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descrição', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa ação.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("💰 Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("📉 Oscilações")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("📈 Tabela Geral de Ações")
        st.dataframe(self.acoes)

# Execução
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para análise fundamentalista de ações da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"Não foram encontrados arquivos .txt no diretório {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conteúdo do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conteúdo HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conteúdo HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se está correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configuração de vídeos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configuração de áudios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcrições e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diretório
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diretório.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do último arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conteúdo do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimensões do embedding:", embedding.shape)

# Caminho do diretório de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o último embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configuração de transcrições e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diretório onde está procurando as transcrições
        print(f"Diretório de entrada das transcrições: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcrição no diretório de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcrição encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo não encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcrição do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} está vazio.")
                    return

            # Gerar o embedding da transcrição
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcrição e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de áudio
    with sr.Microphone() as source:
        print("Diga algo em português...")

        while True:
            try:
                # Escuta o áudio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {texto_portugues}")

                # Traduz o texto para o inglês
                traducao = translator.translate(texto_portugues)
                print(f"Tradução para o inglês: {traducao}")

            except sr.UnknownValueError:
                print("Não foi possível entender o áudio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usuário")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configurações de áudio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de áudio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em português...")

try:
    audio_buffer = []

    while True:
        # Lê dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o áudio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o áudio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradução
            print(f"Tradução para o inglês: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usuário")

    # Fecha o stream de áudio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O vídeo {video_path} não contém áudio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de vídeo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de vídeo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de vídeo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Conversão de vídeo para áudio concluída!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configurações da API
handler = genai('gemini-1.5-flash')

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Função para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conteúdo com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares únicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcrição obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' não é uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuniões: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de reuniões: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|tópicos da|pontos (relevantes|principais)|análise)\b.*\b(reunião|última (reunião|conversa|sessão))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como análise de reunião.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padrão.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que são muito similares ao comando atual para evitar redundância.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padrão: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # Métodos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de análise de reunião.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conteúdo usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inválida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar variáveis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conteúdo com base no prompt, contexto e reunião"""
        try:
            # Supondo que a API espera um dicionário com os parâmetros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisição para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar variáveis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Criação da conclusão do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que você acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["faça um resumo da última reunião.", "tópicos da última reunião", "resuma a última reunião", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Você é responsável por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - Não acrescentar título nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informações acima e forneça uma solução detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Você é responsável analisar com detalhes a reunião de [str_meeting], e fornecer uma longa estória sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precisão da resposta.
            - Não acrescente título nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e faça uma pesquisa online para uma resposta mais precisa das questões de [str_text]
        - Não acrescente título nas respostas.
        
        ------
        [context]: Regras Básicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("Áudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum áudio detectado.")
                return None
            except sr.UnknownValueError:
                print("Não foi possível entender o áudio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configurações da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicionário
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta não está no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API: {e}")
        return None

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: quais IDES já possuem integração com alguma IA que consegue receber o seu código como contexto e fornecer assistencia no desenvolvimento.???

Atualmente só conheço o VScode com o copilot e o gemini integrados.
Assistant: Com base no código fornecido e nas suas instruções, compilei uma lista de IDEs (Ambientes de Desenvolvimento Integrados) que já possuem integração com alguma IA capaz de receber o seu código como contexto e fornecer assistência no desenvolvimento:

*   **VS Code:** Já possui integrações com o GitHub Copilot e o Gemini.

A pesquisa foi realizada utilizando o código fonte para identificar as ferramentas e frameworks mencionados, e, com base nisso, fornecer uma resposta precisa.
User: mesmo instalando o pacote não reconhece.
Assistant: Com base no contexto do código fornecido, aqui estão alguns dos possíveis problemas e soluções que podem causar problemas de reconhecimento de pacotes:

1.  **Problema de ambiente:** Certifique-se de que o pacote google-generativeai esteja instalado no ambiente Python correto. Às vezes, pode ser instalado em um ambiente diferente do que está sendo usado.

    *   **Solução:** Ative o ambiente virtual apropriado e use `pip show google-generativeai` para verificar se o pacote está instalado naquele ambiente. Se não estiver, instale-o usando `pip install google-generativeai`.

2.  **Problema de cache:** Às vezes, o pip pode ter problemas de cache que impedem que ele reconheça pacotes recém-instalados.

    *   **Solução:** Tente atualizar o pip e limpar o cache do pip antes de instalar o pacote novamente. Use os seguintes comandos:

    ```bash
    pip install --upgrade pip
    pip cache purge
    pip install google-generativeai
    ```

3.  **Problema de caminho:** Às vezes, o Python pode não encontrar o pacote instalado porque o caminho de instalação do pacote não está no `sys.path`.

    *   **Solução:** Adicione o diretório do pacote ao `sys.path` em seu script Python ou bloco de anotações. Você pode encontrar o diretório de instalação do pacote usando `pip show google-generativeai -f`. Em seguida, adicione esse caminho ao `sys.path` em seu script:

    ```python
    import sys
    sys.path.append('/caminho/para/site-packages')  # Substitua com o caminho real
    import google.generativeai as genai
    ```

4.  **Problema de nome:** Às vezes, o nome do pacote no código não corresponde ao nome real do pacote instalado.

    *   **Solução:** Verifique se o nome do pacote está correto em sua instrução `import`. Nesse caso, ele deve ser `import google.generativeai as genai`.

5.  **Problema de versão:** Às vezes, os pacotes podem apresentar problemas de compatibilidade com outras versões de pacotes ou com a versão Python que você está usando.

    *   **Solução:** Considere usar uma versão diferente do pacote que seja compatível com sua configuração.

6.  **IDE ou problema de editor:** Às vezes, o IDE ou editor que você está usando pode não ser capaz de reconhecer pacotes recém-instalados.

    *   **Solução:** Reinicie o IDE ou editor ou recarregue o ambiente Python.
User: parece que é algo com ambiente...porque outras aplicações estão funcionando com essa lib instalada.
Assistant: Compreendo. Se outras aplicações estão funcionando corretamente com a `google-generativeai` instalada, o problema provavelmente reside no ambiente específico em que a aplicação atual está sendo executada. Aqui estão algumas áreas para investigar e possíveis soluções:

1.  **Ambiente Virtual:**
    *   **Verificação:** Certifique-se de que a aplicação esteja rodando dentro do ambiente virtual correto. Se você estiver utilizando um ambiente virtual (como `venv` ou `conda`), ative-o antes de executar a aplicação.

    ```bash
    # Para venv
    source venv/bin/activate  # ou venv\Scripts\activate no Windows

    # Para conda
    conda activate meu_ambiente
    ```
    *   **Confirmação:** Verifique se o ambiente virtual ativado é o mesmo onde `google-generativeai` está instalado. Utilize `pip show google-generativeai` para confirmar a instalação e o caminho do pacote.

2.  **Caminhos de Importação:**
    *   **Investigação:** Verifique se o caminho para o diretório do pacote está corretamente configurado no `sys.path`. Isso pode variar entre diferentes ambientes e instalações.
    *   **Solução:** Adicione o caminho correto ao `sys.path` no início do script. Para descobrir o caminho correto, execute o seguinte no ambiente onde a lib funciona:

    ```python
    import google.generativeai
    print(google.generativeai.__file__)
    ```
    Copie o diretório onde o arquivo `__init__.py` está localizado e adicione ao `sys.path` no seu script.

    ```python
    import sys
    sys.path.append('/caminho/para/o/site-packages')
    import google.generativeai as genai
    ```

3.  **Conflitos de Dependências:**
    *   **Investigação:** Pode haver conflitos entre as dependências de diferentes pacotes no seu ambiente.
    *   **Solução:** Tente criar um ambiente virtual limpo e instalar apenas as dependências necessárias para a sua aplicação atual.

    ```bash
    python -m venv novo_ambiente
    source novo_ambiente/bin/activate  # ou novo_ambiente\Scripts\activate no Windows
    pip install google-generativeai
    # Instale outros pacotes necessários para esta aplicação
    ```

4.  **Cache do Editor/IDE:**
    *   **Investigação:** O editor ou IDE pode estar usando um cache que não está atualizado com as alterações no ambiente.
    *   **Solução:** Reinicie o editor ou IDE, ou force a atualização do cache. No VS Code, isso pode ser feito através do comando "Developer: Reload Window".

5.  **Variáveis de Ambiente:**
    *   **Investigação:** Verifique se todas as variáveis de ambiente necessárias (como `API_KEY_GEMINI`) estão corretamente configuradas e acessíveis no ambiente em que a aplicação está rodando.
    *   **Solução:** Certifique-se de que as variáveis de ambiente estão definidas corretamente e recarregue o ambiente se necessário.

6.  **Permissões:**
    *   **Investigação:** Verifique se o usuário que está executando a aplicação tem permissões para acessar os arquivos e diretórios necessários.
    *   **Solução:** Ajuste as permissões conforme necessário para garantir que o usuário tenha acesso aos arquivos e diretórios requeridos.

7.  **Versões do Python:**
    *   **Investigação:** Confirme que a versão do Python em uso no ambiente é compatível com a `google-generativeai`.
    *   **Solução:** Verifique a versão do Python utilizada (`python --version`) e, se necessário, instale uma versão compatível com a biblioteca.

Com esses passos, você deve conseguir identificar e resolver o problema de ambiente que está impedindo a sua aplicação de reconhecer o pacote `google-generativeai`.
User: ocorreu esse erro na imagem.
Assistant: The image shows that the virtual environment is not activated.

Here's how to fix this:

1.  **Verify the virtual environment's `venv` directory:** The error message  `bash: venv/bin/activate: No such file or directory` indicates that the `activate` script is not found in `venv/bin/`. This usually happens if the virtual environment was not created properly or the `venv` directory was renamed or deleted.  You can see from the file explorer in the image that there is a `venv` directory in the project root.
2.  **Check the correct activation command:**  The command `source venv/bin/activate` is usually used on Linux/macOS systems. For Windows, the activation script is located at `venv\Scripts\activate`. Therefore, try running  `venv\Scripts\activate` in the terminal instead.
3.  **Recreate the virtual environment (if necessary):** If you've confirmed that the virtual environment is broken, delete the existing `venv` directory and create a new virtual environment using  `python -m venv venv`. After creating the new environment, try activating it again.

After resolving the issue with the virtual environment, install all dependencies: `pip install -r requirements.txt` if you have a requirements file or install them manually. Then, rerun the application.
User: estou usando windows... mas no terminal é o git bash
Assistant: Okay, I see the problem. Even though you're on Windows, because you're using Git Bash as your terminal, you need to use the Linux-style activation command.  The fact that it's not working suggests that *either* the `venv` directory doesn't contain the `bin/activate` script (as the error message says) *or* there's some kind of permission or execution issue with the script itself in the Git Bash environment.

Here's a breakdown of the likely problem and solutions:

**The Problem:**

Git Bash (MinGW64 specifically) is a Unix-like environment on Windows. While it uses many Linux commands, it still operates within the Windows file system.  Therefore, even when using `source venv/bin/activate`, Git Bash is looking for a proper executable file inside that bin directory, but in your case, that doesn't exist in the proper format.

**Solutions:**

1.  **Use the `.` command as alias to `source`:**  In some Git Bash setups, `.` is explicitly aliased to `source`. Try using it:

    ```bash
    . venv/bin/activate
    ```

    This is a slight variation of the `source` command, but it might work in your particular Git Bash environment.
2.  **Use the correct script in Git Bash (if it exists):** Some virtual environment setups for Git Bash will create a script that is more appropriate for the shell.  Look within your `venv` directory; in some cases, you will see a file `venv/Scripts/activate`. If this file exists, it should be ran directly in Git Bash. However, if that's the case, the path would more likely be `source venv/Scripts/activate`. In this case, that is *not* what you are looking for. You can use the next solution for the `activate` script.
3.  **Execute the Activate.ps1 Script:** Git Bash can run PowerShell scripts directly. Run the PowerShell script activate.ps1 inside powershell. To do this, you can run `powershell -ExecutionPolicy Bypass -File ./venv/Scripts/Activate.ps1` in your terminal. This method is likely the most reliable method to run virtual environments on Git Bash in Windows.

**Troubleshooting steps to confirm the root cause:**

1.  **List the contents of `venv/bin`:**  In your Git Bash terminal, run the command `ls -l venv/bin`.  Examine the output.  You should see a file named `activate` (without a file extension).  If it's not there, that's your primary problem - the venv wasn't created properly.
2.  **Check permissions:** If the `activate` script *is* there, check its permissions. It should have execute permissions. The output of `ls -l venv/bin` will show you the permissions. If the file doesn't have an `x` in the permissions for the user, group, or others, the script won't be executable.  (Note:  Git Bash's permissions model is somewhat emulated and not always perfectly accurate, but it's worth checking.)

If the directory `venv/bin` does *not* contain the activate script, then you should delete the `venv` folder and re-create the virtual environment.

In summary, even if the most likely solution fails, use the troubleshooting steps to confirm that `venv/bin/activate` is an executable.
User: Agora deu esse erro.
Assistant: Based on the screenshot provided, you're still facing an issue related to the virtual environment, but now it's a different error. The error is:

```
/c/Users/jfreis/Documents/agents_ia/comandAI/venv/bin/python: can't open file '/c/Users/jfreis/Documents/agents_ia/comandAI/main.py': [Errno 2] No such file or directory
```

Here's what this means and how to troubleshoot it:

**The Problem:**

The error "No such file or directory" clearly indicates that the Python interpreter within your activated virtual environment cannot find the `main.py` file you're trying to execute. This could be due to several reasons:

1.  **Incorrect working directory:** You might be running the command from a directory where `main.py` doesn't exist.

2.  **Incorrect file path:** The file path in your command (`/c/Users/jfreis/Documents/agents_ia/comandAI/main.py`) might be wrong.

3.  **File doesn't exist:** The `main.py` file may simply not exist at the specified location.

**Troubleshooting Steps:**

1.  **Verify file existence:**

    *   Double-check that `main.py` exists at the path `/c/Users/jfreis/Documents/agents_ia/comandAI/main.py`.  Use your file explorer to confirm this.
    *   Make sure you didn't accidentally rename or move the file.

2.  **Check working directory:**

    *   Run the command `pwd` (print working directory) in your Git Bash terminal.  This will tell you the current directory from which you're running the command.
    *   If the current directory isn't `/c/Users/jfreis/Documents/agents_ia/comandAI`, you need to change it. You can use the `cd` command to navigate to the correct directory:
    ```bash
    cd /c/Users/jfreis/Documents/agents_ia/comandAI
    ```

3.  **Check the command:**
If your pwd is on directory, check the command used to execute the file. If you executed the command like `python /c/Users/jfreis/Documents/agents_ia/comandAI/main.py`, you can simply run `python main.py`.

4.  **Double-check case sensitivity:**  Git Bash, being a Unix-like environment, is case-sensitive. Make sure the file name "main.py" matches the case exactly.

**Example:**

Let's say `pwd` shows that you're currently in `/c/Users/jfreis/`

*   You would run:

    ```bash
    cd Documents/agents_ia/comandAI
    python main.py
    ```

After you've checked the file path and working directory, you should be able to run your Python script without the "No such file or directory" error.

User: reponda em portugues.
2025-04-14 15:12:47,936 [INFO] Resposta da IA (imagem): Com base na captura de tela fornecida, você ainda está enfrentando um problema relacionado ao ambiente virtual, mas agora é um erro diferente. O erro é:

```
/c/Users/jfreis/Documents/agents_ia/comandAI/venv/bin/python: can't open file '/c/Users/jfreis/Documents/agents_ia/comandAI/main.py': [Errno 2] No such file or directory
```

Veja o que isso significa e como solucionar o problema:

**O Problema:**

O erro "No such file or directory" (Arquivo ou diretório inexistente) indica claramente que o interpretador Python dentro do seu ambiente virtual ativado não consegue encontrar o arquivo `main.py` que você está tentando executar. Isso pode ocorrer por vários motivos:

1.  **Diretório de trabalho incorreto:** Você pode estar executando o comando a partir de um diretório onde `main.py` não existe.

2.  **Caminho do arquivo incorreto:** O caminho do arquivo no seu comando (`/c/Users/jfreis/Documents/agents_ia/comandAI/main.py`) pode estar errado.

3.  **Arquivo não existe:** O arquivo `main.py` pode simplesmente não existir no local especificado.

**Etapas de Solução de Problemas:**

1.  **Verifique a existência do arquivo:**

    *   Verifique se `main.py` existe no caminho `/c/Users/jfreis/Documents/agents_ia/comandAI/main.py`. Use o explorador de arquivos para confirmar isso.
    *   Certifique-se de não ter renomeado ou movido o arquivo acidentalmente.

2.  **Verifique o diretório de trabalho:**

    *   Execute o comando `pwd` (print working directory - imprimir diretório de trabalho) no seu terminal Git Bash. Isso dirá qual é o diretório atual de onde você está executando o comando.
    *   Se o diretório atual não for `/c/Users/jfreis/Documents/agents_ia/comandAI`, você precisará alterá-lo. Você pode usar o comando `cd` para navegar até o diretório correto:
    ```bash
    cd /c/Users/jfreis/Documents/agents_ia/comandAI
    ```

3.  **Verifique o comando:**
Se o seu pwd estiver no diretório correto, verifique o comando usado para executar o arquivo. Se você executou o comando como `python /c/Users/jfreis/Documents/agents_ia/comandAI/main.py`, você pode simplesmente executar `python main.py`.

4.  **Verifique a diferenciação de maiúsculas e minúsculas:** O Git Bash, sendo um ambiente semelhante ao Unix, diferencia maiúsculas e minúsculas. Certifique-se de que o nome do arquivo "main.py" corresponda ao caso exatamente.

**Exemplo:**

Digamos que `pwd` mostre que você está atualmente em `/c/Users/jfreis/`

*   Você executaria:

    ```bash
    cd Documents/agents_ia/comandAI
    python main.py
    ```

Depois de verificar o caminho do arquivo e o diretório de trabalho, você deve conseguir executar seu script Python sem o erro "No such file or directory".

2025-04-14 15:12:48,039 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 15:12:48,041 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 15:12:48,043 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 15:12:48,045 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 15:12:48,046 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 15:12:48,048 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 15:12:48,050 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 15:12:48,052 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 15:12:48,054 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 15:12:48,055 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 15:12:48,056 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 15:12:48,058 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 15:12:48,059 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 15:12:48,060 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 15:12:48,062 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 15:12:48,063 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 15:12:48,064 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 15:12:48,065 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 15:12:48,066 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 15:12:48,068 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 15:12:48,071 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 15:12:48,072 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 15:12:48,073 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 15:12:48,075 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 15:12:48,076 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 15:12:48,078 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 15:12:48,080 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 15:12:48,082 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 15:12:48,083 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 15:12:48,085 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 15:12:48,087 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 15:12:48,088 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 15:12:48,090 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 15:12:48,091 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 15:12:48,092 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 15:12:48,093 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 15:12:48,095 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 15:12:48,096 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 15:12:48,097 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 15:12:48,099 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 15:12:48,101 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 15:12:48,103 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 15:12:48,105 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 15:12:48,106 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 15:12:48,108 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 15:12:48,109 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 15:12:48,111 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 15:12:48,113 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 15:12:48,114 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 15:12:48,115 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 15:12:48,118 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 15:12:48,121 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 15:12:48,122 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 15:13:25,983 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 15:13:25,985 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 15:13:25,986 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 15:13:25,988 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 15:13:25,989 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 15:13:25,991 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 15:13:25,992 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 15:13:25,994 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 15:13:25,996 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 15:13:25,997 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 15:13:25,999 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 15:13:26,000 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 15:13:26,002 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 15:13:26,003 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 15:13:26,004 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 15:13:26,006 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 15:13:26,007 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 15:13:26,008 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 15:13:26,009 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 15:13:26,010 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 15:13:26,012 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 15:13:26,013 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 15:13:26,014 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 15:13:26,016 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 15:13:26,017 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 15:13:26,018 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 15:13:26,019 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 15:13:26,021 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 15:13:26,022 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 15:13:26,023 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 15:13:26,025 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 15:13:26,027 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 15:13:26,028 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 15:13:26,029 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 15:13:26,031 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 15:13:26,033 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 15:13:26,034 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 15:13:26,036 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 15:13:26,038 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 15:13:26,040 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 15:13:26,041 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 15:13:26,042 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 15:13:26,043 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 15:13:26,044 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 15:13:26,045 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 15:13:26,046 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 15:13:26,048 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 15:13:26,049 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 15:13:26,050 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 15:13:26,051 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 15:13:26,052 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 15:13:26,053 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 15:13:26,054 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 15:13:26,200 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 15:13:26,201 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 15:13:26,202 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 15:13:26,204 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 15:13:26,205 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 15:13:26,207 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 15:13:26,208 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 15:13:26,210 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 15:13:26,211 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 15:13:26,212 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 15:13:26,213 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 15:13:26,215 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 15:13:26,216 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 15:13:26,217 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 15:13:26,219 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 15:13:26,220 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 15:13:26,222 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 15:13:26,223 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 15:13:26,224 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 15:13:26,226 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 15:13:26,228 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 15:13:26,229 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 15:13:26,231 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 15:13:26,233 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 15:13:26,235 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 15:13:26,236 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 15:13:26,238 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 15:13:26,239 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 15:13:26,240 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 15:13:26,242 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 15:13:26,243 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 15:13:26,244 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 15:13:26,246 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 15:13:26,247 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 15:13:26,248 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 15:13:26,249 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 15:13:26,250 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 15:13:26,252 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 15:13:26,254 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 15:13:26,256 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 15:13:26,257 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 15:13:26,259 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 15:13:26,260 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 15:13:26,261 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 15:13:26,262 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 15:13:26,263 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 15:13:26,264 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 15:13:26,266 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 15:13:26,267 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 15:13:26,268 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 15:13:26,269 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 15:13:26,270 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 15:13:26,271 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 15:13:26,278 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\agents_ia\comandAI\assets\20250414151326_clipboard_20250414145716.png, Prompt: Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diretório atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Definição dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Criação dos diretórios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configuração do layout do Streamlit
st.set_page_config(
    page_title="Análise de Ações",
    layout="wide",
    page_icon="📈"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscilações

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Período', 'Oscilação'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("📊 Dashboard de Análise de Ações")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma ação", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"📌 Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("🔍 Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descrição', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa ação.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("💰 Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("📉 Oscilações")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("📈 Tabela Geral de Ações")
        st.dataframe(self.acoes)

# Execução
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para análise fundamentalista de ações da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"Não foram encontrados arquivos .txt no diretório {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conteúdo do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conteúdo HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conteúdo HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se está correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configuração de vídeos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configuração de áudios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcrições e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diretório
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diretório.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do último arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conteúdo do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimensões do embedding:", embedding.shape)

# Caminho do diretório de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o último embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configuração de transcrições e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diretório onde está procurando as transcrições
        print(f"Diretório de entrada das transcrições: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcrição no diretório de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcrição encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo não encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcrição do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} está vazio.")
                    return

            # Gerar o embedding da transcrição
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcrição e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de áudio
    with sr.Microphone() as source:
        print("Diga algo em português...")

        while True:
            try:
                # Escuta o áudio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {texto_portugues}")

                # Traduz o texto para o inglês
                traducao = translator.translate(texto_portugues)
                print(f"Tradução para o inglês: {traducao}")

            except sr.UnknownValueError:
                print("Não foi possível entender o áudio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usuário")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configurações de áudio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de áudio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em português...")

try:
    audio_buffer = []

    while True:
        # Lê dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o áudio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o áudio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradução
            print(f"Tradução para o inglês: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usuário")

    # Fecha o stream de áudio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O vídeo {video_path} não contém áudio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de vídeo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de vídeo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de vídeo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Conversão de vídeo para áudio concluída!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configurações da API
handler = genai('gemini-1.5-flash')

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Função para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conteúdo com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares únicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcrição obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' não é uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuniões: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de reuniões: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|tópicos da|pontos (relevantes|principais)|análise)\b.*\b(reunião|última (reunião|conversa|sessão))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como análise de reunião.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padrão.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que são muito similares ao comando atual para evitar redundância.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padrão: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # Métodos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de análise de reunião.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conteúdo usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inválida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar variáveis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conteúdo com base no prompt, contexto e reunião"""
        try:
            # Supondo que a API espera um dicionário com os parâmetros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisição para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar variáveis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Criação da conclusão do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que você acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["faça um resumo da última reunião.", "tópicos da última reunião", "resuma a última reunião", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Você é responsável por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - Não acrescentar título nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informações acima e forneça uma solução detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Você é responsável analisar com detalhes a reunião de [str_meeting], e fornecer uma longa estória sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precisão da resposta.
            - Não acrescente título nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e faça uma pesquisa online para uma resposta mais precisa das questões de [str_text]
        - Não acrescente título nas respostas.
        
        ------
        [context]: Regras Básicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("Áudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum áudio detectado.")
                return None
            except sr.UnknownValueError:
                print("Não foi possível entender o áudio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configurações da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicionário
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta não está no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API: {e}")
        return None

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: quais IDES já possuem integração com alguma IA que consegue receber o seu código como contexto e fornecer assistencia no desenvolvimento.???

Atualmente só conheço o VScode com o copilot e o gemini integrados.
Assistant: Com base no código fornecido e nas suas instruções, compilei uma lista de IDEs (Ambientes de Desenvolvimento Integrados) que já possuem integração com alguma IA capaz de receber o seu código como contexto e fornecer assistência no desenvolvimento:

*   **VS Code:** Já possui integrações com o GitHub Copilot e o Gemini.

A pesquisa foi realizada utilizando o código fonte para identificar as ferramentas e frameworks mencionados, e, com base nisso, fornecer uma resposta precisa.
User: mesmo instalando o pacote não reconhece.
Assistant: Com base no contexto do código fornecido, aqui estão alguns dos possíveis problemas e soluções que podem causar problemas de reconhecimento de pacotes:

1.  **Problema de ambiente:** Certifique-se de que o pacote google-generativeai esteja instalado no ambiente Python correto. Às vezes, pode ser instalado em um ambiente diferente do que está sendo usado.

    *   **Solução:** Ative o ambiente virtual apropriado e use `pip show google-generativeai` para verificar se o pacote está instalado naquele ambiente. Se não estiver, instale-o usando `pip install google-generativeai`.

2.  **Problema de cache:** Às vezes, o pip pode ter problemas de cache que impedem que ele reconheça pacotes recém-instalados.

    *   **Solução:** Tente atualizar o pip e limpar o cache do pip antes de instalar o pacote novamente. Use os seguintes comandos:

    ```bash
    pip install --upgrade pip
    pip cache purge
    pip install google-generativeai
    ```

3.  **Problema de caminho:** Às vezes, o Python pode não encontrar o pacote instalado porque o caminho de instalação do pacote não está no `sys.path`.

    *   **Solução:** Adicione o diretório do pacote ao `sys.path` em seu script Python ou bloco de anotações. Você pode encontrar o diretório de instalação do pacote usando `pip show google-generativeai -f`. Em seguida, adicione esse caminho ao `sys.path` em seu script:

    ```python
    import sys
    sys.path.append('/caminho/para/site-packages')  # Substitua com o caminho real
    import google.generativeai as genai
    ```

4.  **Problema de nome:** Às vezes, o nome do pacote no código não corresponde ao nome real do pacote instalado.

    *   **Solução:** Verifique se o nome do pacote está correto em sua instrução `import`. Nesse caso, ele deve ser `import google.generativeai as genai`.

5.  **Problema de versão:** Às vezes, os pacotes podem apresentar problemas de compatibilidade com outras versões de pacotes ou com a versão Python que você está usando.

    *   **Solução:** Considere usar uma versão diferente do pacote que seja compatível com sua configuração.

6.  **IDE ou problema de editor:** Às vezes, o IDE ou editor que você está usando pode não ser capaz de reconhecer pacotes recém-instalados.

    *   **Solução:** Reinicie o IDE ou editor ou recarregue o ambiente Python.
User: parece que é algo com ambiente...porque outras aplicações estão funcionando com essa lib instalada.
Assistant: Compreendo. Se outras aplicações estão funcionando corretamente com a `google-generativeai` instalada, o problema provavelmente reside no ambiente específico em que a aplicação atual está sendo executada. Aqui estão algumas áreas para investigar e possíveis soluções:

1.  **Ambiente Virtual:**
    *   **Verificação:** Certifique-se de que a aplicação esteja rodando dentro do ambiente virtual correto. Se você estiver utilizando um ambiente virtual (como `venv` ou `conda`), ative-o antes de executar a aplicação.

    ```bash
    # Para venv
    source venv/bin/activate  # ou venv\Scripts\activate no Windows

    # Para conda
    conda activate meu_ambiente
    ```
    *   **Confirmação:** Verifique se o ambiente virtual ativado é o mesmo onde `google-generativeai` está instalado. Utilize `pip show google-generativeai` para confirmar a instalação e o caminho do pacote.

2.  **Caminhos de Importação:**
    *   **Investigação:** Verifique se o caminho para o diretório do pacote está corretamente configurado no `sys.path`. Isso pode variar entre diferentes ambientes e instalações.
    *   **Solução:** Adicione o caminho correto ao `sys.path` no início do script. Para descobrir o caminho correto, execute o seguinte no ambiente onde a lib funciona:

    ```python
    import google.generativeai
    print(google.generativeai.__file__)
    ```
    Copie o diretório onde o arquivo `__init__.py` está localizado e adicione ao `sys.path` no seu script.

    ```python
    import sys
    sys.path.append('/caminho/para/o/site-packages')
    import google.generativeai as genai
    ```

3.  **Conflitos de Dependências:**
    *   **Investigação:** Pode haver conflitos entre as dependências de diferentes pacotes no seu ambiente.
    *   **Solução:** Tente criar um ambiente virtual limpo e instalar apenas as dependências necessárias para a sua aplicação atual.

    ```bash
    python -m venv novo_ambiente
    source novo_ambiente/bin/activate  # ou novo_ambiente\Scripts\activate no Windows
    pip install google-generativeai
    # Instale outros pacotes necessários para esta aplicação
    ```

4.  **Cache do Editor/IDE:**
    *   **Investigação:** O editor ou IDE pode estar usando um cache que não está atualizado com as alterações no ambiente.
    *   **Solução:** Reinicie o editor ou IDE, ou force a atualização do cache. No VS Code, isso pode ser feito através do comando "Developer: Reload Window".

5.  **Variáveis de Ambiente:**
    *   **Investigação:** Verifique se todas as variáveis de ambiente necessárias (como `API_KEY_GEMINI`) estão corretamente configuradas e acessíveis no ambiente em que a aplicação está rodando.
    *   **Solução:** Certifique-se de que as variáveis de ambiente estão definidas corretamente e recarregue o ambiente se necessário.

6.  **Permissões:**
    *   **Investigação:** Verifique se o usuário que está executando a aplicação tem permissões para acessar os arquivos e diretórios necessários.
    *   **Solução:** Ajuste as permissões conforme necessário para garantir que o usuário tenha acesso aos arquivos e diretórios requeridos.

7.  **Versões do Python:**
    *   **Investigação:** Confirme que a versão do Python em uso no ambiente é compatível com a `google-generativeai`.
    *   **Solução:** Verifique a versão do Python utilizada (`python --version`) e, se necessário, instale uma versão compatível com a biblioteca.

Com esses passos, você deve conseguir identificar e resolver o problema de ambiente que está impedindo a sua aplicação de reconhecer o pacote `google-generativeai`.
User: ocorreu esse erro na imagem.
Assistant: The image shows that the virtual environment is not activated.

Here's how to fix this:

1.  **Verify the virtual environment's `venv` directory:** The error message  `bash: venv/bin/activate: No such file or directory` indicates that the `activate` script is not found in `venv/bin/`. This usually happens if the virtual environment was not created properly or the `venv` directory was renamed or deleted.  You can see from the file explorer in the image that there is a `venv` directory in the project root.
2.  **Check the correct activation command:**  The command `source venv/bin/activate` is usually used on Linux/macOS systems. For Windows, the activation script is located at `venv\Scripts\activate`. Therefore, try running  `venv\Scripts\activate` in the terminal instead.
3.  **Recreate the virtual environment (if necessary):** If you've confirmed that the virtual environment is broken, delete the existing `venv` directory and create a new virtual environment using  `python -m venv venv`. After creating the new environment, try activating it again.

After resolving the issue with the virtual environment, install all dependencies: `pip install -r requirements.txt` if you have a requirements file or install them manually. Then, rerun the application.
User: estou usando windows... mas no terminal é o git bash
Assistant: Okay, I see the problem. Even though you're on Windows, because you're using Git Bash as your terminal, you need to use the Linux-style activation command.  The fact that it's not working suggests that *either* the `venv` directory doesn't contain the `bin/activate` script (as the error message says) *or* there's some kind of permission or execution issue with the script itself in the Git Bash environment.

Here's a breakdown of the likely problem and solutions:

**The Problem:**

Git Bash (MinGW64 specifically) is a Unix-like environment on Windows. While it uses many Linux commands, it still operates within the Windows file system.  Therefore, even when using `source venv/bin/activate`, Git Bash is looking for a proper executable file inside that bin directory, but in your case, that doesn't exist in the proper format.

**Solutions:**

1.  **Use the `.` command as alias to `source`:**  In some Git Bash setups, `.` is explicitly aliased to `source`. Try using it:

    ```bash
    . venv/bin/activate
    ```

    This is a slight variation of the `source` command, but it might work in your particular Git Bash environment.
2.  **Use the correct script in Git Bash (if it exists):** Some virtual environment setups for Git Bash will create a script that is more appropriate for the shell.  Look within your `venv` directory; in some cases, you will see a file `venv/Scripts/activate`. If this file exists, it should be ran directly in Git Bash. However, if that's the case, the path would more likely be `source venv/Scripts/activate`. In this case, that is *not* what you are looking for. You can use the next solution for the `activate` script.
3.  **Execute the Activate.ps1 Script:** Git Bash can run PowerShell scripts directly. Run the PowerShell script activate.ps1 inside powershell. To do this, you can run `powershell -ExecutionPolicy Bypass -File ./venv/Scripts/Activate.ps1` in your terminal. This method is likely the most reliable method to run virtual environments on Git Bash in Windows.

**Troubleshooting steps to confirm the root cause:**

1.  **List the contents of `venv/bin`:**  In your Git Bash terminal, run the command `ls -l venv/bin`.  Examine the output.  You should see a file named `activate` (without a file extension).  If it's not there, that's your primary problem - the venv wasn't created properly.
2.  **Check permissions:** If the `activate` script *is* there, check its permissions. It should have execute permissions. The output of `ls -l venv/bin` will show you the permissions. If the file doesn't have an `x` in the permissions for the user, group, or others, the script won't be executable.  (Note:  Git Bash's permissions model is somewhat emulated and not always perfectly accurate, but it's worth checking.)

If the directory `venv/bin` does *not* contain the activate script, then you should delete the `venv` folder and re-create the virtual environment.

In summary, even if the most likely solution fails, use the troubleshooting steps to confirm that `venv/bin/activate` is an executable.
User: Agora deu esse erro.
Assistant: Based on the screenshot provided, you're still facing an issue related to the virtual environment, but now it's a different error. The error is:

```
/c/Users/jfreis/Documents/agents_ia/comandAI/venv/bin/python: can't open file '/c/Users/jfreis/Documents/agents_ia/comandAI/main.py': [Errno 2] No such file or directory
```

Here's what this means and how to troubleshoot it:

**The Problem:**

The error "No such file or directory" clearly indicates that the Python interpreter within your activated virtual environment cannot find the `main.py` file you're trying to execute. This could be due to several reasons:

1.  **Incorrect working directory:** You might be running the command from a directory where `main.py` doesn't exist.

2.  **Incorrect file path:** The file path in your command (`/c/Users/jfreis/Documents/agents_ia/comandAI/main.py`) might be wrong.

3.  **File doesn't exist:** The `main.py` file may simply not exist at the specified location.

**Troubleshooting Steps:**

1.  **Verify file existence:**

    *   Double-check that `main.py` exists at the path `/c/Users/jfreis/Documents/agents_ia/comandAI/main.py`.  Use your file explorer to confirm this.
    *   Make sure you didn't accidentally rename or move the file.

2.  **Check working directory:**

    *   Run the command `pwd` (print working directory) in your Git Bash terminal.  This will tell you the current directory from which you're running the command.
    *   If the current directory isn't `/c/Users/jfreis/Documents/agents_ia/comandAI`, you need to change it. You can use the `cd` command to navigate to the correct directory:
    ```bash
    cd /c/Users/jfreis/Documents/agents_ia/comandAI
    ```

3.  **Check the command:**
If your pwd is on directory, check the command used to execute the file. If you executed the command like `python /c/Users/jfreis/Documents/agents_ia/comandAI/main.py`, you can simply run `python main.py`.

4.  **Double-check case sensitivity:**  Git Bash, being a Unix-like environment, is case-sensitive. Make sure the file name "main.py" matches the case exactly.

**Example:**

Let's say `pwd` shows that you're currently in `/c/Users/jfreis/`

*   You would run:

    ```bash
    cd Documents/agents_ia/comandAI
    python main.py
    ```

After you've checked the file path and working directory, you should be able to run your Python script without the "No such file or directory" error.

User: reponda em portugues.
Assistant: Com base na captura de tela fornecida, você ainda está enfrentando um problema relacionado ao ambiente virtual, mas agora é um erro diferente. O erro é:

```
/c/Users/jfreis/Documents/agents_ia/comandAI/venv/bin/python: can't open file '/c/Users/jfreis/Documents/agents_ia/comandAI/main.py': [Errno 2] No such file or directory
```

Veja o que isso significa e como solucionar o problema:

**O Problema:**

O erro "No such file or directory" (Arquivo ou diretório inexistente) indica claramente que o interpretador Python dentro do seu ambiente virtual ativado não consegue encontrar o arquivo `main.py` que você está tentando executar. Isso pode ocorrer por vários motivos:

1.  **Diretório de trabalho incorreto:** Você pode estar executando o comando a partir de um diretório onde `main.py` não existe.

2.  **Caminho do arquivo incorreto:** O caminho do arquivo no seu comando (`/c/Users/jfreis/Documents/agents_ia/comandAI/main.py`) pode estar errado.

3.  **Arquivo não existe:** O arquivo `main.py` pode simplesmente não existir no local especificado.

**Etapas de Solução de Problemas:**

1.  **Verifique a existência do arquivo:**

    *   Verifique se `main.py` existe no caminho `/c/Users/jfreis/Documents/agents_ia/comandAI/main.py`. Use o explorador de arquivos para confirmar isso.
    *   Certifique-se de não ter renomeado ou movido o arquivo acidentalmente.

2.  **Verifique o diretório de trabalho:**

    *   Execute o comando `pwd` (print working directory - imprimir diretório de trabalho) no seu terminal Git Bash. Isso dirá qual é o diretório atual de onde você está executando o comando.
    *   Se o diretório atual não for `/c/Users/jfreis/Documents/agents_ia/comandAI`, você precisará alterá-lo. Você pode usar o comando `cd` para navegar até o diretório correto:
    ```bash
    cd /c/Users/jfreis/Documents/agents_ia/comandAI
    ```

3.  **Verifique o comando:**
Se o seu pwd estiver no diretório correto, verifique o comando usado para executar o arquivo. Se você executou o comando como `python /c/Users/jfreis/Documents/agents_ia/comandAI/main.py`, você pode simplesmente executar `python main.py`.

4.  **Verifique a diferenciação de maiúsculas e minúsculas:** O Git Bash, sendo um ambiente semelhante ao Unix, diferencia maiúsculas e minúsculas. Certifique-se de que o nome do arquivo "main.py" corresponda ao caso exatamente.

**Exemplo:**

Digamos que `pwd` mostre que você está atualmente em `/c/Users/jfreis/`

*   Você executaria:

    ```bash
    cd Documents/agents_ia/comandAI
    python main.py
    ```

Depois de verificar o caminho do arquivo e o diretório de trabalho, você deve conseguir executar seu script Python sem o erro "No such file or directory".

User: mas eu não excutei o main... eu tentei ativar o ambiente virtual.
2025-04-14 15:13:30,557 [INFO] Resposta da IA (imagem): Compreendo. Se você não tentou executar o `main.py`, mas sim ativar o ambiente virtual e obteve o erro, o problema é *definitivamente* com a ativação do ambiente e não com a execução do script em si. O que o terminal está dizendo é que ao tentar ativar o ambiente virtual, ele está tentando executar um script (aparentemente o `main.py`) e não consegue encontrar esse script no local especificado.

Isso sugere que:

1. **Algo está sobrescrevendo a maneira como o ambiente virtual é ativado.** Normalmente, a ativação não tenta executar um script externo.
2. **O interpretador do python no venv não é o correto, e está tentando buscar o caminho principal.**
3. **O comando de ativação está configurado incorretamente ou corrompido.**

Aqui estão as etapas a seguir, levando em conta que você está usando o Git Bash no Windows e que tentou ativar o ambiente virtual, não executar um script:

1. **Desative qualquer ambiente virtual ativo:** Se algum ambiente estiver ativado, desative-o primeiro com o comando `deactivate`. Certifique-se que o prefixo `(venv)` não esteja mais visível no prompt do seu terminal.

2. **Verifique se a ativação padrão está funcionando:**

   * Utilize o comando de ativação padrão `source venv/bin/activate` ou `. venv/bin/activate` (se `.` for um alias para `source`).
   * Se este comando falhar com os mesmos erros, o problema não está relacionado a nenhuma configuração de execução de script adicional.

3. **Verifique o arquivo `activate`:**
   1. **Existe?** Verifique que o arquivo `venv/bin/activate` existe.
   2. **Conteúdo:** Se o arquivo existir, execute `cat venv/bin/activate` para ler o conteúdo do arquivo no terminal. Procure por algo que tente executar o arquivo `main.py` dentro desse script.

4.  **Reconstrua o ambiente virtual (opção mais segura):**

    *   **Desative o ambiente atual** (se ainda estiver ativado): `deactivate`
    *   **Exclua o diretório `venv`:** Use o comando `rm -rf venv` (isso removerá o diretório `venv` e todo o seu conteúdo. Use com extrema cautela, certificando-se de que você está no diretório correto!)
    *   **Crie um novo ambiente virtual:** Execute `python -m venv venv`
    *   **Ative o novo ambiente virtual:** Tente `source venv/bin/activate` ou `. venv/bin/activate` ou `powershell -ExecutionPolicy Bypass -File ./venv/Scripts/Activate.ps1`
    *   **Reinstale as dependências:** `pip install -r requirements.txt`

5.  **Verifique as variáveis de ambiente:**
    1.  Examine as variáveis de ambiente que podem estar relacionadas a caminhos ou Python.
    2.  Para exibir as variáveis de ambiente, use o comando `env` no Git Bash.

Com essas etapas, você deve conseguir identificar e resolver o problema ao ativar o ambiente virtual, garantindo que a ativação não tente executar um script que não existe.

O problema é com as configurações de ativação de ambiente virtual, e as etapas acima visam limpar e recriar o ambiente virtual, resolvendo o erro.
2025-04-14 15:13:30,649 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 15:13:30,650 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 15:13:30,652 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 15:13:30,654 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 15:13:30,656 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 15:13:30,658 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 15:13:30,659 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 15:13:30,660 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 15:13:30,661 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 15:13:30,663 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 15:13:30,664 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 15:13:30,665 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 15:13:30,666 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 15:13:30,667 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 15:13:30,670 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 15:13:30,671 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 15:13:30,673 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 15:13:30,674 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 15:13:30,675 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 15:13:30,677 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 15:13:30,678 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-14 15:13:30,679 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 15:13:30,680 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 15:13:30,682 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 15:13:30,683 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 15:13:30,684 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 15:13:30,686 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 15:13:30,687 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 15:13:30,688 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-14 15:13:30,689 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-14 15:13:30,691 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 15:13:30,692 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 15:13:30,694 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 15:13:30,695 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 15:13:30,696 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 15:13:30,697 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 15:13:30,698 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 15:13:30,699 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 15:13:30,700 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 15:13:30,701 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 15:13:30,702 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 15:13:30,703 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 15:13:30,705 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 15:13:30,706 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 15:13:30,707 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 15:13:30,708 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 15:13:30,709 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 15:13:30,710 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 15:13:30,711 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 15:13:30,712 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 15:13:30,713 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 15:13:30,714 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 15:13:30,716 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:19:09,399 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:19:09,400 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:19:09,402 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:19:09,402 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:19:09,404 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:19:09,404 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:19:09,406 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:19:09,406 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:19:09,408 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:19:09,408 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:19:09,411 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:19:09,411 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:19:09,413 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:19:09,414 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:19:09,416 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:19:09,417 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:19:09,420 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:19:09,422 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:19:09,423 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:19:09,424 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:19:09,425 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:19:09,427 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:19:09,429 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:19:09,430 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:19:09,431 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:19:09,434 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:19:09,435 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:19:09,439 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:19:09,440 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:19:09,441 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:19:09,442 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:19:09,444 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:19:09,446 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:19:09,446 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:19:09,448 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:19:09,449 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:19:09,450 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:19:09,452 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:19:09,455 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:19:09,456 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:19:09,458 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:19:09,459 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:19:09,460 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:19:09,462 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:19:09,464 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:19:09,465 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:19:09,467 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:19:09,469 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:19:09,470 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:19:09,472 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:19:09,473 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:19:09,474 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:19:09,475 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:19:09,477 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:19:09,478 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:19:09,479 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:19:09,480 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:19:09,483 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:19:09,485 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:19:09,486 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:19:09,488 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:19:09,489 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:19:09,490 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:19:09,492 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:19:09,493 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:19:09,494 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:19:09,496 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:19:09,497 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:19:09,498 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:19:09,499 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:19:09,501 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:19:09,503 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:19:09,504 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:19:09,506 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:19:09,506 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:19:09,509 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:19:09,510 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:19:09,512 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:19:09,513 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:19:09,515 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:19:09,516 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:19:09,518 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:19:09,520 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:19:09,521 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:19:09,522 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:19:09,523 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:19:09,526 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:19:09,526 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:19:09,528 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:19:09,529 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:19:09,530 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:19:09,531 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:19:09,532 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:19:09,534 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:19:09,536 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:19:09,537 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:19:09,539 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:19:09,540 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:19:09,542 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:19:09,548 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:19:09,557 [INFO] Modelo Gemini 'gemini-2.0-flash-exp' inicializado com sucesso.
2025-04-14 16:19:16,443 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:19:16,445 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:19:16,448 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:19:16,450 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:19:16,452 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:19:16,455 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:19:16,457 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:19:16,458 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:19:16,461 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:19:16,464 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:19:16,466 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:19:16,469 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:19:16,471 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:19:16,473 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:19:16,476 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:19:16,478 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:19:16,482 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:19:16,483 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:19:16,488 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:19:16,490 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:19:16,492 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:19:16,495 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:19:16,498 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:19:16,499 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:19:16,502 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:19:16,505 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:19:16,507 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:19:16,510 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:19:16,514 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:19:16,516 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:19:16,520 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:19:16,523 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:19:16,525 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:19:16,526 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:19:16,529 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:19:16,531 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:19:16,534 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:19:16,538 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:19:16,541 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:19:16,545 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:19:16,547 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:19:16,550 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:19:16,554 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:19:16,558 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:19:16,561 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:19:16,563 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:19:16,565 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:19:16,568 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:19:16,571 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:19:16,574 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:20:38,489 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:20:38,491 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:20:38,493 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:20:38,495 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:20:38,497 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:20:38,499 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:20:38,501 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:20:38,503 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:20:38,505 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:20:38,506 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:20:38,508 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:20:38,510 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:20:38,511 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:20:38,514 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:20:38,516 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:20:38,519 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:20:38,521 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:20:38,523 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:20:38,525 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:20:38,528 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:20:38,531 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:20:38,533 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:20:38,535 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:20:38,537 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:20:38,538 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:20:38,539 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:20:38,541 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:20:38,543 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:20:38,545 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:20:38,547 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:20:38,549 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:20:38,550 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:20:38,551 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:20:38,552 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:20:38,554 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:20:38,555 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:20:38,556 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:20:38,558 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:20:38,560 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:20:38,561 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:20:38,563 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:20:38,565 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:20:38,566 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:20:38,567 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:20:38,568 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:20:38,570 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:20:38,571 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:20:38,573 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:20:38,574 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:20:38,577 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:20:38,674 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:20:38,677 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:20:38,679 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:20:38,681 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:20:38,683 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:20:38,685 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:20:38,687 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:20:38,689 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:20:38,691 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:20:38,694 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:20:38,696 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:20:38,700 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:20:38,702 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:20:38,704 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:20:38,706 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:20:38,708 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:20:38,709 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:20:38,712 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:20:38,713 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:20:38,716 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:20:38,717 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:20:38,719 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:20:38,721 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:20:38,722 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:20:38,724 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:20:38,726 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:20:38,728 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:20:38,733 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:20:38,735 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:20:38,738 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:20:38,740 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:20:38,741 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:20:38,744 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:20:38,746 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:20:38,749 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:20:38,751 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:20:38,753 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:20:38,755 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:20:38,757 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:20:38,760 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:20:38,762 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:20:38,763 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:20:38,765 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:20:38,767 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:20:38,768 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:20:38,769 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:20:38,771 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:20:38,772 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:20:38,773 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:20:38,775 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:20:38,778 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diretório atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Definição dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Criação dos diretórios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configuração do layout do Streamlit
st.set_page_config(
    page_title="Análise de Ações",
    layout="wide",
    page_icon="📈"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscilações

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Período', 'Oscilação'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("📊 Dashboard de Análise de Ações")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma ação", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"📌 Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("🔍 Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descrição', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa ação.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("💰 Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("📉 Oscilações")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("📈 Tabela Geral de Ações")
        st.dataframe(self.acoes)

# Execução
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para análise fundamentalista de ações da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se está correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configuração de vídeos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configuração de áudios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcrições e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diretório
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diretório.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do último arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conteúdo do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimensões do embedding:", embedding.shape)

# Caminho do diretório de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o último embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configuração de transcrições e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diretório onde está procurando as transcrições
        print(f"Diretório de entrada das transcrições: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcrição no diretório de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcrição encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo não encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcrição do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} está vazio.")
                    return

            # Gerar o embedding da transcrição
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcrição e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O vídeo {video_path} não contém áudio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de vídeo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de vídeo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de vídeo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Conversão de vídeo para áudio concluída!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configurações da API
handler = genai('gemini-1.5-flash')

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Função para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conteúdo com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares únicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcrição obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' não é uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuniões: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de reuniões: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|tópicos da|pontos (relevantes|principais)|análise)\b.*\b(reunião|última (reunião|conversa|sessão))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como análise de reunião.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padrão.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que são muito similares ao comando atual para evitar redundância.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padrão: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # Métodos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de análise de reunião.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conteúdo usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inválida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar variáveis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conteúdo com base no prompt, contexto e reunião"""
        try:
            # Supondo que a API espera um dicionário com os parâmetros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisição para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar variáveis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Criação da conclusão do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que você acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["faça um resumo da última reunião.", "tópicos da última reunião", "resuma a última reunião", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Você é responsável por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - Não acrescentar título nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informações acima e forneça uma solução detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Você é responsável analisar com detalhes a reunião de [str_meeting], e fornecer uma longa estória sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precisão da resposta.
            - Não acrescente título nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e faça uma pesquisa online para uma resposta mais precisa das questões de [str_text]
        - Não acrescente título nas respostas.
        
        ------
        [context]: Regras Básicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("Áudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum áudio detectado.")
                return None
            except sr.UnknownValueError:
                print("Não foi possível entender o áudio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configurações da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicionário
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta não está no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API: {e}")
        return None

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: Faça uma limpeza na pasta class_voice_assistent
2025-04-14 16:20:47,658 [INFO] Resposta da IA (texto): Com base nos arquivos fornecidos, aqui está um resumo da estrutura e funcionalidade do código, juntamente com as informações sobre a limpeza da pasta `class_voice_assistent`:

**Visão geral do projeto:**

Este projeto parece ser um assistente de voz que usa o Google Gemini para processamento de linguagem natural. Ele é estruturado em vários módulos e classes para lidar com diferentes aspectos, como captura de áudio, conversão de texto em fala, interpretação de comandos e interação com APIs externas.

**Principais componentes:**

*   **Captura de comandos:**
    *   `voice_assistent\class_voice_assistent\voice_command_hendler.py`: Lida com a captura de comandos de voz usando a biblioteca `speech_recognition`.
    *   `voice_assistent\class_voice_assistent\text_command_hendler.py`: Lida com a captura de comandos de texto do usuário.

*   **Processamento de linguagem natural:**
    *   `voice_assistent\class_voice_assistent\command_interpreter.py`: Interpreta os comandos do usuário e determina a ação apropriada a ser executada.
    *   `voice_assistent\class_voice_assistent\question_answers_service.py`: Converte texto em embeddings para encontrar respostas semelhantes.
    *   `voice_assistent\class_voice_assistent\context_manager.py`: Gerencia o contexto da conversa, armazenando as interações recentes.

*   **Comunicação com o Google Gemini:**
    *   `voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py`: Lida com a comunicação com a API do Google Gemini.
    *   `voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py`: Define o modelo generativo e a configuração da API Google Gemini.

*   **Geração de prompt:**
    *   `voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py`: Gera prompts padrão para o modelo de linguagem.
    *   `voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py`: Gera prompts para análise de reuniões.
    *   `voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py`: Gera prompts para pesquisas online.
    *   `voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py`: Define a classe abstrata para geradores de prompt.

*   **Texto para fala:**
    *   `voice_assistent\class_voice_assistent\text_to_speech.py`: Converte texto em fala usando a biblioteca `pyttsx3`.

*   **Processamento de texto:**
    *   `voice_assistent\class_voice_assistent\text_processor.py`: Fornece funções para extrair valores de JSON e texto de HTML.

*   **Interface principal:**
    *   `voice_assistent\class_voice_assistent\main.py`: Inicializa e coordena os diferentes componentes do assistente de voz.

**Limpeza da pasta `class_voice_assistent`:**

O comando para limpar uma pasta não faz parte diretamente do código fornecido. Para implementar essa funcionalidade, você precisaria adicionar um código que use a biblioteca `os` ou `shutil` para remover arquivos e subdiretórios da pasta `class_voice_assistent`.

**Exemplo de como implementar a limpeza da pasta:**

```python
import os
import shutil

def clean_directory(path):
    """Remove todos os arquivos e subdiretórios em um diretório."""
    for filename in os.listdir(path):
        file_path = os.path.join(path, filename)
        try:
            if os.path.isfile(file_path) or os.path.islink(file_path):
                os.unlink(file_path)
            elif os.path.isdir(file_path):
                shutil.rmtree(file_path)
        except Exception as e:
            print(f'Falha ao excluir {file_path}. Motivo: {e}')

# Caminho para a pasta class_voice_assistent
path_to_clean = 'voice_assistent/class_voice_assistent'

# Chame a função para limpar a pasta
clean_directory(path_to_clean)
```

**Considerações:**

1.  **Importe os módulos necessários**: `os` e `shutil`.
2.  **Defina a função `clean_directory`**: Esta função itera sobre todos os itens no diretório fornecido e tenta remover cada um.
3.  **Tratamento de erros**: A função inclui um bloco `try...except` para lidar com possíveis erros durante a remoção de arquivos ou diretórios.
4.  **Chame a função `clean_directory`**: Passe o caminho do diretório que você deseja limpar.

**Onde adicionar este código:**

Você pode integrar esta função dentro do `command_interpreter.py` ou `main.py`. Por exemplo, dentro do `interpret_command`, você pode adicionar uma condição para detectar o comando "limpar pasta" e, em seguida, chamar a função `clean_directory` com o caminho apropriado.

**Aviso**: Esta operação é irreversível e excluirá permanentemente todos os arquivos e pastas dentro do diretório especificado. Certifique-se de ter um backup se necessário.

**Segurança:**

Certifique-se de que o caminho da pasta a ser limpa seja validado e seguro para evitar exclusões acidentais ou maliciosas.

2025-04-14 16:20:47,758 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:20:47,760 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:20:47,762 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:20:47,763 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:20:47,765 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:20:47,766 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:20:47,767 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:20:47,769 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:20:47,770 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:20:47,771 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:20:47,773 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:20:47,774 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:20:47,775 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:20:47,777 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:20:47,778 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:20:47,780 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:20:47,781 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:20:47,782 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:20:47,783 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:20:47,784 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:20:47,786 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:20:47,787 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:20:47,789 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:20:47,790 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:20:47,791 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:20:47,792 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:20:47,793 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:20:47,795 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:20:47,797 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:20:47,798 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:20:47,799 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:20:47,801 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:20:47,802 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:20:47,803 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:20:47,804 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:20:47,805 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:20:47,806 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:20:47,807 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:20:47,808 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:20:47,810 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:20:47,812 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:20:47,814 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:20:47,816 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:20:47,817 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:20:47,818 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:20:47,820 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:20:47,821 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:20:47,822 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:20:47,823 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:20:47,824 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:21:59,128 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:21:59,130 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:21:59,132 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:21:59,133 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:21:59,136 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:21:59,137 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:21:59,139 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:21:59,141 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:21:59,143 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:21:59,145 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:21:59,146 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:21:59,147 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:21:59,149 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:21:59,152 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:21:59,153 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:21:59,159 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:21:59,171 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:21:59,179 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:21:59,181 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:21:59,185 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:21:59,187 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:21:59,189 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:21:59,191 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:21:59,192 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:21:59,194 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:21:59,195 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:21:59,196 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:21:59,197 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:21:59,199 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:21:59,200 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:21:59,202 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:21:59,204 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:21:59,206 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:21:59,208 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:21:59,209 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:21:59,211 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:21:59,213 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:21:59,214 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:21:59,216 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:21:59,218 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:21:59,219 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:21:59,222 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:21:59,224 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:21:59,225 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:21:59,227 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:21:59,228 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:21:59,230 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:21:59,231 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:21:59,233 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:21:59,236 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:21:59,333 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:21:59,335 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:21:59,337 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:21:59,339 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:21:59,340 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:21:59,342 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:21:59,343 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:21:59,345 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:21:59,347 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:21:59,349 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:21:59,351 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:21:59,354 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:21:59,356 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:21:59,357 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:21:59,359 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:21:59,361 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:21:59,363 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:21:59,364 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:21:59,366 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:21:59,370 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:21:59,374 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:21:59,376 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:21:59,378 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:21:59,380 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:21:59,382 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:21:59,386 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:21:59,388 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:21:59,391 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:21:59,393 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:21:59,394 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:21:59,396 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:21:59,398 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:21:59,401 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:21:59,403 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:21:59,405 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:21:59,406 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:21:59,407 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:21:59,408 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:21:59,410 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:21:59,411 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:21:59,412 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:21:59,413 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:21:59,415 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:21:59,416 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:21:59,418 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:21:59,419 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:21:59,421 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:21:59,422 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:21:59,424 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:21:59,425 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:21:59,427 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diretório atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Definição dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Criação dos diretórios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configuração do layout do Streamlit
st.set_page_config(
    page_title="Análise de Ações",
    layout="wide",
    page_icon="📈"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscilações

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Período', 'Oscilação'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("📊 Dashboard de Análise de Ações")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma ação", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"📌 Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("🔍 Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descrição', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa ação.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("💰 Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("📉 Oscilações")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("📈 Tabela Geral de Ações")
        st.dataframe(self.acoes)

# Execução
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para análise fundamentalista de ações da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se está correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configuração de vídeos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configuração de áudios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcrições e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diretório
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diretório.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do último arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conteúdo do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimensões do embedding:", embedding.shape)

# Caminho do diretório de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o último embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configuração de transcrições e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diretório onde está procurando as transcrições
        print(f"Diretório de entrada das transcrições: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcrição no diretório de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcrição encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo não encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcrição do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} está vazio.")
                    return

            # Gerar o embedding da transcrição
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcrição e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O vídeo {video_path} não contém áudio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de vídeo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de vídeo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de vídeo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Conversão de vídeo para áudio concluída!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configurações da API
handler = genai('gemini-1.5-flash')

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Função para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conteúdo com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares únicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcrição obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' não é uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuniões: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de reuniões: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|tópicos da|pontos (relevantes|principais)|análise)\b.*\b(reunião|última (reunião|conversa|sessão))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como análise de reunião.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padrão.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que são muito similares ao comando atual para evitar redundância.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padrão: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # Métodos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de análise de reunião.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conteúdo usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inválida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar variáveis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conteúdo com base no prompt, contexto e reunião"""
        try:
            # Supondo que a API espera um dicionário com os parâmetros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisição para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar variáveis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Criação da conclusão do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que você acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["faça um resumo da última reunião.", "tópicos da última reunião", "resuma a última reunião", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Você é responsável por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - Não acrescentar título nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informações acima e forneça uma solução detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Você é responsável analisar com detalhes a reunião de [str_meeting], e fornecer uma longa estória sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precisão da resposta.
            - Não acrescente título nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e faça uma pesquisa online para uma resposta mais precisa das questões de [str_text]
        - Não acrescente título nas respostas.
        
        ------
        [context]: Regras Básicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("Áudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum áudio detectado.")
                return None
            except sr.UnknownValueError:
                print("Não foi possível entender o áudio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configurações da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicionário
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta não está no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API: {e}")
        return None

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: Faça uma limpeza na pasta class_voice_assistent
Assistant: Com base nos arquivos fornecidos, aqui está um resumo da estrutura e funcionalidade do código, juntamente com as informações sobre a limpeza da pasta `class_voice_assistent`:

**Visão geral do projeto:**

Este projeto parece ser um assistente de voz que usa o Google Gemini para processamento de linguagem natural. Ele é estruturado em vários módulos e classes para lidar com diferentes aspectos, como captura de áudio, conversão de texto em fala, interpretação de comandos e interação com APIs externas.

**Principais componentes:**

*   **Captura de comandos:**
    *   `voice_assistent\class_voice_assistent\voice_command_hendler.py`: Lida com a captura de comandos de voz usando a biblioteca `speech_recognition`.
    *   `voice_assistent\class_voice_assistent\text_command_hendler.py`: Lida com a captura de comandos de texto do usuário.

*   **Processamento de linguagem natural:**
    *   `voice_assistent\class_voice_assistent\command_interpreter.py`: Interpreta os comandos do usuário e determina a ação apropriada a ser executada.
    *   `voice_assistent\class_voice_assistent\question_answers_service.py`: Converte texto em embeddings para encontrar respostas semelhantes.
    *   `voice_assistent\class_voice_assistent\context_manager.py`: Gerencia o contexto da conversa, armazenando as interações recentes.

*   **Comunicação com o Google Gemini:**
    *   `voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py`: Lida com a comunicação com a API do Google Gemini.
    *   `voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py`: Define o modelo generativo e a configuração da API Google Gemini.

*   **Geração de prompt:**
    *   `voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py`: Gera prompts padrão para o modelo de linguagem.
    *   `voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py`: Gera prompts para análise de reuniões.
    *   `voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py`: Gera prompts para pesquisas online.
    *   `voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py`: Define a classe abstrata para geradores de prompt.

*   **Texto para fala:**
    *   `voice_assistent\class_voice_assistent\text_to_speech.py`: Converte texto em fala usando a biblioteca `pyttsx3`.

*   **Processamento de texto:**
    *   `voice_assistent\class_voice_assistent\text_processor.py`: Fornece funções para extrair valores de JSON e texto de HTML.

*   **Interface principal:**
    *   `voice_assistent\class_voice_assistent\main.py`: Inicializa e coordena os diferentes componentes do assistente de voz.

**Limpeza da pasta `class_voice_assistent`:**

O comando para limpar uma pasta não faz parte diretamente do código fornecido. Para implementar essa funcionalidade, você precisaria adicionar um código que use a biblioteca `os` ou `shutil` para remover arquivos e subdiretórios da pasta `class_voice_assistent`.

**Exemplo de como implementar a limpeza da pasta:**

```python
import os
import shutil

def clean_directory(path):
    """Remove todos os arquivos e subdiretórios em um diretório."""
    for filename in os.listdir(path):
        file_path = os.path.join(path, filename)
        try:
            if os.path.isfile(file_path) or os.path.islink(file_path):
                os.unlink(file_path)
            elif os.path.isdir(file_path):
                shutil.rmtree(file_path)
        except Exception as e:
            print(f'Falha ao excluir {file_path}. Motivo: {e}')

# Caminho para a pasta class_voice_assistent
path_to_clean = 'voice_assistent/class_voice_assistent'

# Chame a função para limpar a pasta
clean_directory(path_to_clean)
```

**Considerações:**

1.  **Importe os módulos necessários**: `os` e `shutil`.
2.  **Defina a função `clean_directory`**: Esta função itera sobre todos os itens no diretório fornecido e tenta remover cada um.
3.  **Tratamento de erros**: A função inclui um bloco `try...except` para lidar com possíveis erros durante a remoção de arquivos ou diretórios.
4.  **Chame a função `clean_directory`**: Passe o caminho do diretório que você deseja limpar.

**Onde adicionar este código:**

Você pode integrar esta função dentro do `command_interpreter.py` ou `main.py`. Por exemplo, dentro do `interpret_command`, você pode adicionar uma condição para detectar o comando "limpar pasta" e, em seguida, chamar a função `clean_directory` com o caminho apropriado.

**Aviso**: Esta operação é irreversível e excluirá permanentemente todos os arquivos e pastas dentro do diretório especificado. Certifique-se de ter um backup se necessário.

**Segurança:**

Certifique-se de que o caminho da pasta a ser limpa seja validado e seguro para evitar exclusões acidentais ou maliciosas.

User: quero remover códigos desnecessários na pasta voice_assistent no projeto.
2025-04-14 16:22:08,049 [INFO] Resposta da IA (texto): Assistant: Para remover códigos desnecessários na pasta `voice_assistent` do projeto, aqui está uma abordagem detalhada:

**1. Visão Geral:**

O objetivo é identificar e remover arquivos e funcionalidades redundantes ou não utilizados na pasta `voice_assistent`. Isso envolve uma análise cuidadosa do código, das dependências e do fluxo do programa para garantir que nenhuma funcionalidade essencial seja comprometida.

**2. Etapas Detalhadas:**

*   **Backup:**
    *   Antes de fazer qualquer alteração, faça um backup completo da pasta `voice_assistent`. Isso permitirá restaurar o estado anterior em caso de remoção acidental de código importante.

*   **Análise da Estrutura do Código:**
    *   **Identifique os principais módulos:** Analise todos os arquivos Python na pasta `voice_assistent` e suas subpastas para entender sua função e como eles se interconectam.
    *   **Módulos Essenciais:**
        *   `class_voice_assistent/main.py`: Coordena a operação do assistente de voz.
        *   `class_voice_assistent/command_interpreter.py`: Interpreta comandos do usuário.
        *   `class_voice_assistent/gpt_communication/gemini_gpt.py`: Lida com a comunicação com o Google Gemini.
        *   `class_voice_assistent/voice_command_handler.py` e `class_voice_assistent/text_command_handler.py`: Capturam comandos de voz e texto, respectivamente.
        *   `class_voice_assistent/text_to_speech.py`: Converte texto em fala.

*   **Identificação de Código Não Utilizado:**
    *   **Remova imports não utilizados:** Abra cada arquivo e verifique se todos os módulos importados estão realmente sendo usados. Remova aqueles que não estão.

    *   **Analise funções não utilizadas:** Verifique se todas as funções definidas são chamadas em algum lugar do código. Funções que não são chamadas podem ser removidas com segurança.
    *   **Avalie código comentado:** Avalie se os blocos de código comentados ainda são relevantes ou podem ser removidos.

*   **Análise das Dependências:**
    *   **Arquivo requirements.txt ou pyproject.toml:** Verifique se todas as bibliotecas listadas nos arquivos de dependência são necessárias para o funcionamento do assistente de voz. Remova as dependências que não estão sendo usadas.

*   **Simplificação da Lógica:**
    *   **Refatore código duplicado:** Se houver trechos de código repetidos, crie funções ou classes para reutilizá-los, reduzindo a redundância.
    *   **Simplifique funções complexas:** Divida funções muito longas ou complexas em funções menores e mais fáceis de entender e manter.

*   **Remoção de Código Desnecessário:**
    *   **Exclua arquivos desnecessários:** Remova arquivos que não contribuem para a funcionalidade principal do assistente de voz.

*   **Teste:**
    *   Após cada remoção ou modificação significativa, execute os testes para garantir que o assistente de voz continue funcionando corretamente.
    *   Crie testes automatizados para os principais componentes para facilitar a detecção de erros após futuras alterações.

**3. Implementação:**

*   **Edite os Arquivos:**
    *   Abra cada arquivo Python relevante e faça as modificações necessárias, como remover imports não utilizados, funções desnecessárias e refatorar o código.
*   **Remova Arquivos Desnecessários:**
    *   Exclua arquivos que não são mais necessários. Certifique-se de que nenhum outro componente depende desses arquivos antes de removê-los.

**4. Exemplo Prático:**

*   **Avaliação de `template.py`:**
    *   O arquivo `template.py` parece ser uma versão mais antiga ou um modelo inicial do assistente de voz. Compare-o com os outros arquivos (principalmente os arquivos em `class_voice_assistent`) para identificar se alguma parte do código em `template.py` ainda é usada. Se não for, considere remover este arquivo.

*   **Exemplo de refatoração:**
    *   Se você encontrar código duplicado entre `voice_command_hendler.py` e `text_command_hendler.py`, você pode criar uma classe base ou função utilitária para lidar com a lógica comum, reduzindo a duplicação.

**5. Exemplo de Código para Remover Arquivos:**

Para remover um arquivo específico, você pode usar o seguinte código:

```python
import os

file_path = "voice_assistent/class_voice_assistent/arquivo_desnecessario.py"
try:
    os.remove(file_path)
    print(f"Arquivo {file_path} removido com sucesso.")
except FileNotFoundError:
    print(f"Arquivo {file_path} não encontrado.")
except Exception as e:
    print(f"Erro ao remover {file_path}: {e}")
```

**Considerações Adicionais:**

*   **Documentação:** Atualize a documentação do projeto para refletir as mudanças realizadas. Isso ajudará outros desenvolvedores (ou você mesmo no futuro) a entender a nova estrutura do código.
*   **Versionamento:** Use um sistema de controle de versão (como Git) para gerenciar as alterações no código. Isso facilita a reversão de alterações e a colaboração com outros desenvolvedores.

Seguindo essas etapas, você poderá remover códigos desnecessários da pasta `voice_assistent`, tornando o projeto mais limpo, eficiente e fácil de manter.
2025-04-14 16:22:08,145 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:22:08,147 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:22:08,149 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:22:08,151 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:22:08,154 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:22:08,155 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:22:08,157 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:22:08,158 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:22:08,161 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:22:08,163 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:22:08,164 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:22:08,166 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:22:08,167 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:22:08,169 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:22:08,171 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:22:08,172 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:22:08,173 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:22:08,174 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:22:08,177 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:22:08,179 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:22:08,180 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:22:08,181 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:22:08,183 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:22:08,184 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:22:08,185 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:22:08,187 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:22:08,189 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:22:08,190 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:22:08,191 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:22:08,193 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:22:08,196 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:22:08,197 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:22:08,198 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:22:08,199 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:22:08,201 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:22:08,203 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:22:08,205 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:22:08,207 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:22:08,208 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:22:08,209 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:22:08,210 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:22:08,212 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:22:08,213 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:22:08,214 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:22:08,215 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:22:08,217 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:22:08,218 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:22:08,220 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:22:08,221 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:22:08,222 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:24:31,011 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:24:31,012 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:24:31,014 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:24:31,015 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:24:31,016 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:24:31,017 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:24:31,019 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:24:31,020 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:24:31,022 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:24:31,023 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:24:31,025 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:24:31,027 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:24:31,029 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:24:31,031 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:24:31,033 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:24:31,036 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:24:31,038 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:24:31,040 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:24:31,043 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:24:31,045 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:24:31,046 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:24:31,047 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:24:31,048 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:24:31,050 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:24:31,052 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:24:31,053 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:24:31,054 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:24:31,055 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:24:31,056 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:24:31,058 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:24:31,059 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:24:31,061 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:24:31,062 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:24:31,064 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:24:31,065 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:24:31,067 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:24:31,069 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:24:31,070 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:24:31,072 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:24:31,073 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:24:31,075 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:24:31,077 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:24:31,079 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:24:31,080 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:24:31,082 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:24:31,084 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:24:31,086 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:24:31,087 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:24:31,089 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:24:31,091 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:24:31,194 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:24:31,197 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:24:31,198 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:24:31,200 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:24:31,201 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:24:31,203 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:24:31,204 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:24:31,205 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:24:31,207 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:24:31,209 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:24:31,210 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:24:31,212 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:24:31,213 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:24:31,215 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:24:31,218 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:24:31,219 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:24:31,221 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:24:31,222 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:24:31,224 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:24:31,225 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:24:31,227 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:24:31,228 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:24:31,230 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:24:31,231 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:24:31,233 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:24:31,236 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:24:31,237 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:24:31,239 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:24:31,241 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:24:31,243 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:24:31,244 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:24:31,246 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:24:31,249 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:24:31,251 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:24:31,253 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:24:31,255 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:24:31,256 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:24:31,258 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:24:31,260 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:24:31,261 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:24:31,263 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:24:31,267 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:24:31,278 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:24:31,279 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:24:31,283 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:24:31,285 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:24:31,287 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:24:31,288 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:24:31,289 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:24:31,291 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:24:31,292 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diretório atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Definição dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Criação dos diretórios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configuração do layout do Streamlit
st.set_page_config(
    page_title="Análise de Ações",
    layout="wide",
    page_icon="📈"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscilações

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Período', 'Oscilação'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("📊 Dashboard de Análise de Ações")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma ação", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"📌 Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("🔍 Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descrição', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa ação.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("💰 Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("📉 Oscilações")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("📈 Tabela Geral de Ações")
        st.dataframe(self.acoes)

# Execução
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para análise fundamentalista de ações da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se está correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configuração de vídeos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configuração de áudios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcrições e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diretório
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diretório.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do último arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conteúdo do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimensões do embedding:", embedding.shape)

# Caminho do diretório de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o último embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configuração de transcrições e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diretório onde está procurando as transcrições
        print(f"Diretório de entrada das transcrições: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcrição no diretório de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcrição encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo não encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcrição do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} está vazio.")
                    return

            # Gerar o embedding da transcrição
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcrição e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O vídeo {video_path} não contém áudio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de vídeo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de vídeo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de vídeo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Conversão de vídeo para áudio concluída!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configurações da API
handler = genai('gemini-1.5-flash')

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Função para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conteúdo com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares únicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcrição obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' não é uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuniões: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de reuniões: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|tópicos da|pontos (relevantes|principais)|análise)\b.*\b(reunião|última (reunião|conversa|sessão))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como análise de reunião.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padrão.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que são muito similares ao comando atual para evitar redundância.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padrão: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # Métodos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de análise de reunião.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conteúdo usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inválida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar variáveis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conteúdo com base no prompt, contexto e reunião"""
        try:
            # Supondo que a API espera um dicionário com os parâmetros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisição para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar variáveis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Criação da conclusão do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que você acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["faça um resumo da última reunião.", "tópicos da última reunião", "resuma a última reunião", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Você é responsável por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - Não acrescentar título nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informações acima e forneça uma solução detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Você é responsável analisar com detalhes a reunião de [str_meeting], e fornecer uma longa estória sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precisão da resposta.
            - Não acrescente título nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e faça uma pesquisa online para uma resposta mais precisa das questões de [str_text]
        - Não acrescente título nas respostas.
        
        ------
        [context]: Regras Básicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("Áudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum áudio detectado.")
                return None
            except sr.UnknownValueError:
                print("Não foi possível entender o áudio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configurações da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicionário
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta não está no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API: {e}")
        return None

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: Faça uma limpeza na pasta class_voice_assistent
Assistant: Com base nos arquivos fornecidos, aqui está um resumo da estrutura e funcionalidade do código, juntamente com as informações sobre a limpeza da pasta `class_voice_assistent`:

**Visão geral do projeto:**

Este projeto parece ser um assistente de voz que usa o Google Gemini para processamento de linguagem natural. Ele é estruturado em vários módulos e classes para lidar com diferentes aspectos, como captura de áudio, conversão de texto em fala, interpretação de comandos e interação com APIs externas.

**Principais componentes:**

*   **Captura de comandos:**
    *   `voice_assistent\class_voice_assistent\voice_command_hendler.py`: Lida com a captura de comandos de voz usando a biblioteca `speech_recognition`.
    *   `voice_assistent\class_voice_assistent\text_command_hendler.py`: Lida com a captura de comandos de texto do usuário.

*   **Processamento de linguagem natural:**
    *   `voice_assistent\class_voice_assistent\command_interpreter.py`: Interpreta os comandos do usuário e determina a ação apropriada a ser executada.
    *   `voice_assistent\class_voice_assistent\question_answers_service.py`: Converte texto em embeddings para encontrar respostas semelhantes.
    *   `voice_assistent\class_voice_assistent\context_manager.py`: Gerencia o contexto da conversa, armazenando as interações recentes.

*   **Comunicação com o Google Gemini:**
    *   `voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py`: Lida com a comunicação com a API do Google Gemini.
    *   `voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py`: Define o modelo generativo e a configuração da API Google Gemini.

*   **Geração de prompt:**
    *   `voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py`: Gera prompts padrão para o modelo de linguagem.
    *   `voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py`: Gera prompts para análise de reuniões.
    *   `voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py`: Gera prompts para pesquisas online.
    *   `voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py`: Define a classe abstrata para geradores de prompt.

*   **Texto para fala:**
    *   `voice_assistent\class_voice_assistent\text_to_speech.py`: Converte texto em fala usando a biblioteca `pyttsx3`.

*   **Processamento de texto:**
    *   `voice_assistent\class_voice_assistent\text_processor.py`: Fornece funções para extrair valores de JSON e texto de HTML.

*   **Interface principal:**
    *   `voice_assistent\class_voice_assistent\main.py`: Inicializa e coordena os diferentes componentes do assistente de voz.

**Limpeza da pasta `class_voice_assistent`:**

O comando para limpar uma pasta não faz parte diretamente do código fornecido. Para implementar essa funcionalidade, você precisaria adicionar um código que use a biblioteca `os` ou `shutil` para remover arquivos e subdiretórios da pasta `class_voice_assistent`.

**Exemplo de como implementar a limpeza da pasta:**

```python
import os
import shutil

def clean_directory(path):
    """Remove todos os arquivos e subdiretórios em um diretório."""
    for filename in os.listdir(path):
        file_path = os.path.join(path, filename)
        try:
            if os.path.isfile(file_path) or os.path.islink(file_path):
                os.unlink(file_path)
            elif os.path.isdir(file_path):
                shutil.rmtree(file_path)
        except Exception as e:
            print(f'Falha ao excluir {file_path}. Motivo: {e}')

# Caminho para a pasta class_voice_assistent
path_to_clean = 'voice_assistent/class_voice_assistent'

# Chame a função para limpar a pasta
clean_directory(path_to_clean)
```

**Considerações:**

1.  **Importe os módulos necessários**: `os` e `shutil`.
2.  **Defina a função `clean_directory`**: Esta função itera sobre todos os itens no diretório fornecido e tenta remover cada um.
3.  **Tratamento de erros**: A função inclui um bloco `try...except` para lidar com possíveis erros durante a remoção de arquivos ou diretórios.
4.  **Chame a função `clean_directory`**: Passe o caminho do diretório que você deseja limpar.

**Onde adicionar este código:**

Você pode integrar esta função dentro do `command_interpreter.py` ou `main.py`. Por exemplo, dentro do `interpret_command`, você pode adicionar uma condição para detectar o comando "limpar pasta" e, em seguida, chamar a função `clean_directory` com o caminho apropriado.

**Aviso**: Esta operação é irreversível e excluirá permanentemente todos os arquivos e pastas dentro do diretório especificado. Certifique-se de ter um backup se necessário.

**Segurança:**

Certifique-se de que o caminho da pasta a ser limpa seja validado e seguro para evitar exclusões acidentais ou maliciosas.

User: quero remover códigos desnecessários na pasta voice_assistent no projeto.
Assistant: Assistant: Para remover códigos desnecessários na pasta `voice_assistent` do projeto, aqui está uma abordagem detalhada:

**1. Visão Geral:**

O objetivo é identificar e remover arquivos e funcionalidades redundantes ou não utilizados na pasta `voice_assistent`. Isso envolve uma análise cuidadosa do código, das dependências e do fluxo do programa para garantir que nenhuma funcionalidade essencial seja comprometida.

**2. Etapas Detalhadas:**

*   **Backup:**
    *   Antes de fazer qualquer alteração, faça um backup completo da pasta `voice_assistent`. Isso permitirá restaurar o estado anterior em caso de remoção acidental de código importante.

*   **Análise da Estrutura do Código:**
    *   **Identifique os principais módulos:** Analise todos os arquivos Python na pasta `voice_assistent` e suas subpastas para entender sua função e como eles se interconectam.
    *   **Módulos Essenciais:**
        *   `class_voice_assistent/main.py`: Coordena a operação do assistente de voz.
        *   `class_voice_assistent/command_interpreter.py`: Interpreta comandos do usuário.
        *   `class_voice_assistent/gpt_communication/gemini_gpt.py`: Lida com a comunicação com o Google Gemini.
        *   `class_voice_assistent/voice_command_handler.py` e `class_voice_assistent/text_command_handler.py`: Capturam comandos de voz e texto, respectivamente.
        *   `class_voice_assistent/text_to_speech.py`: Converte texto em fala.

*   **Identificação de Código Não Utilizado:**
    *   **Remova imports não utilizados:** Abra cada arquivo e verifique se todos os módulos importados estão realmente sendo usados. Remova aqueles que não estão.

    *   **Analise funções não utilizadas:** Verifique se todas as funções definidas são chamadas em algum lugar do código. Funções que não são chamadas podem ser removidas com segurança.
    *   **Avalie código comentado:** Avalie se os blocos de código comentados ainda são relevantes ou podem ser removidos.

*   **Análise das Dependências:**
    *   **Arquivo requirements.txt ou pyproject.toml:** Verifique se todas as bibliotecas listadas nos arquivos de dependência são necessárias para o funcionamento do assistente de voz. Remova as dependências que não estão sendo usadas.

*   **Simplificação da Lógica:**
    *   **Refatore código duplicado:** Se houver trechos de código repetidos, crie funções ou classes para reutilizá-los, reduzindo a redundância.
    *   **Simplifique funções complexas:** Divida funções muito longas ou complexas em funções menores e mais fáceis de entender e manter.

*   **Remoção de Código Desnecessário:**
    *   **Exclua arquivos desnecessários:** Remova arquivos que não contribuem para a funcionalidade principal do assistente de voz.

*   **Teste:**
    *   Após cada remoção ou modificação significativa, execute os testes para garantir que o assistente de voz continue funcionando corretamente.
    *   Crie testes automatizados para os principais componentes para facilitar a detecção de erros após futuras alterações.

**3. Implementação:**

*   **Edite os Arquivos:**
    *   Abra cada arquivo Python relevante e faça as modificações necessárias, como remover imports não utilizados, funções desnecessárias e refatorar o código.
*   **Remova Arquivos Desnecessários:**
    *   Exclua arquivos que não são mais necessários. Certifique-se de que nenhum outro componente depende desses arquivos antes de removê-los.

**4. Exemplo Prático:**

*   **Avaliação de `template.py`:**
    *   O arquivo `template.py` parece ser uma versão mais antiga ou um modelo inicial do assistente de voz. Compare-o com os outros arquivos (principalmente os arquivos em `class_voice_assistent`) para identificar se alguma parte do código em `template.py` ainda é usada. Se não for, considere remover este arquivo.

*   **Exemplo de refatoração:**
    *   Se você encontrar código duplicado entre `voice_command_hendler.py` e `text_command_hendler.py`, você pode criar uma classe base ou função utilitária para lidar com a lógica comum, reduzindo a duplicação.

**5. Exemplo de Código para Remover Arquivos:**

Para remover um arquivo específico, você pode usar o seguinte código:

```python
import os

file_path = "voice_assistent/class_voice_assistent/arquivo_desnecessario.py"
try:
    os.remove(file_path)
    print(f"Arquivo {file_path} removido com sucesso.")
except FileNotFoundError:
    print(f"Arquivo {file_path} não encontrado.")
except Exception as e:
    print(f"Erro ao remover {file_path}: {e}")
```

**Considerações Adicionais:**

*   **Documentação:** Atualize a documentação do projeto para refletir as mudanças realizadas. Isso ajudará outros desenvolvedores (ou você mesmo no futuro) a entender a nova estrutura do código.
*   **Versionamento:** Use um sistema de controle de versão (como Git) para gerenciar as alterações no código. Isso facilita a reversão de alterações e a colaboração com outros desenvolvedores.

Seguindo essas etapas, você poderá remover códigos desnecessários da pasta `voice_assistent`, tornando o projeto mais limpo, eficiente e fácil de manter.
User: você tem o código no contexto. você é meu assistente e deve fazer a análise para mim.
2025-04-14 16:24:40,808 [INFO] Resposta da IA (texto): Com prazer! Analisei o código que você forneceu e identifiquei algumas áreas onde podemos remover código desnecessário e refatorar para melhor clareza e eficiência.

**1. Análise Detalhada e Sugestões:**

*   **`template.py`:**

    *   **Status:** Este arquivo parece ser uma versão anterior ou um modelo desatualizado do assistente de voz.
    *   **Ação:** Comparar o código deste arquivo com os componentes atuais em `class_voice_assistent`. Se o código não for mais utilizado, **remover o arquivo `template.py`**.

*   **`voice_assistent\config.py`:**

    *   **Status:** Este arquivo contém configurações básicas, mas algumas dessas configurações podem estar duplicadas ou não sendo usadas.
    *   **Ação:**
        *   **apiKey e url:** Verificar se essas variáveis ainda são necessárias e se não estão duplicadas em outro lugar (por exemplo, em um arquivo `.env`).
        *   **engine, recent_context, nlp:** Estes itens são inicializados novamente em outros arquivos. Avaliar se é melhor centralizar a inicialização em um único local (possivelmente em `class_voice_assistent/main.py`) e importar as instâncias onde necessário.

*   **`class_voice_assistent\conversation_history.py`:**

    *   **Status:** Este arquivo está vazio.
    *   **Ação:** **Remover este arquivo** se ele não for usado. Se a intenção era armazenar o histórico de conversas, a funcionalidade deve ser movida para um arquivo relevante ou integrada em `class_voice_assistent\context_manager.py`.

*   **`class_voice_assistent\gpt_communication\groq._gpt.py`:**

    *   **Status:** Este arquivo demonstra o uso da API Groq, mas não parece estar integrado ao resto do código.
    *   **Ação:** Se a API Groq não for utilizada, **remover este arquivo**. Caso contrário, integrar corretamente e remover a duplicação de configurações e inicializações.

*   **`class_voice_assistent\prompt_generator\prompt_generator.py`:**

    *   **Status:** Definição de uma classe abstrata.
    *   **Ação:** Verificar se a classe abstrata `PromptGenerator` é usada corretamente pelas classes filhas. Se as classes filhas não implementam os métodos abstratos corretamente, refatorar ou remover a classe abstrata.

*   **Módulos de Comando (`voice_command_hendler.py` e `text_command_hendler.py`):**

    *   **Status:** Lidam com a captura de comandos.
    *   **Ação:** Verificar se há duplicação de código entre esses dois módulos (por exemplo, tratamento de erros, prompts ao usuário). Se houver, criar uma classe base ou função utilitária para lidar com a lógica comum.

*   **Remoção de código não utilizado:**

    *   Utilize um linter (como `flake8`) para identificar imports não utilizados e código não referenciado nos arquivos.

**2. Plano de Ação Detalhado:**

1.  **Backup:** Criar um backup da pasta `voice_assistent`.
2.  **Remover `template.py`:**
    ```bash
    rm voice_assistent/template.py
    ```
3.  **Remover `class_voice_assistent\conversation_history.py`:**
    ```bash
    rm voice_assistent/class_voice_assistent/conversation_history.py
    ```
4.  **Remover `class_voice_assistent\gpt_communication\groq._gpt.py`:**
    ```bash
    rm voice_assistent/class_voice_assistent/gpt_communication/groq._gpt.py
    ```
5.  **Refatorar Configurações:**
    *   Mover as configurações essenciais de `voice_assistent\config.py` para um arquivo `.env` ou para `class_voice_assistent\main.py`.
    *   Remover o arquivo `voice_assistent\config.py` se ele não for mais necessário.
6.  **Refatorar Módulos de Comando:**
    *   Criar uma classe base ou função utilitária para consolidar a lógica comum em `voice_command_hendler.py` e `text_command_hendler.py`.
7.  **Executar Linter:**
    ```bash
    pip install flake8
    flake8 voice_assistent
    ```
    *   Remover imports não utilizados e código não referenciado.
8.  **Testar:**
    *   Após cada passo, testar o assistente de voz para garantir que ele continue funcionando corretamente.
9.  **Documentar:**
    *   Atualizar a documentação do projeto para refletir as mudanças realizadas.

**3. Implementação:**

Remover código não utilizado

```python
# Exemplo de como remover um arquivo
import os

file_path = "voice_assistent/class_voice_assistent/gpt_communication/groq._gpt.py"
try:
    os.remove(file_path)
    print(f"Arquivo {file_path} removido com sucesso.")
except FileNotFoundError:
    print(f"Arquivo {file_path} não encontrado.")
except Exception as e:
    print(f"Erro ao remover {file_path}: {e}")
```

**Centralizar configurações**

```python
# Exemplo de como centralizar configurações em main.py
import os
from dotenv import load_dotenv

load_dotenv()

api_key = os.getenv("API_KEY")
# ... outras configurações
```

**4. Considerações Adicionais:**

*   **Testes:** Criar testes unitários para garantir a estabilidade dos componentes principais.
*   **Versionamento:** Usar Git para controlar as alterações e facilitar a reversão, se necessário.
*   **Segurança:** Validar o caminho da pasta para evitar exclusões acidentais.

Com essas ações, a pasta `voice_assistent` ficará mais organizada, eficiente e fácil de manter. Se precisar de ajuda com algum passo específico ou detalhe adicional, é só me avisar!

2025-04-14 16:24:40,950 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:24:40,953 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:24:40,955 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:24:40,957 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:24:40,959 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:24:40,961 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:24:40,962 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:24:40,964 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:24:40,966 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:24:40,969 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:24:40,970 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:24:40,972 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:24:40,974 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:24:40,976 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:24:40,977 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:24:40,979 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:24:40,981 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:24:40,983 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:24:40,985 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:24:40,987 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:24:40,989 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:24:40,991 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:24:40,993 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:24:40,995 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:24:40,997 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:24:41,000 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:24:41,002 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:24:41,004 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:24:41,005 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:24:41,007 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:24:41,009 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:24:41,011 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:24:41,013 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:24:41,015 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:24:41,017 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:24:41,019 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:24:41,021 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:24:41,022 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:24:41,024 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:24:41,026 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:24:41,028 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:24:41,031 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:24:41,033 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:24:41,035 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:24:41,037 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:24:41,039 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:24:41,041 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:24:41,043 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:24:41,045 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:24:41,047 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:25:47,327 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:25:47,328 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:25:47,330 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:25:47,331 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:25:47,332 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:25:47,334 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:25:47,335 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:25:47,336 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:25:47,337 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:25:47,338 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:25:47,340 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:25:47,341 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:25:47,343 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:25:47,345 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:25:47,346 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:25:47,348 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:25:47,349 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:25:47,350 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:25:47,352 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:25:47,354 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:25:47,355 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:25:47,359 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:25:47,361 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:25:47,362 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:25:47,363 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:25:47,364 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:25:47,365 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:25:47,366 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:25:47,367 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:25:47,369 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:25:47,370 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:25:47,371 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:25:47,372 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:25:47,373 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:25:47,374 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:25:47,377 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:25:47,378 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:25:47,379 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:25:47,380 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:25:47,381 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:25:47,382 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:25:47,383 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:25:47,384 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:25:47,385 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:25:47,386 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:25:47,387 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:25:47,388 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:25:47,390 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:25:47,391 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:25:47,393 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:25:47,556 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:25:47,557 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:25:47,560 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:25:47,561 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:25:47,563 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:25:47,565 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:25:47,566 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:25:47,567 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:25:47,568 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:25:47,570 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:25:47,571 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:25:47,573 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:25:47,574 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:25:47,576 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:25:47,577 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:25:47,578 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:25:47,579 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:25:47,580 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:25:47,581 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:25:47,582 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:25:47,583 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:25:47,585 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:25:47,586 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:25:47,587 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:25:47,588 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:25:47,589 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:25:47,591 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:25:47,592 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:25:47,593 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:25:47,595 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:25:47,596 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:25:47,597 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:25:47,598 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:25:47,599 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:25:47,601 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:25:47,602 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:25:47,603 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:25:47,604 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:25:47,605 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:25:47,606 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:25:47,608 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:25:47,609 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:25:47,610 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:25:47,612 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:25:47,613 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:25:47,614 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:25:47,615 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:25:47,616 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:25:47,617 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:25:47,618 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:25:56,993 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:25:56,995 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:25:56,996 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:25:56,998 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:25:57,000 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:25:57,002 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:25:57,003 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:25:57,005 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:25:57,007 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:25:57,009 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:25:57,011 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:25:57,012 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:25:57,014 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:25:57,015 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:25:57,016 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:25:57,017 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:25:57,019 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:25:57,020 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:25:57,059 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:25:57,062 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:25:57,064 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:25:57,065 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:25:57,067 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:25:57,069 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:25:57,070 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:25:57,074 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:25:57,081 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:25:57,082 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:25:57,083 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:25:57,085 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:25:57,086 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:25:57,087 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:25:57,088 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:25:57,089 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:25:57,091 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:25:57,092 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:25:57,094 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:25:57,095 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:25:57,096 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:25:57,098 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:25:57,099 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:25:57,100 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:25:57,101 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:25:57,102 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:25:57,103 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:25:57,104 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:25:57,105 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:25:57,106 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:25:57,107 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:25:57,109 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:25:57,206 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:25:57,208 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:25:57,211 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:25:57,214 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:25:57,215 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:25:57,217 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:25:57,218 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:25:57,219 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:25:57,221 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:25:57,222 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:25:57,224 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:25:57,225 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:25:57,227 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:25:57,228 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:25:57,229 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:25:57,231 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:25:57,232 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:25:57,233 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:25:57,234 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:25:57,236 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:25:57,237 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:25:57,238 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:25:57,240 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:25:57,243 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:25:57,244 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:25:57,245 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:25:57,247 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:25:57,249 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:25:57,251 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:25:57,253 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:25:57,256 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:25:57,257 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:25:57,259 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:25:57,261 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:25:57,262 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:25:57,264 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:25:57,265 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:25:57,267 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:25:57,268 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:25:57,270 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:25:57,271 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:25:57,273 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:25:57,275 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:25:57,277 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:25:57,279 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:25:57,280 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:25:57,281 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:25:57,282 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:25:57,283 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:25:57,284 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:25:57,300 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\agents_ia\comandAI\assets\20250414162557_clipboard_20250414162547.png, Prompt: Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diretório atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Definição dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Criação dos diretórios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configuração do layout do Streamlit
st.set_page_config(
    page_title="Análise de Ações",
    layout="wide",
    page_icon="📈"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscilações

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Período', 'Oscilação'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("📊 Dashboard de Análise de Ações")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma ação", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"📌 Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("🔍 Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descrição', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa ação.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("💰 Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("📉 Oscilações")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("📈 Tabela Geral de Ações")
        st.dataframe(self.acoes)

# Execução
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para análise fundamentalista de ações da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se está correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configuração de vídeos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configuração de áudios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcrições e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diretório
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diretório.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do último arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conteúdo do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimensões do embedding:", embedding.shape)

# Caminho do diretório de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o último embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configuração de transcrições e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diretório onde está procurando as transcrições
        print(f"Diretório de entrada das transcrições: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcrição no diretório de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcrição encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo não encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcrição do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} está vazio.")
                    return

            # Gerar o embedding da transcrição
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcrição e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O vídeo {video_path} não contém áudio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de vídeo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de vídeo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de vídeo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Conversão de vídeo para áudio concluída!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configurações da API
handler = genai('gemini-1.5-flash')

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Função para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conteúdo com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares únicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcrição obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' não é uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuniões: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de reuniões: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|tópicos da|pontos (relevantes|principais)|análise)\b.*\b(reunião|última (reunião|conversa|sessão))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como análise de reunião.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padrão.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que são muito similares ao comando atual para evitar redundância.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padrão: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # Métodos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de análise de reunião.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conteúdo usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inválida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar variáveis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conteúdo com base no prompt, contexto e reunião"""
        try:
            # Supondo que a API espera um dicionário com os parâmetros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisição para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar variáveis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Criação da conclusão do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que você acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["faça um resumo da última reunião.", "tópicos da última reunião", "resuma a última reunião", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Você é responsável por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - Não acrescentar título nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informações acima e forneça uma solução detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Você é responsável analisar com detalhes a reunião de [str_meeting], e fornecer uma longa estória sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precisão da resposta.
            - Não acrescente título nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e faça uma pesquisa online para uma resposta mais precisa das questões de [str_text]
        - Não acrescente título nas respostas.
        
        ------
        [context]: Regras Básicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("Áudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum áudio detectado.")
                return None
            except sr.UnknownValueError:
                print("Não foi possível entender o áudio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configurações da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicionário
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta não está no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API: {e}")
        return None

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: Faça uma limpeza na pasta class_voice_assistent
Assistant: Com base nos arquivos fornecidos, aqui está um resumo da estrutura e funcionalidade do código, juntamente com as informações sobre a limpeza da pasta `class_voice_assistent`:

**Visão geral do projeto:**

Este projeto parece ser um assistente de voz que usa o Google Gemini para processamento de linguagem natural. Ele é estruturado em vários módulos e classes para lidar com diferentes aspectos, como captura de áudio, conversão de texto em fala, interpretação de comandos e interação com APIs externas.

**Principais componentes:**

*   **Captura de comandos:**
    *   `voice_assistent\class_voice_assistent\voice_command_hendler.py`: Lida com a captura de comandos de voz usando a biblioteca `speech_recognition`.
    *   `voice_assistent\class_voice_assistent\text_command_hendler.py`: Lida com a captura de comandos de texto do usuário.

*   **Processamento de linguagem natural:**
    *   `voice_assistent\class_voice_assistent\command_interpreter.py`: Interpreta os comandos do usuário e determina a ação apropriada a ser executada.
    *   `voice_assistent\class_voice_assistent\question_answers_service.py`: Converte texto em embeddings para encontrar respostas semelhantes.
    *   `voice_assistent\class_voice_assistent\context_manager.py`: Gerencia o contexto da conversa, armazenando as interações recentes.

*   **Comunicação com o Google Gemini:**
    *   `voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py`: Lida com a comunicação com a API do Google Gemini.
    *   `voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py`: Define o modelo generativo e a configuração da API Google Gemini.

*   **Geração de prompt:**
    *   `voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py`: Gera prompts padrão para o modelo de linguagem.
    *   `voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py`: Gera prompts para análise de reuniões.
    *   `voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py`: Gera prompts para pesquisas online.
    *   `voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py`: Define a classe abstrata para geradores de prompt.

*   **Texto para fala:**
    *   `voice_assistent\class_voice_assistent\text_to_speech.py`: Converte texto em fala usando a biblioteca `pyttsx3`.

*   **Processamento de texto:**
    *   `voice_assistent\class_voice_assistent\text_processor.py`: Fornece funções para extrair valores de JSON e texto de HTML.

*   **Interface principal:**
    *   `voice_assistent\class_voice_assistent\main.py`: Inicializa e coordena os diferentes componentes do assistente de voz.

**Limpeza da pasta `class_voice_assistent`:**

O comando para limpar uma pasta não faz parte diretamente do código fornecido. Para implementar essa funcionalidade, você precisaria adicionar um código que use a biblioteca `os` ou `shutil` para remover arquivos e subdiretórios da pasta `class_voice_assistent`.

**Exemplo de como implementar a limpeza da pasta:**

```python
import os
import shutil

def clean_directory(path):
    """Remove todos os arquivos e subdiretórios em um diretório."""
    for filename in os.listdir(path):
        file_path = os.path.join(path, filename)
        try:
            if os.path.isfile(file_path) or os.path.islink(file_path):
                os.unlink(file_path)
            elif os.path.isdir(file_path):
                shutil.rmtree(file_path)
        except Exception as e:
            print(f'Falha ao excluir {file_path}. Motivo: {e}')

# Caminho para a pasta class_voice_assistent
path_to_clean = 'voice_assistent/class_voice_assistent'

# Chame a função para limpar a pasta
clean_directory(path_to_clean)
```

**Considerações:**

1.  **Importe os módulos necessários**: `os` e `shutil`.
2.  **Defina a função `clean_directory`**: Esta função itera sobre todos os itens no diretório fornecido e tenta remover cada um.
3.  **Tratamento de erros**: A função inclui um bloco `try...except` para lidar com possíveis erros durante a remoção de arquivos ou diretórios.
4.  **Chame a função `clean_directory`**: Passe o caminho do diretório que você deseja limpar.

**Onde adicionar este código:**

Você pode integrar esta função dentro do `command_interpreter.py` ou `main.py`. Por exemplo, dentro do `interpret_command`, você pode adicionar uma condição para detectar o comando "limpar pasta" e, em seguida, chamar a função `clean_directory` com o caminho apropriado.

**Aviso**: Esta operação é irreversível e excluirá permanentemente todos os arquivos e pastas dentro do diretório especificado. Certifique-se de ter um backup se necessário.

**Segurança:**

Certifique-se de que o caminho da pasta a ser limpa seja validado e seguro para evitar exclusões acidentais ou maliciosas.

User: quero remover códigos desnecessários na pasta voice_assistent no projeto.
Assistant: Assistant: Para remover códigos desnecessários na pasta `voice_assistent` do projeto, aqui está uma abordagem detalhada:

**1. Visão Geral:**

O objetivo é identificar e remover arquivos e funcionalidades redundantes ou não utilizados na pasta `voice_assistent`. Isso envolve uma análise cuidadosa do código, das dependências e do fluxo do programa para garantir que nenhuma funcionalidade essencial seja comprometida.

**2. Etapas Detalhadas:**

*   **Backup:**
    *   Antes de fazer qualquer alteração, faça um backup completo da pasta `voice_assistent`. Isso permitirá restaurar o estado anterior em caso de remoção acidental de código importante.

*   **Análise da Estrutura do Código:**
    *   **Identifique os principais módulos:** Analise todos os arquivos Python na pasta `voice_assistent` e suas subpastas para entender sua função e como eles se interconectam.
    *   **Módulos Essenciais:**
        *   `class_voice_assistent/main.py`: Coordena a operação do assistente de voz.
        *   `class_voice_assistent/command_interpreter.py`: Interpreta comandos do usuário.
        *   `class_voice_assistent/gpt_communication/gemini_gpt.py`: Lida com a comunicação com o Google Gemini.
        *   `class_voice_assistent/voice_command_handler.py` e `class_voice_assistent/text_command_handler.py`: Capturam comandos de voz e texto, respectivamente.
        *   `class_voice_assistent/text_to_speech.py`: Converte texto em fala.

*   **Identificação de Código Não Utilizado:**
    *   **Remova imports não utilizados:** Abra cada arquivo e verifique se todos os módulos importados estão realmente sendo usados. Remova aqueles que não estão.

    *   **Analise funções não utilizadas:** Verifique se todas as funções definidas são chamadas em algum lugar do código. Funções que não são chamadas podem ser removidas com segurança.
    *   **Avalie código comentado:** Avalie se os blocos de código comentados ainda são relevantes ou podem ser removidos.

*   **Análise das Dependências:**
    *   **Arquivo requirements.txt ou pyproject.toml:** Verifique se todas as bibliotecas listadas nos arquivos de dependência são necessárias para o funcionamento do assistente de voz. Remova as dependências que não estão sendo usadas.

*   **Simplificação da Lógica:**
    *   **Refatore código duplicado:** Se houver trechos de código repetidos, crie funções ou classes para reutilizá-los, reduzindo a redundância.
    *   **Simplifique funções complexas:** Divida funções muito longas ou complexas em funções menores e mais fáceis de entender e manter.

*   **Remoção de Código Desnecessário:**
    *   **Exclua arquivos desnecessários:** Remova arquivos que não contribuem para a funcionalidade principal do assistente de voz.

*   **Teste:**
    *   Após cada remoção ou modificação significativa, execute os testes para garantir que o assistente de voz continue funcionando corretamente.
    *   Crie testes automatizados para os principais componentes para facilitar a detecção de erros após futuras alterações.

**3. Implementação:**

*   **Edite os Arquivos:**
    *   Abra cada arquivo Python relevante e faça as modificações necessárias, como remover imports não utilizados, funções desnecessárias e refatorar o código.
*   **Remova Arquivos Desnecessários:**
    *   Exclua arquivos que não são mais necessários. Certifique-se de que nenhum outro componente depende desses arquivos antes de removê-los.

**4. Exemplo Prático:**

*   **Avaliação de `template.py`:**
    *   O arquivo `template.py` parece ser uma versão mais antiga ou um modelo inicial do assistente de voz. Compare-o com os outros arquivos (principalmente os arquivos em `class_voice_assistent`) para identificar se alguma parte do código em `template.py` ainda é usada. Se não for, considere remover este arquivo.

*   **Exemplo de refatoração:**
    *   Se você encontrar código duplicado entre `voice_command_hendler.py` e `text_command_hendler.py`, você pode criar uma classe base ou função utilitária para lidar com a lógica comum, reduzindo a duplicação.

**5. Exemplo de Código para Remover Arquivos:**

Para remover um arquivo específico, você pode usar o seguinte código:

```python
import os

file_path = "voice_assistent/class_voice_assistent/arquivo_desnecessario.py"
try:
    os.remove(file_path)
    print(f"Arquivo {file_path} removido com sucesso.")
except FileNotFoundError:
    print(f"Arquivo {file_path} não encontrado.")
except Exception as e:
    print(f"Erro ao remover {file_path}: {e}")
```

**Considerações Adicionais:**

*   **Documentação:** Atualize a documentação do projeto para refletir as mudanças realizadas. Isso ajudará outros desenvolvedores (ou você mesmo no futuro) a entender a nova estrutura do código.
*   **Versionamento:** Use um sistema de controle de versão (como Git) para gerenciar as alterações no código. Isso facilita a reversão de alterações e a colaboração com outros desenvolvedores.

Seguindo essas etapas, você poderá remover códigos desnecessários da pasta `voice_assistent`, tornando o projeto mais limpo, eficiente e fácil de manter.
User: você tem o código no contexto. você é meu assistente e deve fazer a análise para mim.
Assistant: Com prazer! Analisei o código que você forneceu e identifiquei algumas áreas onde podemos remover código desnecessário e refatorar para melhor clareza e eficiência.

**1. Análise Detalhada e Sugestões:**

*   **`template.py`:**

    *   **Status:** Este arquivo parece ser uma versão anterior ou um modelo desatualizado do assistente de voz.
    *   **Ação:** Comparar o código deste arquivo com os componentes atuais em `class_voice_assistent`. Se o código não for mais utilizado, **remover o arquivo `template.py`**.

*   **`voice_assistent\config.py`:**

    *   **Status:** Este arquivo contém configurações básicas, mas algumas dessas configurações podem estar duplicadas ou não sendo usadas.
    *   **Ação:**
        *   **apiKey e url:** Verificar se essas variáveis ainda são necessárias e se não estão duplicadas em outro lugar (por exemplo, em um arquivo `.env`).
        *   **engine, recent_context, nlp:** Estes itens são inicializados novamente em outros arquivos. Avaliar se é melhor centralizar a inicialização em um único local (possivelmente em `class_voice_assistent/main.py`) e importar as instâncias onde necessário.

*   **`class_voice_assistent\conversation_history.py`:**

    *   **Status:** Este arquivo está vazio.
    *   **Ação:** **Remover este arquivo** se ele não for usado. Se a intenção era armazenar o histórico de conversas, a funcionalidade deve ser movida para um arquivo relevante ou integrada em `class_voice_assistent\context_manager.py`.

*   **`class_voice_assistent\gpt_communication\groq._gpt.py`:**

    *   **Status:** Este arquivo demonstra o uso da API Groq, mas não parece estar integrado ao resto do código.
    *   **Ação:** Se a API Groq não for utilizada, **remover este arquivo**. Caso contrário, integrar corretamente e remover a duplicação de configurações e inicializações.

*   **`class_voice_assistent\prompt_generator\prompt_generator.py`:**

    *   **Status:** Definição de uma classe abstrata.
    *   **Ação:** Verificar se a classe abstrata `PromptGenerator` é usada corretamente pelas classes filhas. Se as classes filhas não implementam os métodos abstratos corretamente, refatorar ou remover a classe abstrata.

*   **Módulos de Comando (`voice_command_hendler.py` e `text_command_hendler.py`):**

    *   **Status:** Lidam com a captura de comandos.
    *   **Ação:** Verificar se há duplicação de código entre esses dois módulos (por exemplo, tratamento de erros, prompts ao usuário). Se houver, criar uma classe base ou função utilitária para lidar com a lógica comum.

*   **Remoção de código não utilizado:**

    *   Utilize um linter (como `flake8`) para identificar imports não utilizados e código não referenciado nos arquivos.

**2. Plano de Ação Detalhado:**

1.  **Backup:** Criar um backup da pasta `voice_assistent`.
2.  **Remover `template.py`:**
    ```bash
    rm voice_assistent/template.py
    ```
3.  **Remover `class_voice_assistent\conversation_history.py`:**
    ```bash
    rm voice_assistent/class_voice_assistent/conversation_history.py
    ```
4.  **Remover `class_voice_assistent\gpt_communication\groq._gpt.py`:**
    ```bash
    rm voice_assistent/class_voice_assistent/gpt_communication/groq._gpt.py
    ```
5.  **Refatorar Configurações:**
    *   Mover as configurações essenciais de `voice_assistent\config.py` para um arquivo `.env` ou para `class_voice_assistent\main.py`.
    *   Remover o arquivo `voice_assistent\config.py` se ele não for mais necessário.
6.  **Refatorar Módulos de Comando:**
    *   Criar uma classe base ou função utilitária para consolidar a lógica comum em `voice_command_hendler.py` e `text_command_hendler.py`.
7.  **Executar Linter:**
    ```bash
    pip install flake8
    flake8 voice_assistent
    ```
    *   Remover imports não utilizados e código não referenciado.
8.  **Testar:**
    *   Após cada passo, testar o assistente de voz para garantir que ele continue funcionando corretamente.
9.  **Documentar:**
    *   Atualizar a documentação do projeto para refletir as mudanças realizadas.

**3. Implementação:**

Remover código não utilizado

```python
# Exemplo de como remover um arquivo
import os

file_path = "voice_assistent/class_voice_assistent/gpt_communication/groq._gpt.py"
try:
    os.remove(file_path)
    print(f"Arquivo {file_path} removido com sucesso.")
except FileNotFoundError:
    print(f"Arquivo {file_path} não encontrado.")
except Exception as e:
    print(f"Erro ao remover {file_path}: {e}")
```

**Centralizar configurações**

```python
# Exemplo de como centralizar configurações em main.py
import os
from dotenv import load_dotenv

load_dotenv()

api_key = os.getenv("API_KEY")
# ... outras configurações
```

**4. Considerações Adicionais:**

*   **Testes:** Criar testes unitários para garantir a estabilidade dos componentes principais.
*   **Versionamento:** Usar Git para controlar as alterações e facilitar a reversão, se necessário.
*   **Segurança:** Validar o caminho da pasta para evitar exclusões acidentais.

Com essas ações, a pasta `voice_assistent` ficará mais organizada, eficiente e fácil de manter. Se precisar de ajuda com algum passo específico ou detalhe adicional, é só me avisar!

User: Preciso importar essas libs 
2025-04-14 16:26:02,032 [INFO] Resposta da IA (imagem): ```python
import os
from dotenv import load_dotenv
import os
import shutil
import pyttsx3
import spacy
import speech_recognition as sr
import requests
import re
from collections import deque
from bs4 import BeautifulSoup
import google.generativeai as genai
```

Essas são as bibliotecas que foram mencionadas ao longo da conversa.

Aqui está um exemplo prático de como importar essas bibliotecas no início de um arquivo Python:

```python
# Importações
import os
from dotenv import load_dotenv
import shutil
import pyttsx3
import spacy
import speech_recognition as sr
import requests
import re
from collections import deque
from bs4 import BeautifulSoup
import google.generativeai as genai

# Seu código continua aqui...
```

Essas importações fornecerão acesso às funcionalidades de cada biblioteca ao longo do seu código.

Além disso, aqui está uma breve descrição de cada biblioteca, juntamente com um link para a documentação, para que você possa aprender mais sobre como utilizá-las de forma eficaz:

-   **os:** Fornece uma maneira de usar funcionalidades dependentes do sistema operacional (documentação: <https://docs.python.org/3/library/os.html>)
-   **dotenv:** Permite carregar variáveis de ambiente de um arquivo `.env` (documentação: <https://pypi.org/project/python-dotenv/>)
-   **shutil:** Oferece um conjunto de operações de alto nível em arquivos e coleções de arquivos, como copiar e remover arquivos e diretórios (documentação: <https://docs.python.org/3/library/shutil.html>)
-   **pyttsx3:** Biblioteca de conversão de texto em fala que funciona offline (documentação: <https://pyttsx3.readthedocs.io/en/latest/>)
-   **spacy:** Uma biblioteca de processamento de linguagem natural avançada (documentação: <https://spacy.io/>)
-   **speech\_recognition:** Biblioteca para reconhecimento de fala, suportando várias APIs (documentação: <https://pypi.org/project/SpeechRecognition/>)
-   **requests:** Biblioteca para fazer requisições HTTP (documentação: <https://requests.readthedocs.io/en/latest/>)
-   **re:** Fornece operações de expressão regular (documentação: <https://docs.python.org/3/library/re.html>)
-   **collections.deque:** Uma generalização da pilha e da fila que suporta anexar e remover de qualquer extremidade (documentação: <https://docs.python.org/3/library/collections.html#collections.deque>)
-   **bs4 (Beautiful Soup):** Biblioteca para extrair informações de páginas web em formato HTML ou XML (documentação: <https://www.crummy.com/software/BeautifulSoup/bs4/doc/>)
-   **google.generativeai:** Biblioteca para usar os modelos de linguagem generativa do Google (documentação: verificar a documentação específica para os modelos do Google Gemini no Google Cloud).

Com essas importações e a documentação correspondente, você está bem preparado para continuar sua análise e refatoração do código!
2025-04-14 16:26:02,147 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:26:02,149 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:26:02,151 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:26:02,152 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:26:02,154 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:26:02,156 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:26:02,157 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:26:02,159 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:26:02,160 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:26:02,161 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:26:02,163 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:26:02,164 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:26:02,165 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:26:02,166 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:26:02,167 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:26:02,168 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:26:02,169 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:26:02,170 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:26:02,171 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:26:02,172 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:26:02,174 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:26:02,175 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:26:02,177 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:26:02,178 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:26:02,179 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:26:02,181 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:26:02,182 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:26:02,183 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:26:02,184 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:26:02,185 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:26:02,186 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:26:02,187 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:26:02,188 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:26:02,189 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:26:02,190 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:26:02,192 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:26:02,193 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:26:02,194 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:26:02,195 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:26:02,197 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:26:02,198 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:26:02,199 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:26:02,200 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:26:02,201 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:26:02,202 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:26:02,203 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:26:02,204 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:26:02,205 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:26:02,206 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:26:02,207 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:27:24,710 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:27:24,712 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:27:24,714 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:27:24,715 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:27:24,718 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:27:24,720 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:27:24,722 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:27:24,724 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:27:24,726 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:27:24,727 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:27:24,729 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:27:24,731 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:27:24,746 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:27:24,748 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:27:24,757 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:27:24,759 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:27:24,761 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:27:24,764 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:27:24,766 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:27:24,770 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:27:24,772 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:27:24,774 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:27:24,776 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:27:24,777 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:27:24,779 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:27:24,782 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:27:24,784 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:27:24,786 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:27:24,788 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:27:24,789 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:27:24,791 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:27:24,793 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:27:24,794 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:27:24,796 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:27:24,798 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:27:24,800 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:27:24,802 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:27:24,804 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:27:24,806 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:27:24,808 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:27:24,809 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:27:24,811 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:27:24,813 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:27:24,815 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:27:24,817 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:27:24,820 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:27:24,822 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:27:24,824 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:27:24,825 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:27:24,827 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:27:24,975 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:27:24,977 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:27:24,979 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:27:24,980 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:27:24,982 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:27:24,985 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:27:24,987 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:27:24,989 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:27:24,991 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:27:24,993 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:27:24,994 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:27:24,997 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:27:24,998 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:27:25,003 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:27:25,005 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:27:25,009 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:27:25,010 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:27:25,012 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:27:25,014 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:27:25,016 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:27:25,020 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:27:25,022 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:27:25,024 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:27:25,026 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:27:25,029 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:27:25,032 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:27:25,033 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:27:25,037 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:27:25,039 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:27:25,040 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:27:25,042 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:27:25,044 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:27:25,046 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:27:25,047 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:27:25,049 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:27:25,052 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:27:25,055 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:27:25,057 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:27:25,059 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:27:25,061 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:27:25,063 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:27:25,065 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:27:25,068 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:27:25,071 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:27:25,074 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:27:25,076 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:27:25,078 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:27:25,082 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:27:25,085 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:27:25,088 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:27:25,107 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\agents_ia\comandAI\assets\20250414162725_clipboard_20250414162547.png, Prompt: Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diretório atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Definição dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Criação dos diretórios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configuração do layout do Streamlit
st.set_page_config(
    page_title="Análise de Ações",
    layout="wide",
    page_icon="📈"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscilações

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Período', 'Oscilação'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("📊 Dashboard de Análise de Ações")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma ação", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"📌 Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("🔍 Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descrição', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa ação.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("💰 Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("📉 Oscilações")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("📈 Tabela Geral de Ações")
        st.dataframe(self.acoes)

# Execução
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para análise fundamentalista de ações da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se está correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configuração de vídeos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configuração de áudios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcrições e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diretório
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diretório.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do último arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conteúdo do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimensões do embedding:", embedding.shape)

# Caminho do diretório de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o último embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configuração de transcrições e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diretório onde está procurando as transcrições
        print(f"Diretório de entrada das transcrições: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcrição no diretório de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcrição encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo não encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcrição do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} está vazio.")
                    return

            # Gerar o embedding da transcrição
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcrição e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O vídeo {video_path} não contém áudio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de vídeo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de vídeo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de vídeo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Conversão de vídeo para áudio concluída!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configurações da API
handler = genai('gemini-1.5-flash')

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Função para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conteúdo com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares únicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcrição obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' não é uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuniões: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de reuniões: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|tópicos da|pontos (relevantes|principais)|análise)\b.*\b(reunião|última (reunião|conversa|sessão))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como análise de reunião.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padrão.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que são muito similares ao comando atual para evitar redundância.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padrão: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # Métodos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de análise de reunião.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conteúdo usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inválida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar variáveis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conteúdo com base no prompt, contexto e reunião"""
        try:
            # Supondo que a API espera um dicionário com os parâmetros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisição para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar variáveis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Criação da conclusão do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que você acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["faça um resumo da última reunião.", "tópicos da última reunião", "resuma a última reunião", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Você é responsável por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - Não acrescentar título nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informações acima e forneça uma solução detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Você é responsável analisar com detalhes a reunião de [str_meeting], e fornecer uma longa estória sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precisão da resposta.
            - Não acrescente título nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e faça uma pesquisa online para uma resposta mais precisa das questões de [str_text]
        - Não acrescente título nas respostas.
        
        ------
        [context]: Regras Básicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("Áudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum áudio detectado.")
                return None
            except sr.UnknownValueError:
                print("Não foi possível entender o áudio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configurações da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicionário
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta não está no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API: {e}")
        return None

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: Faça uma limpeza na pasta class_voice_assistent
Assistant: Com base nos arquivos fornecidos, aqui está um resumo da estrutura e funcionalidade do código, juntamente com as informações sobre a limpeza da pasta `class_voice_assistent`:

**Visão geral do projeto:**

Este projeto parece ser um assistente de voz que usa o Google Gemini para processamento de linguagem natural. Ele é estruturado em vários módulos e classes para lidar com diferentes aspectos, como captura de áudio, conversão de texto em fala, interpretação de comandos e interação com APIs externas.

**Principais componentes:**

*   **Captura de comandos:**
    *   `voice_assistent\class_voice_assistent\voice_command_hendler.py`: Lida com a captura de comandos de voz usando a biblioteca `speech_recognition`.
    *   `voice_assistent\class_voice_assistent\text_command_hendler.py`: Lida com a captura de comandos de texto do usuário.

*   **Processamento de linguagem natural:**
    *   `voice_assistent\class_voice_assistent\command_interpreter.py`: Interpreta os comandos do usuário e determina a ação apropriada a ser executada.
    *   `voice_assistent\class_voice_assistent\question_answers_service.py`: Converte texto em embeddings para encontrar respostas semelhantes.
    *   `voice_assistent\class_voice_assistent\context_manager.py`: Gerencia o contexto da conversa, armazenando as interações recentes.

*   **Comunicação com o Google Gemini:**
    *   `voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py`: Lida com a comunicação com a API do Google Gemini.
    *   `voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py`: Define o modelo generativo e a configuração da API Google Gemini.

*   **Geração de prompt:**
    *   `voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py`: Gera prompts padrão para o modelo de linguagem.
    *   `voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py`: Gera prompts para análise de reuniões.
    *   `voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py`: Gera prompts para pesquisas online.
    *   `voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py`: Define a classe abstrata para geradores de prompt.

*   **Texto para fala:**
    *   `voice_assistent\class_voice_assistent\text_to_speech.py`: Converte texto em fala usando a biblioteca `pyttsx3`.

*   **Processamento de texto:**
    *   `voice_assistent\class_voice_assistent\text_processor.py`: Fornece funções para extrair valores de JSON e texto de HTML.

*   **Interface principal:**
    *   `voice_assistent\class_voice_assistent\main.py`: Inicializa e coordena os diferentes componentes do assistente de voz.

**Limpeza da pasta `class_voice_assistent`:**

O comando para limpar uma pasta não faz parte diretamente do código fornecido. Para implementar essa funcionalidade, você precisaria adicionar um código que use a biblioteca `os` ou `shutil` para remover arquivos e subdiretórios da pasta `class_voice_assistent`.

**Exemplo de como implementar a limpeza da pasta:**

```python
import os
import shutil

def clean_directory(path):
    """Remove todos os arquivos e subdiretórios em um diretório."""
    for filename in os.listdir(path):
        file_path = os.path.join(path, filename)
        try:
            if os.path.isfile(file_path) or os.path.islink(file_path):
                os.unlink(file_path)
            elif os.path.isdir(file_path):
                shutil.rmtree(file_path)
        except Exception as e:
            print(f'Falha ao excluir {file_path}. Motivo: {e}')

# Caminho para a pasta class_voice_assistent
path_to_clean = 'voice_assistent/class_voice_assistent'

# Chame a função para limpar a pasta
clean_directory(path_to_clean)
```

**Considerações:**

1.  **Importe os módulos necessários**: `os` e `shutil`.
2.  **Defina a função `clean_directory`**: Esta função itera sobre todos os itens no diretório fornecido e tenta remover cada um.
3.  **Tratamento de erros**: A função inclui um bloco `try...except` para lidar com possíveis erros durante a remoção de arquivos ou diretórios.
4.  **Chame a função `clean_directory`**: Passe o caminho do diretório que você deseja limpar.

**Onde adicionar este código:**

Você pode integrar esta função dentro do `command_interpreter.py` ou `main.py`. Por exemplo, dentro do `interpret_command`, você pode adicionar uma condição para detectar o comando "limpar pasta" e, em seguida, chamar a função `clean_directory` com o caminho apropriado.

**Aviso**: Esta operação é irreversível e excluirá permanentemente todos os arquivos e pastas dentro do diretório especificado. Certifique-se de ter um backup se necessário.

**Segurança:**

Certifique-se de que o caminho da pasta a ser limpa seja validado e seguro para evitar exclusões acidentais ou maliciosas.

User: quero remover códigos desnecessários na pasta voice_assistent no projeto.
Assistant: Assistant: Para remover códigos desnecessários na pasta `voice_assistent` do projeto, aqui está uma abordagem detalhada:

**1. Visão Geral:**

O objetivo é identificar e remover arquivos e funcionalidades redundantes ou não utilizados na pasta `voice_assistent`. Isso envolve uma análise cuidadosa do código, das dependências e do fluxo do programa para garantir que nenhuma funcionalidade essencial seja comprometida.

**2. Etapas Detalhadas:**

*   **Backup:**
    *   Antes de fazer qualquer alteração, faça um backup completo da pasta `voice_assistent`. Isso permitirá restaurar o estado anterior em caso de remoção acidental de código importante.

*   **Análise da Estrutura do Código:**
    *   **Identifique os principais módulos:** Analise todos os arquivos Python na pasta `voice_assistent` e suas subpastas para entender sua função e como eles se interconectam.
    *   **Módulos Essenciais:**
        *   `class_voice_assistent/main.py`: Coordena a operação do assistente de voz.
        *   `class_voice_assistent/command_interpreter.py`: Interpreta comandos do usuário.
        *   `class_voice_assistent/gpt_communication/gemini_gpt.py`: Lida com a comunicação com o Google Gemini.
        *   `class_voice_assistent/voice_command_handler.py` e `class_voice_assistent/text_command_handler.py`: Capturam comandos de voz e texto, respectivamente.
        *   `class_voice_assistent/text_to_speech.py`: Converte texto em fala.

*   **Identificação de Código Não Utilizado:**
    *   **Remova imports não utilizados:** Abra cada arquivo e verifique se todos os módulos importados estão realmente sendo usados. Remova aqueles que não estão.

    *   **Analise funções não utilizadas:** Verifique se todas as funções definidas são chamadas em algum lugar do código. Funções que não são chamadas podem ser removidas com segurança.
    *   **Avalie código comentado:** Avalie se os blocos de código comentados ainda são relevantes ou podem ser removidos.

*   **Análise das Dependências:**
    *   **Arquivo requirements.txt ou pyproject.toml:** Verifique se todas as bibliotecas listadas nos arquivos de dependência são necessárias para o funcionamento do assistente de voz. Remova as dependências que não estão sendo usadas.

*   **Simplificação da Lógica:**
    *   **Refatore código duplicado:** Se houver trechos de código repetidos, crie funções ou classes para reutilizá-los, reduzindo a redundância.
    *   **Simplifique funções complexas:** Divida funções muito longas ou complexas em funções menores e mais fáceis de entender e manter.

*   **Remoção de Código Desnecessário:**
    *   **Exclua arquivos desnecessários:** Remova arquivos que não contribuem para a funcionalidade principal do assistente de voz.

*   **Teste:**
    *   Após cada remoção ou modificação significativa, execute os testes para garantir que o assistente de voz continue funcionando corretamente.
    *   Crie testes automatizados para os principais componentes para facilitar a detecção de erros após futuras alterações.

**3. Implementação:**

*   **Edite os Arquivos:**
    *   Abra cada arquivo Python relevante e faça as modificações necessárias, como remover imports não utilizados, funções desnecessárias e refatorar o código.
*   **Remova Arquivos Desnecessários:**
    *   Exclua arquivos que não são mais necessários. Certifique-se de que nenhum outro componente depende desses arquivos antes de removê-los.

**4. Exemplo Prático:**

*   **Avaliação de `template.py`:**
    *   O arquivo `template.py` parece ser uma versão mais antiga ou um modelo inicial do assistente de voz. Compare-o com os outros arquivos (principalmente os arquivos em `class_voice_assistent`) para identificar se alguma parte do código em `template.py` ainda é usada. Se não for, considere remover este arquivo.

*   **Exemplo de refatoração:**
    *   Se você encontrar código duplicado entre `voice_command_hendler.py` e `text_command_hendler.py`, você pode criar uma classe base ou função utilitária para lidar com a lógica comum, reduzindo a duplicação.

**5. Exemplo de Código para Remover Arquivos:**

Para remover um arquivo específico, você pode usar o seguinte código:

```python
import os

file_path = "voice_assistent/class_voice_assistent/arquivo_desnecessario.py"
try:
    os.remove(file_path)
    print(f"Arquivo {file_path} removido com sucesso.")
except FileNotFoundError:
    print(f"Arquivo {file_path} não encontrado.")
except Exception as e:
    print(f"Erro ao remover {file_path}: {e}")
```

**Considerações Adicionais:**

*   **Documentação:** Atualize a documentação do projeto para refletir as mudanças realizadas. Isso ajudará outros desenvolvedores (ou você mesmo no futuro) a entender a nova estrutura do código.
*   **Versionamento:** Use um sistema de controle de versão (como Git) para gerenciar as alterações no código. Isso facilita a reversão de alterações e a colaboração com outros desenvolvedores.

Seguindo essas etapas, você poderá remover códigos desnecessários da pasta `voice_assistent`, tornando o projeto mais limpo, eficiente e fácil de manter.
User: você tem o código no contexto. você é meu assistente e deve fazer a análise para mim.
Assistant: Com prazer! Analisei o código que você forneceu e identifiquei algumas áreas onde podemos remover código desnecessário e refatorar para melhor clareza e eficiência.

**1. Análise Detalhada e Sugestões:**

*   **`template.py`:**

    *   **Status:** Este arquivo parece ser uma versão anterior ou um modelo desatualizado do assistente de voz.
    *   **Ação:** Comparar o código deste arquivo com os componentes atuais em `class_voice_assistent`. Se o código não for mais utilizado, **remover o arquivo `template.py`**.

*   **`voice_assistent\config.py`:**

    *   **Status:** Este arquivo contém configurações básicas, mas algumas dessas configurações podem estar duplicadas ou não sendo usadas.
    *   **Ação:**
        *   **apiKey e url:** Verificar se essas variáveis ainda são necessárias e se não estão duplicadas em outro lugar (por exemplo, em um arquivo `.env`).
        *   **engine, recent_context, nlp:** Estes itens são inicializados novamente em outros arquivos. Avaliar se é melhor centralizar a inicialização em um único local (possivelmente em `class_voice_assistent/main.py`) e importar as instâncias onde necessário.

*   **`class_voice_assistent\conversation_history.py`:**

    *   **Status:** Este arquivo está vazio.
    *   **Ação:** **Remover este arquivo** se ele não for usado. Se a intenção era armazenar o histórico de conversas, a funcionalidade deve ser movida para um arquivo relevante ou integrada em `class_voice_assistent\context_manager.py`.

*   **`class_voice_assistent\gpt_communication\groq._gpt.py`:**

    *   **Status:** Este arquivo demonstra o uso da API Groq, mas não parece estar integrado ao resto do código.
    *   **Ação:** Se a API Groq não for utilizada, **remover este arquivo**. Caso contrário, integrar corretamente e remover a duplicação de configurações e inicializações.

*   **`class_voice_assistent\prompt_generator\prompt_generator.py`:**

    *   **Status:** Definição de uma classe abstrata.
    *   **Ação:** Verificar se a classe abstrata `PromptGenerator` é usada corretamente pelas classes filhas. Se as classes filhas não implementam os métodos abstratos corretamente, refatorar ou remover a classe abstrata.

*   **Módulos de Comando (`voice_command_hendler.py` e `text_command_hendler.py`):**

    *   **Status:** Lidam com a captura de comandos.
    *   **Ação:** Verificar se há duplicação de código entre esses dois módulos (por exemplo, tratamento de erros, prompts ao usuário). Se houver, criar uma classe base ou função utilitária para lidar com a lógica comum.

*   **Remoção de código não utilizado:**

    *   Utilize um linter (como `flake8`) para identificar imports não utilizados e código não referenciado nos arquivos.

**2. Plano de Ação Detalhado:**

1.  **Backup:** Criar um backup da pasta `voice_assistent`.
2.  **Remover `template.py`:**
    ```bash
    rm voice_assistent/template.py
    ```
3.  **Remover `class_voice_assistent\conversation_history.py`:**
    ```bash
    rm voice_assistent/class_voice_assistent/conversation_history.py
    ```
4.  **Remover `class_voice_assistent\gpt_communication\groq._gpt.py`:**
    ```bash
    rm voice_assistent/class_voice_assistent/gpt_communication/groq._gpt.py
    ```
5.  **Refatorar Configurações:**
    *   Mover as configurações essenciais de `voice_assistent\config.py` para um arquivo `.env` ou para `class_voice_assistent\main.py`.
    *   Remover o arquivo `voice_assistent\config.py` se ele não for mais necessário.
6.  **Refatorar Módulos de Comando:**
    *   Criar uma classe base ou função utilitária para consolidar a lógica comum em `voice_command_hendler.py` e `text_command_hendler.py`.
7.  **Executar Linter:**
    ```bash
    pip install flake8
    flake8 voice_assistent
    ```
    *   Remover imports não utilizados e código não referenciado.
8.  **Testar:**
    *   Após cada passo, testar o assistente de voz para garantir que ele continue funcionando corretamente.
9.  **Documentar:**
    *   Atualizar a documentação do projeto para refletir as mudanças realizadas.

**3. Implementação:**

Remover código não utilizado

```python
# Exemplo de como remover um arquivo
import os

file_path = "voice_assistent/class_voice_assistent/gpt_communication/groq._gpt.py"
try:
    os.remove(file_path)
    print(f"Arquivo {file_path} removido com sucesso.")
except FileNotFoundError:
    print(f"Arquivo {file_path} não encontrado.")
except Exception as e:
    print(f"Erro ao remover {file_path}: {e}")
```

**Centralizar configurações**

```python
# Exemplo de como centralizar configurações em main.py
import os
from dotenv import load_dotenv

load_dotenv()

api_key = os.getenv("API_KEY")
# ... outras configurações
```

**4. Considerações Adicionais:**

*   **Testes:** Criar testes unitários para garantir a estabilidade dos componentes principais.
*   **Versionamento:** Usar Git para controlar as alterações e facilitar a reversão, se necessário.
*   **Segurança:** Validar o caminho da pasta para evitar exclusões acidentais.

Com essas ações, a pasta `voice_assistent` ficará mais organizada, eficiente e fácil de manter. Se precisar de ajuda com algum passo específico ou detalhe adicional, é só me avisar!

User: Preciso importar essas libs 
Assistant: ```python
import os
from dotenv import load_dotenv
import os
import shutil
import pyttsx3
import spacy
import speech_recognition as sr
import requests
import re
from collections import deque
from bs4 import BeautifulSoup
import google.generativeai as genai
```

Essas são as bibliotecas que foram mencionadas ao longo da conversa.

Aqui está um exemplo prático de como importar essas bibliotecas no início de um arquivo Python:

```python
# Importações
import os
from dotenv import load_dotenv
import shutil
import pyttsx3
import spacy
import speech_recognition as sr
import requests
import re
from collections import deque
from bs4 import BeautifulSoup
import google.generativeai as genai

# Seu código continua aqui...
```

Essas importações fornecerão acesso às funcionalidades de cada biblioteca ao longo do seu código.

Além disso, aqui está uma breve descrição de cada biblioteca, juntamente com um link para a documentação, para que você possa aprender mais sobre como utilizá-las de forma eficaz:

-   **os:** Fornece uma maneira de usar funcionalidades dependentes do sistema operacional (documentação: <https://docs.python.org/3/library/os.html>)
-   **dotenv:** Permite carregar variáveis de ambiente de um arquivo `.env` (documentação: <https://pypi.org/project/python-dotenv/>)
-   **shutil:** Oferece um conjunto de operações de alto nível em arquivos e coleções de arquivos, como copiar e remover arquivos e diretórios (documentação: <https://docs.python.org/3/library/shutil.html>)
-   **pyttsx3:** Biblioteca de conversão de texto em fala que funciona offline (documentação: <https://pyttsx3.readthedocs.io/en/latest/>)
-   **spacy:** Uma biblioteca de processamento de linguagem natural avançada (documentação: <https://spacy.io/>)
-   **speech\_recognition:** Biblioteca para reconhecimento de fala, suportando várias APIs (documentação: <https://pypi.org/project/SpeechRecognition/>)
-   **requests:** Biblioteca para fazer requisições HTTP (documentação: <https://requests.readthedocs.io/en/latest/>)
-   **re:** Fornece operações de expressão regular (documentação: <https://docs.python.org/3/library/re.html>)
-   **collections.deque:** Uma generalização da pilha e da fila que suporta anexar e remover de qualquer extremidade (documentação: <https://docs.python.org/3/library/collections.html#collections.deque>)
-   **bs4 (Beautiful Soup):** Biblioteca para extrair informações de páginas web em formato HTML ou XML (documentação: <https://www.crummy.com/software/BeautifulSoup/bs4/doc/>)
-   **google.generativeai:** Biblioteca para usar os modelos de linguagem generativa do Google (documentação: verificar a documentação específica para os modelos do Google Gemini no Google Cloud).

Com essas importações e a documentação correspondente, você está bem preparado para continuar sua análise e refatoração do código!
User: como faço para instalar as que estão dando erro na imagem ?
2025-04-14 16:27:29,472 [INFO] Resposta da IA (imagem): Com base na imagem que você forneceu, as bibliotecas que precisam ser instaladas são:

*   **speech\_recognition:** Para reconhecimento de fala.
*   **pyttsx3:** Para síntese de fala (conversão de texto em voz).
*   **spacy:** Para processamento de linguagem natural.
*   **google.generativeai:** Para interagir com os modelos generativos do Google.

Aqui estão os passos para instalar essas bibliotecas usando `pip`:

**1. Abra o Terminal ou Prompt de Comando:**

*   No Windows, você pode usar o "Prompt de Comando" ou o "PowerShell".
*   No macOS ou Linux, use o "Terminal".

**2. Instale as Bibliotecas:**

Use o comando `pip install` seguido pelo nome das bibliotecas. Execute os seguintes comandos um por um para instalar cada biblioteca:

```bash
pip install SpeechRecognition
pip install pyttsx3
pip install spacy
pip install google-generativeai
```

**3. Instale o Modelo de Linguagem do SpaCy:**

Para usar a biblioteca SpaCy corretamente, você precisa baixar um modelo de linguagem. Para português (pt), você pode instalar o modelo `pt_core_news_sm`:

```bash
python -m spacy download pt_core_news_sm
```

**4. Verificar a Instalação:**

Para verificar se as bibliotecas foram instaladas corretamente, você pode executar o seguinte código Python em um interpretador Python ou em um arquivo `.py`:

```python
import speech_recognition as sr
import pyttsx3
import spacy
import google.generativeai as genai

print("SpeechRecognition versão:", sr.__version__)
print("pyttsx3 ok")
print("Spacy versão:", spacy.__version__)
print("google-generativeai ok")
```

Se não houver erros ao executar este código, significa que as bibliotecas foram instaladas corretamente.

**Informações Adicionais:**

*   **Se você estiver usando um ambiente virtual (`venv`):** Certifique-se de ativar o ambiente virtual antes de instalar as bibliotecas. Isso garante que as bibliotecas sejam instaladas apenas dentro do ambiente virtual e não no sistema global.
*   **Problemas de Permissão:** Se você encontrar problemas de permissão ao instalar as bibliotecas, tente executar o comando com privilégios de administrador (por exemplo, use `sudo pip install` no macOS/Linux).
*   **Versões das Bibliotecas:** Em alguns casos, versões mais recentes das bibliotecas podem ter problemas de compatibilidade com o seu código. Se você tiver problemas, pode tentar instalar versões mais antigas das bibliotecas.

Com estes passos, você deverá conseguir instalar as bibliotecas que estão faltando e resolver os erros no seu código!
2025-04-14 16:27:29,592 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:27:29,594 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:27:29,595 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:27:29,597 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:27:29,599 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:27:29,600 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:27:29,602 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:27:29,604 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:27:29,606 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:27:29,608 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:27:29,609 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:27:29,610 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:27:29,611 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:27:29,613 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:27:29,614 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:27:29,616 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:27:29,617 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:27:29,619 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:27:29,621 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:27:29,623 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:27:29,624 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:27:29,625 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:27:29,627 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:27:29,628 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:27:29,630 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:27:29,631 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:27:29,632 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:27:29,634 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:27:29,636 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:27:29,638 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:27:29,639 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:27:29,641 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:27:29,642 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:27:29,643 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:27:29,645 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:27:29,646 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:27:29,647 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:27:29,649 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:27:29,650 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:27:29,653 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:27:29,656 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:27:29,657 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:27:29,659 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:27:29,661 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:27:29,662 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:27:29,664 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:27:29,665 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:27:29,667 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:27:29,669 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:27:29,671 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:31:47,337 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:31:47,338 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:31:47,340 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:31:47,342 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:31:47,345 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:31:47,347 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:31:47,349 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:31:47,350 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:31:47,352 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:31:47,353 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:31:47,355 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:31:47,357 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:31:47,359 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:31:47,360 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:31:47,362 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:31:47,364 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:31:47,365 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:31:47,366 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:31:47,368 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:31:47,369 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:31:47,371 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:31:47,372 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:31:47,374 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:31:47,375 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:31:47,377 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:31:47,380 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:31:47,381 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:31:47,383 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:31:47,385 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:31:47,387 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:31:47,388 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:31:47,390 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:31:47,391 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:31:47,393 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:31:47,395 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:31:47,396 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:31:47,398 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:31:47,399 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:31:47,400 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:31:47,402 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:31:47,403 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:31:47,405 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:31:47,406 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:31:47,408 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:31:47,410 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:31:47,412 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:31:47,414 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:31:47,415 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:31:47,416 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:31:47,417 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:31:47,554 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:31:47,556 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:31:47,557 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:31:47,559 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:31:47,560 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:31:47,562 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:31:47,564 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:31:47,565 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:31:47,567 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:31:47,568 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:31:47,570 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:31:47,571 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:31:47,573 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:31:47,574 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:31:47,577 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:31:47,579 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:31:47,580 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:31:47,582 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:31:47,584 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:31:47,585 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:31:47,587 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:31:47,588 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:31:47,590 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:31:47,591 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:31:47,593 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:31:47,595 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:31:47,597 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:31:47,599 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:31:47,600 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:31:47,602 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:31:47,603 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:31:47,605 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:31:47,606 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:31:47,607 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:31:47,609 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:31:47,610 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:31:47,612 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:31:47,614 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:31:47,615 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:31:47,616 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:31:47,617 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:31:47,619 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:31:47,620 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:31:47,621 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:31:47,623 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:31:47,624 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:31:47,626 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:31:47,627 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:31:47,629 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:31:47,630 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:31:47,650 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\agents_ia\comandAI\assets\20250414163147_clipboard_20250414162547.png, Prompt: Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diretório atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Definição dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Criação dos diretórios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configuração do layout do Streamlit
st.set_page_config(
    page_title="Análise de Ações",
    layout="wide",
    page_icon="📈"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscilações

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Período', 'Oscilação'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("📊 Dashboard de Análise de Ações")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma ação", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"📌 Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("🔍 Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descrição', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa ação.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("💰 Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("📉 Oscilações")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("📈 Tabela Geral de Ações")
        st.dataframe(self.acoes)

# Execução
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para análise fundamentalista de ações da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se está correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configuração de vídeos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configuração de áudios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcrições e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diretório
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diretório.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do último arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conteúdo do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimensões do embedding:", embedding.shape)

# Caminho do diretório de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o último embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configuração de transcrições e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diretório onde está procurando as transcrições
        print(f"Diretório de entrada das transcrições: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcrição no diretório de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcrição encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo não encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcrição do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} está vazio.")
                    return

            # Gerar o embedding da transcrição
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcrição e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O vídeo {video_path} não contém áudio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de vídeo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de vídeo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de vídeo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Conversão de vídeo para áudio concluída!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configurações da API
handler = genai('gemini-1.5-flash')

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Função para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conteúdo com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares únicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcrição obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' não é uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuniões: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de reuniões: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|tópicos da|pontos (relevantes|principais)|análise)\b.*\b(reunião|última (reunião|conversa|sessão))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como análise de reunião.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padrão.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que são muito similares ao comando atual para evitar redundância.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padrão: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # Métodos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de análise de reunião.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conteúdo usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inválida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar variáveis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conteúdo com base no prompt, contexto e reunião"""
        try:
            # Supondo que a API espera um dicionário com os parâmetros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisição para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar variáveis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Criação da conclusão do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que você acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["faça um resumo da última reunião.", "tópicos da última reunião", "resuma a última reunião", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Você é responsável por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - Não acrescentar título nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informações acima e forneça uma solução detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Você é responsável analisar com detalhes a reunião de [str_meeting], e fornecer uma longa estória sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precisão da resposta.
            - Não acrescente título nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e faça uma pesquisa online para uma resposta mais precisa das questões de [str_text]
        - Não acrescente título nas respostas.
        
        ------
        [context]: Regras Básicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("Áudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum áudio detectado.")
                return None
            except sr.UnknownValueError:
                print("Não foi possível entender o áudio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configurações da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicionário
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta não está no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API: {e}")
        return None

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: Faça uma limpeza na pasta class_voice_assistent
Assistant: Com base nos arquivos fornecidos, aqui está um resumo da estrutura e funcionalidade do código, juntamente com as informações sobre a limpeza da pasta `class_voice_assistent`:

**Visão geral do projeto:**

Este projeto parece ser um assistente de voz que usa o Google Gemini para processamento de linguagem natural. Ele é estruturado em vários módulos e classes para lidar com diferentes aspectos, como captura de áudio, conversão de texto em fala, interpretação de comandos e interação com APIs externas.

**Principais componentes:**

*   **Captura de comandos:**
    *   `voice_assistent\class_voice_assistent\voice_command_hendler.py`: Lida com a captura de comandos de voz usando a biblioteca `speech_recognition`.
    *   `voice_assistent\class_voice_assistent\text_command_hendler.py`: Lida com a captura de comandos de texto do usuário.

*   **Processamento de linguagem natural:**
    *   `voice_assistent\class_voice_assistent\command_interpreter.py`: Interpreta os comandos do usuário e determina a ação apropriada a ser executada.
    *   `voice_assistent\class_voice_assistent\question_answers_service.py`: Converte texto em embeddings para encontrar respostas semelhantes.
    *   `voice_assistent\class_voice_assistent\context_manager.py`: Gerencia o contexto da conversa, armazenando as interações recentes.

*   **Comunicação com o Google Gemini:**
    *   `voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py`: Lida com a comunicação com a API do Google Gemini.
    *   `voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py`: Define o modelo generativo e a configuração da API Google Gemini.

*   **Geração de prompt:**
    *   `voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py`: Gera prompts padrão para o modelo de linguagem.
    *   `voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py`: Gera prompts para análise de reuniões.
    *   `voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py`: Gera prompts para pesquisas online.
    *   `voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py`: Define a classe abstrata para geradores de prompt.

*   **Texto para fala:**
    *   `voice_assistent\class_voice_assistent\text_to_speech.py`: Converte texto em fala usando a biblioteca `pyttsx3`.

*   **Processamento de texto:**
    *   `voice_assistent\class_voice_assistent\text_processor.py`: Fornece funções para extrair valores de JSON e texto de HTML.

*   **Interface principal:**
    *   `voice_assistent\class_voice_assistent\main.py`: Inicializa e coordena os diferentes componentes do assistente de voz.

**Limpeza da pasta `class_voice_assistent`:**

O comando para limpar uma pasta não faz parte diretamente do código fornecido. Para implementar essa funcionalidade, você precisaria adicionar um código que use a biblioteca `os` ou `shutil` para remover arquivos e subdiretórios da pasta `class_voice_assistent`.

**Exemplo de como implementar a limpeza da pasta:**

```python
import os
import shutil

def clean_directory(path):
    """Remove todos os arquivos e subdiretórios em um diretório."""
    for filename in os.listdir(path):
        file_path = os.path.join(path, filename)
        try:
            if os.path.isfile(file_path) or os.path.islink(file_path):
                os.unlink(file_path)
            elif os.path.isdir(file_path):
                shutil.rmtree(file_path)
        except Exception as e:
            print(f'Falha ao excluir {file_path}. Motivo: {e}')

# Caminho para a pasta class_voice_assistent
path_to_clean = 'voice_assistent/class_voice_assistent'

# Chame a função para limpar a pasta
clean_directory(path_to_clean)
```

**Considerações:**

1.  **Importe os módulos necessários**: `os` e `shutil`.
2.  **Defina a função `clean_directory`**: Esta função itera sobre todos os itens no diretório fornecido e tenta remover cada um.
3.  **Tratamento de erros**: A função inclui um bloco `try...except` para lidar com possíveis erros durante a remoção de arquivos ou diretórios.
4.  **Chame a função `clean_directory`**: Passe o caminho do diretório que você deseja limpar.

**Onde adicionar este código:**

Você pode integrar esta função dentro do `command_interpreter.py` ou `main.py`. Por exemplo, dentro do `interpret_command`, você pode adicionar uma condição para detectar o comando "limpar pasta" e, em seguida, chamar a função `clean_directory` com o caminho apropriado.

**Aviso**: Esta operação é irreversível e excluirá permanentemente todos os arquivos e pastas dentro do diretório especificado. Certifique-se de ter um backup se necessário.

**Segurança:**

Certifique-se de que o caminho da pasta a ser limpa seja validado e seguro para evitar exclusões acidentais ou maliciosas.

User: quero remover códigos desnecessários na pasta voice_assistent no projeto.
Assistant: Assistant: Para remover códigos desnecessários na pasta `voice_assistent` do projeto, aqui está uma abordagem detalhada:

**1. Visão Geral:**

O objetivo é identificar e remover arquivos e funcionalidades redundantes ou não utilizados na pasta `voice_assistent`. Isso envolve uma análise cuidadosa do código, das dependências e do fluxo do programa para garantir que nenhuma funcionalidade essencial seja comprometida.

**2. Etapas Detalhadas:**

*   **Backup:**
    *   Antes de fazer qualquer alteração, faça um backup completo da pasta `voice_assistent`. Isso permitirá restaurar o estado anterior em caso de remoção acidental de código importante.

*   **Análise da Estrutura do Código:**
    *   **Identifique os principais módulos:** Analise todos os arquivos Python na pasta `voice_assistent` e suas subpastas para entender sua função e como eles se interconectam.
    *   **Módulos Essenciais:**
        *   `class_voice_assistent/main.py`: Coordena a operação do assistente de voz.
        *   `class_voice_assistent/command_interpreter.py`: Interpreta comandos do usuário.
        *   `class_voice_assistent/gpt_communication/gemini_gpt.py`: Lida com a comunicação com o Google Gemini.
        *   `class_voice_assistent/voice_command_handler.py` e `class_voice_assistent/text_command_handler.py`: Capturam comandos de voz e texto, respectivamente.
        *   `class_voice_assistent/text_to_speech.py`: Converte texto em fala.

*   **Identificação de Código Não Utilizado:**
    *   **Remova imports não utilizados:** Abra cada arquivo e verifique se todos os módulos importados estão realmente sendo usados. Remova aqueles que não estão.

    *   **Analise funções não utilizadas:** Verifique se todas as funções definidas são chamadas em algum lugar do código. Funções que não são chamadas podem ser removidas com segurança.
    *   **Avalie código comentado:** Avalie se os blocos de código comentados ainda são relevantes ou podem ser removidos.

*   **Análise das Dependências:**
    *   **Arquivo requirements.txt ou pyproject.toml:** Verifique se todas as bibliotecas listadas nos arquivos de dependência são necessárias para o funcionamento do assistente de voz. Remova as dependências que não estão sendo usadas.

*   **Simplificação da Lógica:**
    *   **Refatore código duplicado:** Se houver trechos de código repetidos, crie funções ou classes para reutilizá-los, reduzindo a redundância.
    *   **Simplifique funções complexas:** Divida funções muito longas ou complexas em funções menores e mais fáceis de entender e manter.

*   **Remoção de Código Desnecessário:**
    *   **Exclua arquivos desnecessários:** Remova arquivos que não contribuem para a funcionalidade principal do assistente de voz.

*   **Teste:**
    *   Após cada remoção ou modificação significativa, execute os testes para garantir que o assistente de voz continue funcionando corretamente.
    *   Crie testes automatizados para os principais componentes para facilitar a detecção de erros após futuras alterações.

**3. Implementação:**

*   **Edite os Arquivos:**
    *   Abra cada arquivo Python relevante e faça as modificações necessárias, como remover imports não utilizados, funções desnecessárias e refatorar o código.
*   **Remova Arquivos Desnecessários:**
    *   Exclua arquivos que não são mais necessários. Certifique-se de que nenhum outro componente depende desses arquivos antes de removê-los.

**4. Exemplo Prático:**

*   **Avaliação de `template.py`:**
    *   O arquivo `template.py` parece ser uma versão mais antiga ou um modelo inicial do assistente de voz. Compare-o com os outros arquivos (principalmente os arquivos em `class_voice_assistent`) para identificar se alguma parte do código em `template.py` ainda é usada. Se não for, considere remover este arquivo.

*   **Exemplo de refatoração:**
    *   Se você encontrar código duplicado entre `voice_command_hendler.py` e `text_command_hendler.py`, você pode criar uma classe base ou função utilitária para lidar com a lógica comum, reduzindo a duplicação.

**5. Exemplo de Código para Remover Arquivos:**

Para remover um arquivo específico, você pode usar o seguinte código:

```python
import os

file_path = "voice_assistent/class_voice_assistent/arquivo_desnecessario.py"
try:
    os.remove(file_path)
    print(f"Arquivo {file_path} removido com sucesso.")
except FileNotFoundError:
    print(f"Arquivo {file_path} não encontrado.")
except Exception as e:
    print(f"Erro ao remover {file_path}: {e}")
```

**Considerações Adicionais:**

*   **Documentação:** Atualize a documentação do projeto para refletir as mudanças realizadas. Isso ajudará outros desenvolvedores (ou você mesmo no futuro) a entender a nova estrutura do código.
*   **Versionamento:** Use um sistema de controle de versão (como Git) para gerenciar as alterações no código. Isso facilita a reversão de alterações e a colaboração com outros desenvolvedores.

Seguindo essas etapas, você poderá remover códigos desnecessários da pasta `voice_assistent`, tornando o projeto mais limpo, eficiente e fácil de manter.
User: você tem o código no contexto. você é meu assistente e deve fazer a análise para mim.
Assistant: Com prazer! Analisei o código que você forneceu e identifiquei algumas áreas onde podemos remover código desnecessário e refatorar para melhor clareza e eficiência.

**1. Análise Detalhada e Sugestões:**

*   **`template.py`:**

    *   **Status:** Este arquivo parece ser uma versão anterior ou um modelo desatualizado do assistente de voz.
    *   **Ação:** Comparar o código deste arquivo com os componentes atuais em `class_voice_assistent`. Se o código não for mais utilizado, **remover o arquivo `template.py`**.

*   **`voice_assistent\config.py`:**

    *   **Status:** Este arquivo contém configurações básicas, mas algumas dessas configurações podem estar duplicadas ou não sendo usadas.
    *   **Ação:**
        *   **apiKey e url:** Verificar se essas variáveis ainda são necessárias e se não estão duplicadas em outro lugar (por exemplo, em um arquivo `.env`).
        *   **engine, recent_context, nlp:** Estes itens são inicializados novamente em outros arquivos. Avaliar se é melhor centralizar a inicialização em um único local (possivelmente em `class_voice_assistent/main.py`) e importar as instâncias onde necessário.

*   **`class_voice_assistent\conversation_history.py`:**

    *   **Status:** Este arquivo está vazio.
    *   **Ação:** **Remover este arquivo** se ele não for usado. Se a intenção era armazenar o histórico de conversas, a funcionalidade deve ser movida para um arquivo relevante ou integrada em `class_voice_assistent\context_manager.py`.

*   **`class_voice_assistent\gpt_communication\groq._gpt.py`:**

    *   **Status:** Este arquivo demonstra o uso da API Groq, mas não parece estar integrado ao resto do código.
    *   **Ação:** Se a API Groq não for utilizada, **remover este arquivo**. Caso contrário, integrar corretamente e remover a duplicação de configurações e inicializações.

*   **`class_voice_assistent\prompt_generator\prompt_generator.py`:**

    *   **Status:** Definição de uma classe abstrata.
    *   **Ação:** Verificar se a classe abstrata `PromptGenerator` é usada corretamente pelas classes filhas. Se as classes filhas não implementam os métodos abstratos corretamente, refatorar ou remover a classe abstrata.

*   **Módulos de Comando (`voice_command_hendler.py` e `text_command_hendler.py`):**

    *   **Status:** Lidam com a captura de comandos.
    *   **Ação:** Verificar se há duplicação de código entre esses dois módulos (por exemplo, tratamento de erros, prompts ao usuário). Se houver, criar uma classe base ou função utilitária para lidar com a lógica comum.

*   **Remoção de código não utilizado:**

    *   Utilize um linter (como `flake8`) para identificar imports não utilizados e código não referenciado nos arquivos.

**2. Plano de Ação Detalhado:**

1.  **Backup:** Criar um backup da pasta `voice_assistent`.
2.  **Remover `template.py`:**
    ```bash
    rm voice_assistent/template.py
    ```
3.  **Remover `class_voice_assistent\conversation_history.py`:**
    ```bash
    rm voice_assistent/class_voice_assistent/conversation_history.py
    ```
4.  **Remover `class_voice_assistent\gpt_communication\groq._gpt.py`:**
    ```bash
    rm voice_assistent/class_voice_assistent/gpt_communication/groq._gpt.py
    ```
5.  **Refatorar Configurações:**
    *   Mover as configurações essenciais de `voice_assistent\config.py` para um arquivo `.env` ou para `class_voice_assistent\main.py`.
    *   Remover o arquivo `voice_assistent\config.py` se ele não for mais necessário.
6.  **Refatorar Módulos de Comando:**
    *   Criar uma classe base ou função utilitária para consolidar a lógica comum em `voice_command_hendler.py` e `text_command_hendler.py`.
7.  **Executar Linter:**
    ```bash
    pip install flake8
    flake8 voice_assistent
    ```
    *   Remover imports não utilizados e código não referenciado.
8.  **Testar:**
    *   Após cada passo, testar o assistente de voz para garantir que ele continue funcionando corretamente.
9.  **Documentar:**
    *   Atualizar a documentação do projeto para refletir as mudanças realizadas.

**3. Implementação:**

Remover código não utilizado

```python
# Exemplo de como remover um arquivo
import os

file_path = "voice_assistent/class_voice_assistent/gpt_communication/groq._gpt.py"
try:
    os.remove(file_path)
    print(f"Arquivo {file_path} removido com sucesso.")
except FileNotFoundError:
    print(f"Arquivo {file_path} não encontrado.")
except Exception as e:
    print(f"Erro ao remover {file_path}: {e}")
```

**Centralizar configurações**

```python
# Exemplo de como centralizar configurações em main.py
import os
from dotenv import load_dotenv

load_dotenv()

api_key = os.getenv("API_KEY")
# ... outras configurações
```

**4. Considerações Adicionais:**

*   **Testes:** Criar testes unitários para garantir a estabilidade dos componentes principais.
*   **Versionamento:** Usar Git para controlar as alterações e facilitar a reversão, se necessário.
*   **Segurança:** Validar o caminho da pasta para evitar exclusões acidentais.

Com essas ações, a pasta `voice_assistent` ficará mais organizada, eficiente e fácil de manter. Se precisar de ajuda com algum passo específico ou detalhe adicional, é só me avisar!

User: Preciso importar essas libs 
Assistant: ```python
import os
from dotenv import load_dotenv
import os
import shutil
import pyttsx3
import spacy
import speech_recognition as sr
import requests
import re
from collections import deque
from bs4 import BeautifulSoup
import google.generativeai as genai
```

Essas são as bibliotecas que foram mencionadas ao longo da conversa.

Aqui está um exemplo prático de como importar essas bibliotecas no início de um arquivo Python:

```python
# Importações
import os
from dotenv import load_dotenv
import shutil
import pyttsx3
import spacy
import speech_recognition as sr
import requests
import re
from collections import deque
from bs4 import BeautifulSoup
import google.generativeai as genai

# Seu código continua aqui...
```

Essas importações fornecerão acesso às funcionalidades de cada biblioteca ao longo do seu código.

Além disso, aqui está uma breve descrição de cada biblioteca, juntamente com um link para a documentação, para que você possa aprender mais sobre como utilizá-las de forma eficaz:

-   **os:** Fornece uma maneira de usar funcionalidades dependentes do sistema operacional (documentação: <https://docs.python.org/3/library/os.html>)
-   **dotenv:** Permite carregar variáveis de ambiente de um arquivo `.env` (documentação: <https://pypi.org/project/python-dotenv/>)
-   **shutil:** Oferece um conjunto de operações de alto nível em arquivos e coleções de arquivos, como copiar e remover arquivos e diretórios (documentação: <https://docs.python.org/3/library/shutil.html>)
-   **pyttsx3:** Biblioteca de conversão de texto em fala que funciona offline (documentação: <https://pyttsx3.readthedocs.io/en/latest/>)
-   **spacy:** Uma biblioteca de processamento de linguagem natural avançada (documentação: <https://spacy.io/>)
-   **speech\_recognition:** Biblioteca para reconhecimento de fala, suportando várias APIs (documentação: <https://pypi.org/project/SpeechRecognition/>)
-   **requests:** Biblioteca para fazer requisições HTTP (documentação: <https://requests.readthedocs.io/en/latest/>)
-   **re:** Fornece operações de expressão regular (documentação: <https://docs.python.org/3/library/re.html>)
-   **collections.deque:** Uma generalização da pilha e da fila que suporta anexar e remover de qualquer extremidade (documentação: <https://docs.python.org/3/library/collections.html#collections.deque>)
-   **bs4 (Beautiful Soup):** Biblioteca para extrair informações de páginas web em formato HTML ou XML (documentação: <https://www.crummy.com/software/BeautifulSoup/bs4/doc/>)
-   **google.generativeai:** Biblioteca para usar os modelos de linguagem generativa do Google (documentação: verificar a documentação específica para os modelos do Google Gemini no Google Cloud).

Com essas importações e a documentação correspondente, você está bem preparado para continuar sua análise e refatoração do código!
User: como faço para instalar as que estão dando erro na imagem ?
Assistant: Com base na imagem que você forneceu, as bibliotecas que precisam ser instaladas são:

*   **speech\_recognition:** Para reconhecimento de fala.
*   **pyttsx3:** Para síntese de fala (conversão de texto em voz).
*   **spacy:** Para processamento de linguagem natural.
*   **google.generativeai:** Para interagir com os modelos generativos do Google.

Aqui estão os passos para instalar essas bibliotecas usando `pip`:

**1. Abra o Terminal ou Prompt de Comando:**

*   No Windows, você pode usar o "Prompt de Comando" ou o "PowerShell".
*   No macOS ou Linux, use o "Terminal".

**2. Instale as Bibliotecas:**

Use o comando `pip install` seguido pelo nome das bibliotecas. Execute os seguintes comandos um por um para instalar cada biblioteca:

```bash
pip install SpeechRecognition
pip install pyttsx3
pip install spacy
pip install google-generativeai
```

**3. Instale o Modelo de Linguagem do SpaCy:**

Para usar a biblioteca SpaCy corretamente, você precisa baixar um modelo de linguagem. Para português (pt), você pode instalar o modelo `pt_core_news_sm`:

```bash
python -m spacy download pt_core_news_sm
```

**4. Verificar a Instalação:**

Para verificar se as bibliotecas foram instaladas corretamente, você pode executar o seguinte código Python em um interpretador Python ou em um arquivo `.py`:

```python
import speech_recognition as sr
import pyttsx3
import spacy
import google.generativeai as genai

print("SpeechRecognition versão:", sr.__version__)
print("pyttsx3 ok")
print("Spacy versão:", spacy.__version__)
print("google-generativeai ok")
```

Se não houver erros ao executar este código, significa que as bibliotecas foram instaladas corretamente.

**Informações Adicionais:**

*   **Se você estiver usando um ambiente virtual (`venv`):** Certifique-se de ativar o ambiente virtual antes de instalar as bibliotecas. Isso garante que as bibliotecas sejam instaladas apenas dentro do ambiente virtual e não no sistema global.
*   **Problemas de Permissão:** Se você encontrar problemas de permissão ao instalar as bibliotecas, tente executar o comando com privilégios de administrador (por exemplo, use `sudo pip install` no macOS/Linux).
*   **Versões das Bibliotecas:** Em alguns casos, versões mais recentes das bibliotecas podem ter problemas de compatibilidade com o seu código. Se você tiver problemas, pode tentar instalar versões mais antigas das bibliotecas.

Com estes passos, você deverá conseguir instalar as bibliotecas que estão faltando e resolver os erros no seu código!
User: jfreis@STFSAON006541-L MINGW64 ~/Documents/agents_ia (master)
$ pip install spacy
Collecting spacy
  Using cached spacy-3.8.2.tar.gz (1.3 MB)
  Installing build dependencies ... error
  error: subprocess-exited-with-error

  × pip subprocess to install build dependencies did not run successfully.
  │ exit code: 1
  ╰─> [67 lines of output]
      Ignoring numpy: markers 'python_version < "3.9"' don't match your environment
      Collecting setuptools
        Using cached setuptools-78.1.0-py3-none-any.whl.metadata (6.6 kB)
      Collecting cython<3.0,>=0.25
        Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)
      Collecting cymem<2.1.0,>=2.0.2
        Using cached cymem-2.0.11-cp313-cp313-win_amd64.whl.metadata (8.8 kB)
      Collecting preshed<3.1.0,>=3.0.2
        Using cached preshed-3.0.9.tar.gz (14 kB)
        Installing build dependencies: started
        Installing build dependencies: finished with status 'done'
        Getting requirements to build wheel: started
        Getting requirements to build wheel: finished with status 'done'
        Preparing metadata (pyproject.toml): started
        Preparing metadata (pyproject.toml): finished with status 'done'
      Collecting murmurhash<1.1.0,>=0.28.0
        Using cached murmurhash-1.0.12-cp313-cp313-win_amd64.whl.metadata (2.2 kB)
      Collecting thinc<8.4.0,>=8.3.0
        Using cached thinc-8.3.6-cp313-cp313-win_amd64.whl.metadata (15 kB)
      Collecting numpy<2.1.0,>=2.0.0
        Using cached numpy-2.0.2.tar.gz (18.9 MB)
        Installing build dependencies: started
        Installing build dependencies: finished with status 'done'
        Getting requirements to build wheel: started
        Getting requirements to build wheel: finished with status 'done'
        Installing backend dependencies: started
        Installing backend dependencies: finished with status 'done'
        Preparing metadata (pyproject.toml): started
        Preparing metadata (pyproject.toml): finished with status 'error'
        error: subprocess-exited-with-error
     
        Preparing metadata (pyproject.toml) did not run successfully.
        exit code: 1
     
        [21 lines of output]
        + C:\Users\jfreis\AppData\Local\Programs\Python\Python313\python.exe C:\Users\jfreis\AppData\Local\Temp\pip-install-_5m3qz36\numpy_125b75e27d3645fc84b36c82dbcdef81\vendored-meson\meson\meson.py setup C:\Users\jfreis\AppData\Local\Temp\pip-install-_5m3qz36\numpy_125b75e27d3645fc84b36c82dbcdef81 C:\Users\jfreis\AppData\Local\Temp\pip-install-_5m3qz36\numpy_125b75e27d3645fc84b36c82dbcdef81\.mesonpy-hs6rlqj9 -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\Users\jfreis\AppData\Local\Temp\pip-install-_5m3qz36\numpy_125b75e27d3645fc84b36c82dbcdef81\.mesonpy-hs6rlqj9\meson-python-native-file.ini
        The Meson build system
        Version: 1.4.99
        Source dir: C:\Users\jfreis\AppData\Local\Temp\pip-install-_5m3qz36\numpy_125b75e27d3645fc84b36c82dbcdef81
        Build dir: C:\Users\jfreis\AppData\Local\Temp\pip-install-_5m3qz36\numpy_125b75e27d3645fc84b36c82dbcdef81\.mesonpy-hs6rlqj9
        Build type: native build
        Project name: NumPy
        Project version: 2.0.2
        WARNING: Failed to activate VS environment: Could not parse vswhere.exe output
     
        ..\meson.build:1:0: ERROR: Unknown compiler(s): [['icl'], ['cl'], ['cc'], ['gcc'], ['clang'], ['clang-cl'], ['pgcc']]  
        The following exception(s) were encountered:
        Running `icl ""` gave "[WinError 2] O sistema não pode encontrar o arquivo especificado"
        Running `cl /?` gave "[WinError 2] O sistema não pode encontrar o arquivo especificado"
        Running `cc --version` gave "[WinError 2] O sistema não pode encontrar o arquivo especificado"
        Running `gcc --version` gave "[WinError 2] O sistema não pode encontrar o arquivo especificado"
        Running `clang --version` gave "[WinError 2] O sistema não pode encontrar o arquivo especificado"
        Running `clang-cl /?` gave "[WinError 2] O sistema não pode encontrar o arquivo especificado"
        Running `pgcc --version` gave "[WinError 2] O sistema não pode encontrar o arquivo especificado"
     
        A full log can be found at C:\Users\jfreis\AppData\Local\Temp\pip-install-_5m3qz36\numpy_125b75e27d3645fc84b36c82dbcdef81\.mesonpy-hs6rlqj9\meson-logs\meson-log.txt
        [end of output]
     
        note: This error originates from a subprocess, and is likely not a problem with pip.
      error: metadata-generation-failed
     
      Encountered error while generating package metadata.
     
      See above for output.
     
      note: This is an issue with the package mentioned above, not pip.
      hint: See above for details.
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

× pip subprocess to install build dependencies did not run successfully.
│ exit code: 1
╰─> See above for output.

note: This error originates from a subprocess, and is likely not a problem with pip.

jfreis@STFSAON006541-L MINGW64 ~/Documents/agents_ia (master)
$
2025-04-14 16:31:54,553 [INFO] Resposta da IA (imagem): The error message indicates that the build process for `numpy` (a dependency of `spacy`) is failing because it cannot find a suitable compiler (like `icl`, `cl`, `cc`, `gcc`, etc.) on your system. This typically happens on Windows when the required build tools for compiling Python packages are not installed.

Here's a breakdown of how to fix this, along with detailed steps:

**1. Install Microsoft Visual C++ Build Tools:**

This is the most common solution. Many Python packages (including NumPy and some others) rely on C/C++ code that needs to be compiled during installation.

*   **Download the Build Tools:**  Go to the following Microsoft website:
    [https://visualstudio.microsoft.com/visual-cpp-build-tools/](https://visualstudio.microsoft.com/visual-cpp-build-tools/)

*   **Select the Installer:** Look for "Build Tools for Visual Studio" and download the installer.

*   **Run the Installer:**

    *   When the installer opens, make sure to select the "C++ build tools" workload.  This includes the C++ compiler, libraries, and other necessary tools. It's important to select this option.

    *   You might also want to select "Windows 10 SDK" (or a more recent version, if applicable) if it's not already selected.

*   **Complete the Installation:** Follow the prompts to complete the installation. This may take some time.

**2. Restart Your Computer (Important):**

After installing the Visual C++ Build Tools, *restart your computer*. This ensures that the system picks up the changes to the environment variables that the Build Tools installer makes.  Failing to restart is a common cause of continuing problems.

**3. Try Installing SpaCy Again:**

Open a new terminal window (or restart your existing one) and try installing SpaCy again:

```bash
pip install spacy
```

This time, the installation should proceed successfully because the `numpy` build process will be able to find the necessary C++ compiler.

**4. Handle Potential Environment Issues (If the Above Doesn't Work):**

Sometimes, even after installing the Build Tools, the necessary environment variables aren't set up correctly. You can try manually setting the environment variables:

*   **Find `vcvarsall.bat`:**  The `vcvarsall.bat` file is a batch script that sets up the environment variables for the Visual C++ compiler.  It's usually located in a directory like this (adjust for your Visual Studio version):

    `C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Auxiliary\Build`
    or
    `C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Auxiliary\Build`

*   **Run `vcvarsall.bat`:**  Open a command prompt (not PowerShell) and navigate to the directory containing `vcvarsall.bat`. Then, run the script:

    ```bash
    vcvarsall.bat amd64
    ```

    *   `amd64` specifies the target architecture (64-bit). If you're on a 32-bit system, use `x86` instead.

*   **Test and Reinstall:** After running `vcvarsall.bat`, try running `pip install spacy` in the *same* command prompt window.  The environment variables set by `vcvarsall.bat` are only valid for that specific command prompt session.

**5. Consider Conda (Alternative to pip):**

If you're still having trouble with `pip`, consider using Conda, a package, dependency, and environment manager that's often more reliable for installing complex scientific computing packages on Windows.

*   **Install Conda:**  If you don't have Conda installed, download and install Anaconda or Miniconda from here: [https://docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html)

*   **Create an Environment (Recommended):**  Create a new Conda environment to avoid conflicts with your existing Python installation:

    ```bash
    conda create -n spacy_env python=3.9  # Or a compatible Python version
    conda activate spacy_env
    ```

*   **Install SpaCy with Conda:**

    ```bash
    conda install -c conda-forge spacy
    ```
*   **Install the Language Model:**
     ```bash
    python -m spacy download pt_core_news_sm
    ```

Conda often handles the underlying build tool dependencies more effectively than `pip` on Windows.

**Troubleshooting Steps if You Still Have Issues:**

*   **Check your Python version:** Ensure that you have a compatible Python version installed. The error message might provide clues if the Python version is the issue.  Spacy generally supports Python 3.7+.
*   **Update pip:** Make sure you have the latest version of pip: `python -m pip install --upgrade pip`
*   **Check your PATH:** Verify that the Python installation directory and the Scripts directory (where `pip` is located) are in your system's `PATH` environment variable.
*   **Firewall/Antivirus:** In rare cases, a firewall or antivirus program might interfere with the installation process. Temporarily disable them and try again.

By following these steps in order, you should be able to resolve the error and successfully install SpaCy and its dependencies. Remember to restart your computer after installing the Visual C++ Build Tools, and try each subsequent step in a new terminal window.

2025-04-14 16:31:54,675 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:31:54,677 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:31:54,679 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:31:54,681 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:31:54,683 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:31:54,686 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:31:54,688 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:31:54,690 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:31:54,692 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:31:54,694 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:31:54,696 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:31:54,698 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:31:54,700 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:31:54,702 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:31:54,704 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:31:54,707 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:31:54,710 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:31:54,712 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:31:54,715 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:31:54,718 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:31:54,719 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:31:54,720 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:31:54,722 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:31:54,724 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:31:54,726 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:31:54,727 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:31:54,729 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:31:54,732 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:31:54,734 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:31:54,736 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:31:54,737 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:31:54,739 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:31:54,741 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:31:54,743 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:31:54,745 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:31:54,749 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:31:54,751 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:31:54,753 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:31:54,755 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:31:54,757 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:31:54,759 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:31:54,761 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:31:54,765 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:31:54,767 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:31:54,770 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:31:54,772 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:31:54,774 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:31:54,775 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:31:54,777 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:31:54,779 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:34:01,609 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:34:01,611 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:34:01,612 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:34:01,614 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:34:01,617 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:34:01,619 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:34:01,620 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:34:01,622 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:34:01,623 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:34:01,625 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:34:01,627 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:34:01,629 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:34:01,631 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:34:01,633 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:34:01,634 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:34:01,636 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:34:01,639 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:34:01,640 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:34:01,643 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:34:01,645 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:34:01,647 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:34:01,649 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:34:01,651 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:34:01,654 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:34:01,656 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:34:01,657 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:34:01,659 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:34:01,661 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:34:01,663 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:34:01,664 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:34:01,666 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:34:01,668 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:34:01,669 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:34:01,670 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:34:01,672 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:34:01,673 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:34:01,674 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:34:01,676 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:34:01,677 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:34:01,679 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:34:01,680 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:34:01,681 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:34:01,683 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:34:01,684 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:34:01,685 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:34:01,687 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:34:01,688 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:34:01,689 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:34:01,691 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:34:01,693 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:34:01,831 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:34:01,832 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:34:01,835 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:34:01,837 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:34:01,839 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:34:01,841 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:34:01,842 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:34:01,844 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:34:01,845 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:34:01,847 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:34:01,848 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:34:01,850 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:34:01,851 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:34:01,853 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:34:01,855 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:34:01,857 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:34:01,858 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:34:01,860 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:34:01,861 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:34:01,862 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:34:01,864 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:34:01,865 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:34:01,867 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:34:01,868 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:34:01,870 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:34:01,871 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:34:01,873 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:34:01,874 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:34:01,876 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:34:01,878 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:34:01,879 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:34:01,882 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:34:01,884 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:34:01,885 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:34:01,886 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:34:01,888 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:34:01,890 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:34:01,891 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:34:01,894 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:34:01,895 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:34:01,897 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:34:01,898 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:34:01,900 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:34:01,901 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:34:01,903 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:34:01,915 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:34:01,917 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:34:01,919 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:34:01,921 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:34:01,922 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:34:01,939 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\agents_ia\comandAI\assets\20250414163401_clipboard_20250414162547.png, Prompt: Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diretório atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Definição dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Criação dos diretórios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configuração do layout do Streamlit
st.set_page_config(
    page_title="Análise de Ações",
    layout="wide",
    page_icon="📈"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscilações

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Período', 'Oscilação'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("📊 Dashboard de Análise de Ações")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma ação", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"📌 Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("🔍 Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descrição', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa ação.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("💰 Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("📉 Oscilações")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("📈 Tabela Geral de Ações")
        st.dataframe(self.acoes)

# Execução
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para análise fundamentalista de ações da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se está correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # # Configuração de vídeos
    # video_config = VideoConfig(root_path=root_path)
    # video_processor = VideoToAudioConverter(video_config=video_config)
    # video_processor.process_videos()
    
    # # # Configuração de áudios
    # audio_config = AudioConfig(root_path=root_path)
    # audio_processor = AudioToConverter(audio_config=audio_config)
    # audio_processor.process_audio_files()
    
    # Processamento de transcrições e envio de embeddings
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diretório
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diretório.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do último arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conteúdo do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimensões do embedding:", embedding.shape)

# Caminho do diretório de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o último embedding
load_and_print_embedding(embedding_directory)


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configuração de transcrições e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diretório onde está procurando as transcrições
        print(f"Diretório de entrada das transcrições: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcrição no diretório de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcrição encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo não encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcrição do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} está vazio.")
                    return

            # Gerar o embedding da transcrição
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcrição e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O vídeo {video_path} não contém áudio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de vídeo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de vídeo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de vídeo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Conversão de vídeo para áudio concluída!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configurações da API
handler = genai('gemini-1.5-flash')

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Função para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conteúdo com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares únicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcrição obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' não é uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuniões: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de reuniões: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|tópicos da|pontos (relevantes|principais)|análise)\b.*\b(reunião|última (reunião|conversa|sessão))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como análise de reunião.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padrão.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que são muito similares ao comando atual para evitar redundância.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padrão: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # Métodos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de análise de reunião.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conteúdo usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inválida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar variáveis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conteúdo com base no prompt, contexto e reunião"""
        try:
            # Supondo que a API espera um dicionário com os parâmetros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisição para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar variáveis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Criação da conclusão do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que você acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["faça um resumo da última reunião.", "tópicos da última reunião", "resuma a última reunião", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Você é responsável por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - Não acrescentar título nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informações acima e forneça uma solução detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Você é responsável analisar com detalhes a reunião de [str_meeting], e fornecer uma longa estória sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precisão da resposta.
            - Não acrescente título nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e faça uma pesquisa online para uma resposta mais precisa das questões de [str_text]
        - Não acrescente título nas respostas.
        
        ------
        [context]: Regras Básicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("Áudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum áudio detectado.")
                return None
            except sr.UnknownValueError:
                print("Não foi possível entender o áudio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configurações da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicionário
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta não está no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API: {e}")
        return None

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: Faça uma limpeza na pasta class_voice_assistent
Assistant: Com base nos arquivos fornecidos, aqui está um resumo da estrutura e funcionalidade do código, juntamente com as informações sobre a limpeza da pasta `class_voice_assistent`:

**Visão geral do projeto:**

Este projeto parece ser um assistente de voz que usa o Google Gemini para processamento de linguagem natural. Ele é estruturado em vários módulos e classes para lidar com diferentes aspectos, como captura de áudio, conversão de texto em fala, interpretação de comandos e interação com APIs externas.

**Principais componentes:**

*   **Captura de comandos:**
    *   `voice_assistent\class_voice_assistent\voice_command_hendler.py`: Lida com a captura de comandos de voz usando a biblioteca `speech_recognition`.
    *   `voice_assistent\class_voice_assistent\text_command_hendler.py`: Lida com a captura de comandos de texto do usuário.

*   **Processamento de linguagem natural:**
    *   `voice_assistent\class_voice_assistent\command_interpreter.py`: Interpreta os comandos do usuário e determina a ação apropriada a ser executada.
    *   `voice_assistent\class_voice_assistent\question_answers_service.py`: Converte texto em embeddings para encontrar respostas semelhantes.
    *   `voice_assistent\class_voice_assistent\context_manager.py`: Gerencia o contexto da conversa, armazenando as interações recentes.

*   **Comunicação com o Google Gemini:**
    *   `voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py`: Lida com a comunicação com a API do Google Gemini.
    *   `voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py`: Define o modelo generativo e a configuração da API Google Gemini.

*   **Geração de prompt:**
    *   `voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py`: Gera prompts padrão para o modelo de linguagem.
    *   `voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py`: Gera prompts para análise de reuniões.
    *   `voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py`: Gera prompts para pesquisas online.
    *   `voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py`: Define a classe abstrata para geradores de prompt.

*   **Texto para fala:**
    *   `voice_assistent\class_voice_assistent\text_to_speech.py`: Converte texto em fala usando a biblioteca `pyttsx3`.

*   **Processamento de texto:**
    *   `voice_assistent\class_voice_assistent\text_processor.py`: Fornece funções para extrair valores de JSON e texto de HTML.

*   **Interface principal:**
    *   `voice_assistent\class_voice_assistent\main.py`: Inicializa e coordena os diferentes componentes do assistente de voz.

**Limpeza da pasta `class_voice_assistent`:**

O comando para limpar uma pasta não faz parte diretamente do código fornecido. Para implementar essa funcionalidade, você precisaria adicionar um código que use a biblioteca `os` ou `shutil` para remover arquivos e subdiretórios da pasta `class_voice_assistent`.

**Exemplo de como implementar a limpeza da pasta:**

```python
import os
import shutil

def clean_directory(path):
    """Remove todos os arquivos e subdiretórios em um diretório."""
    for filename in os.listdir(path):
        file_path = os.path.join(path, filename)
        try:
            if os.path.isfile(file_path) or os.path.islink(file_path):
                os.unlink(file_path)
            elif os.path.isdir(file_path):
                shutil.rmtree(file_path)
        except Exception as e:
            print(f'Falha ao excluir {file_path}. Motivo: {e}')

# Caminho para a pasta class_voice_assistent
path_to_clean = 'voice_assistent/class_voice_assistent'

# Chame a função para limpar a pasta
clean_directory(path_to_clean)
```

**Considerações:**

1.  **Importe os módulos necessários**: `os` e `shutil`.
2.  **Defina a função `clean_directory`**: Esta função itera sobre todos os itens no diretório fornecido e tenta remover cada um.
3.  **Tratamento de erros**: A função inclui um bloco `try...except` para lidar com possíveis erros durante a remoção de arquivos ou diretórios.
4.  **Chame a função `clean_directory`**: Passe o caminho do diretório que você deseja limpar.

**Onde adicionar este código:**

Você pode integrar esta função dentro do `command_interpreter.py` ou `main.py`. Por exemplo, dentro do `interpret_command`, você pode adicionar uma condição para detectar o comando "limpar pasta" e, em seguida, chamar a função `clean_directory` com o caminho apropriado.

**Aviso**: Esta operação é irreversível e excluirá permanentemente todos os arquivos e pastas dentro do diretório especificado. Certifique-se de ter um backup se necessário.

**Segurança:**

Certifique-se de que o caminho da pasta a ser limpa seja validado e seguro para evitar exclusões acidentais ou maliciosas.

User: quero remover códigos desnecessários na pasta voice_assistent no projeto.
Assistant: Assistant: Para remover códigos desnecessários na pasta `voice_assistent` do projeto, aqui está uma abordagem detalhada:

**1. Visão Geral:**

O objetivo é identificar e remover arquivos e funcionalidades redundantes ou não utilizados na pasta `voice_assistent`. Isso envolve uma análise cuidadosa do código, das dependências e do fluxo do programa para garantir que nenhuma funcionalidade essencial seja comprometida.

**2. Etapas Detalhadas:**

*   **Backup:**
    *   Antes de fazer qualquer alteração, faça um backup completo da pasta `voice_assistent`. Isso permitirá restaurar o estado anterior em caso de remoção acidental de código importante.

*   **Análise da Estrutura do Código:**
    *   **Identifique os principais módulos:** Analise todos os arquivos Python na pasta `voice_assistent` e suas subpastas para entender sua função e como eles se interconectam.
    *   **Módulos Essenciais:**
        *   `class_voice_assistent/main.py`: Coordena a operação do assistente de voz.
        *   `class_voice_assistent/command_interpreter.py`: Interpreta comandos do usuário.
        *   `class_voice_assistent/gpt_communication/gemini_gpt.py`: Lida com a comunicação com o Google Gemini.
        *   `class_voice_assistent/voice_command_handler.py` e `class_voice_assistent/text_command_handler.py`: Capturam comandos de voz e texto, respectivamente.
        *   `class_voice_assistent/text_to_speech.py`: Converte texto em fala.

*   **Identificação de Código Não Utilizado:**
    *   **Remova imports não utilizados:** Abra cada arquivo e verifique se todos os módulos importados estão realmente sendo usados. Remova aqueles que não estão.

    *   **Analise funções não utilizadas:** Verifique se todas as funções definidas são chamadas em algum lugar do código. Funções que não são chamadas podem ser removidas com segurança.
    *   **Avalie código comentado:** Avalie se os blocos de código comentados ainda são relevantes ou podem ser removidos.

*   **Análise das Dependências:**
    *   **Arquivo requirements.txt ou pyproject.toml:** Verifique se todas as bibliotecas listadas nos arquivos de dependência são necessárias para o funcionamento do assistente de voz. Remova as dependências que não estão sendo usadas.

*   **Simplificação da Lógica:**
    *   **Refatore código duplicado:** Se houver trechos de código repetidos, crie funções ou classes para reutilizá-los, reduzindo a redundância.
    *   **Simplifique funções complexas:** Divida funções muito longas ou complexas em funções menores e mais fáceis de entender e manter.

*   **Remoção de Código Desnecessário:**
    *   **Exclua arquivos desnecessários:** Remova arquivos que não contribuem para a funcionalidade principal do assistente de voz.

*   **Teste:**
    *   Após cada remoção ou modificação significativa, execute os testes para garantir que o assistente de voz continue funcionando corretamente.
    *   Crie testes automatizados para os principais componentes para facilitar a detecção de erros após futuras alterações.

**3. Implementação:**

*   **Edite os Arquivos:**
    *   Abra cada arquivo Python relevante e faça as modificações necessárias, como remover imports não utilizados, funções desnecessárias e refatorar o código.
*   **Remova Arquivos Desnecessários:**
    *   Exclua arquivos que não são mais necessários. Certifique-se de que nenhum outro componente depende desses arquivos antes de removê-los.

**4. Exemplo Prático:**

*   **Avaliação de `template.py`:**
    *   O arquivo `template.py` parece ser uma versão mais antiga ou um modelo inicial do assistente de voz. Compare-o com os outros arquivos (principalmente os arquivos em `class_voice_assistent`) para identificar se alguma parte do código em `template.py` ainda é usada. Se não for, considere remover este arquivo.

*   **Exemplo de refatoração:**
    *   Se você encontrar código duplicado entre `voice_command_hendler.py` e `text_command_hendler.py`, você pode criar uma classe base ou função utilitária para lidar com a lógica comum, reduzindo a duplicação.

**5. Exemplo de Código para Remover Arquivos:**

Para remover um arquivo específico, você pode usar o seguinte código:

```python
import os

file_path = "voice_assistent/class_voice_assistent/arquivo_desnecessario.py"
try:
    os.remove(file_path)
    print(f"Arquivo {file_path} removido com sucesso.")
except FileNotFoundError:
    print(f"Arquivo {file_path} não encontrado.")
except Exception as e:
    print(f"Erro ao remover {file_path}: {e}")
```

**Considerações Adicionais:**

*   **Documentação:** Atualize a documentação do projeto para refletir as mudanças realizadas. Isso ajudará outros desenvolvedores (ou você mesmo no futuro) a entender a nova estrutura do código.
*   **Versionamento:** Use um sistema de controle de versão (como Git) para gerenciar as alterações no código. Isso facilita a reversão de alterações e a colaboração com outros desenvolvedores.

Seguindo essas etapas, você poderá remover códigos desnecessários da pasta `voice_assistent`, tornando o projeto mais limpo, eficiente e fácil de manter.
User: você tem o código no contexto. você é meu assistente e deve fazer a análise para mim.
Assistant: Com prazer! Analisei o código que você forneceu e identifiquei algumas áreas onde podemos remover código desnecessário e refatorar para melhor clareza e eficiência.

**1. Análise Detalhada e Sugestões:**

*   **`template.py`:**

    *   **Status:** Este arquivo parece ser uma versão anterior ou um modelo desatualizado do assistente de voz.
    *   **Ação:** Comparar o código deste arquivo com os componentes atuais em `class_voice_assistent`. Se o código não for mais utilizado, **remover o arquivo `template.py`**.

*   **`voice_assistent\config.py`:**

    *   **Status:** Este arquivo contém configurações básicas, mas algumas dessas configurações podem estar duplicadas ou não sendo usadas.
    *   **Ação:**
        *   **apiKey e url:** Verificar se essas variáveis ainda são necessárias e se não estão duplicadas em outro lugar (por exemplo, em um arquivo `.env`).
        *   **engine, recent_context, nlp:** Estes itens são inicializados novamente em outros arquivos. Avaliar se é melhor centralizar a inicialização em um único local (possivelmente em `class_voice_assistent/main.py`) e importar as instâncias onde necessário.

*   **`class_voice_assistent\conversation_history.py`:**

    *   **Status:** Este arquivo está vazio.
    *   **Ação:** **Remover este arquivo** se ele não for usado. Se a intenção era armazenar o histórico de conversas, a funcionalidade deve ser movida para um arquivo relevante ou integrada em `class_voice_assistent\context_manager.py`.

*   **`class_voice_assistent\gpt_communication\groq._gpt.py`:**

    *   **Status:** Este arquivo demonstra o uso da API Groq, mas não parece estar integrado ao resto do código.
    *   **Ação:** Se a API Groq não for utilizada, **remover este arquivo**. Caso contrário, integrar corretamente e remover a duplicação de configurações e inicializações.

*   **`class_voice_assistent\prompt_generator\prompt_generator.py`:**

    *   **Status:** Definição de uma classe abstrata.
    *   **Ação:** Verificar se a classe abstrata `PromptGenerator` é usada corretamente pelas classes filhas. Se as classes filhas não implementam os métodos abstratos corretamente, refatorar ou remover a classe abstrata.

*   **Módulos de Comando (`voice_command_hendler.py` e `text_command_hendler.py`):**

    *   **Status:** Lidam com a captura de comandos.
    *   **Ação:** Verificar se há duplicação de código entre esses dois módulos (por exemplo, tratamento de erros, prompts ao usuário). Se houver, criar uma classe base ou função utilitária para lidar com a lógica comum.

*   **Remoção de código não utilizado:**

    *   Utilize um linter (como `flake8`) para identificar imports não utilizados e código não referenciado nos arquivos.

**2. Plano de Ação Detalhado:**

1.  **Backup:** Criar um backup da pasta `voice_assistent`.
2.  **Remover `template.py`:**
    ```bash
    rm voice_assistent/template.py
    ```
3.  **Remover `class_voice_assistent\conversation_history.py`:**
    ```bash
    rm voice_assistent/class_voice_assistent/conversation_history.py
    ```
4.  **Remover `class_voice_assistent\gpt_communication\groq._gpt.py`:**
    ```bash
    rm voice_assistent/class_voice_assistent/gpt_communication/groq._gpt.py
    ```
5.  **Refatorar Configurações:**
    *   Mover as configurações essenciais de `voice_assistent\config.py` para um arquivo `.env` ou para `class_voice_assistent\main.py`.
    *   Remover o arquivo `voice_assistent\config.py` se ele não for mais necessário.
6.  **Refatorar Módulos de Comando:**
    *   Criar uma classe base ou função utilitária para consolidar a lógica comum em `voice_command_hendler.py` e `text_command_hendler.py`.
7.  **Executar Linter:**
    ```bash
    pip install flake8
    flake8 voice_assistent
    ```
    *   Remover imports não utilizados e código não referenciado.
8.  **Testar:**
    *   Após cada passo, testar o assistente de voz para garantir que ele continue funcionando corretamente.
9.  **Documentar:**
    *   Atualizar a documentação do projeto para refletir as mudanças realizadas.

**3. Implementação:**

Remover código não utilizado

```python
# Exemplo de como remover um arquivo
import os

file_path = "voice_assistent/class_voice_assistent/gpt_communication/groq._gpt.py"
try:
    os.remove(file_path)
    print(f"Arquivo {file_path} removido com sucesso.")
except FileNotFoundError:
    print(f"Arquivo {file_path} não encontrado.")
except Exception as e:
    print(f"Erro ao remover {file_path}: {e}")
```

**Centralizar configurações**

```python
# Exemplo de como centralizar configurações em main.py
import os
from dotenv import load_dotenv

load_dotenv()

api_key = os.getenv("API_KEY")
# ... outras configurações
```

**4. Considerações Adicionais:**

*   **Testes:** Criar testes unitários para garantir a estabilidade dos componentes principais.
*   **Versionamento:** Usar Git para controlar as alterações e facilitar a reversão, se necessário.
*   **Segurança:** Validar o caminho da pasta para evitar exclusões acidentais.

Com essas ações, a pasta `voice_assistent` ficará mais organizada, eficiente e fácil de manter. Se precisar de ajuda com algum passo específico ou detalhe adicional, é só me avisar!

User: Preciso importar essas libs 
Assistant: ```python
import os
from dotenv import load_dotenv
import os
import shutil
import pyttsx3
import spacy
import speech_recognition as sr
import requests
import re
from collections import deque
from bs4 import BeautifulSoup
import google.generativeai as genai
```

Essas são as bibliotecas que foram mencionadas ao longo da conversa.

Aqui está um exemplo prático de como importar essas bibliotecas no início de um arquivo Python:

```python
# Importações
import os
from dotenv import load_dotenv
import shutil
import pyttsx3
import spacy
import speech_recognition as sr
import requests
import re
from collections import deque
from bs4 import BeautifulSoup
import google.generativeai as genai

# Seu código continua aqui...
```

Essas importações fornecerão acesso às funcionalidades de cada biblioteca ao longo do seu código.

Além disso, aqui está uma breve descrição de cada biblioteca, juntamente com um link para a documentação, para que você possa aprender mais sobre como utilizá-las de forma eficaz:

-   **os:** Fornece uma maneira de usar funcionalidades dependentes do sistema operacional (documentação: <https://docs.python.org/3/library/os.html>)
-   **dotenv:** Permite carregar variáveis de ambiente de um arquivo `.env` (documentação: <https://pypi.org/project/python-dotenv/>)
-   **shutil:** Oferece um conjunto de operações de alto nível em arquivos e coleções de arquivos, como copiar e remover arquivos e diretórios (documentação: <https://docs.python.org/3/library/shutil.html>)
-   **pyttsx3:** Biblioteca de conversão de texto em fala que funciona offline (documentação: <https://pyttsx3.readthedocs.io/en/latest/>)
-   **spacy:** Uma biblioteca de processamento de linguagem natural avançada (documentação: <https://spacy.io/>)
-   **speech\_recognition:** Biblioteca para reconhecimento de fala, suportando várias APIs (documentação: <https://pypi.org/project/SpeechRecognition/>)
-   **requests:** Biblioteca para fazer requisições HTTP (documentação: <https://requests.readthedocs.io/en/latest/>)
-   **re:** Fornece operações de expressão regular (documentação: <https://docs.python.org/3/library/re.html>)
-   **collections.deque:** Uma generalização da pilha e da fila que suporta anexar e remover de qualquer extremidade (documentação: <https://docs.python.org/3/library/collections.html#collections.deque>)
-   **bs4 (Beautiful Soup):** Biblioteca para extrair informações de páginas web em formato HTML ou XML (documentação: <https://www.crummy.com/software/BeautifulSoup/bs4/doc/>)
-   **google.generativeai:** Biblioteca para usar os modelos de linguagem generativa do Google (documentação: verificar a documentação específica para os modelos do Google Gemini no Google Cloud).

Com essas importações e a documentação correspondente, você está bem preparado para continuar sua análise e refatoração do código!
User: como faço para instalar as que estão dando erro na imagem ?
Assistant: Com base na imagem que você forneceu, as bibliotecas que precisam ser instaladas são:

*   **speech\_recognition:** Para reconhecimento de fala.
*   **pyttsx3:** Para síntese de fala (conversão de texto em voz).
*   **spacy:** Para processamento de linguagem natural.
*   **google.generativeai:** Para interagir com os modelos generativos do Google.

Aqui estão os passos para instalar essas bibliotecas usando `pip`:

**1. Abra o Terminal ou Prompt de Comando:**

*   No Windows, você pode usar o "Prompt de Comando" ou o "PowerShell".
*   No macOS ou Linux, use o "Terminal".

**2. Instale as Bibliotecas:**

Use o comando `pip install` seguido pelo nome das bibliotecas. Execute os seguintes comandos um por um para instalar cada biblioteca:

```bash
pip install SpeechRecognition
pip install pyttsx3
pip install spacy
pip install google-generativeai
```

**3. Instale o Modelo de Linguagem do SpaCy:**

Para usar a biblioteca SpaCy corretamente, você precisa baixar um modelo de linguagem. Para português (pt), você pode instalar o modelo `pt_core_news_sm`:

```bash
python -m spacy download pt_core_news_sm
```

**4. Verificar a Instalação:**

Para verificar se as bibliotecas foram instaladas corretamente, você pode executar o seguinte código Python em um interpretador Python ou em um arquivo `.py`:

```python
import speech_recognition as sr
import pyttsx3
import spacy
import google.generativeai as genai

print("SpeechRecognition versão:", sr.__version__)
print("pyttsx3 ok")
print("Spacy versão:", spacy.__version__)
print("google-generativeai ok")
```

Se não houver erros ao executar este código, significa que as bibliotecas foram instaladas corretamente.

**Informações Adicionais:**

*   **Se você estiver usando um ambiente virtual (`venv`):** Certifique-se de ativar o ambiente virtual antes de instalar as bibliotecas. Isso garante que as bibliotecas sejam instaladas apenas dentro do ambiente virtual e não no sistema global.
*   **Problemas de Permissão:** Se você encontrar problemas de permissão ao instalar as bibliotecas, tente executar o comando com privilégios de administrador (por exemplo, use `sudo pip install` no macOS/Linux).
*   **Versões das Bibliotecas:** Em alguns casos, versões mais recentes das bibliotecas podem ter problemas de compatibilidade com o seu código. Se você tiver problemas, pode tentar instalar versões mais antigas das bibliotecas.

Com estes passos, você deverá conseguir instalar as bibliotecas que estão faltando e resolver os erros no seu código!
User: jfreis@STFSAON006541-L MINGW64 ~/Documents/agents_ia (master)
$ pip install spacy
Collecting spacy
  Using cached spacy-3.8.2.tar.gz (1.3 MB)
  Installing build dependencies ... error
  error: subprocess-exited-with-error

  × pip subprocess to install build dependencies did not run successfully.
  │ exit code: 1
  ╰─> [67 lines of output]
      Ignoring numpy: markers 'python_version < "3.9"' don't match your environment
      Collecting setuptools
        Using cached setuptools-78.1.0-py3-none-any.whl.metadata (6.6 kB)
      Collecting cython<3.0,>=0.25
        Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)
      Collecting cymem<2.1.0,>=2.0.2
        Using cached cymem-2.0.11-cp313-cp313-win_amd64.whl.metadata (8.8 kB)
      Collecting preshed<3.1.0,>=3.0.2
        Using cached preshed-3.0.9.tar.gz (14 kB)
        Installing build dependencies: started
        Installing build dependencies: finished with status 'done'
        Getting requirements to build wheel: started
        Getting requirements to build wheel: finished with status 'done'
        Preparing metadata (pyproject.toml): started
        Preparing metadata (pyproject.toml): finished with status 'done'
      Collecting murmurhash<1.1.0,>=0.28.0
        Using cached murmurhash-1.0.12-cp313-cp313-win_amd64.whl.metadata (2.2 kB)
      Collecting thinc<8.4.0,>=8.3.0
        Using cached thinc-8.3.6-cp313-cp313-win_amd64.whl.metadata (15 kB)
      Collecting numpy<2.1.0,>=2.0.0
        Using cached numpy-2.0.2.tar.gz (18.9 MB)
        Installing build dependencies: started
        Installing build dependencies: finished with status 'done'
        Getting requirements to build wheel: started
        Getting requirements to build wheel: finished with status 'done'
        Installing backend dependencies: started
        Installing backend dependencies: finished with status 'done'
        Preparing metadata (pyproject.toml): started
        Preparing metadata (pyproject.toml): finished with status 'error'
        error: subprocess-exited-with-error
     
        Preparing metadata (pyproject.toml) did not run successfully.
        exit code: 1
     
        [21 lines of output]
        + C:\Users\jfreis\AppData\Local\Programs\Python\Python313\python.exe C:\Users\jfreis\AppData\Local\Temp\pip-install-_5m3qz36\numpy_125b75e27d3645fc84b36c82dbcdef81\vendored-meson\meson\meson.py setup C:\Users\jfreis\AppData\Local\Temp\pip-install-_5m3qz36\numpy_125b75e27d3645fc84b36c82dbcdef81 C:\Users\jfreis\AppData\Local\Temp\pip-install-_5m3qz36\numpy_125b75e27d3645fc84b36c82dbcdef81\.mesonpy-hs6rlqj9 -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\Users\jfreis\AppData\Local\Temp\pip-install-_5m3qz36\numpy_125b75e27d3645fc84b36c82dbcdef81\.mesonpy-hs6rlqj9\meson-python-native-file.ini
        The Meson build system
        Version: 1.4.99
        Source dir: C:\Users\jfreis\AppData\Local\Temp\pip-install-_5m3qz36\numpy_125b75e27d3645fc84b36c82dbcdef81
        Build dir: C:\Users\jfreis\AppData\Local\Temp\pip-install-_5m3qz36\numpy_125b75e27d3645fc84b36c82dbcdef81\.mesonpy-hs6rlqj9
        Build type: native build
        Project name: NumPy
        Project version: 2.0.2
        WARNING: Failed to activate VS environment: Could not parse vswhere.exe output
     
        ..\meson.build:1:0: ERROR: Unknown compiler(s): [['icl'], ['cl'], ['cc'], ['gcc'], ['clang'], ['clang-cl'], ['pgcc']]  
        The following exception(s) were encountered:
        Running `icl ""` gave "[WinError 2] O sistema não pode encontrar o arquivo especificado"
        Running `cl /?` gave "[WinError 2] O sistema não pode encontrar o arquivo especificado"
        Running `cc --version` gave "[WinError 2] O sistema não pode encontrar o arquivo especificado"
        Running `gcc --version` gave "[WinError 2] O sistema não pode encontrar o arquivo especificado"
        Running `clang --version` gave "[WinError 2] O sistema não pode encontrar o arquivo especificado"
        Running `clang-cl /?` gave "[WinError 2] O sistema não pode encontrar o arquivo especificado"
        Running `pgcc --version` gave "[WinError 2] O sistema não pode encontrar o arquivo especificado"
     
        A full log can be found at C:\Users\jfreis\AppData\Local\Temp\pip-install-_5m3qz36\numpy_125b75e27d3645fc84b36c82dbcdef81\.mesonpy-hs6rlqj9\meson-logs\meson-log.txt
        [end of output]
     
        note: This error originates from a subprocess, and is likely not a problem with pip.
      error: metadata-generation-failed
     
      Encountered error while generating package metadata.
     
      See above for output.
     
      note: This is an issue with the package mentioned above, not pip.
      hint: See above for details.
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

× pip subprocess to install build dependencies did not run successfully.
│ exit code: 1
╰─> See above for output.

note: This error originates from a subprocess, and is likely not a problem with pip.

jfreis@STFSAON006541-L MINGW64 ~/Documents/agents_ia (master)
$
Assistant: The error message indicates that the build process for `numpy` (a dependency of `spacy`) is failing because it cannot find a suitable compiler (like `icl`, `cl`, `cc`, `gcc`, etc.) on your system. This typically happens on Windows when the required build tools for compiling Python packages are not installed.

Here's a breakdown of how to fix this, along with detailed steps:

**1. Install Microsoft Visual C++ Build Tools:**

This is the most common solution. Many Python packages (including NumPy and some others) rely on C/C++ code that needs to be compiled during installation.

*   **Download the Build Tools:**  Go to the following Microsoft website:
    [https://visualstudio.microsoft.com/visual-cpp-build-tools/](https://visualstudio.microsoft.com/visual-cpp-build-tools/)

*   **Select the Installer:** Look for "Build Tools for Visual Studio" and download the installer.

*   **Run the Installer:**

    *   When the installer opens, make sure to select the "C++ build tools" workload.  This includes the C++ compiler, libraries, and other necessary tools. It's important to select this option.

    *   You might also want to select "Windows 10 SDK" (or a more recent version, if applicable) if it's not already selected.

*   **Complete the Installation:** Follow the prompts to complete the installation. This may take some time.

**2. Restart Your Computer (Important):**

After installing the Visual C++ Build Tools, *restart your computer*. This ensures that the system picks up the changes to the environment variables that the Build Tools installer makes.  Failing to restart is a common cause of continuing problems.

**3. Try Installing SpaCy Again:**

Open a new terminal window (or restart your existing one) and try installing SpaCy again:

```bash
pip install spacy
```

This time, the installation should proceed successfully because the `numpy` build process will be able to find the necessary C++ compiler.

**4. Handle Potential Environment Issues (If the Above Doesn't Work):**

Sometimes, even after installing the Build Tools, the necessary environment variables aren't set up correctly. You can try manually setting the environment variables:

*   **Find `vcvarsall.bat`:**  The `vcvarsall.bat` file is a batch script that sets up the environment variables for the Visual C++ compiler.  It's usually located in a directory like this (adjust for your Visual Studio version):

    `C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Auxiliary\Build`
    or
    `C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Auxiliary\Build`

*   **Run `vcvarsall.bat`:**  Open a command prompt (not PowerShell) and navigate to the directory containing `vcvarsall.bat`. Then, run the script:

    ```bash
    vcvarsall.bat amd64
    ```

    *   `amd64` specifies the target architecture (64-bit). If you're on a 32-bit system, use `x86` instead.

*   **Test and Reinstall:** After running `vcvarsall.bat`, try running `pip install spacy` in the *same* command prompt window.  The environment variables set by `vcvarsall.bat` are only valid for that specific command prompt session.

**5. Consider Conda (Alternative to pip):**

If you're still having trouble with `pip`, consider using Conda, a package, dependency, and environment manager that's often more reliable for installing complex scientific computing packages on Windows.

*   **Install Conda:**  If you don't have Conda installed, download and install Anaconda or Miniconda from here: [https://docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html)

*   **Create an Environment (Recommended):**  Create a new Conda environment to avoid conflicts with your existing Python installation:

    ```bash
    conda create -n spacy_env python=3.9  # Or a compatible Python version
    conda activate spacy_env
    ```

*   **Install SpaCy with Conda:**

    ```bash
    conda install -c conda-forge spacy
    ```
*   **Install the Language Model:**
     ```bash
    python -m spacy download pt_core_news_sm
    ```

Conda often handles the underlying build tool dependencies more effectively than `pip` on Windows.

**Troubleshooting Steps if You Still Have Issues:**

*   **Check your Python version:** Ensure that you have a compatible Python version installed. The error message might provide clues if the Python version is the issue.  Spacy generally supports Python 3.7+.
*   **Update pip:** Make sure you have the latest version of pip: `python -m pip install --upgrade pip`
*   **Check your PATH:** Verify that the Python installation directory and the Scripts directory (where `pip` is located) are in your system's `PATH` environment variable.
*   **Firewall/Antivirus:** In rare cases, a firewall or antivirus program might interfere with the installation process. Temporarily disable them and try again.

By following these steps in order, you should be able to resolve the error and successfully install SpaCy and its dependencies. Remember to restart your computer after installing the Visual C++ Build Tools, and try each subsequent step in a new terminal window.

User: explique em portugues.
2025-04-14 16:34:08,126 [INFO] Resposta da IA (imagem): Claro, aqui está a explicação detalhada em português para resolver o erro de instalação do SpaCy:

**O Problema:**

A mensagem de erro indica que o processo de construção do `numpy` (uma dependência do `spacy`) está falhando porque não consegue encontrar um compilador adequado (como `icl`, `cl`, `cc`, `gcc`, etc.) no seu sistema. Isso geralmente acontece no Windows quando as ferramentas de construção necessárias para compilar pacotes Python não estão instaladas.

**A Solução:**

Para resolver esse problema, você precisa instalar as ferramentas de construção da Microsoft Visual C++. Veja como:

**1. Instale as Ferramentas de Construção do Microsoft Visual C++:**

Esta é a solução mais comum. Muitos pacotes Python (incluindo NumPy e alguns outros) dependem de código C/C++ que precisa ser compilado durante a instalação.

*   **Baixe as Ferramentas de Construção:** Acesse o seguinte site da Microsoft:
    [https://visualstudio.microsoft.com/visual-cpp-build-tools/](https://visualstudio.microsoft.com/visual-cpp-build-tools/)

*   **Selecione o Instalador:** Procure por "Build Tools for Visual Studio" e baixe o instalador.

*   **Execute o Instalador:**
    *   Quando o instalador abrir, certifique-se de selecionar a carga de trabalho "Ferramentas de construção C++". Isso inclui o compilador C++, bibliotecas e outras ferramentas necessárias. É importante selecionar esta opção.

    *   Você também pode querer selecionar "SDK do Windows 10" (ou uma versão mais recente, se aplicável) se já não estiver selecionado.

*   **Conclua a Instalação:** Siga as instruções para concluir a instalação. Isso pode demorar algum tempo.

**2. Reinicie o Seu Computador (Importante):**

Após instalar as Ferramentas de Construção do Visual C++, *reinicie o seu computador*. Isso garante que o sistema detecte as alterações nas variáveis de ambiente que o instalador das Ferramentas de Construção faz. Não reiniciar é uma causa comum de problemas persistentes.

**3. Tente Instalar o SpaCy Novamente:**

Abra uma nova janela de terminal (ou reinicie a sua existente) e tente instalar o SpaCy novamente:

```bash
pip install spacy
```

Desta vez, a instalação deve prosseguir com sucesso porque o processo de construção do `numpy` será capaz de encontrar o compilador C++ necessário.

**4. Lidar com Possíveis Problemas de Ambiente (Se o Acima Não Funcionar):**

Às vezes, mesmo após instalar as Ferramentas de Construção, as variáveis de ambiente necessárias não são configuradas corretamente. Você pode tentar configurar manualmente as variáveis de ambiente:

*   **Encontre `vcvarsall.bat`:** O arquivo `vcvarsall.bat` é um script em lote que configura as variáveis de ambiente para o compilador Visual C++. Geralmente está localizado em um diretório como este (ajuste para a sua versão do Visual Studio):
    `C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Auxiliary\Build`
    ou
    `C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Auxiliary\Build`

*   **Execute `vcvarsall.bat`:** Abra um prompt de comando (não PowerShell) e navegue até o diretório que contém `vcvarsall.bat`. Em seguida, execute o script:

    ```bash
    vcvarsall.bat amd64
    ```

    *   `amd64` especifica a arquitetura de destino (64-bit). Se você estiver em um sistema de 32-bit, use `x86` em vez disso.

*   **Teste e Reinstale:** Depois de executar `vcvarsall.bat`, tente executar `pip install spacy` na *mesma* janela do prompt de comando. As variáveis de ambiente definidas por `vcvarsall.bat` são válidas apenas para essa sessão específica do prompt de comando.

**5. Considere Conda (Alternativa ao pip):**

Se você ainda estiver tendo problemas com o `pip`, considere usar o Conda, um gerenciador de pacotes, dependências e ambientes que é frequentemente mais confiável para instalar pacotes complexos de computação científica no Windows.

*   **Instale o Conda:** Se você não tiver o Conda instalado, baixe e instale o Anaconda ou Miniconda daqui: [https://docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html)

*   **Crie um Ambiente (Recomendado):** Crie um novo ambiente Conda para evitar conflitos com a sua instalação Python existente:

    ```bash
    conda create -n spacy_env python=3.9  # Ou uma versão Python compatível
    conda activate spacy_env
    ```

*   **Instale o SpaCy com Conda:**
    ```bash
    conda install -c conda-forge spacy
    ```
    *   **Instale o Language Model:**
        ```bash
    python -m spacy download pt_core_news_sm
    ```

O Conda geralmente lida com as dependências subjacentes das ferramentas de construção de forma mais eficaz do que o `pip` no Windows.

**Passos de Solução de Problemas Adicionais Se Você Ainda Tiver Problemas:**

*   **Verifique a sua versão do Python:** Certifique-se de que você tem uma versão compatível do Python instalada. A mensagem de erro pode fornecer pistas se a versão do Python é o problema. Spacy geralmente suporta Python 3.7+.
*   **Atualize o pip:** Certifique-se de ter a versão mais recente do pip: `python -m pip install --upgrade pip`
*   **Verifique o seu PATH:** Verifique se o diretório de instalação do Python e o diretório Scripts (onde o `pip` está localizado) estão na variável de ambiente `PATH` do seu sistema.
*   **Firewall/Antivírus:** Em casos raros, um firewall ou programa antivírus pode interferir no processo de instalação. Desative-os temporariamente e tente novamente.

Seguindo estes passos em ordem, você deverá conseguir resolver o erro e instalar o SpaCy e suas dependências com sucesso. Lembre-se de reiniciar o seu computador após instalar as Ferramentas de Construção do Visual C++, e tente cada passo subsequente numa nova janela de terminal.
2025-04-14 16:34:08,274 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-14 16:34:08,276 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-14 16:34:08,277 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-14 16:34:08,279 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:34:08,281 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:34:08,282 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:34:08,284 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:34:08,286 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:34:08,288 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:34:08,289 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:34:08,291 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:34:08,293 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:34:08,295 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:34:08,297 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:34:08,299 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:34:08,301 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:34:08,303 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-14 16:34:08,304 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-14 16:34:08,307 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-14 16:34:08,309 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-14 16:34:08,311 [INFO] Arquivo lido com sucesso: main.py
2025-04-14 16:34:08,313 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-14 16:34:08,315 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-14 16:34:08,317 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-14 16:34:08,320 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-14 16:34:08,322 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-14 16:34:08,324 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-14 16:34:08,326 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-14 16:34:08,328 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-14 16:34:08,331 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-14 16:34:08,333 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-14 16:34:08,335 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-14 16:34:08,338 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-14 16:34:08,340 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-14 16:34:08,342 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-14 16:34:08,345 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-14 16:34:08,347 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-14 16:34:08,349 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-14 16:34:08,351 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-14 16:34:08,352 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-14 16:34:08,354 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-14 16:34:08,356 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-14 16:34:08,358 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-14 16:34:08,361 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-14 16:34:08,363 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-14 16:34:08,365 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-14 16:34:08,367 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-14 16:34:08,368 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-14 16:34:08,370 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-14 16:34:08,371 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-14 16:43:41,553 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:43:41,555 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:43:41,556 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:43:41,557 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:43:41,558 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:43:41,560 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:43:41,561 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:43:41,562 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:43:41,563 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:43:41,564 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:43:41,566 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:43:41,567 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:43:41,568 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:43:42,114 [ERROR] API Key não encontrada nas variáveis de ambiente
2025-04-14 16:45:16,582 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:45:16,584 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:45:16,585 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:45:16,586 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:45:16,587 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:45:16,588 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:45:16,589 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:45:16,590 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:45:16,591 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:45:16,592 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:45:16,593 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:45:16,594 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:45:16,596 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:45:16,601 [INFO] Modelo Gemini 'gemini-2.0-flash-exp' inicializado com sucesso.
2025-04-14 16:45:33,246 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:45:33,248 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:45:33,250 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:45:33,251 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:45:33,253 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:45:33,254 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:45:33,256 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:45:33,258 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:45:33,259 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:45:33,260 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:45:33,262 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:45:33,264 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:45:33,265 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:45:33,377 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:45:33,378 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:45:33,380 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:45:33,382 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:45:33,383 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:45:33,384 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:45:33,386 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:45:33,387 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:45:33,389 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:45:33,390 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:45:33,392 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:45:33,394 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:45:33,396 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:45:33,398 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: Resuma essa aplicação 
2025-04-14 16:45:41,104 [INFO] Resposta da IA (texto): Esta aplicação Streamlit oferece um chatbot inteligente que interage com o usuário por meio de texto e imagens, utilizando o modelo Gemini da Google. Aqui está um resumo das principais funcionalidades e componentes:

**Funcionalidades Principais:**

*   **Chatbot Multimodal:** Permite conversas textuais e análise de imagens. O usuário pode enviar mensagens de texto e, opcionalmente, anexar imagens para obter informações ou análises específicas sobre elas.
*   **Geração de Imagens:** Integra um recurso para gerar imagens a partir de prompts de texto usando o Gemini.
*   **Histórico de Conversa:** Mantém um histórico das mensagens trocadas, limitado a um número máximo para evitar sobrecarga.
*   **Integração com Área de Transferência:** Possibilita colar imagens diretamente da área de transferência, facilitando o envio de capturas de tela ou outras imagens copiadas.
*   **Upload de Imagens:** Permite o envio de imagens a partir do sistema de arquivos do usuário.
*   **Limpeza de Imagens:** Oferece botões para limpar imagens carregadas ou coladas, permitindo ao usuário começar com uma nova imagem sem recarregar a página.
*   **Contexto Dinâmico:** O chatbot inclui o código fonte da própria aplicação como contexto, permitindo perguntas sobre a estrutura e o funcionamento interno do aplicativo.
*   **Rate Limiter:** Implementa um sistema de rate limiting para controlar o número de requisições à API do Gemini, evitando sobrecarga e garantindo um uso justo dos recursos.

**Componentes e Arquitetura:**

*   **Streamlit:** A interface do usuário é construída com Streamlit, uma biblioteca Python para criação rápida de aplicativos web interativos.
*   **Gemini API:** Utiliza a API do Gemini, modelo de linguagem da Google, para gerar respostas inteligentes com base no texto e nas imagens fornecidas.
*   **Handlers:** A interação com o Gemini é encapsulada em um `GeminiHandler`, que abstrai a complexidade da API e gerencia o rate limiting.
*   **Configuração:** As configurações da aplicação, como caminhos de arquivos, limites de taxa e prompts, são centralizadas em um módulo de configuração (`config.py`).
*   **Logs:**  Utiliza um sistema de logs para registrar informações sobre o funcionamento da aplicação, erros e interações com a API.
*   **Estados de Sessão:** O `session_state` do Streamlit é utilizado para armazenar e gerenciar o estado da aplicação, como histórico de mensagens, imagens carregadas e status de processamento.
*   **Serviços:** A aplicação é organizada em serviços, como:
    *   `gpt_services.py`: Lida com a interação direta com a API do Gemini.
    *   `document_service.py`: Responsável pela criação e formatação de documentos (Word).
    *   `markdown_service.py`: Responsável pela criação de documentos Markdown.
    *   `image_processor.py`: Processa imagens, extrai informações e atualiza o histórico.
    *   `search_files.py`:  Lê o código fonte da aplicação para fornecer contexto ao chatbot.

**Fluxo de Operação:**

1.  **Entrada do Usuário:** O usuário interage com a aplicação através da interface Streamlit, enviando mensagens de texto e, opcionalmente, carregando ou colando imagens.
2.  **Processamento da Mensagem:** A aplicação verifica se há uma imagem anexada. Se houver, prepara a imagem para envio à API do Gemini.
3.  **Geração da Resposta:** O `GeminiHandler` envia o prompt de texto e a imagem (se houver) para a API do Gemini.
4.  **Exibição da Resposta:** A resposta gerada pelo Gemini é exibida na interface do usuário, juntamente com a imagem (se houver).
5.  **Histórico:** As mensagens do usuário e as respostas do Gemini são adicionadas ao histórico da conversa.

**Em resumo, esta aplicação oferece um chatbot versátil e inteligente, capaz de interagir com o usuário por meio de texto e imagens, utilizando o poder da API do Gemini e uma arquitetura bem organizada.**

2025-04-14 16:45:41,223 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:45:41,224 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:45:41,226 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:45:41,227 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:45:41,228 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:45:41,229 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:45:41,231 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:45:41,233 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:45:41,234 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:45:41,235 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:45:41,236 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:45:41,237 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:45:41,239 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:50:08,376 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



User: Resuma essa aplicação 
Assistant: Esta aplicação Streamlit oferece um chatbot inteligente que interage com o usuário por meio de texto e imagens, utilizando o modelo Gemini da Google. Aqui está um resumo das principais funcionalidades e componentes:

**Funcionalidades Principais:**

*   **Chatbot Multimodal:** Permite conversas textuais e análise de imagens. O usuário pode enviar mensagens de texto e, opcionalmente, anexar imagens para obter informações ou análises específicas sobre elas.
*   **Geração de Imagens:** Integra um recurso para gerar imagens a partir de prompts de texto usando o Gemini.
*   **Histórico de Conversa:** Mantém um histórico das mensagens trocadas, limitado a um número máximo para evitar sobrecarga.
*   **Integração com Área de Transferência:** Possibilita colar imagens diretamente da área de transferência, facilitando o envio de capturas de tela ou outras imagens copiadas.
*   **Upload de Imagens:** Permite o envio de imagens a partir do sistema de arquivos do usuário.
*   **Limpeza de Imagens:** Oferece botões para limpar imagens carregadas ou coladas, permitindo ao usuário começar com uma nova imagem sem recarregar a página.
*   **Contexto Dinâmico:** O chatbot inclui o código fonte da própria aplicação como contexto, permitindo perguntas sobre a estrutura e o funcionamento interno do aplicativo.
*   **Rate Limiter:** Implementa um sistema de rate limiting para controlar o número de requisições à API do Gemini, evitando sobrecarga e garantindo um uso justo dos recursos.

**Componentes e Arquitetura:**

*   **Streamlit:** A interface do usuário é construída com Streamlit, uma biblioteca Python para criação rápida de aplicativos web interativos.
*   **Gemini API:** Utiliza a API do Gemini, modelo de linguagem da Google, para gerar respostas inteligentes com base no texto e nas imagens fornecidas.
*   **Handlers:** A interação com o Gemini é encapsulada em um `GeminiHandler`, que abstrai a complexidade da API e gerencia o rate limiting.
*   **Configuração:** As configurações da aplicação, como caminhos de arquivos, limites de taxa e prompts, são centralizadas em um módulo de configuração (`config.py`).
*   **Logs:**  Utiliza um sistema de logs para registrar informações sobre o funcionamento da aplicação, erros e interações com a API.
*   **Estados de Sessão:** O `session_state` do Streamlit é utilizado para armazenar e gerenciar o estado da aplicação, como histórico de mensagens, imagens carregadas e status de processamento.
*   **Serviços:** A aplicação é organizada em serviços, como:
    *   `gpt_services.py`: Lida com a interação direta com a API do Gemini.
    *   `document_service.py`: Responsável pela criação e formatação de documentos (Word).
    *   `markdown_service.py`: Responsável pela criação de documentos Markdown.
    *   `image_processor.py`: Processa imagens, extrai informações e atualiza o histórico.
    *   `search_files.py`:  Lê o código fonte da aplicação para fornecer contexto ao chatbot.

**Fluxo de Operação:**

1.  **Entrada do Usuário:** O usuário interage com a aplicação através da interface Streamlit, enviando mensagens de texto e, opcionalmente, carregando ou colando imagens.
2.  **Processamento da Mensagem:** A aplicação verifica se há uma imagem anexada. Se houver, prepara a imagem para envio à API do Gemini.
3.  **Geração da Resposta:** O `GeminiHandler` envia o prompt de texto e a imagem (se houver) para a API do Gemini.
4.  **Exibição da Resposta:** A resposta gerada pelo Gemini é exibida na interface do usuário, juntamente com a imagem (se houver).
5.  **Histórico:** As mensagens do usuário e as respostas do Gemini são adicionadas ao histórico da conversa.

**Em resumo, esta aplicação oferece um chatbot versátil e inteligente, capaz de interagir com o usuário por meio de texto e imagens, utilizando o poder da API do Gemini e uma arquitetura bem organizada.**

User: leia a aplicação.
2025-04-14 16:50:11,720 [INFO] Resposta da IA (texto): Para ler a aplicação, preciso ter acesso ao código fonte. Como sou um modelo de linguagem, não consigo acessar diretamente arquivos no seu computador ou em um repositório online.

Para que eu possa "ler" a aplicação e te ajudar melhor, você pode:

1.  **Compartilhar o código:** Cole o conteúdo dos principais arquivos da aplicação aqui, um por vez. Comece pelos arquivos que você considera mais importantes para entender a lógica geral, como `app.py` (o arquivo principal do Streamlit), `gpt_services.py`, `config.py` e qualquer outro arquivo central na arquitetura.
2.  **Fazer perguntas específicas:** Se você tem dúvidas sobre partes específicas do código, como o funcionamento de uma função ou a finalidade de um determinado módulo, pode me perguntar diretamente.

Ao me fornecer o código ou fazer perguntas direcionadas, posso analisar a aplicação e te auxiliar com:

*   **Explicação do código:** Descrever o que cada parte do código faz.
*   **Identificação de problemas:** Ajudar a encontrar erros ou áreas que podem ser otimizadas.
*   **Sugestões de melhorias:** Propor mudanças no código para torná-lo mais legível, eficiente ou robusto.
*   **Responder a perguntas sobre o funcionamento:** Explicar como diferentes partes da aplicação interagem entre si.

Por favor, forneça o código ou faça perguntas específicas para que eu possa te ajudar a entender a aplicação.

2025-04-14 16:51:21,514 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



User: Resuma essa aplicação 
Assistant: Esta aplicação Streamlit oferece um chatbot inteligente que interage com o usuário por meio de texto e imagens, utilizando o modelo Gemini da Google. Aqui está um resumo das principais funcionalidades e componentes:

**Funcionalidades Principais:**

*   **Chatbot Multimodal:** Permite conversas textuais e análise de imagens. O usuário pode enviar mensagens de texto e, opcionalmente, anexar imagens para obter informações ou análises específicas sobre elas.
*   **Geração de Imagens:** Integra um recurso para gerar imagens a partir de prompts de texto usando o Gemini.
*   **Histórico de Conversa:** Mantém um histórico das mensagens trocadas, limitado a um número máximo para evitar sobrecarga.
*   **Integração com Área de Transferência:** Possibilita colar imagens diretamente da área de transferência, facilitando o envio de capturas de tela ou outras imagens copiadas.
*   **Upload de Imagens:** Permite o envio de imagens a partir do sistema de arquivos do usuário.
*   **Limpeza de Imagens:** Oferece botões para limpar imagens carregadas ou coladas, permitindo ao usuário começar com uma nova imagem sem recarregar a página.
*   **Contexto Dinâmico:** O chatbot inclui o código fonte da própria aplicação como contexto, permitindo perguntas sobre a estrutura e o funcionamento interno do aplicativo.
*   **Rate Limiter:** Implementa um sistema de rate limiting para controlar o número de requisições à API do Gemini, evitando sobrecarga e garantindo um uso justo dos recursos.

**Componentes e Arquitetura:**

*   **Streamlit:** A interface do usuário é construída com Streamlit, uma biblioteca Python para criação rápida de aplicativos web interativos.
*   **Gemini API:** Utiliza a API do Gemini, modelo de linguagem da Google, para gerar respostas inteligentes com base no texto e nas imagens fornecidas.
*   **Handlers:** A interação com o Gemini é encapsulada em um `GeminiHandler`, que abstrai a complexidade da API e gerencia o rate limiting.
*   **Configuração:** As configurações da aplicação, como caminhos de arquivos, limites de taxa e prompts, são centralizadas em um módulo de configuração (`config.py`).
*   **Logs:**  Utiliza um sistema de logs para registrar informações sobre o funcionamento da aplicação, erros e interações com a API.
*   **Estados de Sessão:** O `session_state` do Streamlit é utilizado para armazenar e gerenciar o estado da aplicação, como histórico de mensagens, imagens carregadas e status de processamento.
*   **Serviços:** A aplicação é organizada em serviços, como:
    *   `gpt_services.py`: Lida com a interação direta com a API do Gemini.
    *   `document_service.py`: Responsável pela criação e formatação de documentos (Word).
    *   `markdown_service.py`: Responsável pela criação de documentos Markdown.
    *   `image_processor.py`: Processa imagens, extrai informações e atualiza o histórico.
    *   `search_files.py`:  Lê o código fonte da aplicação para fornecer contexto ao chatbot.

**Fluxo de Operação:**

1.  **Entrada do Usuário:** O usuário interage com a aplicação através da interface Streamlit, enviando mensagens de texto e, opcionalmente, carregando ou colando imagens.
2.  **Processamento da Mensagem:** A aplicação verifica se há uma imagem anexada. Se houver, prepara a imagem para envio à API do Gemini.
3.  **Geração da Resposta:** O `GeminiHandler` envia o prompt de texto e a imagem (se houver) para a API do Gemini.
4.  **Exibição da Resposta:** A resposta gerada pelo Gemini é exibida na interface do usuário, juntamente com a imagem (se houver).
5.  **Histórico:** As mensagens do usuário e as respostas do Gemini são adicionadas ao histórico da conversa.

**Em resumo, esta aplicação oferece um chatbot versátil e inteligente, capaz de interagir com o usuário por meio de texto e imagens, utilizando o poder da API do Gemini e uma arquitetura bem organizada.**

User: leia a aplicação.
Assistant: Para ler a aplicação, preciso ter acesso ao código fonte. Como sou um modelo de linguagem, não consigo acessar diretamente arquivos no seu computador ou em um repositório online.

Para que eu possa "ler" a aplicação e te ajudar melhor, você pode:

1.  **Compartilhar o código:** Cole o conteúdo dos principais arquivos da aplicação aqui, um por vez. Comece pelos arquivos que você considera mais importantes para entender a lógica geral, como `app.py` (o arquivo principal do Streamlit), `gpt_services.py`, `config.py` e qualquer outro arquivo central na arquitetura.
2.  **Fazer perguntas específicas:** Se você tem dúvidas sobre partes específicas do código, como o funcionamento de uma função ou a finalidade de um determinado módulo, pode me perguntar diretamente.

Ao me fornecer o código ou fazer perguntas direcionadas, posso analisar a aplicação e te auxiliar com:

*   **Explicação do código:** Descrever o que cada parte do código faz.
*   **Identificação de problemas:** Ajudar a encontrar erros ou áreas que podem ser otimizadas.
*   **Sugestões de melhorias:** Propor mudanças no código para torná-lo mais legível, eficiente ou robusto.
*   **Responder a perguntas sobre o funcionamento:** Explicar como diferentes partes da aplicação interagem entre si.

Por favor, forneça o código ou faça perguntas específicas para que eu possa te ajudar a entender a aplicação.

User: import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py', '*.java')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

Esse código pode ser usado para ler arquivos python e java tbm ?


2025-04-14 16:51:28,660 [INFO] Resposta da IA (texto): Sim, o código que você forneceu pode ser usado para ler arquivos Python e Java, mas com algumas ressalvas e possíveis ajustes:

**Pontos Positivos:**

*   **Flexibilidade do `glob`:** A função `glob.glob(padrao_busca, recursive=True)` é poderosa e permite buscar arquivos recursivamente em subdiretórios.  O `recursive=True` é essencial para percorrer toda a estrutura de diretórios.
*   **Tratamento de erros:** O bloco `try...except` dentro do loop é importante para lidar com possíveis erros de leitura de arquivos (por exemplo, permissões insuficientes ou arquivos corrompidos) e evitar que a função pare de funcionar.  O logging dos erros é excelente para depuração.
*   **Caminho Relativo:** Usar `os.path.relpath` para obter o caminho relativo ao diretório `src_dir` é uma boa prática, pois torna o output mais legível e independente da localização absoluta do diretório.
*   **Logging:** A inclusão de logging para informar sobre o sucesso ou falha na leitura dos arquivos é crucial para monitorar o processo e identificar problemas.

**Possíveis Melhorias e Considerações:**

*   **Padrão de busca (`padrao_busca`):** Atualmente, o padrão de busca é  `os.path.join(src_dir.as_posix(), '**', '*.py', '*.java')`.  O problema é que `glob` não aceita multiplas extenções como essa `'*.py', '*.java'` juntas. Então o correto seria:

    ```python
    padrao_busca_py = os.path.join(src_dir.as_posix(), '**', '*.py')
    padrao_busca_java = os.path.join(src_dir.as_posix(), '**', '*.java')

    arquivos_py = glob.glob(padrao_busca_py, recursive=True)
    arquivos_java = glob.glob(padrao_busca_java, recursive=True)

    arquivos = arquivos_py + arquivos_java
    ```

*   **Manipulação do caminho do arquivo:** `src_dir.as_posix()`  já retorna o caminho no formato string (POSIX), então não precisa usar `os.path.join` passando para string novamente.

*   **Codificação:** Especificar `encoding='utf-8'` ao abrir os arquivos é uma excelente prática para garantir que caracteres especiais sejam lidos corretamente.

**Código Revisado:**

Aqui está uma versão revisada do código, incorporando as melhorias sugeridas:

```python
import os
import glob
from pathlib import Path
from config.config import Config
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python_e_java() -> str:
    """Lê todo o conteúdo de todos os arquivos .py e .java a partir de src/"""
    src_dir = Config.BASE_DIR

    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    # Busca arquivos .py e .java separadamente
    padrao_busca_py = os.path.join(src_dir, '**', '*.py')
    padrao_busca_java = os.path.join(src_dir, '**', '*.java')

    arquivos_py = glob.glob(padrao_busca_py, recursive=True)
    arquivos_java = glob.glob(padrao_busca_java, recursive=True)

    arquivos = arquivos_py + arquivos_java
    arquivos.sort() # Ordena a lista completa

    for arquivo in arquivos:
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")
            continue

    return conteudo_total
```

**Como usar:**

1.  **Certifique-se de ter a biblioteca `glob` instalada.** Ela geralmente vem com a instalação padrão do Python.
2.  **Substitua `Config.BASE_DIR` pelo caminho correto para o seu diretório "src".**  É importante que `Config.BASE_DIR` seja um objeto `Path` do módulo `pathlib`.
3.  **Execute a função `ler_todos_arquivos_python_e_java()`**.  A função retornará uma string contendo o conteúdo de todos os arquivos .py e .java encontrados em "src" e seus subdiretórios, separados por cabeçalhos contendo o caminho relativo de cada arquivo.

Essa versão revisada é mais robusta e segura, garantindo que o código explore os arquivos .java e .py da melhor forma possível.

2025-04-14 16:54:23,434 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\API_CommandAI\assets\20250414165423_clipboard_20250414165400.png, Prompt: Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



User: import os
import glob
from pathlib import Path
from config.config import Config
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python_e_java() -> str:
    """Lê todo o conteúdo de todos os arquivos .py e .java a partir de src/"""
    src_dir = Config.BASE_DIR

    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    # Busca arquivos .py e .java separadamente
    padrao_busca_py = os.path.join(src_dir, '**', '*.py')
    padrao_busca_java = os.path.join(src_dir, '**', '*.java')

    arquivos_py = glob.glob(padrao_busca_py, recursive=True)
    arquivos_java = glob.glob(padrao_busca_java, recursive=True)

    arquivos = arquivos_py + arquivos_java
    arquivos.sort() # Ordena a lista completa

    for arquivo in arquivos:
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")
            continue

    return conteudo_total
2025-04-14 16:54:29,910 [INFO] Resposta da IA (imagem): Okay, I see the error message:

**ImportError: cannot import name 'ler_todos_arquivos_python' from 'services.search_files'**

This error indicates that your Python code in `chat_streamlit.py` is trying to import a function or class named `ler_todos_arquivos_python` from the module `services.search_files`, but that name doesn't exist in that module.

Based on your prompt, the function `ler_todos_arquivos_python_e_java` exists but has a different name.

Here are the most likely causes and solutions:

1.  **Misspelled Name:** You might have misspelled the name when importing. Double-check that `ler_todos_arquivos_python` is exactly how it's defined in `services/search_files.py`. This is often the case with long or similar-sounding names.

2.  **Incorrect Function Name:** Your code now has a function named `ler_todos_arquivos_python_e_java` to read files of python and java extensions. You must to change the function call for this new function or rename the function to `ler_todos_arquivos_python`.

3.  **Function Not Defined or Misnamed:** The function `ler_todos_arquivos_python` might not be defined within the `services/search_files.py` file at all, or it might have a slightly different name there. Look at the source code of `services/search_files.py` to confirm the actual name of the function you want to import.

4.  **Incorrect Module Path:** You might be trying to import from the wrong module altogether. Double-check that the `services/search_files.py` is the correct location of the file containing the function.

5.  **Module Not Properly Saved:** If you've made changes to `services/search_files.py`, make sure you've saved the file after making the changes. Sometimes the interpreter loads an older version.

To Fix This Error:

1.  **Examine `services/search_files.py`:** Open the `services/search_files.py` file and carefully check the following:

    *   Confirm that a function or class named either `ler_todos_arquivos_python` or `ler_todos_arquivos_python_e_java` exists. If it doesn't exist, rename the function or class to be called `ler_todos_arquivos_python`
    *   Ensure there are no typos in the name's definition.
    *   Verify that the function is actually defined (not commented out or within an `if False` block).

2.  **Update the Import Statement:** Modify your import statement in `chat_streamlit.py` to match the actual name of the function.  For example:

    ```python
    from services.search_files import ler_todos_arquivos_python_e_java  # Use the correct name
    ```

3.  **Double-Check Paths:** Ensure that your Python environment is correctly finding the `services/search_files.py` file. If your project has a complex directory structure, you might need to adjust the Python path or use relative imports more carefully.

**Example (assuming `ler_todos_arquivos_python_e_java` is the correct name):**

In `chat_streamlit.py`, change:

```python
from services.search_files import ler_todos_arquivos_python
```

To:

```python
from services.search_files import ler_todos_arquivos_python_e_java
```

Then, in the code where you are using the function, make sure that you are calling it by its correct name.

By carefully checking the name and location of the function you're trying to import, you should be able to resolve this `ImportError`.

2025-04-14 16:54:56,793 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:54:56,794 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:54:56,795 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:54:56,797 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:54:56,798 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:54:56,799 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:54:56,801 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:54:56,802 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:54:56,804 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:54:56,805 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:54:56,807 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:54:56,808 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:54:56,810 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:54:56,814 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 16:54:56,822 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 16:54:56,829 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 16:54:56,833 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 16:54:56,838 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 16:54:56,842 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 16:54:56,849 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 16:54:56,854 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 16:54:56,859 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 16:54:56,866 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 16:54:56,871 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 16:54:56,876 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 16:54:56,881 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 16:54:56,887 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 16:54:56,891 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 16:54:56,897 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 16:54:56,902 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 16:54:56,908 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 16:54:56,913 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 16:54:56,917 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 16:54:56,923 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 16:54:56,928 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 16:54:56,933 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 16:55:07,745 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:55:07,746 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:55:07,747 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:55:07,749 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:55:07,750 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:55:07,752 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:55:07,754 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:55:07,756 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:55:07,757 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:55:07,759 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:55:07,760 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:55:07,761 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:55:07,763 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:55:07,764 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 16:55:07,766 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 16:55:07,767 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 16:55:07,769 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 16:55:07,770 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 16:55:07,773 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 16:55:07,775 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 16:55:07,777 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 16:55:07,778 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 16:55:07,779 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 16:55:07,780 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 16:55:07,781 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 16:55:07,782 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 16:55:07,783 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 16:55:07,784 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 16:55:07,785 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 16:55:07,786 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 16:55:07,788 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 16:55:07,790 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 16:55:07,792 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 16:55:07,793 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 16:55:07,794 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 16:55:07,795 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 16:55:07,925 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:55:07,926 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:55:07,928 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:55:07,929 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:55:07,931 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:55:07,932 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:55:07,934 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:55:07,936 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:55:07,937 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:55:07,939 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:55:07,940 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:55:07,942 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:55:07,943 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:55:07,944 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 16:55:07,945 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 16:55:07,946 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 16:55:07,948 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 16:55:07,949 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 16:55:07,950 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 16:55:07,952 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 16:55:07,964 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 16:55:07,966 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 16:55:07,969 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 16:55:07,971 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 16:55:07,973 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 16:55:07,975 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 16:55:07,976 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 16:55:07,978 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 16:55:07,980 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 16:55:07,981 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 16:55:07,982 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 16:55:07,983 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 16:55:07,985 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 16:55:07,986 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 16:55:07,987 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 16:55:07,988 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 16:55:08,000 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\API_CommandAI\assets\20250414165507_clipboard_20250414165400.png, Prompt: Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py e .java a partir de src/"""
    src_dir = Config.BASE_DIR

    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    # Busca arquivos .py e .java separadamente
    padrao_busca_py = os.path.join(src_dir, '**', '*.py')
    padrao_busca_java = os.path.join(src_dir, '**', '*.java')

    arquivos_py = glob.glob(padrao_busca_py, recursive=True)
    arquivos_java = glob.glob(padrao_busca_java, recursive=True)

    arquivos = arquivos_py + arquivos_java
    arquivos.sort() # Ordena a lista completa

    for arquivo in arquivos:
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# main\java\com\commandAI\commandAI\CommandAiApplication.java

package com.commandAI.commandAI;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class CommandAiApplication {

	public static void main(String[] args) {
		SpringApplication.run(CommandAiApplication.class, args);
	}

}


# main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java

package com.commandAI.commandAI.modules.AIComunication.controller;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import com.commandAI.commandAI.modules.AIComunication.model.dto.GTPCommunicationRequestDTO;
import com.commandAI.commandAI.modules.AIComunication.service.IGPTCommunicationService;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.RequiredArgsConstructor;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.Arrays;
import java.util.List;

@RequiredArgsConstructor
@RestController
@RequestMapping("/api/question_answers")
public class GPTCommunicationController {

    private final IGPTCommunicationService service;

    @PostMapping("/save")
    public ResponseEntity<GPTCommunication> saveCommunication(@RequestBody GTPCommunicationRequestDTO dto) {
        try {
            GPTCommunication savedCommunication = service.saveCommunication(dto);
            return new ResponseEntity<>(savedCommunication, HttpStatus.CREATED);
        } catch (Exception e) {
            // Log error and return an appropriate response
            System.err.println("Erro ao salvar comunicação: " + e.getMessage());
            return new ResponseEntity<>(HttpStatus.INTERNAL_SERVER_ERROR);
        }
    }

    @PostMapping("/similar")
    public ResponseEntity<List<GTPCommunicationRequestDTO>> getSimilarCommunications(@RequestBody JsonNode requestBody) {
        try {
            // Validar se é um array
            if (!requestBody.isArray()) {
                throw new IllegalArgumentException("Esperado um array");
            }

            // Converter JsonNode para array de float
            float[] embedding = new ObjectMapper().convertValue(requestBody, float[].class);

            // Log para verificar o embedding recebido
            System.out.println("Embedding recebido: " + Arrays.toString(embedding));

            // Chamar o serviço com o array de float
            List<GTPCommunicationRequestDTO> similarCommunications = service.findSimilarCommunications(embedding);
            return ResponseEntity.ok(similarCommunications);
        } catch (IllegalArgumentException e) {
            System.err.println("Erro de validação: " + e.getMessage());
            return ResponseEntity.status(HttpStatus.BAD_REQUEST).body(null);
        } catch (Exception e) {
            // Log error and return an appropriate response
            System.err.println("Erro ao processar a solicitação: " + e.getMessage());
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(null);
        }
    }
}


# main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java

package com.commandAI.commandAI.modules.AIComunication.model;

import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import jakarta.persistence.*;
import lombok.Data;

import java.time.LocalDateTime;
@Data
@Entity
@JsonIgnoreProperties(ignoreUnknown = true)
@Table(name = "gpt_communication")
public class GPTCommunication {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(name = "question", nullable = false, columnDefinition = "TEXT")
    private String question;

    @Column(name = "answer", nullable = false, columnDefinition = "TEXT")
    private String answer;

    @Column(name = "question_embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] questionEmbedding;

    @Column(name = "answer_embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] answerEmbedding;

    @Column(name = "created_at", nullable = false)
    private LocalDateTime createdAt;

}

# main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java

package com.commandAI.commandAI.modules.AIComunication.model.dto;

import com.fasterxml.jackson.annotation.JsonProperty;

public record GTPCommunicationRequestDTO(
        String question,
        float[] questionEmbedding,
        String answer,
        float[] answerEmbedding
) {}



# main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java

package com.commandAI.commandAI.modules.AIComunication.repository;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

@Repository
public interface IGPTCommunicationRepository extends JpaRepository<GPTCommunication, Long> {

}

# main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java

package com.commandAI.commandAI.modules.AIComunication.service;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import com.commandAI.commandAI.modules.AIComunication.model.dto.GTPCommunicationRequestDTO;

import java.util.List;

public interface IGPTCommunicationService {
    GPTCommunication saveCommunication(GTPCommunicationRequestDTO dto);
    List<GTPCommunicationRequestDTO> findSimilarCommunications(float[] embedding);
}

# main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java

package com.commandAI.commandAI.modules.AIComunication.service.impl;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import com.commandAI.commandAI.modules.AIComunication.model.dto.GTPCommunicationRequestDTO;
import com.commandAI.commandAI.modules.AIComunication.repository.IGPTCommunicationRepository;
import com.commandAI.commandAI.modules.AIComunication.service.IGPTCommunicationService;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;

import java.time.LocalDateTime;
import java.util.Arrays;
import java.util.List;
import java.util.stream.Collectors;

@RequiredArgsConstructor
@Service
public class GPTCommunicationServiceImpl implements IGPTCommunicationService {

    private final IGPTCommunicationRepository repository;

    @Override
    public GPTCommunication saveCommunication(GTPCommunicationRequestDTO dto) {
        GPTCommunication communication = new GPTCommunication();
        communication.setQuestion(dto.question());
        communication.setQuestionEmbedding(dto.questionEmbedding());
        communication.setAnswer(dto.answer());
        communication.setAnswerEmbedding(dto.answerEmbedding());
        communication.setCreatedAt(LocalDateTime.now());
        return repository.save(communication);
    }

    @Override
    public List<GTPCommunicationRequestDTO> findSimilarCommunications(float[] embedding) {
        List<GPTCommunication> allCommunications = repository.findAll();

        return allCommunications.stream()
                .filter(comm -> isSimilar(embedding, comm.getQuestionEmbedding()))
                .map(comm -> new GTPCommunicationRequestDTO(
                        comm.getQuestion(),
                        comm.getQuestionEmbedding(),
                        comm.getAnswer(),
                        comm.getAnswerEmbedding()))
                .collect(Collectors.toList());
    }

    private boolean isSimilar(float[] embedding1, float[] embedding2) {
        // Verificar se os embeddings têm o mesmo tamanho
        if (embedding1.length != embedding2.length) {
            System.err.println("Os embeddings têm tamanhos diferentes.");
            return false;
        }

        System.out.println("Embedding1: " + Arrays.toString(embedding1));
        System.out.println("Embedding2: " + Arrays.toString(embedding2));

        double dotProduct = 0.0;
        double normA = 0.0;
        double normB = 0.0;
        for (int i = 0; i < embedding1.length; i++) {
            dotProduct += embedding1[i] * embedding2[i];
            normA += Math.pow(embedding1[i], 2);
            normB += Math.pow(embedding2[i], 2);
        }
        return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB)) > 0.8; // Ajuste o limiar conforme necessário
    }
}


# main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java

package com.commandAI.commandAI.modules.context.controller;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.context.service.IContextService;
import com.commandAI.commandAI.modules.context.model.context.Context;
import lombok.RequiredArgsConstructor;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.validation.annotation.Validated;
import org.springframework.web.bind.annotation.*;

import java.util.List;
import java.util.Map;

@RequiredArgsConstructor
@RestController
@Validated
@ResponseBody
@RequestMapping("/api/contexts")
public class ContextController {

    @Autowired
    private final IContextService contextService;

    @PostMapping("/save")
    public ResponseEntity<Map<String, Object>> saveContext(@RequestBody ContextDTO data) {
        Context newContext = contextService.saveContext(data);
        Map<String, Object> response = Map.of("result", newContext);
        return ResponseEntity.status(HttpStatus.CREATED).body(response);
    }


    @GetMapping("/last")
    public ResponseEntity<ContextDTO> getLastContext() {
        ContextDTO lastContext = contextService.getLastContext();
        return ResponseEntity.ok(lastContext);
    }

    @GetMapping("/all")
    public ResponseEntity<Map<String, Object>> getAllContexts() {
        List<Context> contexts = contextService.findAllContext();
        return ResponseEntity.ok(Map.of("contexts", contexts));
    }
}


# main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java

package com.commandAI.commandAI.modules.context.controller.hendler;

import com.commandAI.commandAI.modules.context.service.validation.ContextNotFoundException;
import lombok.Data;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.MissingServletRequestParameterException;
import org.springframework.web.bind.annotation.ControllerAdvice;
import org.springframework.web.bind.annotation.ExceptionHandler;
import org.springframework.web.context.request.WebRequest;

import java.time.LocalDateTime;
import java.util.HashMap;
import java.util.Map;

@Data
@ControllerAdvice
public class GlobalExceptionHandler {

    @ExceptionHandler(ContextNotFoundException.class)
    public ResponseEntity<Object> handleContextNotFoundException(ContextNotFoundException ex, WebRequest request) {
        Map<String, Object> body = new HashMap<>();
        body.put("timestamp", LocalDateTime.now());
        body.put("message", ex.getMessage());

        return new ResponseEntity<>(body, HttpStatus.NOT_FOUND);
    }
    @ExceptionHandler(MissingServletRequestParameterException.class)
    public ResponseEntity<Object> handleMissingServletRequestParameterException(MissingServletRequestParameterException ex, WebRequest request) {
        Map<String, Object> body = new HashMap<>();
        body.put("timestamp", LocalDateTime.now());
        body.put("message", "Parâmetro de requisição ausente: " + ex.getParameterName());

        return new ResponseEntity<>(body, HttpStatus.BAD_REQUEST);
    }

    // Outros handlers de exceção podem ser adicionados aqui
}

# main\java\com\commandAI\commandAI\modules\context\model\context\Context.java

    package com.commandAI.commandAI.modules.context.model.context;
    import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
    import jakarta.persistence.*;
    import lombok.Data;

    import java.time.LocalDateTime;
    @Data
    @Table(name = "tb_context")
    @Entity
    @JsonIgnoreProperties(ignoreUnknown = true)
    public class Context {

        @Id
        @GeneratedValue(strategy = GenerationType.IDENTITY)
        private Long id;

        @Column(name = "context", nullable = false, columnDefinition = "TEXT")
        private String context;

        @Column(name = "created_at", nullable = false)
        private LocalDateTime createdAt;

        @PrePersist
        protected void onCreate() {
            createdAt = LocalDateTime.now();
        }
    }



# main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java

package com.commandAI.commandAI.modules.context.model.dto;

import com.commandAI.commandAI.modules.context.model.context.Context;
import java.time.LocalDateTime;
public record ContextDTO(
        Long id,
        String context,
        LocalDateTime createdAt
) {
    public static ContextDTO fromEntity(Context context) {
        return new ContextDTO(context.getId(), context.getContext(), context.getCreatedAt());
    }
}


# main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java

package com.commandAI.commandAI.modules.context.repository;

import com.commandAI.commandAI.modules.context.model.context.Context;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import java.util.Optional;

@Repository
public interface IContextRepository extends JpaRepository<Context, Long> {
    Optional<Context> findTopByOrderByCreatedAtDesc();
}


# main\java\com\commandAI\commandAI\modules\context\service\IContextService.java

package com.commandAI.commandAI.modules.context.service;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;

import java.util.List;

public interface IContextService {
    Context saveContext(ContextDTO data);
    ContextDTO getLastContext();
    List<Context> findAllContext();
}


# main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java

package com.commandAI.commandAI.modules.context.service.impl;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.repository.IContextRepository;
import com.commandAI.commandAI.modules.context.service.IContextService;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.util.List;

@RequiredArgsConstructor
@Service
public class ContextServiceImpl implements IContextService {

    private final IContextRepository contextRepository;

    @Override
    @Transactional
    public Context saveContext(ContextDTO data) {
        Context context = new Context();
        context.setContext(data.context());
        context = contextRepository.save(context);
        return context;
    }

    @Override
    @Transactional
    public ContextDTO getLastContext() {
        Context lastContext = contextRepository.findTopByOrderByCreatedAtDesc()
                .orElseThrow(() -> new RuntimeException("No contexts found"));
        return ContextDTO.fromEntity(lastContext);
    }
    @Override
    @Transactional
    public List<Context> findAllContext() {
        return contextRepository.findAll();
    }
}


# main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java

package com.commandAI.commandAI.modules.context.service.validation;

public class ContextNotFoundException extends RuntimeException {
    public ContextNotFoundException(String message) {
        super(message);
    }
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java

package com.commandAI.commandAI.modules.embeddingTranscription.controller;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.service.IServiceTranscriptionEmbedded;
import lombok.RequiredArgsConstructor;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.List;

@RestController
@RequestMapping("/api/meetings")    
@RequiredArgsConstructor
public class TranscriptionController {

    private final IServiceTranscriptionEmbedded service;

    @PostMapping("/transcriptions")
    public ResponseEntity<Void> addTranscription(@RequestBody TranscriptionEmbeddedDTO transcriptionDTO) {
        service.saveTranscription(transcriptionDTO.transcriptionText(), transcriptionDTO.embedding());
        return ResponseEntity.status(HttpStatus.CREATED).build();
    }

    @GetMapping("/transcriptions")
    public List<TranscriptionEmbeddedDTO> getAllTranscriptions() {
        return service.getAllTranscriptions();
    }

    @GetMapping("/transcriptions/{id}")
    public ResponseEntity<TranscriptionEmbeddedDTO> getTranscriptionById(@PathVariable Long id) {
        TranscriptionEmbeddedDTO transcription = service.getTranscriptionById(id);
        if (transcription == null) {
            return ResponseEntity.status(HttpStatus.NOT_FOUND).build();
        }
        return ResponseEntity.ok(transcription);
    }

    @GetMapping("/transcriptions/similar")
    public List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(@RequestParam float[] queryEmbedding) {
        return service.findSimilarTranscriptions(queryEmbedding);
    }
    @GetMapping("/last")
    public ResponseEntity<TranscriptionEmbeddedDTO> getLastTranscription() {
        TranscriptionEmbeddedDTO lastTranscription = service.getLastTranscription();
        return ResponseEntity.ok(lastTranscription);
    }
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java

package com.commandAI.commandAI.modules.embeddingTranscription.model;

import jakarta.persistence.*;
import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import lombok.Data;

import java.time.LocalDateTime;

@Data
@JsonIgnoreProperties(ignoreUnknown = true)
@Entity
@Table(name = "transcriptionEmbedded")
public class TranscriptionEmbedded {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    @Column(name = "meeting_id", nullable = false)
    private Long meetingId;

    @Column(name = "transcription_text", nullable = false, columnDefinition = "TEXT")
    private String transcriptionText;

    @Column(name = "embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] embedding;

    @Column(name = "created_at", nullable = false)
    private LocalDateTime createdAt;

    @PrePersist
    protected void onCreate() {
        createdAt = LocalDateTime.now();
    }

}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java

package com.commandAI.commandAI.modules.embeddingTranscription.model.dto;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;

import java.time.LocalDateTime;

public record TranscriptionEmbeddedDTO(
        Long meetingId,
        String transcriptionText,
        float[] embedding,
        LocalDateTime createdAt
) {
    public static TranscriptionEmbeddedDTO fromEntity(TranscriptionEmbedded transcriptionEmbedded) {
        return new TranscriptionEmbeddedDTO(transcriptionEmbedded.getMeetingId(), transcriptionEmbedded.getTranscriptionText(), transcriptionEmbedded.getEmbedding(), transcriptionEmbedded.getCreatedAt());
    }
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java

package com.commandAI.commandAI.modules.embeddingTranscription.repository;

import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import java.util.Optional;

@Repository
public interface ITranscriptionEmbeddedRepository extends JpaRepository<TranscriptionEmbedded, Long> {
    Optional<TranscriptionEmbedded> findTopByOrderByCreatedAtDesc();

}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java

package com.commandAI.commandAI.modules.embeddingTranscription.service;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;

import java.util.List;

public interface IServiceTranscriptionEmbedded {
    void saveTranscription(String transcriptionText, float[] embedding);
    List<TranscriptionEmbeddedDTO> getAllTranscriptions();
    TranscriptionEmbeddedDTO getTranscriptionById(Long id);
    List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(float[] queryEmbedding);
    TranscriptionEmbeddedDTO getLastTranscription();
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java

package com.commandAI.commandAI.modules.embeddingTranscription.service.impl;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.repository.ITranscriptionEmbeddedRepository;
import com.commandAI.commandAI.modules.embeddingTranscription.service.IServiceTranscriptionEmbedded;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.util.List;
import java.util.stream.Collectors;

@RequiredArgsConstructor
@Service
public class ServiceTranscriptionEmbeddedImpl implements IServiceTranscriptionEmbedded {

    private final ITranscriptionEmbeddedRepository transcriptionEmbeddedRepository;

    @Override
    @Transactional
    public void saveTranscription(String transcriptionText, float[] embedding) {
        TranscriptionEmbedded transcription = new TranscriptionEmbedded();
        transcription.setTranscriptionText(transcriptionText);
        transcription.setEmbedding(embedding);
        System.out.println(transcription);
        transcriptionEmbeddedRepository.save(transcription);
    }

    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> getAllTranscriptions() {
        return transcriptionEmbeddedRepository.findAll().stream()
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }

    @Override
    @Transactional
    public TranscriptionEmbeddedDTO getTranscriptionById(Long id) {
        TranscriptionEmbedded transcription = transcriptionEmbeddedRepository.findById(id).orElse(null);
        if (transcription == null) {
            return null;
        }
        return new TranscriptionEmbeddedDTO(
                transcription.getMeetingId(),
                transcription.getTranscriptionText(),
                transcription.getEmbedding(),
                transcription.getCreatedAt());
    }

    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(float[] queryEmbedding) {
        List<TranscriptionEmbedded> allTranscriptions = transcriptionEmbeddedRepository.findAll();
        return allTranscriptions.stream()
                .sorted((t1, t2) -> Double.compare(
                        cosineSimilarity(queryEmbedding, t2.getEmbedding()),
                        cosineSimilarity(queryEmbedding, t1.getEmbedding())))
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }
    @Override
    @Transactional
    public TranscriptionEmbeddedDTO getLastTranscription() {
        TranscriptionEmbedded lastTranscription = transcriptionEmbeddedRepository.findTopByOrderByCreatedAtDesc()
                .orElseThrow(() -> new RuntimeException("No contexts found"));
        return TranscriptionEmbeddedDTO.fromEntity(lastTranscription);
    }
    private double cosineSimilarity(float[] vec1, float[] vec2) {
        double dotProduct = 0.0;
        double normA = 0.0;
        double normB = 0.0;
        for (int i = 0; i < vec1.length; i++) {
            dotProduct += vec1[i] * vec2[i];
            normA += Math.pow(vec1[i], 2);
            normB += Math.pow(vec2[i], 2);
        }
        return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
    }
}


# test\java\com\commandAI\commandAI\CommandAiApplicationTests.java

package com.commandAI.commandAI;

import org.junit.jupiter.api.Test;
import org.springframework.boot.test.context.SpringBootTest;

@SpringBootTest
class CommandAiApplicationTests {

	@Test
	void contextLoads() {
	}
}


# test\java\com\commandAI\commandAI\modelsTest\ContextTest.java

package com.commandAI.commandAI.modelsTest;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import com.commandAI.commandAI.modules.context.model.context.Context;
import java.time.LocalDateTime;

import static org.junit.jupiter.api.Assertions.*;

public class ContextTest {

    private Context context;

    @BeforeEach
    public void setUp() {
        context = new Context();
    }

    @Test
    public void testSetAndGetId() {
        Long id = 1L;
        context.setId(id);
        assertEquals(id, context.getId());
    }

    @Test
    public void testSetAndGetContext() {
        String testContext = "Sample context";
        context.setContext(testContext);
        assertEquals(testContext, context.getContext());
    }

    @Test
    public void testSetAndGetCreatedAt() {
        LocalDateTime now = LocalDateTime.now();
        context.setCreatedAt(now);
        assertEquals(now, context.getCreatedAt());
    }
}


User: import os
import glob
from pathlib import Path
from config.config import Config
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python_e_java() -> str:
    """Lê todo o conteúdo de todos os arquivos .py e .java a partir de src/"""
    src_dir = Config.BASE_DIR

    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    # Busca arquivos .py e .java separadamente
    padrao_busca_py = os.path.join(src_dir, '**', '*.py')
    padrao_busca_java = os.path.join(src_dir, '**', '*.java')

    arquivos_py = glob.glob(padrao_busca_py, recursive=True)
    arquivos_java = glob.glob(padrao_busca_java, recursive=True)

    arquivos = arquivos_py + arquivos_java
    arquivos.sort() # Ordena a lista completa

    for arquivo in arquivos:
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")
            continue

    return conteudo_total
Assistant: Okay, I see the error message:

**ImportError: cannot import name 'ler_todos_arquivos_python' from 'services.search_files'**

This error indicates that your Python code in `chat_streamlit.py` is trying to import a function or class named `ler_todos_arquivos_python` from the module `services.search_files`, but that name doesn't exist in that module.

Based on your prompt, the function `ler_todos_arquivos_python_e_java` exists but has a different name.

Here are the most likely causes and solutions:

1.  **Misspelled Name:** You might have misspelled the name when importing. Double-check that `ler_todos_arquivos_python` is exactly how it's defined in `services/search_files.py`. This is often the case with long or similar-sounding names.

2.  **Incorrect Function Name:** Your code now has a function named `ler_todos_arquivos_python_e_java` to read files of python and java extensions. You must to change the function call for this new function or rename the function to `ler_todos_arquivos_python`.

3.  **Function Not Defined or Misnamed:** The function `ler_todos_arquivos_python` might not be defined within the `services/search_files.py` file at all, or it might have a slightly different name there. Look at the source code of `services/search_files.py` to confirm the actual name of the function you want to import.

4.  **Incorrect Module Path:** You might be trying to import from the wrong module altogether. Double-check that the `services/search_files.py` is the correct location of the file containing the function.

5.  **Module Not Properly Saved:** If you've made changes to `services/search_files.py`, make sure you've saved the file after making the changes. Sometimes the interpreter loads an older version.

To Fix This Error:

1.  **Examine `services/search_files.py`:** Open the `services/search_files.py` file and carefully check the following:

    *   Confirm that a function or class named either `ler_todos_arquivos_python` or `ler_todos_arquivos_python_e_java` exists. If it doesn't exist, rename the function or class to be called `ler_todos_arquivos_python`
    *   Ensure there are no typos in the name's definition.
    *   Verify that the function is actually defined (not commented out or within an `if False` block).

2.  **Update the Import Statement:** Modify your import statement in `chat_streamlit.py` to match the actual name of the function.  For example:

    ```python
    from services.search_files import ler_todos_arquivos_python_e_java  # Use the correct name
    ```

3.  **Double-Check Paths:** Ensure that your Python environment is correctly finding the `services/search_files.py` file. If your project has a complex directory structure, you might need to adjust the Python path or use relative imports more carefully.

**Example (assuming `ler_todos_arquivos_python_e_java` is the correct name):**

In `chat_streamlit.py`, change:

```python
from services.search_files import ler_todos_arquivos_python
```

To:

```python
from services.search_files import ler_todos_arquivos_python_e_java
```

Then, in the code where you are using the function, make sure that you are calling it by its correct name.

By carefully checking the name and location of the function you're trying to import, you should be able to resolve this `ImportError`.

User: agora leu ?
2025-04-14 16:55:10,831 [INFO] Resposta da IA (imagem): Assistant: After making the changes discussed and re-running your Streamlit app, if the `ImportError` is gone and the code runs without that specific error, then yes, the code "read" the function successfully!

However, "reading" successfully just means that the import worked and the function is accessible. It *doesn't* guarantee that the function is working correctly or producing the intended results. You still need to check:

1.  **Functionality:** Is the `ler_todos_arquivos_python_e_java` function doing what you expect (reading all the Python and Java files and returning their combined content)? You may want to add print statements inside the function to confirm this.

2.  **Downstream Code:** Does the rest of your `chat_streamlit.py` code correctly handle the output of `ler_todos_arquivos_python_e_java`?

In short, successful import is the *first* step, but thorough testing is still required to ensure everything works as desired.

2025-04-14 16:55:10,962 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:55:10,964 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:55:10,965 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:55:10,967 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:55:10,968 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:55:10,970 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:55:10,971 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:55:10,973 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:55:10,974 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:55:10,976 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:55:10,977 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:55:10,978 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:55:10,979 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:55:10,980 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 16:55:10,981 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 16:55:10,982 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 16:55:10,983 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 16:55:10,984 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 16:55:10,985 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 16:55:10,986 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 16:55:10,988 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 16:55:10,989 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 16:55:10,990 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 16:55:10,991 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 16:55:10,992 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 16:55:10,993 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 16:55:10,994 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 16:55:10,995 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 16:55:10,997 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 16:55:10,998 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 16:55:10,999 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 16:55:11,000 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 16:55:11,001 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 16:55:11,002 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 16:55:11,003 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 16:55:11,005 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 16:56:34,224 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:56:34,225 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:56:34,226 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:56:34,227 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:56:34,228 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:56:34,229 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:56:34,231 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:56:34,233 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:56:34,234 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:56:34,235 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:56:34,237 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:56:34,238 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:56:34,239 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:56:34,240 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 16:56:34,241 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 16:56:34,243 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 16:56:34,244 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 16:56:34,245 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 16:56:34,247 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 16:56:34,248 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 16:56:34,249 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 16:56:34,250 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 16:56:34,252 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 16:56:34,253 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 16:56:34,254 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 16:56:34,256 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 16:56:34,257 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 16:56:34,258 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 16:56:34,259 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 16:56:34,261 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 16:56:34,262 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 16:56:34,264 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 16:56:34,265 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 16:56:34,267 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 16:56:34,268 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 16:56:34,270 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 16:56:44,600 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:56:44,601 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:56:44,602 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:56:44,604 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:56:44,605 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:56:44,606 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:56:44,608 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:56:44,609 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:56:44,611 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:56:44,612 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:56:44,613 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:56:44,615 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:56:44,616 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:56:44,618 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 16:56:44,620 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 16:56:44,623 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 16:56:44,624 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 16:56:44,626 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 16:56:44,628 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 16:56:44,629 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 16:56:44,632 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 16:56:44,635 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 16:56:44,637 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 16:56:44,638 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 16:56:44,639 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 16:56:44,641 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 16:56:44,642 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 16:56:44,644 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 16:56:44,645 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 16:56:44,647 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 16:56:44,649 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 16:56:44,651 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 16:56:44,653 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 16:56:44,654 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 16:56:44,655 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 16:56:44,656 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 16:56:44,780 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:56:44,781 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:56:44,783 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:56:44,785 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:56:44,787 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:56:44,789 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:56:44,790 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:56:44,791 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:56:44,792 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:56:44,794 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:56:44,795 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:56:44,796 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:56:44,798 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:56:44,799 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 16:56:44,801 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 16:56:44,803 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 16:56:44,804 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 16:56:44,806 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 16:56:44,808 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 16:56:44,809 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 16:56:44,810 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 16:56:44,812 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 16:56:44,813 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 16:56:44,814 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 16:56:44,817 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 16:56:44,818 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 16:56:44,820 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 16:56:44,821 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 16:56:44,822 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 16:56:44,823 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 16:56:44,824 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 16:56:44,825 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 16:56:44,826 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 16:56:44,828 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 16:56:44,829 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 16:56:44,830 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 16:56:44,834 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python_e_java

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python_e_java()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python_e_java() -> str:
    """Lê todo o conteúdo de todos os arquivos .py e .java a partir de src/"""
    src_dir = Config.BASE_DIR

    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    # Busca arquivos .py e .java separadamente
    padrao_busca_py = os.path.join(src_dir, '**', '*.py')
    padrao_busca_java = os.path.join(src_dir, '**', '*.java')

    arquivos_py = glob.glob(padrao_busca_py, recursive=True)
    arquivos_java = glob.glob(padrao_busca_java, recursive=True)

    arquivos = arquivos_py + arquivos_java
    arquivos.sort() # Ordena a lista completa

    for arquivo in arquivos:
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# main\java\com\commandAI\commandAI\CommandAiApplication.java

package com.commandAI.commandAI;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class CommandAiApplication {

	public static void main(String[] args) {
		SpringApplication.run(CommandAiApplication.class, args);
	}

}


# main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java

package com.commandAI.commandAI.modules.AIComunication.controller;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import com.commandAI.commandAI.modules.AIComunication.model.dto.GTPCommunicationRequestDTO;
import com.commandAI.commandAI.modules.AIComunication.service.IGPTCommunicationService;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.RequiredArgsConstructor;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.Arrays;
import java.util.List;

@RequiredArgsConstructor
@RestController
@RequestMapping("/api/question_answers")
public class GPTCommunicationController {

    private final IGPTCommunicationService service;

    @PostMapping("/save")
    public ResponseEntity<GPTCommunication> saveCommunication(@RequestBody GTPCommunicationRequestDTO dto) {
        try {
            GPTCommunication savedCommunication = service.saveCommunication(dto);
            return new ResponseEntity<>(savedCommunication, HttpStatus.CREATED);
        } catch (Exception e) {
            // Log error and return an appropriate response
            System.err.println("Erro ao salvar comunicação: " + e.getMessage());
            return new ResponseEntity<>(HttpStatus.INTERNAL_SERVER_ERROR);
        }
    }

    @PostMapping("/similar")
    public ResponseEntity<List<GTPCommunicationRequestDTO>> getSimilarCommunications(@RequestBody JsonNode requestBody) {
        try {
            // Validar se é um array
            if (!requestBody.isArray()) {
                throw new IllegalArgumentException("Esperado um array");
            }

            // Converter JsonNode para array de float
            float[] embedding = new ObjectMapper().convertValue(requestBody, float[].class);

            // Log para verificar o embedding recebido
            System.out.println("Embedding recebido: " + Arrays.toString(embedding));

            // Chamar o serviço com o array de float
            List<GTPCommunicationRequestDTO> similarCommunications = service.findSimilarCommunications(embedding);
            return ResponseEntity.ok(similarCommunications);
        } catch (IllegalArgumentException e) {
            System.err.println("Erro de validação: " + e.getMessage());
            return ResponseEntity.status(HttpStatus.BAD_REQUEST).body(null);
        } catch (Exception e) {
            // Log error and return an appropriate response
            System.err.println("Erro ao processar a solicitação: " + e.getMessage());
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(null);
        }
    }
}


# main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java

package com.commandAI.commandAI.modules.AIComunication.model;

import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import jakarta.persistence.*;
import lombok.Data;

import java.time.LocalDateTime;
@Data
@Entity
@JsonIgnoreProperties(ignoreUnknown = true)
@Table(name = "gpt_communication")
public class GPTCommunication {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(name = "question", nullable = false, columnDefinition = "TEXT")
    private String question;

    @Column(name = "answer", nullable = false, columnDefinition = "TEXT")
    private String answer;

    @Column(name = "question_embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] questionEmbedding;

    @Column(name = "answer_embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] answerEmbedding;

    @Column(name = "created_at", nullable = false)
    private LocalDateTime createdAt;

}

# main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java

package com.commandAI.commandAI.modules.AIComunication.model.dto;

import com.fasterxml.jackson.annotation.JsonProperty;

public record GTPCommunicationRequestDTO(
        String question,
        float[] questionEmbedding,
        String answer,
        float[] answerEmbedding
) {}



# main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java

package com.commandAI.commandAI.modules.AIComunication.repository;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

@Repository
public interface IGPTCommunicationRepository extends JpaRepository<GPTCommunication, Long> {

}

# main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java

package com.commandAI.commandAI.modules.AIComunication.service;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import com.commandAI.commandAI.modules.AIComunication.model.dto.GTPCommunicationRequestDTO;

import java.util.List;

public interface IGPTCommunicationService {
    GPTCommunication saveCommunication(GTPCommunicationRequestDTO dto);
    List<GTPCommunicationRequestDTO> findSimilarCommunications(float[] embedding);
}

# main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java

package com.commandAI.commandAI.modules.AIComunication.service.impl;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import com.commandAI.commandAI.modules.AIComunication.model.dto.GTPCommunicationRequestDTO;
import com.commandAI.commandAI.modules.AIComunication.repository.IGPTCommunicationRepository;
import com.commandAI.commandAI.modules.AIComunication.service.IGPTCommunicationService;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;

import java.time.LocalDateTime;
import java.util.Arrays;
import java.util.List;
import java.util.stream.Collectors;

@RequiredArgsConstructor
@Service
public class GPTCommunicationServiceImpl implements IGPTCommunicationService {

    private final IGPTCommunicationRepository repository;

    @Override
    public GPTCommunication saveCommunication(GTPCommunicationRequestDTO dto) {
        GPTCommunication communication = new GPTCommunication();
        communication.setQuestion(dto.question());
        communication.setQuestionEmbedding(dto.questionEmbedding());
        communication.setAnswer(dto.answer());
        communication.setAnswerEmbedding(dto.answerEmbedding());
        communication.setCreatedAt(LocalDateTime.now());
        return repository.save(communication);
    }

    @Override
    public List<GTPCommunicationRequestDTO> findSimilarCommunications(float[] embedding) {
        List<GPTCommunication> allCommunications = repository.findAll();

        return allCommunications.stream()
                .filter(comm -> isSimilar(embedding, comm.getQuestionEmbedding()))
                .map(comm -> new GTPCommunicationRequestDTO(
                        comm.getQuestion(),
                        comm.getQuestionEmbedding(),
                        comm.getAnswer(),
                        comm.getAnswerEmbedding()))
                .collect(Collectors.toList());
    }

    private boolean isSimilar(float[] embedding1, float[] embedding2) {
        // Verificar se os embeddings têm o mesmo tamanho
        if (embedding1.length != embedding2.length) {
            System.err.println("Os embeddings têm tamanhos diferentes.");
            return false;
        }

        System.out.println("Embedding1: " + Arrays.toString(embedding1));
        System.out.println("Embedding2: " + Arrays.toString(embedding2));

        double dotProduct = 0.0;
        double normA = 0.0;
        double normB = 0.0;
        for (int i = 0; i < embedding1.length; i++) {
            dotProduct += embedding1[i] * embedding2[i];
            normA += Math.pow(embedding1[i], 2);
            normB += Math.pow(embedding2[i], 2);
        }
        return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB)) > 0.8; // Ajuste o limiar conforme necessário
    }
}


# main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java

package com.commandAI.commandAI.modules.context.controller;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.context.service.IContextService;
import com.commandAI.commandAI.modules.context.model.context.Context;
import lombok.RequiredArgsConstructor;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.validation.annotation.Validated;
import org.springframework.web.bind.annotation.*;

import java.util.List;
import java.util.Map;

@RequiredArgsConstructor
@RestController
@Validated
@ResponseBody
@RequestMapping("/api/contexts")
public class ContextController {

    @Autowired
    private final IContextService contextService;

    @PostMapping("/save")
    public ResponseEntity<Map<String, Object>> saveContext(@RequestBody ContextDTO data) {
        Context newContext = contextService.saveContext(data);
        Map<String, Object> response = Map.of("result", newContext);
        return ResponseEntity.status(HttpStatus.CREATED).body(response);
    }


    @GetMapping("/last")
    public ResponseEntity<ContextDTO> getLastContext() {
        ContextDTO lastContext = contextService.getLastContext();
        return ResponseEntity.ok(lastContext);
    }

    @GetMapping("/all")
    public ResponseEntity<Map<String, Object>> getAllContexts() {
        List<Context> contexts = contextService.findAllContext();
        return ResponseEntity.ok(Map.of("contexts", contexts));
    }
}


# main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java

package com.commandAI.commandAI.modules.context.controller.hendler;

import com.commandAI.commandAI.modules.context.service.validation.ContextNotFoundException;
import lombok.Data;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.MissingServletRequestParameterException;
import org.springframework.web.bind.annotation.ControllerAdvice;
import org.springframework.web.bind.annotation.ExceptionHandler;
import org.springframework.web.context.request.WebRequest;

import java.time.LocalDateTime;
import java.util.HashMap;
import java.util.Map;

@Data
@ControllerAdvice
public class GlobalExceptionHandler {

    @ExceptionHandler(ContextNotFoundException.class)
    public ResponseEntity<Object> handleContextNotFoundException(ContextNotFoundException ex, WebRequest request) {
        Map<String, Object> body = new HashMap<>();
        body.put("timestamp", LocalDateTime.now());
        body.put("message", ex.getMessage());

        return new ResponseEntity<>(body, HttpStatus.NOT_FOUND);
    }
    @ExceptionHandler(MissingServletRequestParameterException.class)
    public ResponseEntity<Object> handleMissingServletRequestParameterException(MissingServletRequestParameterException ex, WebRequest request) {
        Map<String, Object> body = new HashMap<>();
        body.put("timestamp", LocalDateTime.now());
        body.put("message", "Parâmetro de requisição ausente: " + ex.getParameterName());

        return new ResponseEntity<>(body, HttpStatus.BAD_REQUEST);
    }

    // Outros handlers de exceção podem ser adicionados aqui
}

# main\java\com\commandAI\commandAI\modules\context\model\context\Context.java

    package com.commandAI.commandAI.modules.context.model.context;
    import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
    import jakarta.persistence.*;
    import lombok.Data;

    import java.time.LocalDateTime;
    @Data
    @Table(name = "tb_context")
    @Entity
    @JsonIgnoreProperties(ignoreUnknown = true)
    public class Context {

        @Id
        @GeneratedValue(strategy = GenerationType.IDENTITY)
        private Long id;

        @Column(name = "context", nullable = false, columnDefinition = "TEXT")
        private String context;

        @Column(name = "created_at", nullable = false)
        private LocalDateTime createdAt;

        @PrePersist
        protected void onCreate() {
            createdAt = LocalDateTime.now();
        }
    }



# main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java

package com.commandAI.commandAI.modules.context.model.dto;

import com.commandAI.commandAI.modules.context.model.context.Context;
import java.time.LocalDateTime;
public record ContextDTO(
        Long id,
        String context,
        LocalDateTime createdAt
) {
    public static ContextDTO fromEntity(Context context) {
        return new ContextDTO(context.getId(), context.getContext(), context.getCreatedAt());
    }
}


# main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java

package com.commandAI.commandAI.modules.context.repository;

import com.commandAI.commandAI.modules.context.model.context.Context;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import java.util.Optional;

@Repository
public interface IContextRepository extends JpaRepository<Context, Long> {
    Optional<Context> findTopByOrderByCreatedAtDesc();
}


# main\java\com\commandAI\commandAI\modules\context\service\IContextService.java

package com.commandAI.commandAI.modules.context.service;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;

import java.util.List;

public interface IContextService {
    Context saveContext(ContextDTO data);
    ContextDTO getLastContext();
    List<Context> findAllContext();
}


# main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java

package com.commandAI.commandAI.modules.context.service.impl;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.repository.IContextRepository;
import com.commandAI.commandAI.modules.context.service.IContextService;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.util.List;

@RequiredArgsConstructor
@Service
public class ContextServiceImpl implements IContextService {

    private final IContextRepository contextRepository;

    @Override
    @Transactional
    public Context saveContext(ContextDTO data) {
        Context context = new Context();
        context.setContext(data.context());
        context = contextRepository.save(context);
        return context;
    }

    @Override
    @Transactional
    public ContextDTO getLastContext() {
        Context lastContext = contextRepository.findTopByOrderByCreatedAtDesc()
                .orElseThrow(() -> new RuntimeException("No contexts found"));
        return ContextDTO.fromEntity(lastContext);
    }
    @Override
    @Transactional
    public List<Context> findAllContext() {
        return contextRepository.findAll();
    }
}


# main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java

package com.commandAI.commandAI.modules.context.service.validation;

public class ContextNotFoundException extends RuntimeException {
    public ContextNotFoundException(String message) {
        super(message);
    }
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java

package com.commandAI.commandAI.modules.embeddingTranscription.controller;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.service.IServiceTranscriptionEmbedded;
import lombok.RequiredArgsConstructor;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.List;

@RestController
@RequestMapping("/api/meetings")    
@RequiredArgsConstructor
public class TranscriptionController {

    private final IServiceTranscriptionEmbedded service;

    @PostMapping("/transcriptions")
    public ResponseEntity<Void> addTranscription(@RequestBody TranscriptionEmbeddedDTO transcriptionDTO) {
        service.saveTranscription(transcriptionDTO.transcriptionText(), transcriptionDTO.embedding());
        return ResponseEntity.status(HttpStatus.CREATED).build();
    }

    @GetMapping("/transcriptions")
    public List<TranscriptionEmbeddedDTO> getAllTranscriptions() {
        return service.getAllTranscriptions();
    }

    @GetMapping("/transcriptions/{id}")
    public ResponseEntity<TranscriptionEmbeddedDTO> getTranscriptionById(@PathVariable Long id) {
        TranscriptionEmbeddedDTO transcription = service.getTranscriptionById(id);
        if (transcription == null) {
            return ResponseEntity.status(HttpStatus.NOT_FOUND).build();
        }
        return ResponseEntity.ok(transcription);
    }

    @GetMapping("/transcriptions/similar")
    public List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(@RequestParam float[] queryEmbedding) {
        return service.findSimilarTranscriptions(queryEmbedding);
    }
    @GetMapping("/last")
    public ResponseEntity<TranscriptionEmbeddedDTO> getLastTranscription() {
        TranscriptionEmbeddedDTO lastTranscription = service.getLastTranscription();
        return ResponseEntity.ok(lastTranscription);
    }
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java

package com.commandAI.commandAI.modules.embeddingTranscription.model;

import jakarta.persistence.*;
import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import lombok.Data;

import java.time.LocalDateTime;

@Data
@JsonIgnoreProperties(ignoreUnknown = true)
@Entity
@Table(name = "transcriptionEmbedded")
public class TranscriptionEmbedded {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    @Column(name = "meeting_id", nullable = false)
    private Long meetingId;

    @Column(name = "transcription_text", nullable = false, columnDefinition = "TEXT")
    private String transcriptionText;

    @Column(name = "embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] embedding;

    @Column(name = "created_at", nullable = false)
    private LocalDateTime createdAt;

    @PrePersist
    protected void onCreate() {
        createdAt = LocalDateTime.now();
    }

}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java

package com.commandAI.commandAI.modules.embeddingTranscription.model.dto;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;

import java.time.LocalDateTime;

public record TranscriptionEmbeddedDTO(
        Long meetingId,
        String transcriptionText,
        float[] embedding,
        LocalDateTime createdAt
) {
    public static TranscriptionEmbeddedDTO fromEntity(TranscriptionEmbedded transcriptionEmbedded) {
        return new TranscriptionEmbeddedDTO(transcriptionEmbedded.getMeetingId(), transcriptionEmbedded.getTranscriptionText(), transcriptionEmbedded.getEmbedding(), transcriptionEmbedded.getCreatedAt());
    }
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java

package com.commandAI.commandAI.modules.embeddingTranscription.repository;

import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import java.util.Optional;

@Repository
public interface ITranscriptionEmbeddedRepository extends JpaRepository<TranscriptionEmbedded, Long> {
    Optional<TranscriptionEmbedded> findTopByOrderByCreatedAtDesc();

}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java

package com.commandAI.commandAI.modules.embeddingTranscription.service;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;

import java.util.List;

public interface IServiceTranscriptionEmbedded {
    void saveTranscription(String transcriptionText, float[] embedding);
    List<TranscriptionEmbeddedDTO> getAllTranscriptions();
    TranscriptionEmbeddedDTO getTranscriptionById(Long id);
    List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(float[] queryEmbedding);
    TranscriptionEmbeddedDTO getLastTranscription();
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java

package com.commandAI.commandAI.modules.embeddingTranscription.service.impl;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.repository.ITranscriptionEmbeddedRepository;
import com.commandAI.commandAI.modules.embeddingTranscription.service.IServiceTranscriptionEmbedded;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.util.List;
import java.util.stream.Collectors;

@RequiredArgsConstructor
@Service
public class ServiceTranscriptionEmbeddedImpl implements IServiceTranscriptionEmbedded {

    private final ITranscriptionEmbeddedRepository transcriptionEmbeddedRepository;

    @Override
    @Transactional
    public void saveTranscription(String transcriptionText, float[] embedding) {
        TranscriptionEmbedded transcription = new TranscriptionEmbedded();
        transcription.setTranscriptionText(transcriptionText);
        transcription.setEmbedding(embedding);
        System.out.println(transcription);
        transcriptionEmbeddedRepository.save(transcription);
    }

    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> getAllTranscriptions() {
        return transcriptionEmbeddedRepository.findAll().stream()
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }

    @Override
    @Transactional
    public TranscriptionEmbeddedDTO getTranscriptionById(Long id) {
        TranscriptionEmbedded transcription = transcriptionEmbeddedRepository.findById(id).orElse(null);
        if (transcription == null) {
            return null;
        }
        return new TranscriptionEmbeddedDTO(
                transcription.getMeetingId(),
                transcription.getTranscriptionText(),
                transcription.getEmbedding(),
                transcription.getCreatedAt());
    }

    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(float[] queryEmbedding) {
        List<TranscriptionEmbedded> allTranscriptions = transcriptionEmbeddedRepository.findAll();
        return allTranscriptions.stream()
                .sorted((t1, t2) -> Double.compare(
                        cosineSimilarity(queryEmbedding, t2.getEmbedding()),
                        cosineSimilarity(queryEmbedding, t1.getEmbedding())))
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }
    @Override
    @Transactional
    public TranscriptionEmbeddedDTO getLastTranscription() {
        TranscriptionEmbedded lastTranscription = transcriptionEmbeddedRepository.findTopByOrderByCreatedAtDesc()
                .orElseThrow(() -> new RuntimeException("No contexts found"));
        return TranscriptionEmbeddedDTO.fromEntity(lastTranscription);
    }
    private double cosineSimilarity(float[] vec1, float[] vec2) {
        double dotProduct = 0.0;
        double normA = 0.0;
        double normB = 0.0;
        for (int i = 0; i < vec1.length; i++) {
            dotProduct += vec1[i] * vec2[i];
            normA += Math.pow(vec1[i], 2);
            normB += Math.pow(vec2[i], 2);
        }
        return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
    }
}


# test\java\com\commandAI\commandAI\CommandAiApplicationTests.java

package com.commandAI.commandAI;

import org.junit.jupiter.api.Test;
import org.springframework.boot.test.context.SpringBootTest;

@SpringBootTest
class CommandAiApplicationTests {

	@Test
	void contextLoads() {
	}
}


# test\java\com\commandAI\commandAI\modelsTest\ContextTest.java

package com.commandAI.commandAI.modelsTest;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import com.commandAI.commandAI.modules.context.model.context.Context;
import java.time.LocalDateTime;

import static org.junit.jupiter.api.Assertions.*;

public class ContextTest {

    private Context context;

    @BeforeEach
    public void setUp() {
        context = new Context();
    }

    @Test
    public void testSetAndGetId() {
        Long id = 1L;
        context.setId(id);
        assertEquals(id, context.getId());
    }

    @Test
    public void testSetAndGetContext() {
        String testContext = "Sample context";
        context.setContext(testContext);
        assertEquals(testContext, context.getContext());
    }

    @Test
    public void testSetAndGetCreatedAt() {
        LocalDateTime now = LocalDateTime.now();
        context.setCreatedAt(now);
        assertEquals(now, context.getCreatedAt());
    }
}


User: resuma a minha aplicação JAVA
2025-04-14 16:56:50,559 [INFO] Resposta da IA (texto): Com certeza! Sua aplicação Java, "CommandAI", é um projeto Spring Boot que visa facilitar a comunicação com inteligência artificial (como o GPT), armazenando contextos e transcrições de forma inteligente. Aqui está um resumo dos principais componentes:

**Componentes Principais:**

*   **Módulos:**
    *   **AIComunication:** Lida com a comunicação com a IA, incluindo salvar as perguntas e respostas, gerar embeddings para o texto e encontrar comunicações similares com base nos embeddings.
    *   **Context:** Permite salvar e recuperar o contexto da conversa, mantendo o histórico das interações.
    *   **EmbeddingTranscription:** Gerencia as transcrições das reuniões com os respectivos "embeddings" (representações vetoriais) para facilitar a busca de transcrições similares.

*   **Entidades (Modelos):**

    *   `GPTCommunication`: Representa uma interação com a IA, armazenando a pergunta, a resposta e os "embeddings" vetoriais correspondentes.
    *   `Context`:  Representa um contexto de conversa.
    *   `TranscriptionEmbedded`: Representa uma transcrição de reunião com o seu embedding.

*   **Repositories:**
    *   Interfaces que estendem `JpaRepository` para interagir com o banco de dados para as entidades: `GPTCommunication`, `Context`, e `TranscriptionEmbedded`.

*   **Serviços:**
    *   Implementações das interfaces de serviço (ex: `GPTCommunicationServiceImpl`, `ContextServiceImpl`, `ServiceTranscriptionEmbeddedImpl`) que contêm a lógica de negócios para salvar, recuperar e encontrar informações relacionadas à comunicação com a IA, contextos e transcrições.

*   **Controladores (Controllers):**

    *   `GPTCommunicationController`:  Expõe endpoints REST para salvar as interações com a IA (`/api/question_answers`) e buscar interações similares.
    *   `ContextController`:  Gerencia os contextos de conversa, permitindo salvar, recuperar o último contexto e todos os contextos (`/api/contexts`).
    *   `TranscriptionController`:  Lida com as transcrições de reuniões, permitindo adicionar, recuperar, buscar transcrições similares e o último contexto (`/api/meetings`).

*   **DTOs (Data Transfer Objects):**

    *   `GTPCommunicationRequestDTO`, `ContextDTO`, `TranscriptionEmbeddedDTO`: Usados para transferir dados entre a aplicação e o mundo externo (ex: requisições e respostas da API).

**Funcionalidades:**

*   **Armazenamento de interações com a IA:** Salva as perguntas e respostas trocadas com a IA, junto com os "embeddings" vetoriais do texto.
*   **Busca de interações similares:** Encontra interações semelhantes com base nos "embeddings" vetoriais, permitindo reutilizar informações e contextos.
*   **Gerenciamento de contexto:** Permite salvar e recuperar o contexto da conversa, garantindo que a IA tenha o histórico necessário para responder adequadamente.
*   **Transcrições de reuniões:** Permite salvar as transcrições de reuniões e associá-las a "embeddings" vetoriais para facilitar a busca de reuniões relevantes.
*   **APIs REST:** Expõe APIs REST para facilitar a integração com outras aplicações e serviços.

**Em resumo:** A aplicação CommandAI é uma solução completa para gerenciar a comunicação com a IA, armazenando contextos e transcrições de forma inteligente, permitindo a busca de informações relevantes e a reutilização de conhecimento.
2025-04-14 16:56:50,728 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 16:56:50,729 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 16:56:50,731 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 16:56:50,732 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 16:56:50,734 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 16:56:50,735 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 16:56:50,736 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 16:56:50,738 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 16:56:50,739 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 16:56:50,740 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 16:56:50,741 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 16:56:50,743 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 16:56:50,744 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 16:56:50,746 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 16:56:50,748 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 16:56:50,749 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 16:56:50,750 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 16:56:50,751 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 16:56:50,752 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 16:56:50,754 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 16:56:50,755 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 16:56:50,756 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 16:56:50,757 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 16:56:50,758 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 16:56:50,759 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 16:56:50,761 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 16:56:50,763 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 16:56:50,764 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 16:56:50,766 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 16:56:50,767 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 16:56:50,769 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 16:56:50,770 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 16:56:50,771 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 16:56:50,772 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 16:56:50,773 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 16:56:50,775 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 17:02:11,357 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 17:02:11,358 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 17:02:11,359 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 17:02:11,361 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 17:02:11,362 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 17:02:11,363 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 17:02:11,365 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 17:02:11,367 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 17:02:11,368 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 17:02:11,369 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 17:02:11,371 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 17:02:11,372 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 17:02:11,374 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 17:02:11,375 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 17:02:11,377 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 17:02:11,378 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 17:02:11,379 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 17:02:11,381 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 17:02:11,383 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 17:02:11,386 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 17:02:11,389 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 17:02:11,390 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 17:02:11,391 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 17:02:11,392 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 17:02:11,393 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 17:02:11,395 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 17:02:11,396 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 17:02:11,397 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 17:02:11,400 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 17:02:11,401 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 17:02:11,403 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 17:02:11,404 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 17:02:11,405 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 17:02:11,407 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 17:02:11,408 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 17:02:11,409 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 17:02:11,533 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 17:02:11,534 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 17:02:11,537 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 17:02:11,538 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 17:02:11,539 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 17:02:11,541 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 17:02:11,542 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 17:02:11,543 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 17:02:11,545 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 17:02:11,546 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 17:02:11,547 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 17:02:11,549 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 17:02:11,551 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 17:02:11,552 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 17:02:11,555 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 17:02:11,556 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 17:02:11,558 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 17:02:11,560 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 17:02:11,561 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 17:02:11,562 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 17:02:11,564 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 17:02:11,565 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 17:02:11,567 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 17:02:11,570 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 17:02:11,572 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 17:02:11,573 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 17:02:11,575 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 17:02:11,576 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 17:02:11,577 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 17:02:11,578 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 17:02:11,579 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 17:02:11,580 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 17:02:11,581 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 17:02:11,583 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 17:02:11,584 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 17:02:11,585 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 17:02:11,587 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python_e_java

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python_e_java()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python_e_java() -> str:
    """Lê todo o conteúdo de todos os arquivos .py e .java a partir de src/"""
    src_dir = Config.BASE_DIR

    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    # Busca arquivos .py e .java separadamente
    padrao_busca_py = os.path.join(src_dir, '**', '*.py')
    padrao_busca_java = os.path.join(src_dir, '**', '*.java')

    arquivos_py = glob.glob(padrao_busca_py, recursive=True)
    arquivos_java = glob.glob(padrao_busca_java, recursive=True)

    arquivos = arquivos_py + arquivos_java
    arquivos.sort() # Ordena a lista completa

    for arquivo in arquivos:
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# main\java\com\commandAI\commandAI\CommandAiApplication.java

package com.commandAI.commandAI;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class CommandAiApplication {

	public static void main(String[] args) {
		SpringApplication.run(CommandAiApplication.class, args);
	}

}


# main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java

package com.commandAI.commandAI.modules.AIComunication.controller;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import com.commandAI.commandAI.modules.AIComunication.model.dto.GTPCommunicationRequestDTO;
import com.commandAI.commandAI.modules.AIComunication.service.IGPTCommunicationService;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.RequiredArgsConstructor;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.Arrays;
import java.util.List;

@RequiredArgsConstructor
@RestController
@RequestMapping("/api/question_answers")
public class GPTCommunicationController {

    private final IGPTCommunicationService service;

    @PostMapping("/save")
    public ResponseEntity<GPTCommunication> saveCommunication(@RequestBody GTPCommunicationRequestDTO dto) {
        try {
            GPTCommunication savedCommunication = service.saveCommunication(dto);
            return new ResponseEntity<>(savedCommunication, HttpStatus.CREATED);
        } catch (Exception e) {
            // Log error and return an appropriate response
            System.err.println("Erro ao salvar comunicação: " + e.getMessage());
            return new ResponseEntity<>(HttpStatus.INTERNAL_SERVER_ERROR);
        }
    }

    @PostMapping("/similar")
    public ResponseEntity<List<GTPCommunicationRequestDTO>> getSimilarCommunications(@RequestBody JsonNode requestBody) {
        try {
            // Validar se é um array
            if (!requestBody.isArray()) {
                throw new IllegalArgumentException("Esperado um array");
            }

            // Converter JsonNode para array de float
            float[] embedding = new ObjectMapper().convertValue(requestBody, float[].class);

            // Log para verificar o embedding recebido
            System.out.println("Embedding recebido: " + Arrays.toString(embedding));

            // Chamar o serviço com o array de float
            List<GTPCommunicationRequestDTO> similarCommunications = service.findSimilarCommunications(embedding);
            return ResponseEntity.ok(similarCommunications);
        } catch (IllegalArgumentException e) {
            System.err.println("Erro de validação: " + e.getMessage());
            return ResponseEntity.status(HttpStatus.BAD_REQUEST).body(null);
        } catch (Exception e) {
            // Log error and return an appropriate response
            System.err.println("Erro ao processar a solicitação: " + e.getMessage());
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(null);
        }
    }
}


# main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java

package com.commandAI.commandAI.modules.AIComunication.model;

import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import jakarta.persistence.*;
import lombok.Data;

import java.time.LocalDateTime;
@Data
@Entity
@JsonIgnoreProperties(ignoreUnknown = true)
@Table(name = "gpt_communication")
public class GPTCommunication {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(name = "question", nullable = false, columnDefinition = "TEXT")
    private String question;

    @Column(name = "answer", nullable = false, columnDefinition = "TEXT")
    private String answer;

    @Column(name = "question_embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] questionEmbedding;

    @Column(name = "answer_embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] answerEmbedding;

    @Column(name = "created_at", nullable = false)
    private LocalDateTime createdAt;

}

# main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java

package com.commandAI.commandAI.modules.AIComunication.model.dto;

import com.fasterxml.jackson.annotation.JsonProperty;

public record GTPCommunicationRequestDTO(
        String question,
        float[] questionEmbedding,
        String answer,
        float[] answerEmbedding
) {}



# main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java

package com.commandAI.commandAI.modules.AIComunication.repository;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

@Repository
public interface IGPTCommunicationRepository extends JpaRepository<GPTCommunication, Long> {

}

# main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java

package com.commandAI.commandAI.modules.AIComunication.service;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import com.commandAI.commandAI.modules.AIComunication.model.dto.GTPCommunicationRequestDTO;

import java.util.List;

public interface IGPTCommunicationService {
    GPTCommunication saveCommunication(GTPCommunicationRequestDTO dto);
    List<GTPCommunicationRequestDTO> findSimilarCommunications(float[] embedding);
}

# main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java

package com.commandAI.commandAI.modules.AIComunication.service.impl;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import com.commandAI.commandAI.modules.AIComunication.model.dto.GTPCommunicationRequestDTO;
import com.commandAI.commandAI.modules.AIComunication.repository.IGPTCommunicationRepository;
import com.commandAI.commandAI.modules.AIComunication.service.IGPTCommunicationService;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;

import java.time.LocalDateTime;
import java.util.Arrays;
import java.util.List;
import java.util.stream.Collectors;

@RequiredArgsConstructor
@Service
public class GPTCommunicationServiceImpl implements IGPTCommunicationService {

    private final IGPTCommunicationRepository repository;

    @Override
    public GPTCommunication saveCommunication(GTPCommunicationRequestDTO dto) {
        GPTCommunication communication = new GPTCommunication();
        communication.setQuestion(dto.question());
        communication.setQuestionEmbedding(dto.questionEmbedding());
        communication.setAnswer(dto.answer());
        communication.setAnswerEmbedding(dto.answerEmbedding());
        communication.setCreatedAt(LocalDateTime.now());
        return repository.save(communication);
    }

    @Override
    public List<GTPCommunicationRequestDTO> findSimilarCommunications(float[] embedding) {
        List<GPTCommunication> allCommunications = repository.findAll();

        return allCommunications.stream()
                .filter(comm -> isSimilar(embedding, comm.getQuestionEmbedding()))
                .map(comm -> new GTPCommunicationRequestDTO(
                        comm.getQuestion(),
                        comm.getQuestionEmbedding(),
                        comm.getAnswer(),
                        comm.getAnswerEmbedding()))
                .collect(Collectors.toList());
    }

    private boolean isSimilar(float[] embedding1, float[] embedding2) {
        // Verificar se os embeddings têm o mesmo tamanho
        if (embedding1.length != embedding2.length) {
            System.err.println("Os embeddings têm tamanhos diferentes.");
            return false;
        }

        System.out.println("Embedding1: " + Arrays.toString(embedding1));
        System.out.println("Embedding2: " + Arrays.toString(embedding2));

        double dotProduct = 0.0;
        double normA = 0.0;
        double normB = 0.0;
        for (int i = 0; i < embedding1.length; i++) {
            dotProduct += embedding1[i] * embedding2[i];
            normA += Math.pow(embedding1[i], 2);
            normB += Math.pow(embedding2[i], 2);
        }
        return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB)) > 0.8; // Ajuste o limiar conforme necessário
    }
}


# main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java

package com.commandAI.commandAI.modules.context.controller;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.context.service.IContextService;
import com.commandAI.commandAI.modules.context.model.context.Context;
import lombok.RequiredArgsConstructor;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.validation.annotation.Validated;
import org.springframework.web.bind.annotation.*;

import java.util.List;
import java.util.Map;

@RequiredArgsConstructor
@RestController
@Validated
@ResponseBody
@RequestMapping("/api/contexts")
public class ContextController {

    @Autowired
    private final IContextService contextService;

    @PostMapping("/save")
    public ResponseEntity<Map<String, Object>> saveContext(@RequestBody ContextDTO data) {
        Context newContext = contextService.saveContext(data);
        Map<String, Object> response = Map.of("result", newContext);
        return ResponseEntity.status(HttpStatus.CREATED).body(response);
    }


    @GetMapping("/last")
    public ResponseEntity<ContextDTO> getLastContext() {
        ContextDTO lastContext = contextService.getLastContext();
        return ResponseEntity.ok(lastContext);
    }

    @GetMapping("/all")
    public ResponseEntity<Map<String, Object>> getAllContexts() {
        List<Context> contexts = contextService.findAllContext();
        return ResponseEntity.ok(Map.of("contexts", contexts));
    }
}


# main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java

package com.commandAI.commandAI.modules.context.controller.hendler;

import com.commandAI.commandAI.modules.context.service.validation.ContextNotFoundException;
import lombok.Data;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.MissingServletRequestParameterException;
import org.springframework.web.bind.annotation.ControllerAdvice;
import org.springframework.web.bind.annotation.ExceptionHandler;
import org.springframework.web.context.request.WebRequest;

import java.time.LocalDateTime;
import java.util.HashMap;
import java.util.Map;

@Data
@ControllerAdvice
public class GlobalExceptionHandler {

    @ExceptionHandler(ContextNotFoundException.class)
    public ResponseEntity<Object> handleContextNotFoundException(ContextNotFoundException ex, WebRequest request) {
        Map<String, Object> body = new HashMap<>();
        body.put("timestamp", LocalDateTime.now());
        body.put("message", ex.getMessage());

        return new ResponseEntity<>(body, HttpStatus.NOT_FOUND);
    }
    @ExceptionHandler(MissingServletRequestParameterException.class)
    public ResponseEntity<Object> handleMissingServletRequestParameterException(MissingServletRequestParameterException ex, WebRequest request) {
        Map<String, Object> body = new HashMap<>();
        body.put("timestamp", LocalDateTime.now());
        body.put("message", "Parâmetro de requisição ausente: " + ex.getParameterName());

        return new ResponseEntity<>(body, HttpStatus.BAD_REQUEST);
    }

    // Outros handlers de exceção podem ser adicionados aqui
}

# main\java\com\commandAI\commandAI\modules\context\model\context\Context.java

    package com.commandAI.commandAI.modules.context.model.context;
    import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
    import jakarta.persistence.*;
    import lombok.Data;

    import java.time.LocalDateTime;
    @Data
    @Table(name = "tb_context")
    @Entity
    @JsonIgnoreProperties(ignoreUnknown = true)
    public class Context {

        @Id
        @GeneratedValue(strategy = GenerationType.IDENTITY)
        private Long id;

        @Column(name = "context", nullable = false, columnDefinition = "TEXT")
        private String context;

        @Column(name = "created_at", nullable = false)
        private LocalDateTime createdAt;

        @PrePersist
        protected void onCreate() {
            createdAt = LocalDateTime.now();
        }
    }



# main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java

package com.commandAI.commandAI.modules.context.model.dto;

import com.commandAI.commandAI.modules.context.model.context.Context;
import java.time.LocalDateTime;
public record ContextDTO(
        Long id,
        String context,
        LocalDateTime createdAt
) {
    public static ContextDTO fromEntity(Context context) {
        return new ContextDTO(context.getId(), context.getContext(), context.getCreatedAt());
    }
}


# main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java

package com.commandAI.commandAI.modules.context.repository;

import com.commandAI.commandAI.modules.context.model.context.Context;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import java.util.Optional;

@Repository
public interface IContextRepository extends JpaRepository<Context, Long> {
    Optional<Context> findTopByOrderByCreatedAtDesc();
}


# main\java\com\commandAI\commandAI\modules\context\service\IContextService.java

package com.commandAI.commandAI.modules.context.service;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;

import java.util.List;

public interface IContextService {
    Context saveContext(ContextDTO data);
    ContextDTO getLastContext();
    List<Context> findAllContext();
}


# main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java

package com.commandAI.commandAI.modules.context.service.impl;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.repository.IContextRepository;
import com.commandAI.commandAI.modules.context.service.IContextService;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.util.List;

@RequiredArgsConstructor
@Service
public class ContextServiceImpl implements IContextService {

    private final IContextRepository contextRepository;

    @Override
    @Transactional
    public Context saveContext(ContextDTO data) {
        Context context = new Context();
        context.setContext(data.context());
        context = contextRepository.save(context);
        return context;
    }

    @Override
    @Transactional
    public ContextDTO getLastContext() {
        Context lastContext = contextRepository.findTopByOrderByCreatedAtDesc()
                .orElseThrow(() -> new RuntimeException("No contexts found"));
        return ContextDTO.fromEntity(lastContext);
    }
    @Override
    @Transactional
    public List<Context> findAllContext() {
        return contextRepository.findAll();
    }
}


# main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java

package com.commandAI.commandAI.modules.context.service.validation;

public class ContextNotFoundException extends RuntimeException {
    public ContextNotFoundException(String message) {
        super(message);
    }
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java

package com.commandAI.commandAI.modules.embeddingTranscription.controller;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.service.IServiceTranscriptionEmbedded;
import lombok.RequiredArgsConstructor;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.List;

@RestController
@RequestMapping("/api/meetings")    
@RequiredArgsConstructor
public class TranscriptionController {

    private final IServiceTranscriptionEmbedded service;

    @PostMapping("/transcriptions")
    public ResponseEntity<Void> addTranscription(@RequestBody TranscriptionEmbeddedDTO transcriptionDTO) {
        service.saveTranscription(transcriptionDTO.transcriptionText(), transcriptionDTO.embedding());
        return ResponseEntity.status(HttpStatus.CREATED).build();
    }

    @GetMapping("/transcriptions")
    public List<TranscriptionEmbeddedDTO> getAllTranscriptions() {
        return service.getAllTranscriptions();
    }

    @GetMapping("/transcriptions/{id}")
    public ResponseEntity<TranscriptionEmbeddedDTO> getTranscriptionById(@PathVariable Long id) {
        TranscriptionEmbeddedDTO transcription = service.getTranscriptionById(id);
        if (transcription == null) {
            return ResponseEntity.status(HttpStatus.NOT_FOUND).build();
        }
        return ResponseEntity.ok(transcription);
    }

    @GetMapping("/transcriptions/similar")
    public List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(@RequestParam float[] queryEmbedding) {
        return service.findSimilarTranscriptions(queryEmbedding);
    }
    @GetMapping("/last")
    public ResponseEntity<TranscriptionEmbeddedDTO> getLastTranscription() {
        TranscriptionEmbeddedDTO lastTranscription = service.getLastTranscription();
        return ResponseEntity.ok(lastTranscription);
    }
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java

package com.commandAI.commandAI.modules.embeddingTranscription.model;

import jakarta.persistence.*;
import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import lombok.Data;

import java.time.LocalDateTime;

@Data
@JsonIgnoreProperties(ignoreUnknown = true)
@Entity
@Table(name = "transcriptionEmbedded")
public class TranscriptionEmbedded {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    @Column(name = "meeting_id", nullable = false)
    private Long meetingId;

    @Column(name = "transcription_text", nullable = false, columnDefinition = "TEXT")
    private String transcriptionText;

    @Column(name = "embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] embedding;

    @Column(name = "created_at", nullable = false)
    private LocalDateTime createdAt;

    @PrePersist
    protected void onCreate() {
        createdAt = LocalDateTime.now();
    }

}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java

package com.commandAI.commandAI.modules.embeddingTranscription.model.dto;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;

import java.time.LocalDateTime;

public record TranscriptionEmbeddedDTO(
        Long meetingId,
        String transcriptionText,
        float[] embedding,
        LocalDateTime createdAt
) {
    public static TranscriptionEmbeddedDTO fromEntity(TranscriptionEmbedded transcriptionEmbedded) {
        return new TranscriptionEmbeddedDTO(transcriptionEmbedded.getMeetingId(), transcriptionEmbedded.getTranscriptionText(), transcriptionEmbedded.getEmbedding(), transcriptionEmbedded.getCreatedAt());
    }
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java

package com.commandAI.commandAI.modules.embeddingTranscription.repository;

import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import java.util.Optional;

@Repository
public interface ITranscriptionEmbeddedRepository extends JpaRepository<TranscriptionEmbedded, Long> {
    Optional<TranscriptionEmbedded> findTopByOrderByCreatedAtDesc();

}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java

package com.commandAI.commandAI.modules.embeddingTranscription.service;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;

import java.util.List;

public interface IServiceTranscriptionEmbedded {
    void saveTranscription(String transcriptionText, float[] embedding);
    List<TranscriptionEmbeddedDTO> getAllTranscriptions();
    TranscriptionEmbeddedDTO getTranscriptionById(Long id);
    List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(float[] queryEmbedding);
    TranscriptionEmbeddedDTO getLastTranscription();
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java

package com.commandAI.commandAI.modules.embeddingTranscription.service.impl;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.repository.ITranscriptionEmbeddedRepository;
import com.commandAI.commandAI.modules.embeddingTranscription.service.IServiceTranscriptionEmbedded;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.util.List;
import java.util.stream.Collectors;

@RequiredArgsConstructor
@Service
public class ServiceTranscriptionEmbeddedImpl implements IServiceTranscriptionEmbedded {

    private final ITranscriptionEmbeddedRepository transcriptionEmbeddedRepository;

    @Override
    @Transactional
    public void saveTranscription(String transcriptionText, float[] embedding) {
        TranscriptionEmbedded transcription = new TranscriptionEmbedded();
        transcription.setTranscriptionText(transcriptionText);
        transcription.setEmbedding(embedding);
        System.out.println(transcription);
        transcriptionEmbeddedRepository.save(transcription);
    }

    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> getAllTranscriptions() {
        return transcriptionEmbeddedRepository.findAll().stream()
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }

    @Override
    @Transactional
    public TranscriptionEmbeddedDTO getTranscriptionById(Long id) {
        TranscriptionEmbedded transcription = transcriptionEmbeddedRepository.findById(id).orElse(null);
        if (transcription == null) {
            return null;
        }
        return new TranscriptionEmbeddedDTO(
                transcription.getMeetingId(),
                transcription.getTranscriptionText(),
                transcription.getEmbedding(),
                transcription.getCreatedAt());
    }

    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(float[] queryEmbedding) {
        List<TranscriptionEmbedded> allTranscriptions = transcriptionEmbeddedRepository.findAll();
        return allTranscriptions.stream()
                .sorted((t1, t2) -> Double.compare(
                        cosineSimilarity(queryEmbedding, t2.getEmbedding()),
                        cosineSimilarity(queryEmbedding, t1.getEmbedding())))
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }
    @Override
    @Transactional
    public TranscriptionEmbeddedDTO getLastTranscription() {
        TranscriptionEmbedded lastTranscription = transcriptionEmbeddedRepository.findTopByOrderByCreatedAtDesc()
                .orElseThrow(() -> new RuntimeException("No contexts found"));
        return TranscriptionEmbeddedDTO.fromEntity(lastTranscription);
    }
    private double cosineSimilarity(float[] vec1, float[] vec2) {
        double dotProduct = 0.0;
        double normA = 0.0;
        double normB = 0.0;
        for (int i = 0; i < vec1.length; i++) {
            dotProduct += vec1[i] * vec2[i];
            normA += Math.pow(vec1[i], 2);
            normB += Math.pow(vec2[i], 2);
        }
        return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
    }
}


# test\java\com\commandAI\commandAI\CommandAiApplicationTests.java

package com.commandAI.commandAI;

import org.junit.jupiter.api.Test;
import org.springframework.boot.test.context.SpringBootTest;

@SpringBootTest
class CommandAiApplicationTests {

	@Test
	void contextLoads() {
	}
}


# test\java\com\commandAI\commandAI\modelsTest\ContextTest.java

package com.commandAI.commandAI.modelsTest;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import com.commandAI.commandAI.modules.context.model.context.Context;
import java.time.LocalDateTime;

import static org.junit.jupiter.api.Assertions.*;

public class ContextTest {

    private Context context;

    @BeforeEach
    public void setUp() {
        context = new Context();
    }

    @Test
    public void testSetAndGetId() {
        Long id = 1L;
        context.setId(id);
        assertEquals(id, context.getId());
    }

    @Test
    public void testSetAndGetContext() {
        String testContext = "Sample context";
        context.setContext(testContext);
        assertEquals(testContext, context.getContext());
    }

    @Test
    public void testSetAndGetCreatedAt() {
        LocalDateTime now = LocalDateTime.now();
        context.setCreatedAt(now);
        assertEquals(now, context.getCreatedAt());
    }
}


User: resuma a minha aplicação JAVA
Assistant: Com certeza! Sua aplicação Java, "CommandAI", é um projeto Spring Boot que visa facilitar a comunicação com inteligência artificial (como o GPT), armazenando contextos e transcrições de forma inteligente. Aqui está um resumo dos principais componentes:

**Componentes Principais:**

*   **Módulos:**
    *   **AIComunication:** Lida com a comunicação com a IA, incluindo salvar as perguntas e respostas, gerar embeddings para o texto e encontrar comunicações similares com base nos embeddings.
    *   **Context:** Permite salvar e recuperar o contexto da conversa, mantendo o histórico das interações.
    *   **EmbeddingTranscription:** Gerencia as transcrições das reuniões com os respectivos "embeddings" (representações vetoriais) para facilitar a busca de transcrições similares.

*   **Entidades (Modelos):**

    *   `GPTCommunication`: Representa uma interação com a IA, armazenando a pergunta, a resposta e os "embeddings" vetoriais correspondentes.
    *   `Context`:  Representa um contexto de conversa.
    *   `TranscriptionEmbedded`: Representa uma transcrição de reunião com o seu embedding.

*   **Repositories:**
    *   Interfaces que estendem `JpaRepository` para interagir com o banco de dados para as entidades: `GPTCommunication`, `Context`, e `TranscriptionEmbedded`.

*   **Serviços:**
    *   Implementações das interfaces de serviço (ex: `GPTCommunicationServiceImpl`, `ContextServiceImpl`, `ServiceTranscriptionEmbeddedImpl`) que contêm a lógica de negócios para salvar, recuperar e encontrar informações relacionadas à comunicação com a IA, contextos e transcrições.

*   **Controladores (Controllers):**

    *   `GPTCommunicationController`:  Expõe endpoints REST para salvar as interações com a IA (`/api/question_answers`) e buscar interações similares.
    *   `ContextController`:  Gerencia os contextos de conversa, permitindo salvar, recuperar o último contexto e todos os contextos (`/api/contexts`).
    *   `TranscriptionController`:  Lida com as transcrições de reuniões, permitindo adicionar, recuperar, buscar transcrições similares e o último contexto (`/api/meetings`).

*   **DTOs (Data Transfer Objects):**

    *   `GTPCommunicationRequestDTO`, `ContextDTO`, `TranscriptionEmbeddedDTO`: Usados para transferir dados entre a aplicação e o mundo externo (ex: requisições e respostas da API).

**Funcionalidades:**

*   **Armazenamento de interações com a IA:** Salva as perguntas e respostas trocadas com a IA, junto com os "embeddings" vetoriais do texto.
*   **Busca de interações similares:** Encontra interações semelhantes com base nos "embeddings" vetoriais, permitindo reutilizar informações e contextos.
*   **Gerenciamento de contexto:** Permite salvar e recuperar o contexto da conversa, garantindo que a IA tenha o histórico necessário para responder adequadamente.
*   **Transcrições de reuniões:** Permite salvar as transcrições de reuniões e associá-las a "embeddings" vetoriais para facilitar a busca de reuniões relevantes.
*   **APIs REST:** Expõe APIs REST para facilitar a integração com outras aplicações e serviços.

**Em resumo:** A aplicação CommandAI é uma solução completa para gerenciar a comunicação com a IA, armazenando contextos e transcrições de forma inteligente, permitindo a busca de informações relevantes e a reutilização de conhecimento.
User: Estou implementando boas práticas de desenvolvimento JAVA ?
2025-04-14 17:02:19,584 [INFO] Resposta da IA (texto): Assistant: Sim, com base na estrutura e nos componentes da sua aplicação CommandAI, parece que você está seguindo muitas boas práticas de desenvolvimento Java e Spring Boot. Aqui está uma análise detalhada:

**Boas Práticas Implementadas:**

*   **Arquitetura em Camadas:** A aplicação está bem organizada em camadas distintas (Controladores, Serviços, Repositórios, Modelos), o que facilita a manutenção, o teste e a evolução do sistema.
*   **Uso do Spring Boot:** O uso do Spring Boot simplifica a configuração e o desenvolvimento, fornecendo "auto-configuração", "starter dependencies" e outras ferramentas que agilizam o processo.
*   **Injeção de Dependência (DI):** O uso de `@Autowired` e `@RequiredArgsConstructor` para injeção de dependência promove o baixo acoplamento e facilita o teste unitário.
*   **DTOs (Data Transfer Objects):** O uso de DTOs para transferir dados entre as camadas e expor dados na API é uma prática recomendada para desacoplar os modelos de dados internos da API.
*   **RESTful APIs:** A criação de APIs RESTful com controladores (Controllers) bem definidos e endpoints claros (ex: `/api/question_answers`, `/api/contexts`, `/api/meetings`) facilita a integração com outros sistemas.
*   **Validação:** O uso de `@Validated` nos controladores indica a preocupação com a validação dos dados de entrada.
*   **Tratamento de Exceções:** A implementação de um `GlobalExceptionHandler` para lidar com exceções de forma centralizada é uma boa prática para fornecer respostas de erro consistentes e informativas.
*   **Log:** O uso de `System.err.println` para log (enquanto não ideal) indica uma preocupação com o registro de eventos e erros para facilitar a depuração e o monitoramento.
*   **JPA e Spring Data JPA:** O uso de JPA (Java Persistence API) com Spring Data JPA simplifica o acesso ao banco de dados, fornecendo uma abstração sobre a camada de persistência.
*   **Embeddings:** A utilização de Embeddings é uma ótima estratégia para similaridade semântica.
*   **Testes:** A presença da classe `CommandAiApplicationTests` e `ContextTest` indica que você está escrevendo testes para sua aplicação, o que é fundamental para garantir a qualidade e a confiabilidade do código.
*   **Uso de Records:** A utilização de Records é excelente para DTOs.

**Sugestões de Melhorias:**

*   **Log:** Substitua `System.err.println` por um framework de log mais robusto, como SLF4J com Logback ou Log4j2. Isso permite configurar diferentes níveis de log, direcionar os logs para arquivos ou outros destinos, e adicionar informações contextuais aos logs.
*   **Tratamento de Exceções:**
    *   Crie classes de exceção personalizadas para representar diferentes tipos de erros na sua aplicação (ex: `AICommunicationException`, `ContextException`, `TranscriptionException`).
    *   Use códigos de status HTTP apropriados para cada tipo de erro.
    *   Forneça mensagens de erro claras e informativas para o cliente.
*   **Segurança:** Implemente mecanismos de autenticação e autorização para proteger suas APIs. Spring Security é uma ótima opção para isso.
*   **Documentação:** Use ferramentas como o Swagger (OpenAPI) para gerar automaticamente a documentação da sua API REST.
*   **Internacionalização (i18n):** Se sua aplicação precisar suportar diferentes idiomas, considere implementar a internacionalização.
*   **Monitoramento:** Integre ferramentas de monitoramento para coletar métricas sobre o desempenho da sua aplicação (ex: tempo de resposta da API, uso de recursos, etc.).
*   **Documentação Javadoc:** Adicione comentários Javadoc para documentar suas classes, métodos e parâmetros. Isso facilita a compreensão do código e a geração de documentação automática.
*   **Cobertura de Testes:** Busque uma alta cobertura de testes unitários e de integração.

**Em resumo:** Sua aplicação CommandAI parece estar bem estruturada e seguindo muitas boas práticas de desenvolvimento Java e Spring Boot. Ao implementar as sugestões de melhorias acima, você pode aumentar ainda mais a qualidade, a manutenibilidade e a segurança do seu sistema.

2025-04-14 17:02:19,703 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 17:02:19,704 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 17:02:19,706 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 17:02:19,707 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 17:02:19,708 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 17:02:19,710 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 17:02:19,711 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 17:02:19,712 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 17:02:19,714 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 17:02:19,715 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 17:02:19,717 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 17:02:19,718 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 17:02:19,720 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 17:02:19,721 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 17:02:19,722 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 17:02:19,724 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 17:02:19,725 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 17:02:19,726 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 17:02:19,728 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 17:02:19,729 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 17:02:19,731 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 17:02:19,732 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 17:02:19,734 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 17:02:19,735 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 17:02:19,736 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 17:02:19,737 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 17:02:19,739 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 17:02:19,740 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 17:02:19,741 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 17:02:19,742 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 17:02:19,743 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 17:02:19,745 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 17:02:19,747 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 17:02:19,749 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 17:02:19,750 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 17:02:19,752 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 19:47:26,357 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 19:47:26,368 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 19:47:26,373 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 19:47:26,379 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 19:47:26,385 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 19:47:26,392 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 19:47:26,401 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 19:47:26,407 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 19:47:26,415 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 19:47:26,422 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 19:47:26,428 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 19:47:26,433 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 19:47:26,438 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 19:47:26,443 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 19:47:26,450 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 19:47:26,456 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 19:47:26,461 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 19:47:26,467 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 19:47:26,472 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 19:47:26,480 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 19:47:26,486 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 19:47:26,494 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 19:47:26,503 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 19:47:26,508 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 19:47:26,513 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 19:47:26,518 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 19:47:26,524 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 19:47:26,528 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 19:47:26,533 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 19:47:26,539 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 19:47:26,544 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 19:47:26,550 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 19:47:26,556 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 19:47:26,564 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 19:47:26,569 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 19:47:26,575 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 19:47:27,083 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 19:47:27,085 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 19:47:27,086 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 19:47:27,088 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 19:47:27,090 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 19:47:27,091 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 19:47:27,094 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 19:47:27,095 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 19:47:27,097 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 19:47:27,099 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 19:47:27,101 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 19:47:27,103 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 19:47:27,106 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 19:47:27,107 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 19:47:27,108 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 19:47:27,110 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 19:47:27,113 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 19:47:27,114 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 19:47:27,115 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 19:47:27,116 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 19:47:27,118 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 19:47:27,119 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 19:47:27,121 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 19:47:27,122 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 19:47:27,124 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 19:47:27,125 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 19:47:27,127 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 19:47:27,128 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 19:47:27,130 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 19:47:27,131 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 19:47:27,132 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 19:47:27,134 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 19:47:27,135 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 19:47:27,137 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 19:47:27,139 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 19:47:27,141 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 19:47:27,145 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python_e_java

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python_e_java()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python_e_java() -> str:
    """Lê todo o conteúdo de todos os arquivos .py e .java a partir de src/"""
    src_dir = Config.BASE_DIR

    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    # Busca arquivos .py e .java separadamente
    padrao_busca_py = os.path.join(src_dir, '**', '*.py')
    padrao_busca_java = os.path.join(src_dir, '**', '*.java')

    arquivos_py = glob.glob(padrao_busca_py, recursive=True)
    arquivos_java = glob.glob(padrao_busca_java, recursive=True)

    arquivos = arquivos_py + arquivos_java
    arquivos.sort() # Ordena a lista completa

    for arquivo in arquivos:
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# main\java\com\commandAI\commandAI\CommandAiApplication.java

package com.commandAI.commandAI;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class CommandAiApplication {

	public static void main(String[] args) {
		SpringApplication.run(CommandAiApplication.class, args);
	}

}


# main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java

package com.commandAI.commandAI.modules.AIComunication.controller;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import com.commandAI.commandAI.modules.AIComunication.model.dto.GTPCommunicationRequestDTO;
import com.commandAI.commandAI.modules.AIComunication.service.IGPTCommunicationService;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.RequiredArgsConstructor;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.Arrays;
import java.util.List;

@RequiredArgsConstructor
@RestController
@RequestMapping("/api/question_answers")
public class GPTCommunicationController {

    private final IGPTCommunicationService service;

    @PostMapping("/save")
    public ResponseEntity<GPTCommunication> saveCommunication(@RequestBody GTPCommunicationRequestDTO dto) {
        try {
            GPTCommunication savedCommunication = service.saveCommunication(dto);
            return new ResponseEntity<>(savedCommunication, HttpStatus.CREATED);
        } catch (Exception e) {
            // Log error and return an appropriate response
            System.err.println("Erro ao salvar comunicação: " + e.getMessage());
            return new ResponseEntity<>(HttpStatus.INTERNAL_SERVER_ERROR);
        }
    }

    @PostMapping("/similar")
    public ResponseEntity<List<GTPCommunicationRequestDTO>> getSimilarCommunications(@RequestBody JsonNode requestBody) {
        try {
            // Validar se é um array
            if (!requestBody.isArray()) {
                throw new IllegalArgumentException("Esperado um array");
            }

            // Converter JsonNode para array de float
            float[] embedding = new ObjectMapper().convertValue(requestBody, float[].class);

            // Log para verificar o embedding recebido
            System.out.println("Embedding recebido: " + Arrays.toString(embedding));

            // Chamar o serviço com o array de float
            List<GTPCommunicationRequestDTO> similarCommunications = service.findSimilarCommunications(embedding);
            return ResponseEntity.ok(similarCommunications);
        } catch (IllegalArgumentException e) {
            System.err.println("Erro de validação: " + e.getMessage());
            return ResponseEntity.status(HttpStatus.BAD_REQUEST).body(null);
        } catch (Exception e) {
            // Log error and return an appropriate response
            System.err.println("Erro ao processar a solicitação: " + e.getMessage());
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(null);
        }
    }
}


# main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java

package com.commandAI.commandAI.modules.AIComunication.model;

import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import jakarta.persistence.*;
import lombok.Data;

import java.time.LocalDateTime;
@Data
@Entity
@JsonIgnoreProperties(ignoreUnknown = true)
@Table(name = "gpt_communication")
public class GPTCommunication {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(name = "question", nullable = false, columnDefinition = "TEXT")
    private String question;

    @Column(name = "answer", nullable = false, columnDefinition = "TEXT")
    private String answer;

    @Column(name = "question_embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] questionEmbedding;

    @Column(name = "answer_embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] answerEmbedding;

    @Column(name = "created_at", nullable = false)
    private LocalDateTime createdAt;

}

# main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java

package com.commandAI.commandAI.modules.AIComunication.model.dto;

import com.fasterxml.jackson.annotation.JsonProperty;

public record GTPCommunicationRequestDTO(
        String question,
        float[] questionEmbedding,
        String answer,
        float[] answerEmbedding
) {}



# main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java

package com.commandAI.commandAI.modules.AIComunication.repository;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

@Repository
public interface IGPTCommunicationRepository extends JpaRepository<GPTCommunication, Long> {

}

# main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java

package com.commandAI.commandAI.modules.AIComunication.service;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import com.commandAI.commandAI.modules.AIComunication.model.dto.GTPCommunicationRequestDTO;

import java.util.List;

public interface IGPTCommunicationService {
    GPTCommunication saveCommunication(GTPCommunicationRequestDTO dto);
    List<GTPCommunicationRequestDTO> findSimilarCommunications(float[] embedding);
}

# main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java

package com.commandAI.commandAI.modules.AIComunication.service.impl;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import com.commandAI.commandAI.modules.AIComunication.model.dto.GTPCommunicationRequestDTO;
import com.commandAI.commandAI.modules.AIComunication.repository.IGPTCommunicationRepository;
import com.commandAI.commandAI.modules.AIComunication.service.IGPTCommunicationService;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;

import java.time.LocalDateTime;
import java.util.Arrays;
import java.util.List;
import java.util.stream.Collectors;

@RequiredArgsConstructor
@Service
public class GPTCommunicationServiceImpl implements IGPTCommunicationService {

    private final IGPTCommunicationRepository repository;

    @Override
    public GPTCommunication saveCommunication(GTPCommunicationRequestDTO dto) {
        GPTCommunication communication = new GPTCommunication();
        communication.setQuestion(dto.question());
        communication.setQuestionEmbedding(dto.questionEmbedding());
        communication.setAnswer(dto.answer());
        communication.setAnswerEmbedding(dto.answerEmbedding());
        communication.setCreatedAt(LocalDateTime.now());
        return repository.save(communication);
    }

    @Override
    public List<GTPCommunicationRequestDTO> findSimilarCommunications(float[] embedding) {
        List<GPTCommunication> allCommunications = repository.findAll();

        return allCommunications.stream()
                .filter(comm -> isSimilar(embedding, comm.getQuestionEmbedding()))
                .map(comm -> new GTPCommunicationRequestDTO(
                        comm.getQuestion(),
                        comm.getQuestionEmbedding(),
                        comm.getAnswer(),
                        comm.getAnswerEmbedding()))
                .collect(Collectors.toList());
    }

    private boolean isSimilar(float[] embedding1, float[] embedding2) {
        // Verificar se os embeddings têm o mesmo tamanho
        if (embedding1.length != embedding2.length) {
            System.err.println("Os embeddings têm tamanhos diferentes.");
            return false;
        }

        System.out.println("Embedding1: " + Arrays.toString(embedding1));
        System.out.println("Embedding2: " + Arrays.toString(embedding2));

        double dotProduct = 0.0;
        double normA = 0.0;
        double normB = 0.0;
        for (int i = 0; i < embedding1.length; i++) {
            dotProduct += embedding1[i] * embedding2[i];
            normA += Math.pow(embedding1[i], 2);
            normB += Math.pow(embedding2[i], 2);
        }
        return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB)) > 0.8; // Ajuste o limiar conforme necessário
    }
}


# main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java

package com.commandAI.commandAI.modules.context.controller;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.context.service.IContextService;
import com.commandAI.commandAI.modules.context.model.context.Context;
import lombok.RequiredArgsConstructor;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.validation.annotation.Validated;
import org.springframework.web.bind.annotation.*;

import java.util.List;
import java.util.Map;

@RequiredArgsConstructor
@RestController
@Validated
@ResponseBody
@RequestMapping("/api/contexts")
public class ContextController {

    @Autowired
    private final IContextService contextService;

    @PostMapping("/save")
    public ResponseEntity<Map<String, Object>> saveContext(@RequestBody ContextDTO data) {
        Context newContext = contextService.saveContext(data);
        Map<String, Object> response = Map.of("result", newContext);
        return ResponseEntity.status(HttpStatus.CREATED).body(response);
    }


    @GetMapping("/last")
    public ResponseEntity<ContextDTO> getLastContext() {
        ContextDTO lastContext = contextService.getLastContext();
        return ResponseEntity.ok(lastContext);
    }

    @GetMapping("/all")
    public ResponseEntity<Map<String, Object>> getAllContexts() {
        List<Context> contexts = contextService.findAllContext();
        return ResponseEntity.ok(Map.of("contexts", contexts));
    }
}


# main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java

package com.commandAI.commandAI.modules.context.controller.hendler;

import com.commandAI.commandAI.modules.context.service.validation.ContextNotFoundException;
import lombok.Data;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.MissingServletRequestParameterException;
import org.springframework.web.bind.annotation.ControllerAdvice;
import org.springframework.web.bind.annotation.ExceptionHandler;
import org.springframework.web.context.request.WebRequest;

import java.time.LocalDateTime;
import java.util.HashMap;
import java.util.Map;

@Data
@ControllerAdvice
public class GlobalExceptionHandler {

    @ExceptionHandler(ContextNotFoundException.class)
    public ResponseEntity<Object> handleContextNotFoundException(ContextNotFoundException ex, WebRequest request) {
        Map<String, Object> body = new HashMap<>();
        body.put("timestamp", LocalDateTime.now());
        body.put("message", ex.getMessage());

        return new ResponseEntity<>(body, HttpStatus.NOT_FOUND);
    }
    @ExceptionHandler(MissingServletRequestParameterException.class)
    public ResponseEntity<Object> handleMissingServletRequestParameterException(MissingServletRequestParameterException ex, WebRequest request) {
        Map<String, Object> body = new HashMap<>();
        body.put("timestamp", LocalDateTime.now());
        body.put("message", "Parâmetro de requisição ausente: " + ex.getParameterName());

        return new ResponseEntity<>(body, HttpStatus.BAD_REQUEST);
    }

    // Outros handlers de exceção podem ser adicionados aqui
}

# main\java\com\commandAI\commandAI\modules\context\model\context\Context.java

    package com.commandAI.commandAI.modules.context.model.context;
    import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
    import jakarta.persistence.*;
    import lombok.Data;

    import java.time.LocalDateTime;
    @Data
    @Table(name = "tb_context")
    @Entity
    @JsonIgnoreProperties(ignoreUnknown = true)
    public class Context {

        @Id
        @GeneratedValue(strategy = GenerationType.IDENTITY)
        private Long id;

        @Column(name = "context", nullable = false, columnDefinition = "TEXT")
        private String context;

        @Column(name = "created_at", nullable = false)
        private LocalDateTime createdAt;

        @PrePersist
        protected void onCreate() {
            createdAt = LocalDateTime.now();
        }
    }



# main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java

package com.commandAI.commandAI.modules.context.model.dto;

import com.commandAI.commandAI.modules.context.model.context.Context;
import java.time.LocalDateTime;
public record ContextDTO(
        Long id,
        String context,
        LocalDateTime createdAt
) {
    public static ContextDTO fromEntity(Context context) {
        return new ContextDTO(context.getId(), context.getContext(), context.getCreatedAt());
    }
}


# main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java

package com.commandAI.commandAI.modules.context.repository;

import com.commandAI.commandAI.modules.context.model.context.Context;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import java.util.Optional;

@Repository
public interface IContextRepository extends JpaRepository<Context, Long> {
    Optional<Context> findTopByOrderByCreatedAtDesc();
}


# main\java\com\commandAI\commandAI\modules\context\service\IContextService.java

package com.commandAI.commandAI.modules.context.service;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;

import java.util.List;

public interface IContextService {
    Context saveContext(ContextDTO data);
    ContextDTO getLastContext();
    List<Context> findAllContext();
}


# main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java

package com.commandAI.commandAI.modules.context.service.impl;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.repository.IContextRepository;
import com.commandAI.commandAI.modules.context.service.IContextService;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.util.List;

@RequiredArgsConstructor
@Service
public class ContextServiceImpl implements IContextService {

    private final IContextRepository contextRepository;

    @Override
    @Transactional
    public Context saveContext(ContextDTO data) {
        Context context = new Context();
        context.setContext(data.context());
        context = contextRepository.save(context);
        return context;
    }

    @Override
    @Transactional
    public ContextDTO getLastContext() {
        Context lastContext = contextRepository.findTopByOrderByCreatedAtDesc()
                .orElseThrow(() -> new RuntimeException("No contexts found"));
        return ContextDTO.fromEntity(lastContext);
    }
    @Override
    @Transactional
    public List<Context> findAllContext() {
        return contextRepository.findAll();
    }
}


# main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java

package com.commandAI.commandAI.modules.context.service.validation;

public class ContextNotFoundException extends RuntimeException {
    public ContextNotFoundException(String message) {
        super(message);
    }
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java

package com.commandAI.commandAI.modules.embeddingTranscription.controller;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.service.IServiceTranscriptionEmbedded;
import lombok.RequiredArgsConstructor;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.List;

@RestController
@RequestMapping("/api/meetings")    
@RequiredArgsConstructor
public class TranscriptionController {

    private final IServiceTranscriptionEmbedded service;

    @PostMapping("/transcriptions")
    public ResponseEntity<Void> addTranscription(@RequestBody TranscriptionEmbeddedDTO transcriptionDTO) {
        service.saveTranscription(transcriptionDTO.transcriptionText(), transcriptionDTO.embedding());
        return ResponseEntity.status(HttpStatus.CREATED).build();
    }

    @GetMapping("/transcriptions")
    public List<TranscriptionEmbeddedDTO> getAllTranscriptions() {
        return service.getAllTranscriptions();
    }

    @GetMapping("/transcriptions/{id}")
    public ResponseEntity<TranscriptionEmbeddedDTO> getTranscriptionById(@PathVariable Long id) {
        TranscriptionEmbeddedDTO transcription = service.getTranscriptionById(id);
        if (transcription == null) {
            return ResponseEntity.status(HttpStatus.NOT_FOUND).build();
        }
        return ResponseEntity.ok(transcription);
    }

    @GetMapping("/transcriptions/similar")
    public List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(@RequestParam float[] queryEmbedding) {
        return service.findSimilarTranscriptions(queryEmbedding);
    }
    @GetMapping("/last")
    public ResponseEntity<TranscriptionEmbeddedDTO> getLastTranscription() {
        TranscriptionEmbeddedDTO lastTranscription = service.getLastTranscription();
        return ResponseEntity.ok(lastTranscription);
    }
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java

package com.commandAI.commandAI.modules.embeddingTranscription.model;

import jakarta.persistence.*;
import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import lombok.Data;

import java.time.LocalDateTime;

@Data
@JsonIgnoreProperties(ignoreUnknown = true)
@Entity
@Table(name = "transcriptionEmbedded")
public class TranscriptionEmbedded {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    @Column(name = "meeting_id", nullable = false)
    private Long meetingId;

    @Column(name = "transcription_text", nullable = false, columnDefinition = "TEXT")
    private String transcriptionText;

    @Column(name = "embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] embedding;

    @Column(name = "created_at", nullable = false)
    private LocalDateTime createdAt;

    @PrePersist
    protected void onCreate() {
        createdAt = LocalDateTime.now();
    }

}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java

package com.commandAI.commandAI.modules.embeddingTranscription.model.dto;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;

import java.time.LocalDateTime;

public record TranscriptionEmbeddedDTO(
        Long meetingId,
        String transcriptionText,
        float[] embedding,
        LocalDateTime createdAt
) {
    public static TranscriptionEmbeddedDTO fromEntity(TranscriptionEmbedded transcriptionEmbedded) {
        return new TranscriptionEmbeddedDTO(transcriptionEmbedded.getMeetingId(), transcriptionEmbedded.getTranscriptionText(), transcriptionEmbedded.getEmbedding(), transcriptionEmbedded.getCreatedAt());
    }
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java

package com.commandAI.commandAI.modules.embeddingTranscription.repository;

import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import java.util.Optional;

@Repository
public interface ITranscriptionEmbeddedRepository extends JpaRepository<TranscriptionEmbedded, Long> {
    Optional<TranscriptionEmbedded> findTopByOrderByCreatedAtDesc();

}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java

package com.commandAI.commandAI.modules.embeddingTranscription.service;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;

import java.util.List;

public interface IServiceTranscriptionEmbedded {
    void saveTranscription(String transcriptionText, float[] embedding);
    List<TranscriptionEmbeddedDTO> getAllTranscriptions();
    TranscriptionEmbeddedDTO getTranscriptionById(Long id);
    List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(float[] queryEmbedding);
    TranscriptionEmbeddedDTO getLastTranscription();
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java

package com.commandAI.commandAI.modules.embeddingTranscription.service.impl;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.repository.ITranscriptionEmbeddedRepository;
import com.commandAI.commandAI.modules.embeddingTranscription.service.IServiceTranscriptionEmbedded;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.util.List;
import java.util.stream.Collectors;

@RequiredArgsConstructor
@Service
public class ServiceTranscriptionEmbeddedImpl implements IServiceTranscriptionEmbedded {

    private final ITranscriptionEmbeddedRepository transcriptionEmbeddedRepository;

    @Override
    @Transactional
    public void saveTranscription(String transcriptionText, float[] embedding) {
        TranscriptionEmbedded transcription = new TranscriptionEmbedded();
        transcription.setTranscriptionText(transcriptionText);
        transcription.setEmbedding(embedding);
        System.out.println(transcription);
        transcriptionEmbeddedRepository.save(transcription);
    }

    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> getAllTranscriptions() {
        return transcriptionEmbeddedRepository.findAll().stream()
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }

    @Override
    @Transactional
    public TranscriptionEmbeddedDTO getTranscriptionById(Long id) {
        TranscriptionEmbedded transcription = transcriptionEmbeddedRepository.findById(id).orElse(null);
        if (transcription == null) {
            return null;
        }
        return new TranscriptionEmbeddedDTO(
                transcription.getMeetingId(),
                transcription.getTranscriptionText(),
                transcription.getEmbedding(),
                transcription.getCreatedAt());
    }

    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(float[] queryEmbedding) {
        List<TranscriptionEmbedded> allTranscriptions = transcriptionEmbeddedRepository.findAll();
        return allTranscriptions.stream()
                .sorted((t1, t2) -> Double.compare(
                        cosineSimilarity(queryEmbedding, t2.getEmbedding()),
                        cosineSimilarity(queryEmbedding, t1.getEmbedding())))
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }
    @Override
    @Transactional
    public TranscriptionEmbeddedDTO getLastTranscription() {
        TranscriptionEmbedded lastTranscription = transcriptionEmbeddedRepository.findTopByOrderByCreatedAtDesc()
                .orElseThrow(() -> new RuntimeException("No contexts found"));
        return TranscriptionEmbeddedDTO.fromEntity(lastTranscription);
    }
    private double cosineSimilarity(float[] vec1, float[] vec2) {
        double dotProduct = 0.0;
        double normA = 0.0;
        double normB = 0.0;
        for (int i = 0; i < vec1.length; i++) {
            dotProduct += vec1[i] * vec2[i];
            normA += Math.pow(vec1[i], 2);
            normB += Math.pow(vec2[i], 2);
        }
        return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
    }
}


# test\java\com\commandAI\commandAI\CommandAiApplicationTests.java

package com.commandAI.commandAI;

import org.junit.jupiter.api.Test;
import org.springframework.boot.test.context.SpringBootTest;

@SpringBootTest
class CommandAiApplicationTests {

	@Test
	void contextLoads() {
	}
}


# test\java\com\commandAI\commandAI\modelsTest\ContextTest.java

package com.commandAI.commandAI.modelsTest;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import com.commandAI.commandAI.modules.context.model.context.Context;
import java.time.LocalDateTime;

import static org.junit.jupiter.api.Assertions.*;

public class ContextTest {

    private Context context;

    @BeforeEach
    public void setUp() {
        context = new Context();
    }

    @Test
    public void testSetAndGetId() {
        Long id = 1L;
        context.setId(id);
        assertEquals(id, context.getId());
    }

    @Test
    public void testSetAndGetContext() {
        String testContext = "Sample context";
        context.setContext(testContext);
        assertEquals(testContext, context.getContext());
    }

    @Test
    public void testSetAndGetCreatedAt() {
        LocalDateTime now = LocalDateTime.now();
        context.setCreatedAt(now);
        assertEquals(now, context.getCreatedAt());
    }
}


User: resuma a minha aplicação JAVA
Assistant: Com certeza! Sua aplicação Java, "CommandAI", é um projeto Spring Boot que visa facilitar a comunicação com inteligência artificial (como o GPT), armazenando contextos e transcrições de forma inteligente. Aqui está um resumo dos principais componentes:

**Componentes Principais:**

*   **Módulos:**
    *   **AIComunication:** Lida com a comunicação com a IA, incluindo salvar as perguntas e respostas, gerar embeddings para o texto e encontrar comunicações similares com base nos embeddings.
    *   **Context:** Permite salvar e recuperar o contexto da conversa, mantendo o histórico das interações.
    *   **EmbeddingTranscription:** Gerencia as transcrições das reuniões com os respectivos "embeddings" (representações vetoriais) para facilitar a busca de transcrições similares.

*   **Entidades (Modelos):**

    *   `GPTCommunication`: Representa uma interação com a IA, armazenando a pergunta, a resposta e os "embeddings" vetoriais correspondentes.
    *   `Context`:  Representa um contexto de conversa.
    *   `TranscriptionEmbedded`: Representa uma transcrição de reunião com o seu embedding.

*   **Repositories:**
    *   Interfaces que estendem `JpaRepository` para interagir com o banco de dados para as entidades: `GPTCommunication`, `Context`, e `TranscriptionEmbedded`.

*   **Serviços:**
    *   Implementações das interfaces de serviço (ex: `GPTCommunicationServiceImpl`, `ContextServiceImpl`, `ServiceTranscriptionEmbeddedImpl`) que contêm a lógica de negócios para salvar, recuperar e encontrar informações relacionadas à comunicação com a IA, contextos e transcrições.

*   **Controladores (Controllers):**

    *   `GPTCommunicationController`:  Expõe endpoints REST para salvar as interações com a IA (`/api/question_answers`) e buscar interações similares.
    *   `ContextController`:  Gerencia os contextos de conversa, permitindo salvar, recuperar o último contexto e todos os contextos (`/api/contexts`).
    *   `TranscriptionController`:  Lida com as transcrições de reuniões, permitindo adicionar, recuperar, buscar transcrições similares e o último contexto (`/api/meetings`).

*   **DTOs (Data Transfer Objects):**

    *   `GTPCommunicationRequestDTO`, `ContextDTO`, `TranscriptionEmbeddedDTO`: Usados para transferir dados entre a aplicação e o mundo externo (ex: requisições e respostas da API).

**Funcionalidades:**

*   **Armazenamento de interações com a IA:** Salva as perguntas e respostas trocadas com a IA, junto com os "embeddings" vetoriais do texto.
*   **Busca de interações similares:** Encontra interações semelhantes com base nos "embeddings" vetoriais, permitindo reutilizar informações e contextos.
*   **Gerenciamento de contexto:** Permite salvar e recuperar o contexto da conversa, garantindo que a IA tenha o histórico necessário para responder adequadamente.
*   **Transcrições de reuniões:** Permite salvar as transcrições de reuniões e associá-las a "embeddings" vetoriais para facilitar a busca de reuniões relevantes.
*   **APIs REST:** Expõe APIs REST para facilitar a integração com outras aplicações e serviços.

**Em resumo:** A aplicação CommandAI é uma solução completa para gerenciar a comunicação com a IA, armazenando contextos e transcrições de forma inteligente, permitindo a busca de informações relevantes e a reutilização de conhecimento.
User: Estou implementando boas práticas de desenvolvimento JAVA ?
Assistant: Assistant: Sim, com base na estrutura e nos componentes da sua aplicação CommandAI, parece que você está seguindo muitas boas práticas de desenvolvimento Java e Spring Boot. Aqui está uma análise detalhada:

**Boas Práticas Implementadas:**

*   **Arquitetura em Camadas:** A aplicação está bem organizada em camadas distintas (Controladores, Serviços, Repositórios, Modelos), o que facilita a manutenção, o teste e a evolução do sistema.
*   **Uso do Spring Boot:** O uso do Spring Boot simplifica a configuração e o desenvolvimento, fornecendo "auto-configuração", "starter dependencies" e outras ferramentas que agilizam o processo.
*   **Injeção de Dependência (DI):** O uso de `@Autowired` e `@RequiredArgsConstructor` para injeção de dependência promove o baixo acoplamento e facilita o teste unitário.
*   **DTOs (Data Transfer Objects):** O uso de DTOs para transferir dados entre as camadas e expor dados na API é uma prática recomendada para desacoplar os modelos de dados internos da API.
*   **RESTful APIs:** A criação de APIs RESTful com controladores (Controllers) bem definidos e endpoints claros (ex: `/api/question_answers`, `/api/contexts`, `/api/meetings`) facilita a integração com outros sistemas.
*   **Validação:** O uso de `@Validated` nos controladores indica a preocupação com a validação dos dados de entrada.
*   **Tratamento de Exceções:** A implementação de um `GlobalExceptionHandler` para lidar com exceções de forma centralizada é uma boa prática para fornecer respostas de erro consistentes e informativas.
*   **Log:** O uso de `System.err.println` para log (enquanto não ideal) indica uma preocupação com o registro de eventos e erros para facilitar a depuração e o monitoramento.
*   **JPA e Spring Data JPA:** O uso de JPA (Java Persistence API) com Spring Data JPA simplifica o acesso ao banco de dados, fornecendo uma abstração sobre a camada de persistência.
*   **Embeddings:** A utilização de Embeddings é uma ótima estratégia para similaridade semântica.
*   **Testes:** A presença da classe `CommandAiApplicationTests` e `ContextTest` indica que você está escrevendo testes para sua aplicação, o que é fundamental para garantir a qualidade e a confiabilidade do código.
*   **Uso de Records:** A utilização de Records é excelente para DTOs.

**Sugestões de Melhorias:**

*   **Log:** Substitua `System.err.println` por um framework de log mais robusto, como SLF4J com Logback ou Log4j2. Isso permite configurar diferentes níveis de log, direcionar os logs para arquivos ou outros destinos, e adicionar informações contextuais aos logs.
*   **Tratamento de Exceções:**
    *   Crie classes de exceção personalizadas para representar diferentes tipos de erros na sua aplicação (ex: `AICommunicationException`, `ContextException`, `TranscriptionException`).
    *   Use códigos de status HTTP apropriados para cada tipo de erro.
    *   Forneça mensagens de erro claras e informativas para o cliente.
*   **Segurança:** Implemente mecanismos de autenticação e autorização para proteger suas APIs. Spring Security é uma ótima opção para isso.
*   **Documentação:** Use ferramentas como o Swagger (OpenAPI) para gerar automaticamente a documentação da sua API REST.
*   **Internacionalização (i18n):** Se sua aplicação precisar suportar diferentes idiomas, considere implementar a internacionalização.
*   **Monitoramento:** Integre ferramentas de monitoramento para coletar métricas sobre o desempenho da sua aplicação (ex: tempo de resposta da API, uso de recursos, etc.).
*   **Documentação Javadoc:** Adicione comentários Javadoc para documentar suas classes, métodos e parâmetros. Isso facilita a compreensão do código e a geração de documentação automática.
*   **Cobertura de Testes:** Busque uma alta cobertura de testes unitários e de integração.

**Em resumo:** Sua aplicação CommandAI parece estar bem estruturada e seguindo muitas boas práticas de desenvolvimento Java e Spring Boot. Ao implementar as sugestões de melhorias acima, você pode aumentar ainda mais a qualidade, a manutenibilidade e a segurança do seu sistema.

User: Preciso implementar a lógica para que um end point possa buscar reuniões com palavras chaves. Preciso implementar a lógica de salvar as reuniões em embeddings ?
2025-04-14 19:47:44,439 [INFO] Resposta da IA (texto): Com certeza! Aqui está um guia de como implementar a lógica para buscar reuniões por palavras-chave usando "embeddings" (representações vetoriais) e como salvar as reuniões em embeddings:

**1. Atualize a Entidade `TranscriptionEmbedded`:**

*   Adicione um campo para as palavras-chave (opcional, mas recomendado para buscas mais rápidas):

```java
package com.commandAI.commandAI.modules.embeddingTranscription.model;

import jakarta.persistence.*;
import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import lombok.Data;

import java.time.LocalDateTime;

@Data
@JsonIgnoreProperties(ignoreUnknown = true)
@Entity
@Table(name = "transcriptionEmbedded")
public class TranscriptionEmbedded {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    @Column(name = "meeting_id", nullable = false)
    private Long meetingId;

    @Column(name = "transcription_text", nullable = false, columnDefinition = "TEXT")
    private String transcriptionText;

    @Column(name = "embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] embedding;

    @Column(name = "keywords", columnDefinition = "TEXT") // Novo campo
    private String keywords;

    @Column(name = "created_at", nullable = false)
    private LocalDateTime createdAt;

    @PrePersist
    protected void onCreate() {
        createdAt = LocalDateTime.now();
    }

}
```

**2. Atualize o DTO `TranscriptionEmbeddedDTO`:**

*   Adicione o campo `keywords` ao DTO:

```java
package com.commandAI.commandAI.modules.embeddingTranscription.model.dto;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;

import java.time.LocalDateTime;

public record TranscriptionEmbeddedDTO(
        Long meetingId,
        String transcriptionText,
        float[] embedding,
        String keywords, // Novo campo
        LocalDateTime createdAt
) {
    public static TranscriptionEmbeddedDTO fromEntity(TranscriptionEmbedded transcriptionEmbedded) {
        return new TranscriptionEmbeddedDTO(transcriptionEmbedded.getMeetingId(), transcriptionEmbedded.getTranscriptionText(), transcriptionEmbedded.getEmbedding(), transcriptionEmbedded.getKeywords(), transcriptionEmbedded.getCreatedAt());
    }
}
```

**3. Atualize o Serviço `IServiceTranscriptionEmbedded`:**

*   Adicione um método para buscar transcrições por palavras-chave:

```java
package com.commandAI.commandAI.modules.embeddingTranscription.service;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;

import java.util.List;

public interface IServiceTranscriptionEmbedded {
    void saveTranscription(String transcriptionText, float[] embedding, String keywords); // Modifiquei
    List<TranscriptionEmbeddedDTO> getAllTranscriptions();
    TranscriptionEmbeddedDTO getTranscriptionById(Long id);
    List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(float[] queryEmbedding);
    TranscriptionEmbeddedDTO getLastTranscription();
    List<TranscriptionEmbeddedDTO> findByKeywords(String keywords);  // Novo método
}
```

**4. Atualize a Implementação do Serviço `ServiceTranscriptionEmbeddedImpl`:**

```java
package com.commandAI.commandAI.modules.embeddingTranscription.service.impl;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.repository.ITranscriptionEmbeddedRepository;
import com.commandAI.commandAI.modules.embeddingTranscription.service.IServiceTranscriptionEmbedded;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.util.Arrays;
import java.util.List;
import java.util.stream.Collectors;

@RequiredArgsConstructor
@Service
public class ServiceTranscriptionEmbeddedImpl implements IServiceTranscriptionEmbedded {

    private final ITranscriptionEmbeddedRepository transcriptionEmbeddedRepository;

    @Override
    @Transactional
    public void saveTranscription(String transcriptionText, float[] embedding, String keywords) {  // Modifiquei
        TranscriptionEmbedded transcription = new TranscriptionEmbedded();
        transcription.setTranscriptionText(transcriptionText);
        transcription.setEmbedding(embedding);
        transcription.setKeywords(keywords); // Adicionei
        System.out.println(transcription);
        transcriptionEmbeddedRepository.save(transcription);
    }

    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> getAllTranscriptions() {
        return transcriptionEmbeddedRepository.findAll().stream()
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                        transcription.getKeywords(), // Adicionei
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }

    @Override
    @Transactional
    public TranscriptionEmbeddedDTO getTranscriptionById(Long id) {
        TranscriptionEmbedded transcription = transcriptionEmbeddedRepository.findById(id).orElse(null);
        if (transcription == null) {
            return null;
        }
        return new TranscriptionEmbeddedDTO(
                transcription.getMeetingId(),
                transcription.getTranscriptionText(),
                transcription.getEmbedding(),
                transcription.getKeywords(), // Adicionei
                transcription.getCreatedAt());
    }

    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(float[] queryEmbedding) {
        List<TranscriptionEmbedded> allTranscriptions = transcriptionEmbeddedRepository.findAll();
        return allTranscriptions.stream()
                .sorted((t1, t2) -> Double.compare(
                        cosineSimilarity(queryEmbedding, t2.getEmbedding()),
                        cosineSimilarity(queryEmbedding, t1.getEmbedding())))
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                         transcription.getKeywords(), // Adicionei
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }

    @Override
    @Transactional
    public TranscriptionEmbeddedDTO getLastTranscription() {
        TranscriptionEmbedded lastTranscription = transcriptionEmbeddedRepository.findTopByOrderByCreatedAtDesc()
                .orElseThrow(() -> new RuntimeException("No contexts found"));
        return TranscriptionEmbeddedDTO.fromEntity(lastTranscription);
    }

    private double cosineSimilarity(float[] vec1, float[] vec2) {
        double dotProduct = 0.0;
        double normA = 0.0;
        double normB = 0.0;
        for (int i = 0; i < vec1.length; i++) {
            dotProduct += vec1[i] * vec2[i];
            normA += Math.pow(vec1[i], 2);
            normB += Math.pow(vec2[i], 2);
        }
        return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
    }

    // Implementação para buscar por palavras-chave
    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> findByKeywords(String keywords) {
        //String[] keywordsArray = keywords.toLowerCase().split("\\s+");  // Divide as palavras-chave
        return transcriptionEmbeddedRepository.findAll().stream()
        .filter(transcription -> transcription.getTranscriptionText().toLowerCase().contains(keywords.toLowerCase()))
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                        transcription.getKeywords(), // Adicionei
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }
}
```

**5. Atualize o Repositório `ITranscriptionEmbeddedRepository`:**

*   Você não precisa modificar o repositório, pois estamos filtrando com JAVA. Caso queira fazer uma query no banco de dados:

```java
package com.commandAI.commandAI.modules.embeddingTranscription.repository;

import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import java.util.List;
import java.util.Optional;

@Repository
public interface ITranscriptionEmbeddedRepository extends JpaRepository<TranscriptionEmbedded, Long> {
    Optional<TranscriptionEmbedded> findTopByOrderByCreatedAtDesc();
    List<TranscriptionEmbedded> findByKeywordsContainingIgnoreCase(String keywords); // Novo método
}
```

**6. Atualize o Controlador `TranscriptionController`:**

*   Adicione o novo endpoint para buscar por palavras-chave:

```java
package com.commandAI.commandAI.modules.embeddingTranscription.controller;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.service.IServiceTranscriptionEmbedded;
import lombok.RequiredArgsConstructor;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.List;

@RestController
@RequestMapping("/api/meetings")    
@RequiredArgsConstructor
public class TranscriptionController {

    private final IServiceTranscriptionEmbedded service;

    @PostMapping("/transcriptions")
    public ResponseEntity<Void> addTranscription(@RequestBody TranscriptionEmbeddedDTO transcriptionDTO) {
        service.saveTranscription(transcriptionDTO.transcriptionText(), transcriptionDTO.embedding(), transcriptionDTO.keywords()); // Modifiquei
        return ResponseEntity.status(HttpStatus.CREATED).build();
    }

    @GetMapping("/transcriptions")
    public List<TranscriptionEmbeddedDTO> getAllTranscriptions() {
        return service.getAllTranscriptions();
    }

    @GetMapping("/transcriptions/{id}")
    public ResponseEntity<TranscriptionEmbeddedDTO> getTranscriptionById(@PathVariable Long id) {
        TranscriptionEmbeddedDTO transcription = service.getTranscriptionById(id);
        if (transcription == null) {
            return ResponseEntity.status(HttpStatus.NOT_FOUND).build();
        }
        return ResponseEntity.ok(transcription);
    }

    @GetMapping("/transcriptions/similar")
    public List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(@RequestParam float[] queryEmbedding) {
        return service.findSimilarTranscriptions(queryEmbedding);
    }
    @GetMapping("/last")
    public ResponseEntity<TranscriptionEmbeddedDTO> getLastTranscription() {
        TranscriptionEmbeddedDTO lastTranscription = service.getLastTranscription();
        return ResponseEntity.ok(lastTranscription);
    }

    @GetMapping("/transcriptions/search") // Novo endpoint
    public List<TranscriptionEmbeddedDTO> searchTranscriptions(@RequestParam String keywords) {
        return service.findByKeywords(keywords);
    }
}
```

**7. Lógica para Salvar Reuniões com Embeddings e Palavras-Chave:**

*   **Obtenha a Transcrição:**  Primeiro, você precisa obter a transcrição da reunião (texto).
*   **Gere o Embedding:** Use um modelo de linguagem (como o Gemini) para gerar o "embedding" vetorial da transcrição.
*   **Extraia Palavras-Chave:** Use um algoritmo de extração de palavras-chave para identificar os termos mais relevantes na transcrição.
*   **Salve no Banco de Dados:** Salve a transcrição, o embedding e as palavras-chave no banco de dados.

**Exemplo de como salvar (adaptado para o seu código):**

```java
// Supondo que você tenha a transcrição, o embedding e as palavras-chave
String transcriptionText = "Transcrição da reunião...";
float[] embedding = // ... seu embedding gerado ...
String keywords = "palavras-chave extraídas...";

// Crie um DTO
TranscriptionEmbeddedDTO transcriptionDTO = new TranscriptionEmbeddedDTO(
    null, // ID será gerado automaticamente
    transcriptionText,
    embedding,
    keywords,
    null // Data será gerada automaticamente
);

// Salve a transcrição
service.saveTranscription(transcriptionDTO.transcriptionText(), transcriptionDTO.embedding(), transcriptionDTO.keywords());
```

**Pontos Importantes:**

*   **Extração de Palavras-Chave:** Você pode usar bibliotecas como Apache OpenNLP, Stanford CoreNLP ou até mesmo APIs online para extrair palavras-chave.
*   **Geração de Embeddings:**  Use a API do Gemini para gerar embeddings para cada frase.
*   **Banco de Dados:** Certifique-se de que o tipo de dados `FLOAT8[]` é suportado pelo seu banco de dados.
*   **Performance:** Se você tiver muitas transcrições, considere usar técnicas de indexação para acelerar as buscas.

Este guia fornece um ponto de partida para implementar a busca de reuniões por palavras-chave e o armazenamento de reuniões com embeddings. Adapte o código e as técnicas às suas necessidades específicas.
2025-04-14 19:47:44,588 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 19:47:44,590 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 19:47:44,592 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 19:47:44,594 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 19:47:44,598 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 19:47:44,600 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 19:47:44,603 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 19:47:44,605 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 19:47:44,607 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 19:47:44,610 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 19:47:44,612 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 19:47:44,614 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 19:47:44,616 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 19:47:44,618 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 19:47:44,620 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 19:47:44,622 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 19:47:44,624 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 19:47:44,626 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 19:47:44,628 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 19:47:44,631 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 19:47:44,633 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 19:47:44,636 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 19:47:44,638 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 19:47:44,641 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 19:47:44,644 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 19:47:44,647 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 19:47:44,649 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 19:47:44,650 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 19:47:44,652 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 19:47:44,654 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 19:47:44,657 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 19:47:44,659 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 19:47:44,661 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 19:47:44,663 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 19:47:44,665 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 19:47:44,668 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 22:39:31,765 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 22:39:31,774 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 22:39:31,777 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 22:39:31,779 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 22:39:31,782 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 22:39:31,784 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 22:39:31,787 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 22:39:31,789 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 22:39:31,791 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 22:39:31,793 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 22:39:31,794 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 22:39:31,798 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 22:39:31,799 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 22:39:31,801 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 22:39:31,803 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 22:39:31,805 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 22:39:31,806 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 22:39:31,808 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 22:39:31,809 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 22:39:31,810 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 22:39:31,812 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 22:39:31,815 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 22:39:31,817 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 22:39:31,819 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 22:39:31,820 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 22:39:31,822 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 22:39:31,824 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 22:39:31,825 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 22:39:31,828 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 22:39:31,832 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 22:39:31,833 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 22:39:31,835 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 22:39:31,837 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 22:39:31,839 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 22:39:31,841 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 22:39:31,843 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 22:39:32,347 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 22:39:32,350 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 22:39:32,352 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 22:39:32,353 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 22:39:32,355 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 22:39:32,356 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 22:39:32,357 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 22:39:32,359 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 22:39:32,360 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 22:39:32,362 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 22:39:32,363 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 22:39:32,365 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 22:39:32,366 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 22:39:32,367 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 22:39:32,369 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 22:39:32,370 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 22:39:32,372 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 22:39:32,373 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 22:39:32,374 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 22:39:32,375 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 22:39:32,376 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 22:39:32,377 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 22:39:32,378 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 22:39:32,380 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 22:39:32,382 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 22:39:32,384 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 22:39:32,386 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 22:39:32,387 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 22:39:32,389 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 22:39:32,391 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 22:39:32,393 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 22:39:32,394 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 22:39:32,395 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 22:39:32,398 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 22:39:32,400 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 22:39:32,401 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 22:39:46,239 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 22:39:46,240 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 22:39:46,241 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 22:39:46,242 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 22:39:46,244 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 22:39:46,245 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 22:39:46,246 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 22:39:46,247 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 22:39:46,248 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 22:39:46,250 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 22:39:46,251 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 22:39:46,252 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 22:39:46,254 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 22:39:46,255 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 22:39:46,256 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 22:39:46,258 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 22:39:46,260 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 22:39:46,261 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 22:39:46,263 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 22:39:46,264 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 22:39:46,266 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 22:39:46,268 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 22:39:46,270 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 22:39:46,271 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 22:39:46,272 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 22:39:46,273 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 22:39:46,274 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 22:39:46,274 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 22:39:46,275 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 22:39:46,276 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 22:39:46,277 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 22:39:46,278 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 22:39:46,279 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 22:39:46,281 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 22:39:46,284 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 22:39:46,285 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 22:39:46,397 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 22:39:46,399 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 22:39:46,400 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 22:39:46,401 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 22:39:46,402 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 22:39:46,404 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 22:39:46,406 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 22:39:46,408 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 22:39:46,410 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 22:39:46,412 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 22:39:46,413 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 22:39:46,415 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 22:39:46,417 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 22:39:46,418 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 22:39:46,419 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 22:39:46,421 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 22:39:46,422 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 22:39:46,424 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 22:39:46,425 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 22:39:46,426 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 22:39:46,427 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 22:39:46,429 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 22:39:46,431 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 22:39:46,432 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 22:39:46,433 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 22:39:46,435 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 22:39:46,436 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 22:39:46,437 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 22:39:46,438 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 22:39:46,440 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 22:39:46,441 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 22:39:46,442 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 22:39:46,443 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 22:39:46,444 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 22:39:46,446 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 22:39:46,447 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
2025-04-14 22:39:46,461 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\API_CommandAI\assets\20250414223946_clipboard_20250414223931.png, Prompt: Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python_e_java

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python_e_java()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python_e_java() -> str:
    """Lê todo o conteúdo de todos os arquivos .py e .java a partir de src/"""
    src_dir = Config.BASE_DIR

    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    # Busca arquivos .py e .java separadamente
    padrao_busca_py = os.path.join(src_dir, '**', '*.py')
    padrao_busca_java = os.path.join(src_dir, '**', '*.java')

    arquivos_py = glob.glob(padrao_busca_py, recursive=True)
    arquivos_java = glob.glob(padrao_busca_java, recursive=True)

    arquivos = arquivos_py + arquivos_java
    arquivos.sort() # Ordena a lista completa

    for arquivo in arquivos:
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# main\java\com\commandAI\commandAI\CommandAiApplication.java

package com.commandAI.commandAI;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class CommandAiApplication {

	public static void main(String[] args) {
		SpringApplication.run(CommandAiApplication.class, args);
	}

}


# main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java

package com.commandAI.commandAI.modules.AIComunication.controller;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import com.commandAI.commandAI.modules.AIComunication.model.dto.GTPCommunicationRequestDTO;
import com.commandAI.commandAI.modules.AIComunication.service.IGPTCommunicationService;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.RequiredArgsConstructor;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.Arrays;
import java.util.List;

@RequiredArgsConstructor
@RestController
@RequestMapping("/api/question_answers")
public class GPTCommunicationController {

    private final IGPTCommunicationService service;

    @PostMapping("/save")
    public ResponseEntity<GPTCommunication> saveCommunication(@RequestBody GTPCommunicationRequestDTO dto) {
        try {
            GPTCommunication savedCommunication = service.saveCommunication(dto);
            return new ResponseEntity<>(savedCommunication, HttpStatus.CREATED);
        } catch (Exception e) {
            // Log error and return an appropriate response
            System.err.println("Erro ao salvar comunicação: " + e.getMessage());
            return new ResponseEntity<>(HttpStatus.INTERNAL_SERVER_ERROR);
        }
    }

    @PostMapping("/similar")
    public ResponseEntity<List<GTPCommunicationRequestDTO>> getSimilarCommunications(@RequestBody JsonNode requestBody) {
        try {
            // Validar se é um array
            if (!requestBody.isArray()) {
                throw new IllegalArgumentException("Esperado um array");
            }

            // Converter JsonNode para array de float
            float[] embedding = new ObjectMapper().convertValue(requestBody, float[].class);

            // Log para verificar o embedding recebido
            System.out.println("Embedding recebido: " + Arrays.toString(embedding));

            // Chamar o serviço com o array de float
            List<GTPCommunicationRequestDTO> similarCommunications = service.findSimilarCommunications(embedding);
            return ResponseEntity.ok(similarCommunications);
        } catch (IllegalArgumentException e) {
            System.err.println("Erro de validação: " + e.getMessage());
            return ResponseEntity.status(HttpStatus.BAD_REQUEST).body(null);
        } catch (Exception e) {
            // Log error and return an appropriate response
            System.err.println("Erro ao processar a solicitação: " + e.getMessage());
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(null);
        }
    }
}


# main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java

package com.commandAI.commandAI.modules.AIComunication.model;

import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import jakarta.persistence.*;
import lombok.Data;

import java.time.LocalDateTime;
@Data
@Entity
@JsonIgnoreProperties(ignoreUnknown = true)
@Table(name = "gpt_communication")
public class GPTCommunication {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(name = "question", nullable = false, columnDefinition = "TEXT")
    private String question;

    @Column(name = "answer", nullable = false, columnDefinition = "TEXT")
    private String answer;

    @Column(name = "question_embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] questionEmbedding;

    @Column(name = "answer_embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] answerEmbedding;

    @Column(name = "created_at", nullable = false)
    private LocalDateTime createdAt;

}

# main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java

package com.commandAI.commandAI.modules.AIComunication.model.dto;

import com.fasterxml.jackson.annotation.JsonProperty;

public record GTPCommunicationRequestDTO(
        String question,
        float[] questionEmbedding,
        String answer,
        float[] answerEmbedding
) {}



# main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java

package com.commandAI.commandAI.modules.AIComunication.repository;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

@Repository
public interface IGPTCommunicationRepository extends JpaRepository<GPTCommunication, Long> {

}

# main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java

package com.commandAI.commandAI.modules.AIComunication.service;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import com.commandAI.commandAI.modules.AIComunication.model.dto.GTPCommunicationRequestDTO;

import java.util.List;

public interface IGPTCommunicationService {
    GPTCommunication saveCommunication(GTPCommunicationRequestDTO dto);
    List<GTPCommunicationRequestDTO> findSimilarCommunications(float[] embedding);
}

# main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java

package com.commandAI.commandAI.modules.AIComunication.service.impl;

import com.commandAI.commandAI.modules.AIComunication.model.GPTCommunication;
import com.commandAI.commandAI.modules.AIComunication.model.dto.GTPCommunicationRequestDTO;
import com.commandAI.commandAI.modules.AIComunication.repository.IGPTCommunicationRepository;
import com.commandAI.commandAI.modules.AIComunication.service.IGPTCommunicationService;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;

import java.time.LocalDateTime;
import java.util.Arrays;
import java.util.List;
import java.util.stream.Collectors;

@RequiredArgsConstructor
@Service
public class GPTCommunicationServiceImpl implements IGPTCommunicationService {

    private final IGPTCommunicationRepository repository;

    @Override
    public GPTCommunication saveCommunication(GTPCommunicationRequestDTO dto) {
        GPTCommunication communication = new GPTCommunication();
        communication.setQuestion(dto.question());
        communication.setQuestionEmbedding(dto.questionEmbedding());
        communication.setAnswer(dto.answer());
        communication.setAnswerEmbedding(dto.answerEmbedding());
        communication.setCreatedAt(LocalDateTime.now());
        return repository.save(communication);
    }

    @Override
    public List<GTPCommunicationRequestDTO> findSimilarCommunications(float[] embedding) {
        List<GPTCommunication> allCommunications = repository.findAll();

        return allCommunications.stream()
                .filter(comm -> isSimilar(embedding, comm.getQuestionEmbedding()))
                .map(comm -> new GTPCommunicationRequestDTO(
                        comm.getQuestion(),
                        comm.getQuestionEmbedding(),
                        comm.getAnswer(),
                        comm.getAnswerEmbedding()))
                .collect(Collectors.toList());
    }

    private boolean isSimilar(float[] embedding1, float[] embedding2) {
        // Verificar se os embeddings têm o mesmo tamanho
        if (embedding1.length != embedding2.length) {
            System.err.println("Os embeddings têm tamanhos diferentes.");
            return false;
        }

        System.out.println("Embedding1: " + Arrays.toString(embedding1));
        System.out.println("Embedding2: " + Arrays.toString(embedding2));

        double dotProduct = 0.0;
        double normA = 0.0;
        double normB = 0.0;
        for (int i = 0; i < embedding1.length; i++) {
            dotProduct += embedding1[i] * embedding2[i];
            normA += Math.pow(embedding1[i], 2);
            normB += Math.pow(embedding2[i], 2);
        }
        return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB)) > 0.8; // Ajuste o limiar conforme necessário
    }
}


# main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java

package com.commandAI.commandAI.modules.context.controller;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.context.service.IContextService;
import com.commandAI.commandAI.modules.context.model.context.Context;
import lombok.RequiredArgsConstructor;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.validation.annotation.Validated;
import org.springframework.web.bind.annotation.*;

import java.util.List;
import java.util.Map;

@RequiredArgsConstructor
@RestController
@Validated
@ResponseBody
@RequestMapping("/api/contexts")
public class ContextController {

    @Autowired
    private final IContextService contextService;

    @PostMapping("/save")
    public ResponseEntity<Map<String, Object>> saveContext(@RequestBody ContextDTO data) {
        Context newContext = contextService.saveContext(data);
        Map<String, Object> response = Map.of("result", newContext);
        return ResponseEntity.status(HttpStatus.CREATED).body(response);
    }


    @GetMapping("/last")
    public ResponseEntity<ContextDTO> getLastContext() {
        ContextDTO lastContext = contextService.getLastContext();
        return ResponseEntity.ok(lastContext);
    }

    @GetMapping("/all")
    public ResponseEntity<Map<String, Object>> getAllContexts() {
        List<Context> contexts = contextService.findAllContext();
        return ResponseEntity.ok(Map.of("contexts", contexts));
    }
}


# main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java

package com.commandAI.commandAI.modules.context.controller.hendler;

import com.commandAI.commandAI.modules.context.service.validation.ContextNotFoundException;
import lombok.Data;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.MissingServletRequestParameterException;
import org.springframework.web.bind.annotation.ControllerAdvice;
import org.springframework.web.bind.annotation.ExceptionHandler;
import org.springframework.web.context.request.WebRequest;

import java.time.LocalDateTime;
import java.util.HashMap;
import java.util.Map;

@Data
@ControllerAdvice
public class GlobalExceptionHandler {

    @ExceptionHandler(ContextNotFoundException.class)
    public ResponseEntity<Object> handleContextNotFoundException(ContextNotFoundException ex, WebRequest request) {
        Map<String, Object> body = new HashMap<>();
        body.put("timestamp", LocalDateTime.now());
        body.put("message", ex.getMessage());

        return new ResponseEntity<>(body, HttpStatus.NOT_FOUND);
    }
    @ExceptionHandler(MissingServletRequestParameterException.class)
    public ResponseEntity<Object> handleMissingServletRequestParameterException(MissingServletRequestParameterException ex, WebRequest request) {
        Map<String, Object> body = new HashMap<>();
        body.put("timestamp", LocalDateTime.now());
        body.put("message", "Parâmetro de requisição ausente: " + ex.getParameterName());

        return new ResponseEntity<>(body, HttpStatus.BAD_REQUEST);
    }

    // Outros handlers de exceção podem ser adicionados aqui
}

# main\java\com\commandAI\commandAI\modules\context\model\context\Context.java

    package com.commandAI.commandAI.modules.context.model.context;
    import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
    import jakarta.persistence.*;
    import lombok.Data;

    import java.time.LocalDateTime;
    @Data
    @Table(name = "tb_context")
    @Entity
    @JsonIgnoreProperties(ignoreUnknown = true)
    public class Context {

        @Id
        @GeneratedValue(strategy = GenerationType.IDENTITY)
        private Long id;

        @Column(name = "context", nullable = false, columnDefinition = "TEXT")
        private String context;

        @Column(name = "created_at", nullable = false)
        private LocalDateTime createdAt;

        @PrePersist
        protected void onCreate() {
            createdAt = LocalDateTime.now();
        }
    }



# main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java

package com.commandAI.commandAI.modules.context.model.dto;

import com.commandAI.commandAI.modules.context.model.context.Context;
import java.time.LocalDateTime;
public record ContextDTO(
        Long id,
        String context,
        LocalDateTime createdAt
) {
    public static ContextDTO fromEntity(Context context) {
        return new ContextDTO(context.getId(), context.getContext(), context.getCreatedAt());
    }
}


# main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java

package com.commandAI.commandAI.modules.context.repository;

import com.commandAI.commandAI.modules.context.model.context.Context;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import java.util.Optional;

@Repository
public interface IContextRepository extends JpaRepository<Context, Long> {
    Optional<Context> findTopByOrderByCreatedAtDesc();
}


# main\java\com\commandAI\commandAI\modules\context\service\IContextService.java

package com.commandAI.commandAI.modules.context.service;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;

import java.util.List;

public interface IContextService {
    Context saveContext(ContextDTO data);
    ContextDTO getLastContext();
    List<Context> findAllContext();
}


# main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java

package com.commandAI.commandAI.modules.context.service.impl;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.repository.IContextRepository;
import com.commandAI.commandAI.modules.context.service.IContextService;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.util.List;

@RequiredArgsConstructor
@Service
public class ContextServiceImpl implements IContextService {

    private final IContextRepository contextRepository;

    @Override
    @Transactional
    public Context saveContext(ContextDTO data) {
        Context context = new Context();
        context.setContext(data.context());
        context = contextRepository.save(context);
        return context;
    }

    @Override
    @Transactional
    public ContextDTO getLastContext() {
        Context lastContext = contextRepository.findTopByOrderByCreatedAtDesc()
                .orElseThrow(() -> new RuntimeException("No contexts found"));
        return ContextDTO.fromEntity(lastContext);
    }
    @Override
    @Transactional
    public List<Context> findAllContext() {
        return contextRepository.findAll();
    }
}


# main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java

package com.commandAI.commandAI.modules.context.service.validation;

public class ContextNotFoundException extends RuntimeException {
    public ContextNotFoundException(String message) {
        super(message);
    }
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java

package com.commandAI.commandAI.modules.embeddingTranscription.controller;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.service.IServiceTranscriptionEmbedded;
import lombok.RequiredArgsConstructor;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.List;

@RestController
@RequestMapping("/api/meetings")    
@RequiredArgsConstructor
public class TranscriptionController {

    private final IServiceTranscriptionEmbedded service;

    @PostMapping("/transcriptions")
    public ResponseEntity<Void> addTranscription(@RequestBody TranscriptionEmbeddedDTO transcriptionDTO) {
        service.saveTranscription(transcriptionDTO.transcriptionText(), transcriptionDTO.embedding());
        return ResponseEntity.status(HttpStatus.CREATED).build();
    }

    @GetMapping("/transcriptions")
    public List<TranscriptionEmbeddedDTO> getAllTranscriptions() {
        return service.getAllTranscriptions();
    }

    @GetMapping("/transcriptions/{id}")
    public ResponseEntity<TranscriptionEmbeddedDTO> getTranscriptionById(@PathVariable Long id) {
        TranscriptionEmbeddedDTO transcription = service.getTranscriptionById(id);
        if (transcription == null) {
            return ResponseEntity.status(HttpStatus.NOT_FOUND).build();
        }
        return ResponseEntity.ok(transcription);
    }

    @GetMapping("/transcriptions/similar")
    public List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(@RequestParam float[] queryEmbedding) {
        return service.findSimilarTranscriptions(queryEmbedding);
    }
    @GetMapping("/last")
    public ResponseEntity<TranscriptionEmbeddedDTO> getLastTranscription() {
        TranscriptionEmbeddedDTO lastTranscription = service.getLastTranscription();
        return ResponseEntity.ok(lastTranscription);
    }
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java

package com.commandAI.commandAI.modules.embeddingTranscription.model;

import jakarta.persistence.*;
import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import lombok.Data;

import java.time.LocalDateTime;

@Data
@JsonIgnoreProperties(ignoreUnknown = true)
@Entity
@Table(name = "transcriptionEmbedded")
public class TranscriptionEmbedded {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    @Column(name = "meeting_id", nullable = false)
    private Long meetingId;

    @Column(name = "transcription_text", nullable = false, columnDefinition = "TEXT")
    private String transcriptionText;

    @Column(name = "embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] embedding;

    @Column(name = "created_at", nullable = false)
    private LocalDateTime createdAt;

    @PrePersist
    protected void onCreate() {
        createdAt = LocalDateTime.now();
    }

}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java

package com.commandAI.commandAI.modules.embeddingTranscription.model.dto;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;

import java.time.LocalDateTime;

public record TranscriptionEmbeddedDTO(
        Long meetingId,
        String transcriptionText,
        float[] embedding,
        LocalDateTime createdAt
) {
    public static TranscriptionEmbeddedDTO fromEntity(TranscriptionEmbedded transcriptionEmbedded) {
        return new TranscriptionEmbeddedDTO(transcriptionEmbedded.getMeetingId(), transcriptionEmbedded.getTranscriptionText(), transcriptionEmbedded.getEmbedding(), transcriptionEmbedded.getCreatedAt());
    }
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java

package com.commandAI.commandAI.modules.embeddingTranscription.repository;

import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import java.util.Optional;

@Repository
public interface ITranscriptionEmbeddedRepository extends JpaRepository<TranscriptionEmbedded, Long> {
    Optional<TranscriptionEmbedded> findTopByOrderByCreatedAtDesc();

}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java

package com.commandAI.commandAI.modules.embeddingTranscription.service;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;

import java.util.List;

public interface IServiceTranscriptionEmbedded {
    void saveTranscription(String transcriptionText, float[] embedding);
    List<TranscriptionEmbeddedDTO> getAllTranscriptions();
    TranscriptionEmbeddedDTO getTranscriptionById(Long id);
    List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(float[] queryEmbedding);
    TranscriptionEmbeddedDTO getLastTranscription();
}


# main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java

package com.commandAI.commandAI.modules.embeddingTranscription.service.impl;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.repository.ITranscriptionEmbeddedRepository;
import com.commandAI.commandAI.modules.embeddingTranscription.service.IServiceTranscriptionEmbedded;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.util.List;
import java.util.stream.Collectors;

@RequiredArgsConstructor
@Service
public class ServiceTranscriptionEmbeddedImpl implements IServiceTranscriptionEmbedded {

    private final ITranscriptionEmbeddedRepository transcriptionEmbeddedRepository;

    @Override
    @Transactional
    public void saveTranscription(String transcriptionText, float[] embedding) {
        TranscriptionEmbedded transcription = new TranscriptionEmbedded();
        transcription.setTranscriptionText(transcriptionText);
        transcription.setEmbedding(embedding);
        System.out.println(transcription);
        transcriptionEmbeddedRepository.save(transcription);
    }

    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> getAllTranscriptions() {
        return transcriptionEmbeddedRepository.findAll().stream()
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }

    @Override
    @Transactional
    public TranscriptionEmbeddedDTO getTranscriptionById(Long id) {
        TranscriptionEmbedded transcription = transcriptionEmbeddedRepository.findById(id).orElse(null);
        if (transcription == null) {
            return null;
        }
        return new TranscriptionEmbeddedDTO(
                transcription.getMeetingId(),
                transcription.getTranscriptionText(),
                transcription.getEmbedding(),
                transcription.getCreatedAt());
    }

    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(float[] queryEmbedding) {
        List<TranscriptionEmbedded> allTranscriptions = transcriptionEmbeddedRepository.findAll();
        return allTranscriptions.stream()
                .sorted((t1, t2) -> Double.compare(
                        cosineSimilarity(queryEmbedding, t2.getEmbedding()),
                        cosineSimilarity(queryEmbedding, t1.getEmbedding())))
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }
    @Override
    @Transactional
    public TranscriptionEmbeddedDTO getLastTranscription() {
        TranscriptionEmbedded lastTranscription = transcriptionEmbeddedRepository.findTopByOrderByCreatedAtDesc()
                .orElseThrow(() -> new RuntimeException("No contexts found"));
        return TranscriptionEmbeddedDTO.fromEntity(lastTranscription);
    }
    private double cosineSimilarity(float[] vec1, float[] vec2) {
        double dotProduct = 0.0;
        double normA = 0.0;
        double normB = 0.0;
        for (int i = 0; i < vec1.length; i++) {
            dotProduct += vec1[i] * vec2[i];
            normA += Math.pow(vec1[i], 2);
            normB += Math.pow(vec2[i], 2);
        }
        return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
    }
}


# test\java\com\commandAI\commandAI\CommandAiApplicationTests.java

package com.commandAI.commandAI;

import org.junit.jupiter.api.Test;
import org.springframework.boot.test.context.SpringBootTest;

@SpringBootTest
class CommandAiApplicationTests {

	@Test
	void contextLoads() {
	}
}


# test\java\com\commandAI\commandAI\modelsTest\ContextTest.java

package com.commandAI.commandAI.modelsTest;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import com.commandAI.commandAI.modules.context.model.context.Context;
import java.time.LocalDateTime;

import static org.junit.jupiter.api.Assertions.*;

public class ContextTest {

    private Context context;

    @BeforeEach
    public void setUp() {
        context = new Context();
    }

    @Test
    public void testSetAndGetId() {
        Long id = 1L;
        context.setId(id);
        assertEquals(id, context.getId());
    }

    @Test
    public void testSetAndGetContext() {
        String testContext = "Sample context";
        context.setContext(testContext);
        assertEquals(testContext, context.getContext());
    }

    @Test
    public void testSetAndGetCreatedAt() {
        LocalDateTime now = LocalDateTime.now();
        context.setCreatedAt(now);
        assertEquals(now, context.getCreatedAt());
    }
}


User: resuma a minha aplicação JAVA
Assistant: Com certeza! Sua aplicação Java, "CommandAI", é um projeto Spring Boot que visa facilitar a comunicação com inteligência artificial (como o GPT), armazenando contextos e transcrições de forma inteligente. Aqui está um resumo dos principais componentes:

**Componentes Principais:**

*   **Módulos:**
    *   **AIComunication:** Lida com a comunicação com a IA, incluindo salvar as perguntas e respostas, gerar embeddings para o texto e encontrar comunicações similares com base nos embeddings.
    *   **Context:** Permite salvar e recuperar o contexto da conversa, mantendo o histórico das interações.
    *   **EmbeddingTranscription:** Gerencia as transcrições das reuniões com os respectivos "embeddings" (representações vetoriais) para facilitar a busca de transcrições similares.

*   **Entidades (Modelos):**

    *   `GPTCommunication`: Representa uma interação com a IA, armazenando a pergunta, a resposta e os "embeddings" vetoriais correspondentes.
    *   `Context`:  Representa um contexto de conversa.
    *   `TranscriptionEmbedded`: Representa uma transcrição de reunião com o seu embedding.

*   **Repositories:**
    *   Interfaces que estendem `JpaRepository` para interagir com o banco de dados para as entidades: `GPTCommunication`, `Context`, e `TranscriptionEmbedded`.

*   **Serviços:**
    *   Implementações das interfaces de serviço (ex: `GPTCommunicationServiceImpl`, `ContextServiceImpl`, `ServiceTranscriptionEmbeddedImpl`) que contêm a lógica de negócios para salvar, recuperar e encontrar informações relacionadas à comunicação com a IA, contextos e transcrições.

*   **Controladores (Controllers):**

    *   `GPTCommunicationController`:  Expõe endpoints REST para salvar as interações com a IA (`/api/question_answers`) e buscar interações similares.
    *   `ContextController`:  Gerencia os contextos de conversa, permitindo salvar, recuperar o último contexto e todos os contextos (`/api/contexts`).
    *   `TranscriptionController`:  Lida com as transcrições de reuniões, permitindo adicionar, recuperar, buscar transcrições similares e o último contexto (`/api/meetings`).

*   **DTOs (Data Transfer Objects):**

    *   `GTPCommunicationRequestDTO`, `ContextDTO`, `TranscriptionEmbeddedDTO`: Usados para transferir dados entre a aplicação e o mundo externo (ex: requisições e respostas da API).

**Funcionalidades:**

*   **Armazenamento de interações com a IA:** Salva as perguntas e respostas trocadas com a IA, junto com os "embeddings" vetoriais do texto.
*   **Busca de interações similares:** Encontra interações semelhantes com base nos "embeddings" vetoriais, permitindo reutilizar informações e contextos.
*   **Gerenciamento de contexto:** Permite salvar e recuperar o contexto da conversa, garantindo que a IA tenha o histórico necessário para responder adequadamente.
*   **Transcrições de reuniões:** Permite salvar as transcrições de reuniões e associá-las a "embeddings" vetoriais para facilitar a busca de reuniões relevantes.
*   **APIs REST:** Expõe APIs REST para facilitar a integração com outras aplicações e serviços.

**Em resumo:** A aplicação CommandAI é uma solução completa para gerenciar a comunicação com a IA, armazenando contextos e transcrições de forma inteligente, permitindo a busca de informações relevantes e a reutilização de conhecimento.
User: Estou implementando boas práticas de desenvolvimento JAVA ?
Assistant: Assistant: Sim, com base na estrutura e nos componentes da sua aplicação CommandAI, parece que você está seguindo muitas boas práticas de desenvolvimento Java e Spring Boot. Aqui está uma análise detalhada:

**Boas Práticas Implementadas:**

*   **Arquitetura em Camadas:** A aplicação está bem organizada em camadas distintas (Controladores, Serviços, Repositórios, Modelos), o que facilita a manutenção, o teste e a evolução do sistema.
*   **Uso do Spring Boot:** O uso do Spring Boot simplifica a configuração e o desenvolvimento, fornecendo "auto-configuração", "starter dependencies" e outras ferramentas que agilizam o processo.
*   **Injeção de Dependência (DI):** O uso de `@Autowired` e `@RequiredArgsConstructor` para injeção de dependência promove o baixo acoplamento e facilita o teste unitário.
*   **DTOs (Data Transfer Objects):** O uso de DTOs para transferir dados entre as camadas e expor dados na API é uma prática recomendada para desacoplar os modelos de dados internos da API.
*   **RESTful APIs:** A criação de APIs RESTful com controladores (Controllers) bem definidos e endpoints claros (ex: `/api/question_answers`, `/api/contexts`, `/api/meetings`) facilita a integração com outros sistemas.
*   **Validação:** O uso de `@Validated` nos controladores indica a preocupação com a validação dos dados de entrada.
*   **Tratamento de Exceções:** A implementação de um `GlobalExceptionHandler` para lidar com exceções de forma centralizada é uma boa prática para fornecer respostas de erro consistentes e informativas.
*   **Log:** O uso de `System.err.println` para log (enquanto não ideal) indica uma preocupação com o registro de eventos e erros para facilitar a depuração e o monitoramento.
*   **JPA e Spring Data JPA:** O uso de JPA (Java Persistence API) com Spring Data JPA simplifica o acesso ao banco de dados, fornecendo uma abstração sobre a camada de persistência.
*   **Embeddings:** A utilização de Embeddings é uma ótima estratégia para similaridade semântica.
*   **Testes:** A presença da classe `CommandAiApplicationTests` e `ContextTest` indica que você está escrevendo testes para sua aplicação, o que é fundamental para garantir a qualidade e a confiabilidade do código.
*   **Uso de Records:** A utilização de Records é excelente para DTOs.

**Sugestões de Melhorias:**

*   **Log:** Substitua `System.err.println` por um framework de log mais robusto, como SLF4J com Logback ou Log4j2. Isso permite configurar diferentes níveis de log, direcionar os logs para arquivos ou outros destinos, e adicionar informações contextuais aos logs.
*   **Tratamento de Exceções:**
    *   Crie classes de exceção personalizadas para representar diferentes tipos de erros na sua aplicação (ex: `AICommunicationException`, `ContextException`, `TranscriptionException`).
    *   Use códigos de status HTTP apropriados para cada tipo de erro.
    *   Forneça mensagens de erro claras e informativas para o cliente.
*   **Segurança:** Implemente mecanismos de autenticação e autorização para proteger suas APIs. Spring Security é uma ótima opção para isso.
*   **Documentação:** Use ferramentas como o Swagger (OpenAPI) para gerar automaticamente a documentação da sua API REST.
*   **Internacionalização (i18n):** Se sua aplicação precisar suportar diferentes idiomas, considere implementar a internacionalização.
*   **Monitoramento:** Integre ferramentas de monitoramento para coletar métricas sobre o desempenho da sua aplicação (ex: tempo de resposta da API, uso de recursos, etc.).
*   **Documentação Javadoc:** Adicione comentários Javadoc para documentar suas classes, métodos e parâmetros. Isso facilita a compreensão do código e a geração de documentação automática.
*   **Cobertura de Testes:** Busque uma alta cobertura de testes unitários e de integração.

**Em resumo:** Sua aplicação CommandAI parece estar bem estruturada e seguindo muitas boas práticas de desenvolvimento Java e Spring Boot. Ao implementar as sugestões de melhorias acima, você pode aumentar ainda mais a qualidade, a manutenibilidade e a segurança do seu sistema.

User: Preciso implementar a lógica para que um end point possa buscar reuniões com palavras chaves. Preciso implementar a lógica de salvar as reuniões em embeddings ?
Assistant: Com certeza! Aqui está um guia de como implementar a lógica para buscar reuniões por palavras-chave usando "embeddings" (representações vetoriais) e como salvar as reuniões em embeddings:

**1. Atualize a Entidade `TranscriptionEmbedded`:**

*   Adicione um campo para as palavras-chave (opcional, mas recomendado para buscas mais rápidas):

```java
package com.commandAI.commandAI.modules.embeddingTranscription.model;

import jakarta.persistence.*;
import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import lombok.Data;

import java.time.LocalDateTime;

@Data
@JsonIgnoreProperties(ignoreUnknown = true)
@Entity
@Table(name = "transcriptionEmbedded")
public class TranscriptionEmbedded {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    @Column(name = "meeting_id", nullable = false)
    private Long meetingId;

    @Column(name = "transcription_text", nullable = false, columnDefinition = "TEXT")
    private String transcriptionText;

    @Column(name = "embedding", columnDefinition = "FLOAT8[]", nullable = false)
    private float[] embedding;

    @Column(name = "keywords", columnDefinition = "TEXT") // Novo campo
    private String keywords;

    @Column(name = "created_at", nullable = false)
    private LocalDateTime createdAt;

    @PrePersist
    protected void onCreate() {
        createdAt = LocalDateTime.now();
    }

}
```

**2. Atualize o DTO `TranscriptionEmbeddedDTO`:**

*   Adicione o campo `keywords` ao DTO:

```java
package com.commandAI.commandAI.modules.embeddingTranscription.model.dto;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;

import java.time.LocalDateTime;

public record TranscriptionEmbeddedDTO(
        Long meetingId,
        String transcriptionText,
        float[] embedding,
        String keywords, // Novo campo
        LocalDateTime createdAt
) {
    public static TranscriptionEmbeddedDTO fromEntity(TranscriptionEmbedded transcriptionEmbedded) {
        return new TranscriptionEmbeddedDTO(transcriptionEmbedded.getMeetingId(), transcriptionEmbedded.getTranscriptionText(), transcriptionEmbedded.getEmbedding(), transcriptionEmbedded.getKeywords(), transcriptionEmbedded.getCreatedAt());
    }
}
```

**3. Atualize o Serviço `IServiceTranscriptionEmbedded`:**

*   Adicione um método para buscar transcrições por palavras-chave:

```java
package com.commandAI.commandAI.modules.embeddingTranscription.service;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;

import java.util.List;

public interface IServiceTranscriptionEmbedded {
    void saveTranscription(String transcriptionText, float[] embedding, String keywords); // Modifiquei
    List<TranscriptionEmbeddedDTO> getAllTranscriptions();
    TranscriptionEmbeddedDTO getTranscriptionById(Long id);
    List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(float[] queryEmbedding);
    TranscriptionEmbeddedDTO getLastTranscription();
    List<TranscriptionEmbeddedDTO> findByKeywords(String keywords);  // Novo método
}
```

**4. Atualize a Implementação do Serviço `ServiceTranscriptionEmbeddedImpl`:**

```java
package com.commandAI.commandAI.modules.embeddingTranscription.service.impl;

import com.commandAI.commandAI.modules.context.model.context.Context;
import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.repository.ITranscriptionEmbeddedRepository;
import com.commandAI.commandAI.modules.embeddingTranscription.service.IServiceTranscriptionEmbedded;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.util.Arrays;
import java.util.List;
import java.util.stream.Collectors;

@RequiredArgsConstructor
@Service
public class ServiceTranscriptionEmbeddedImpl implements IServiceTranscriptionEmbedded {

    private final ITranscriptionEmbeddedRepository transcriptionEmbeddedRepository;

    @Override
    @Transactional
    public void saveTranscription(String transcriptionText, float[] embedding, String keywords) {  // Modifiquei
        TranscriptionEmbedded transcription = new TranscriptionEmbedded();
        transcription.setTranscriptionText(transcriptionText);
        transcription.setEmbedding(embedding);
        transcription.setKeywords(keywords); // Adicionei
        System.out.println(transcription);
        transcriptionEmbeddedRepository.save(transcription);
    }

    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> getAllTranscriptions() {
        return transcriptionEmbeddedRepository.findAll().stream()
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                        transcription.getKeywords(), // Adicionei
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }

    @Override
    @Transactional
    public TranscriptionEmbeddedDTO getTranscriptionById(Long id) {
        TranscriptionEmbedded transcription = transcriptionEmbeddedRepository.findById(id).orElse(null);
        if (transcription == null) {
            return null;
        }
        return new TranscriptionEmbeddedDTO(
                transcription.getMeetingId(),
                transcription.getTranscriptionText(),
                transcription.getEmbedding(),
                transcription.getKeywords(), // Adicionei
                transcription.getCreatedAt());
    }

    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(float[] queryEmbedding) {
        List<TranscriptionEmbedded> allTranscriptions = transcriptionEmbeddedRepository.findAll();
        return allTranscriptions.stream()
                .sorted((t1, t2) -> Double.compare(
                        cosineSimilarity(queryEmbedding, t2.getEmbedding()),
                        cosineSimilarity(queryEmbedding, t1.getEmbedding())))
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                         transcription.getKeywords(), // Adicionei
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }

    @Override
    @Transactional
    public TranscriptionEmbeddedDTO getLastTranscription() {
        TranscriptionEmbedded lastTranscription = transcriptionEmbeddedRepository.findTopByOrderByCreatedAtDesc()
                .orElseThrow(() -> new RuntimeException("No contexts found"));
        return TranscriptionEmbeddedDTO.fromEntity(lastTranscription);
    }

    private double cosineSimilarity(float[] vec1, float[] vec2) {
        double dotProduct = 0.0;
        double normA = 0.0;
        double normB = 0.0;
        for (int i = 0; i < vec1.length; i++) {
            dotProduct += vec1[i] * vec2[i];
            normA += Math.pow(vec1[i], 2);
            normB += Math.pow(vec2[i], 2);
        }
        return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
    }

    // Implementação para buscar por palavras-chave
    @Override
    @Transactional
    public List<TranscriptionEmbeddedDTO> findByKeywords(String keywords) {
        //String[] keywordsArray = keywords.toLowerCase().split("\\s+");  // Divide as palavras-chave
        return transcriptionEmbeddedRepository.findAll().stream()
        .filter(transcription -> transcription.getTranscriptionText().toLowerCase().contains(keywords.toLowerCase()))
                .map(transcription -> new TranscriptionEmbeddedDTO(
                        transcription.getMeetingId(),
                        transcription.getTranscriptionText(),
                        transcription.getEmbedding(),
                        transcription.getKeywords(), // Adicionei
                        transcription.getCreatedAt()))
                .collect(Collectors.toList());
    }
}
```

**5. Atualize o Repositório `ITranscriptionEmbeddedRepository`:**

*   Você não precisa modificar o repositório, pois estamos filtrando com JAVA. Caso queira fazer uma query no banco de dados:

```java
package com.commandAI.commandAI.modules.embeddingTranscription.repository;

import com.commandAI.commandAI.modules.embeddingTranscription.model.TranscriptionEmbedded;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import java.util.List;
import java.util.Optional;

@Repository
public interface ITranscriptionEmbeddedRepository extends JpaRepository<TranscriptionEmbedded, Long> {
    Optional<TranscriptionEmbedded> findTopByOrderByCreatedAtDesc();
    List<TranscriptionEmbedded> findByKeywordsContainingIgnoreCase(String keywords); // Novo método
}
```

**6. Atualize o Controlador `TranscriptionController`:**

*   Adicione o novo endpoint para buscar por palavras-chave:

```java
package com.commandAI.commandAI.modules.embeddingTranscription.controller;

import com.commandAI.commandAI.modules.context.model.dto.ContextDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.model.dto.TranscriptionEmbeddedDTO;
import com.commandAI.commandAI.modules.embeddingTranscription.service.IServiceTranscriptionEmbedded;
import lombok.RequiredArgsConstructor;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.List;

@RestController
@RequestMapping("/api/meetings")    
@RequiredArgsConstructor
public class TranscriptionController {

    private final IServiceTranscriptionEmbedded service;

    @PostMapping("/transcriptions")
    public ResponseEntity<Void> addTranscription(@RequestBody TranscriptionEmbeddedDTO transcriptionDTO) {
        service.saveTranscription(transcriptionDTO.transcriptionText(), transcriptionDTO.embedding(), transcriptionDTO.keywords()); // Modifiquei
        return ResponseEntity.status(HttpStatus.CREATED).build();
    }

    @GetMapping("/transcriptions")
    public List<TranscriptionEmbeddedDTO> getAllTranscriptions() {
        return service.getAllTranscriptions();
    }

    @GetMapping("/transcriptions/{id}")
    public ResponseEntity<TranscriptionEmbeddedDTO> getTranscriptionById(@PathVariable Long id) {
        TranscriptionEmbeddedDTO transcription = service.getTranscriptionById(id);
        if (transcription == null) {
            return ResponseEntity.status(HttpStatus.NOT_FOUND).build();
        }
        return ResponseEntity.ok(transcription);
    }

    @GetMapping("/transcriptions/similar")
    public List<TranscriptionEmbeddedDTO> findSimilarTranscriptions(@RequestParam float[] queryEmbedding) {
        return service.findSimilarTranscriptions(queryEmbedding);
    }
    @GetMapping("/last")
    public ResponseEntity<TranscriptionEmbeddedDTO> getLastTranscription() {
        TranscriptionEmbeddedDTO lastTranscription = service.getLastTranscription();
        return ResponseEntity.ok(lastTranscription);
    }

    @GetMapping("/transcriptions/search") // Novo endpoint
    public List<TranscriptionEmbeddedDTO> searchTranscriptions(@RequestParam String keywords) {
        return service.findByKeywords(keywords);
    }
}
```

**7. Lógica para Salvar Reuniões com Embeddings e Palavras-Chave:**

*   **Obtenha a Transcrição:**  Primeiro, você precisa obter a transcrição da reunião (texto).
*   **Gere o Embedding:** Use um modelo de linguagem (como o Gemini) para gerar o "embedding" vetorial da transcrição.
*   **Extraia Palavras-Chave:** Use um algoritmo de extração de palavras-chave para identificar os termos mais relevantes na transcrição.
*   **Salve no Banco de Dados:** Salve a transcrição, o embedding e as palavras-chave no banco de dados.

**Exemplo de como salvar (adaptado para o seu código):**

```java
// Supondo que você tenha a transcrição, o embedding e as palavras-chave
String transcriptionText = "Transcrição da reunião...";
float[] embedding = // ... seu embedding gerado ...
String keywords = "palavras-chave extraídas...";

// Crie um DTO
TranscriptionEmbeddedDTO transcriptionDTO = new TranscriptionEmbeddedDTO(
    null, // ID será gerado automaticamente
    transcriptionText,
    embedding,
    keywords,
    null // Data será gerada automaticamente
);

// Salve a transcrição
service.saveTranscription(transcriptionDTO.transcriptionText(), transcriptionDTO.embedding(), transcriptionDTO.keywords());
```

**Pontos Importantes:**

*   **Extração de Palavras-Chave:** Você pode usar bibliotecas como Apache OpenNLP, Stanford CoreNLP ou até mesmo APIs online para extrair palavras-chave.
*   **Geração de Embeddings:**  Use a API do Gemini para gerar embeddings para cada frase.
*   **Banco de Dados:** Certifique-se de que o tipo de dados `FLOAT8[]` é suportado pelo seu banco de dados.
*   **Performance:** Se você tiver muitas transcrições, considere usar técnicas de indexação para acelerar as buscas.

Este guia fornece um ponto de partida para implementar a busca de reuniões por palavras-chave e o armazenamento de reuniões com embeddings. Adapte o código e as técnicas às suas necessidades específicas.
User: porque não consegui fazer esse push ? veja imagem ?
2025-04-14 22:39:55,531 [INFO] Resposta da IA (imagem): Com base na saída do terminal que você forneceu, o push falhou devido a um erro de RPC (Remote Procedure Call) com um código de status HTTP 408, que significa "Request Timeout". Adicionalmente, a mensagem "fatal: the remote end hung up unexpectedly" sugere que a conexão com o servidor remoto foi interrompida inesperadamente.

**Possíveis causas e soluções:**

1.  **Tamanho Grande do Push:**

    *   **Causa:** O tamanho dos objetos que você está tentando enviar (1.93 GiB) é relativamente grande. Pode ser que o servidor Git remoto tenha um limite de tempo para o recebimento de dados, e o seu push está excedendo esse limite.
    *   **Solução:**
        *   **Aumente o buffer do Git:** Tente aumentar o tamanho do buffer HTTP do Git. Isso pode ajudar a lidar com grandes quantidades de dados.

            ```bash
            git config http.postBuffer 524288000 # 500MB
            git config http.maxRequestBuffer 100M
            ```

        *   **Divida o Push:** Se possível, tente dividir o push em partes menores. Isso pode ser feito commitando menos alterações de cada vez ou usando `git push --all`.
        *   **Garanta uma boa conexão de rede:** Se a sua conexão for lenta ou instável, isso pode causar timeouts.
        *   **Limpe seu repositório:** Remova arquivos grandes desnecessários do seu repositório, se houver.

2.  **Tempo Limite de Conexão:**

    *   **Causa:** O servidor remoto pode estar fechando a conexão devido a um tempo limite configurado.
    *   **Solução:**
        *   **Aumente o Tempo Limite HTTP:** Aumente o tempo limite HTTP no Git para permitir que a operação seja concluída.

            ```bash
            git config --global http.timeout 300
            ```

            Isso define o tempo limite para 300 segundos (5 minutos). Ajuste conforme necessário.
        *   **Verifique a estabilidade da sua conexão com a internet.**

3.  **Problemas no Servidor Remoto:**

    *   **Causa:** Pode haver um problema temporário no servidor Git remoto (GitHub, GitLab, Bitbucket, etc.).
    *   **Solução:**
        *   **Tente Novamente Mais Tarde:** Espere um pouco e tente o push novamente. O problema pode ser resolvido por si só.
        *   **Verifique o Status do Serviço:** Se você estiver usando um serviço como GitHub ou GitLab, verifique a página de status do serviço para ver se há algum problema conhecido.
        *   **Entre em Contato com o Suporte:** Se o problema persistir, entre em contato com o suporte do serviço Git remoto.

4.  **Problemas de Configuração de Proxy:**

    *   **Causa:** Se você estiver usando um proxy, pode haver um problema com a configuração do proxy que está causando o erro.
    *   **Solução:**
        *   **Verifique as Configurações de Proxy:** Verifique se as configurações de proxy estão corretas.
        *   **Teste Sem o Proxy:** Tente fazer o push sem usar o proxy para ver se o problema é com o proxy.

5.  **Problemas de Firewall ou Antivírus:**

    *   **Causa:** Um firewall ou antivírus pode estar bloqueando a conexão Git.
    *   **Solução:**
        *   **Verifique as Configurações do Firewall/Antivírus:** Verifique as configurações do seu firewall ou antivírus para garantir que o Git não está sendo bloqueado.

**Passos para Diagnosticar e Resolver o Problema:**

1.  **Verifique sua Conexão com a Internet:** Certifique-se de que sua conexão com a internet está funcionando corretamente e é estável.
2.  **Tente um Push Menor:** Faça um pequeno commit e tente enviar esse commit para ver se o problema é com o tamanho do push.
3.  **Aumente o Tempo Limite HTTP:** Execute o comando `git config --global http.timeout 300` para aumentar o tempo limite.
4.  **Aumente o Buffer HTTP:** Execute o comando `git config http.postBuffer 524288000` para aumentar o buffer HTTP.
5.  **Tente Novamente Mais Tarde:** Se as etapas acima não resolverem o problema, espere um pouco e tente novamente.
6.  **Verifique o Status do Serviço:** Se estiver usando um serviço de hospedagem Git, verifique a página de status do serviço.
7.  **Entre em Contato com o Suporte:** Se o problema persistir, entre em contato com o suporte do serviço Git remoto.

Espero que isso ajude a resolver o problema!

2025-04-14 22:39:55,647 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-14 22:39:55,649 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-14 22:39:55,650 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-14 22:39:55,651 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-14 22:39:55,652 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-14 22:39:55,654 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-14 22:39:55,655 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-14 22:39:55,656 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-14 22:39:55,657 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-14 22:39:55,663 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-14 22:39:55,664 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-14 22:39:55,666 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-14 22:39:55,667 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-14 22:39:55,668 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\CommandAiApplication.java
2025-04-14 22:39:55,670 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\controller\GPTCommunicationController.java
2025-04-14 22:39:55,671 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\GPTCommunication.java
2025-04-14 22:39:55,672 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\model\dto\GTPCommunicationRequestDTO.java
2025-04-14 22:39:55,673 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\repository\IGPTCommunicationRepository.java
2025-04-14 22:39:55,675 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\IGPTCommunicationService.java
2025-04-14 22:39:55,676 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\AIComunication\service\impl\GPTCommunicationServiceImpl.java
2025-04-14 22:39:55,677 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\ContextController.java
2025-04-14 22:39:55,678 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\controller\hendler\GlobalExceptionHandler.java
2025-04-14 22:39:55,680 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\context\Context.java
2025-04-14 22:39:55,681 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\model\dto\ContextDTO.java
2025-04-14 22:39:55,682 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\repository\IContextRepository.java
2025-04-14 22:39:55,684 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\IContextService.java
2025-04-14 22:39:55,686 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\impl\ContextServiceImpl.java
2025-04-14 22:39:55,689 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\context\service\validation\ContextNotFoundException.java
2025-04-14 22:39:55,690 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\controller\TranscriptionController.java
2025-04-14 22:39:55,691 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\TranscriptionEmbedded.java
2025-04-14 22:39:55,693 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\model\dto\TranscriptionEmbeddedDTO.java
2025-04-14 22:39:55,694 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\repository\ITranscriptionEmbeddedRepository.java
2025-04-14 22:39:55,695 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\IServiceTranscriptionEmbedded.java
2025-04-14 22:39:55,696 [INFO] Arquivo lido com sucesso: main\java\com\commandAI\commandAI\modules\embeddingTranscription\service\impl\ServiceTranscriptionEmbeddedImpl.java
2025-04-14 22:39:55,698 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\CommandAiApplicationTests.java
2025-04-14 22:39:55,700 [INFO] Arquivo lido com sucesso: test\java\com\commandAI\commandAI\modelsTest\ContextTest.java
