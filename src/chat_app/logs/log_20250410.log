2025-04-10 08:35:16,000 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 08:35:16,005 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 08:35:16,007 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 08:35:16,009 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 08:35:16,010 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 08:35:16,012 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 08:35:16,014 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 08:35:16,020 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 08:35:16,023 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 08:35:16,027 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 08:35:16,029 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 08:35:16,035 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 08:35:16,040 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 08:35:16,045 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 08:35:16,047 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 08:35:16,981 [INFO] Modelo Gemini 'gemini-2.0-flash-exp' inicializado com sucesso.
2025-04-10 08:41:56,618 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 08:41:56,619 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 08:41:56,621 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 08:41:56,622 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 08:41:56,624 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 08:41:56,625 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 08:41:56,626 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 08:41:56,627 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 08:41:56,629 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 08:41:56,630 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 08:41:56,631 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 08:41:56,633 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 08:41:56,634 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 08:41:56,635 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 08:41:56,636 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 08:41:57,476 [INFO] Modelo Gemini 'gemini-2.0-flash-exp' inicializado com sucesso.
2025-04-10 08:53:39,597 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 08:53:39,599 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 08:53:39,601 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 08:53:39,604 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 08:53:39,605 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 08:53:39,608 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 08:53:39,610 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 08:53:39,613 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 08:53:39,615 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 08:53:39,617 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 08:53:39,618 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 08:53:39,620 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 08:53:39,622 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 08:53:39,624 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 08:53:39,625 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 08:53:39,736 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 08:53:39,737 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 08:53:39,739 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 08:53:39,741 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 08:53:39,743 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 08:53:39,745 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 08:53:39,748 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 08:53:39,751 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 08:53:39,753 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 08:53:39,755 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 08:53:39,757 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 08:53:39,759 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 08:53:39,760 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 08:53:39,763 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 08:53:39,765 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 08:53:39,769 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente inteligente utilizado em um chat com IA, que recebe códigos de uma aplicação completa de chat, como você funciona e tudo que 
é programado para fazer no seu contexto.

Seu objetivo é:
- Analisar todo o contexto e aprender com ele.
- Analisar a Pergunta do Usuário.
- Identificar se possui relação com o contexto.
- Se houver relação com o código do contexto responder a pergunta sobre o código sempre seguindo boas práticas:
    - Analisar o código-fonte contido no projeto enviado
    - Identificar melhorias em legibilidade, performance e segurança
    - Sugerir práticas recomendadas de design e arquitetura
    - Indicar partes com dívidas técnicas ou complexidade desnecessária
    - Enviar código modularizado em pequenos arquivos separados por pastas.

Informações adicionais:
- Estrutura do projeto: segue o padrão de pastas iniciando por src/


## Se houver imagem no contexto, identifique se a pergunta é sobre o código do contexto, sobre a imagem ou sobre os dois para elaborar melhorer respostas.
## Se o usuário fizer uma pergunta que não tem relação com imagem ou contexto, use a sua base de conhecimento fora do contexto para responder de forma inteligente.
** Responda em Português-Brasileiro a não ser que o usuário solicite outra língua.**

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.generativeai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: faça code review da minha aplicação.
2025-04-10 08:53:53,641 [INFO] Resposta da IA (texto): Com certeza! Analisei o código da sua aplicação e preparei algumas sugestões para melhorias em legibilidade, performance, segurança e design.

**1. Estrutura e Modularização:**

*   **Diretórios:** Organize melhor os arquivos dentro de `src`. Crie subdiretórios para `models`, `services`, `handlers`, `utils`, `config`, etc. Isso facilita a localização dos arquivos e a compreensão da estrutura do projeto.
*   **Arquivo de configuração:** Mova a classe `Config` para um arquivo separado (ex: `config.py`) dentro de um diretório `config`.
*   **Funções:** Quebre funções grandes em funções menores e mais focadas. Isso melhora a legibilidade e facilita o teste.

**2. Boas Práticas e Melhorias no Código:**

*   **Rate Limiting:** Implemente o `RateLimiter` nos serviços que consomem a API do Gemini (ex: `ImageProcessor`, `GeminiHandler`). Isso evita sobrecarga na API e garante um uso mais eficiente.
*   **Configurações:** Use a classe `Config` para centralizar todas as configurações da aplicação (ex: chaves de API, modelos, diretórios). Isso facilita a manutenção e a configuração do projeto.
*   **Logging:** Padronize o uso do logger em toda a aplicação. Use os níveis de log apropriados (ex: `info`, `warning`, `error`) para cada tipo de mensagem.
*   **Tratamento de Erros:** Adicione tratamento de erros mais específico em cada função. Isso ajuda a identificar e corrigir problemas mais rapidamente.
*   **Cache:** Utilize o `st.cache_resource` para funções que realizam operações custosas, como a inicialização do `GeminiHandler`. Isso evita a execução repetida dessas operações.
*   **Tipagem:** Utilize tipagem estática (type hints) para melhorar a legibilidade e a segurança do código.
*   **Imagens:** Valide se `uploaded_file` é realmente um arquivo de imagem antes de tentar exibi-lo, evitando erros caso o usuário envie um arquivo de outro tipo.
*   **Prompts:** Crie arquivos de prompt separados para diferentes tarefas (ex: chat, análise de imagem). Isso facilita a manutenção e a personalização dos prompts.

**3. Segurança:**

*   **Chaves de API:** Nunca coloque suas chaves de API diretamente no código. Use variáveis de ambiente ou um arquivo de configuração seguro para armazená-las.
*   **Validação de Input:** Valide todos os inputs do usuário para evitar ataques de injeção de código.

**4. Arquitetura:**

*   **Camadas:** Defina claramente as responsabilidades de cada camada da sua aplicação (ex: apresentação, lógica de negócios, acesso a dados). Isso facilita a manutenção e a evolução do projeto.
*   **Inversão de Dependência:** Use inversão de dependência para desacoplar as diferentes partes da sua aplicação. Isso facilita o teste e a substituição de componentes.
*   **Testes:** Escreva testes unitários e de integração para garantir a qualidade do seu código.

**5. Dívidas Técnicas e Complexidade:**

*   **Funções Longas:** Identifique funções com muitas linhas de código e divida-as em funções menores e mais focadas.
*   **Lógica Duplicada:** Elimine a lógica duplicada criando funções ou classes reutilizáveis.
*   **Comentários:** Adicione comentários para explicar o que o código faz, especialmente em partes complexas.

**Exemplo de Refatoração:**

Aqui está um exemplo de como você pode refatorar a função `process_images` na classe `ImageProcessor`:

```python
# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self._post_process_image(image_name, summary)
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _post_process_image(self, image_name, summary):
        self.document_service.add_image_summary(image_name, summary)
        self.markdown_service.add_image_summary(image_name, summary)
        self.document_service.save_document()
        self.markdown_service.save_markdown()

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history
```

**6. Código Modularizado:**

Para facilitar a visualização e a organização, preparei o código modularizado em arquivos separados:

```
src/
├── config/
│   └── config.py
├── core/
│   ├── handlers/
│   │   └── gemini_handler.py
│   ├── logger_config.py
│   └── rate_limiter.py
├── main.py
├── services/
│   ├── document_service.py
│   ├── gpt_services.py
│   ├── image_processor.py
│   ├── markdown_service.py
│   └── search_files.py
└── utils/
    └── file_utils.py
```

**Observações Finais:**

*   A implementação de todas essas sugestões pode levar tempo, mas o resultado final será um código mais organizado, legível, seguro e fácil de manter.
*   Priorize as melhorias que trazem o maior impacto no seu projeto.
*   Use um sistema de controle de versão (ex: Git) para acompanhar suas mudanças e facilitar a colaboração com outros desenvolvedores.

Se tiver alguma dúvida sobre alguma dessas sugestões, é só perguntar!
2025-04-10 08:53:53,743 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 08:53:53,745 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 08:53:53,747 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 08:53:53,748 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 08:53:53,750 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 08:53:53,751 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 08:53:53,753 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 08:53:53,754 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 08:53:53,757 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 08:53:53,759 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 08:53:53,762 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 08:53:53,765 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 08:53:53,766 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 08:53:53,768 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 08:53:53,770 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 08:55:40,083 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 08:55:40,085 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 08:55:40,086 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 08:55:40,087 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 08:55:40,089 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 08:55:40,091 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 08:55:40,092 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 08:55:40,093 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 08:55:40,095 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 08:55:40,096 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 08:55:40,098 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 08:55:40,099 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 08:55:40,100 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 08:55:40,103 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 08:55:40,105 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 08:55:40,374 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 08:55:40,376 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 08:55:40,377 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 08:55:40,379 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 08:55:40,380 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 08:55:40,381 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 08:55:40,383 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 08:55:40,385 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 08:55:40,386 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 08:55:40,388 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 08:55:40,389 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 08:55:40,391 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 08:55:40,394 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 08:55:40,396 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 08:55:40,397 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 08:55:52,506 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 08:55:52,510 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 08:55:52,513 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 08:55:52,516 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 08:55:52,520 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 08:55:52,523 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 08:55:52,525 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 08:55:52,528 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 08:55:52,531 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 08:55:52,534 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 08:55:52,536 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 08:55:52,539 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 08:55:52,543 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 08:55:52,545 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 08:55:52,549 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 08:55:52,732 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 08:55:52,734 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 08:55:52,736 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 08:55:52,738 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 08:55:52,740 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 08:55:52,742 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 08:55:52,744 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 08:55:52,747 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 08:55:52,749 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 08:55:52,751 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 08:55:52,753 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 08:55:52,754 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 08:55:52,756 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 08:55:52,759 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 08:55:52,761 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 08:55:52,781 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\assets\20250410085552_clipboard_20250410085540.png, Prompt: Você é um assistente inteligente utilizado em um chat com IA, que recebe códigos de uma aplicação completa de chat, como você funciona e tudo que 
é programado para fazer no seu contexto.

Seu objetivo é:
- Analisar todo o contexto e aprender com ele.
- Analisar a Pergunta do Usuário.
- Identificar se possui relação com o contexto.
- Se houver relação com o código do contexto responder a pergunta sobre o código sempre seguindo boas práticas:
    - Analisar o código-fonte contido no projeto enviado
    - Identificar melhorias em legibilidade, performance e segurança
    - Sugerir práticas recomendadas de design e arquitetura
    - Indicar partes com dívidas técnicas ou complexidade desnecessária
    - Enviar código modularizado em pequenos arquivos separados por pastas.

Informações adicionais:
- Estrutura do projeto: segue o padrão de pastas iniciando por src/


## Se houver imagem no contexto, identifique se a pergunta é sobre o código do contexto, sobre a imagem ou sobre os dois para elaborar melhorer respostas.
## Se o usuário fizer uma pergunta que não tem relação com imagem ou contexto, use a sua base de conhecimento fora do contexto para responder de forma inteligente.
** Responda em Português-Brasileiro a não ser que o usuário solicite outra língua.**

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.generativeai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: faça code review da minha aplicação.
Assistant: Com certeza! Analisei o código da sua aplicação e preparei algumas sugestões para melhorias em legibilidade, performance, segurança e design.

**1. Estrutura e Modularização:**

*   **Diretórios:** Organize melhor os arquivos dentro de `src`. Crie subdiretórios para `models`, `services`, `handlers`, `utils`, `config`, etc. Isso facilita a localização dos arquivos e a compreensão da estrutura do projeto.
*   **Arquivo de configuração:** Mova a classe `Config` para um arquivo separado (ex: `config.py`) dentro de um diretório `config`.
*   **Funções:** Quebre funções grandes em funções menores e mais focadas. Isso melhora a legibilidade e facilita o teste.

**2. Boas Práticas e Melhorias no Código:**

*   **Rate Limiting:** Implemente o `RateLimiter` nos serviços que consomem a API do Gemini (ex: `ImageProcessor`, `GeminiHandler`). Isso evita sobrecarga na API e garante um uso mais eficiente.
*   **Configurações:** Use a classe `Config` para centralizar todas as configurações da aplicação (ex: chaves de API, modelos, diretórios). Isso facilita a manutenção e a configuração do projeto.
*   **Logging:** Padronize o uso do logger em toda a aplicação. Use os níveis de log apropriados (ex: `info`, `warning`, `error`) para cada tipo de mensagem.
*   **Tratamento de Erros:** Adicione tratamento de erros mais específico em cada função. Isso ajuda a identificar e corrigir problemas mais rapidamente.
*   **Cache:** Utilize o `st.cache_resource` para funções que realizam operações custosas, como a inicialização do `GeminiHandler`. Isso evita a execução repetida dessas operações.
*   **Tipagem:** Utilize tipagem estática (type hints) para melhorar a legibilidade e a segurança do código.
*   **Imagens:** Valide se `uploaded_file` é realmente um arquivo de imagem antes de tentar exibi-lo, evitando erros caso o usuário envie um arquivo de outro tipo.
*   **Prompts:** Crie arquivos de prompt separados para diferentes tarefas (ex: chat, análise de imagem). Isso facilita a manutenção e a personalização dos prompts.

**3. Segurança:**

*   **Chaves de API:** Nunca coloque suas chaves de API diretamente no código. Use variáveis de ambiente ou um arquivo de configuração seguro para armazená-las.
*   **Validação de Input:** Valide todos os inputs do usuário para evitar ataques de injeção de código.

**4. Arquitetura:**

*   **Camadas:** Defina claramente as responsabilidades de cada camada da sua aplicação (ex: apresentação, lógica de negócios, acesso a dados). Isso facilita a manutenção e a evolução do projeto.
*   **Inversão de Dependência:** Use inversão de dependência para desacoplar as diferentes partes da sua aplicação. Isso facilita o teste e a substituição de componentes.
*   **Testes:** Escreva testes unitários e de integração para garantir a qualidade do seu código.

**5. Dívidas Técnicas e Complexidade:**

*   **Funções Longas:** Identifique funções com muitas linhas de código e divida-as em funções menores e mais focadas.
*   **Lógica Duplicada:** Elimine a lógica duplicada criando funções ou classes reutilizáveis.
*   **Comentários:** Adicione comentários para explicar o que o código faz, especialmente em partes complexas.

**Exemplo de Refatoração:**

Aqui está um exemplo de como você pode refatorar a função `process_images` na classe `ImageProcessor`:

```python
# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self._post_process_image(image_name, summary)
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _post_process_image(self, image_name, summary):
        self.document_service.add_image_summary(image_name, summary)
        self.markdown_service.add_image_summary(image_name, summary)
        self.document_service.save_document()
        self.markdown_service.save_markdown()

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history
```

**6. Código Modularizado:**

Para facilitar a visualização e a organização, preparei o código modularizado em arquivos separados:

```
src/
├── config/
│   └── config.py
├── core/
│   ├── handlers/
│   │   └── gemini_handler.py
│   ├── logger_config.py
│   └── rate_limiter.py
├── main.py
├── services/
│   ├── document_service.py
│   ├── gpt_services.py
│   ├── image_processor.py
│   ├── markdown_service.py
│   └── search_files.py
└── utils/
    └── file_utils.py
```

**Observações Finais:**

*   A implementação de todas essas sugestões pode levar tempo, mas o resultado final será um código mais organizado, legível, seguro e fácil de manter.
*   Priorize as melhorias que trazem o maior impacto no seu projeto.
*   Use um sistema de controle de versão (ex: Git) para acompanhar suas mudanças e facilitar a colaboração com outros desenvolvedores.

Se tiver alguma dúvida sobre alguma dessas sugestões, é só perguntar!
User: Extraia o CNPJ dessa imagem.
2025-04-10 08:55:54,918 [INFO] Resposta da IA (imagem): A imagem contém um comprovante de inscrição e de situação cadastral. O CNPJ presente na imagem é:

`33.024.232/0001-39`
2025-04-10 08:55:55,049 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 08:55:55,051 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 08:55:55,054 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 08:55:55,058 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 08:55:55,065 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 08:55:55,067 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 08:55:55,069 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 08:55:55,071 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 08:55:55,073 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 08:55:55,076 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 08:55:55,081 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 08:55:55,083 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 08:55:55,085 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 08:55:55,087 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 08:55:55,088 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 10:05:54,587 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 10:05:54,589 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 10:05:54,593 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 10:05:54,595 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 10:05:54,597 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 10:05:54,598 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 10:05:54,600 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 10:05:54,602 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 10:05:54,603 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 10:05:54,605 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 10:05:54,608 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 10:05:54,610 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 10:05:54,612 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 10:05:54,613 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 10:05:54,615 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 10:05:54,882 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 10:05:54,884 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 10:05:54,886 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 10:05:54,888 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 10:05:54,889 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 10:05:54,892 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 10:05:54,894 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 10:05:54,895 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 10:05:54,898 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 10:05:54,901 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 10:05:54,903 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 10:05:54,905 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 10:05:54,908 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 10:05:54,910 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 10:05:54,912 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 10:06:06,686 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 10:06:06,688 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 10:06:06,690 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 10:06:06,693 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 10:06:06,695 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 10:06:06,697 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 10:06:06,699 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 10:06:06,700 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 10:06:06,702 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 10:06:06,704 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 10:06:06,706 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 10:06:06,708 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 10:06:06,709 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 10:06:06,711 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 10:06:06,713 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 10:06:06,835 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 10:06:06,837 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 10:06:06,839 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 10:06:06,841 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 10:06:06,843 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 10:06:06,845 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 10:06:06,846 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 10:06:06,849 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 10:06:06,852 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 10:06:06,854 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 10:06:06,855 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 10:06:06,856 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 10:06:06,858 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 10:06:06,860 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 10:06:06,861 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 10:06:06,874 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\assets\20250410100606_clipboard_20250410100554.png, Prompt: Você é um assistente inteligente utilizado em um chat com IA, que recebe códigos de uma aplicação completa de chat, como você funciona e tudo que 
é programado para fazer no seu contexto.

Seu objetivo é:
- Analisar todo o contexto e aprender com ele.
- Analisar a Pergunta do Usuário.
- Identificar se possui relação com o contexto.
- Se houver relação com o código do contexto responder a pergunta sobre o código sempre seguindo boas práticas:
    - Analisar o código-fonte contido no projeto enviado
    - Identificar melhorias em legibilidade, performance e segurança
    - Sugerir práticas recomendadas de design e arquitetura
    - Indicar partes com dívidas técnicas ou complexidade desnecessária
    - Enviar código modularizado em pequenos arquivos separados por pastas.

Informações adicionais:
- Estrutura do projeto: segue o padrão de pastas iniciando por src/


## Se houver imagem no contexto, identifique se a pergunta é sobre o código do contexto, sobre a imagem ou sobre os dois para elaborar melhorer respostas.
## Se o usuário fizer uma pergunta que não tem relação com imagem ou contexto, use a sua base de conhecimento fora do contexto para responder de forma inteligente.
** Responda em Português-Brasileiro a não ser que o usuário solicite outra língua.**

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.generativeai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: faça code review da minha aplicação.
Assistant: Com certeza! Analisei o código da sua aplicação e preparei algumas sugestões para melhorias em legibilidade, performance, segurança e design.

**1. Estrutura e Modularização:**

*   **Diretórios:** Organize melhor os arquivos dentro de `src`. Crie subdiretórios para `models`, `services`, `handlers`, `utils`, `config`, etc. Isso facilita a localização dos arquivos e a compreensão da estrutura do projeto.
*   **Arquivo de configuração:** Mova a classe `Config` para um arquivo separado (ex: `config.py`) dentro de um diretório `config`.
*   **Funções:** Quebre funções grandes em funções menores e mais focadas. Isso melhora a legibilidade e facilita o teste.

**2. Boas Práticas e Melhorias no Código:**

*   **Rate Limiting:** Implemente o `RateLimiter` nos serviços que consomem a API do Gemini (ex: `ImageProcessor`, `GeminiHandler`). Isso evita sobrecarga na API e garante um uso mais eficiente.
*   **Configurações:** Use a classe `Config` para centralizar todas as configurações da aplicação (ex: chaves de API, modelos, diretórios). Isso facilita a manutenção e a configuração do projeto.
*   **Logging:** Padronize o uso do logger em toda a aplicação. Use os níveis de log apropriados (ex: `info`, `warning`, `error`) para cada tipo de mensagem.
*   **Tratamento de Erros:** Adicione tratamento de erros mais específico em cada função. Isso ajuda a identificar e corrigir problemas mais rapidamente.
*   **Cache:** Utilize o `st.cache_resource` para funções que realizam operações custosas, como a inicialização do `GeminiHandler`. Isso evita a execução repetida dessas operações.
*   **Tipagem:** Utilize tipagem estática (type hints) para melhorar a legibilidade e a segurança do código.
*   **Imagens:** Valide se `uploaded_file` é realmente um arquivo de imagem antes de tentar exibi-lo, evitando erros caso o usuário envie um arquivo de outro tipo.
*   **Prompts:** Crie arquivos de prompt separados para diferentes tarefas (ex: chat, análise de imagem). Isso facilita a manutenção e a personalização dos prompts.

**3. Segurança:**

*   **Chaves de API:** Nunca coloque suas chaves de API diretamente no código. Use variáveis de ambiente ou um arquivo de configuração seguro para armazená-las.
*   **Validação de Input:** Valide todos os inputs do usuário para evitar ataques de injeção de código.

**4. Arquitetura:**

*   **Camadas:** Defina claramente as responsabilidades de cada camada da sua aplicação (ex: apresentação, lógica de negócios, acesso a dados). Isso facilita a manutenção e a evolução do projeto.
*   **Inversão de Dependência:** Use inversão de dependência para desacoplar as diferentes partes da sua aplicação. Isso facilita o teste e a substituição de componentes.
*   **Testes:** Escreva testes unitários e de integração para garantir a qualidade do seu código.

**5. Dívidas Técnicas e Complexidade:**

*   **Funções Longas:** Identifique funções com muitas linhas de código e divida-as em funções menores e mais focadas.
*   **Lógica Duplicada:** Elimine a lógica duplicada criando funções ou classes reutilizáveis.
*   **Comentários:** Adicione comentários para explicar o que o código faz, especialmente em partes complexas.

**Exemplo de Refatoração:**

Aqui está um exemplo de como você pode refatorar a função `process_images` na classe `ImageProcessor`:

```python
# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self._post_process_image(image_name, summary)
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _post_process_image(self, image_name, summary):
        self.document_service.add_image_summary(image_name, summary)
        self.markdown_service.add_image_summary(image_name, summary)
        self.document_service.save_document()
        self.markdown_service.save_markdown()

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history
```

**6. Código Modularizado:**

Para facilitar a visualização e a organização, preparei o código modularizado em arquivos separados:

```
src/
├── config/
│   └── config.py
├── core/
│   ├── handlers/
│   │   └── gemini_handler.py
│   ├── logger_config.py
│   └── rate_limiter.py
├── main.py
├── services/
│   ├── document_service.py
│   ├── gpt_services.py
│   ├── image_processor.py
│   ├── markdown_service.py
│   └── search_files.py
└── utils/
    └── file_utils.py
```

**Observações Finais:**

*   A implementação de todas essas sugestões pode levar tempo, mas o resultado final será um código mais organizado, legível, seguro e fácil de manter.
*   Priorize as melhorias que trazem o maior impacto no seu projeto.
*   Use um sistema de controle de versão (ex: Git) para acompanhar suas mudanças e facilitar a colaboração com outros desenvolvedores.

Se tiver alguma dúvida sobre alguma dessas sugestões, é só perguntar!
User: Extraia o CNPJ dessa imagem.
Assistant: A imagem contém um comprovante de inscrição e de situação cadastral. O CNPJ presente na imagem é:

`33.024.232/0001-39`
User: extraia o número do chamado que começa com  2025
2025-04-10 10:06:09,693 [INFO] Resposta da IA (imagem): O número do chamado que começa com 2025 na imagem é:

`2025040185005611`

2025-04-10 10:06:09,784 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 10:06:09,786 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 10:06:09,787 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 10:06:09,789 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 10:06:09,790 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 10:06:09,791 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 10:06:09,793 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 10:06:09,795 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 10:06:09,797 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 10:06:09,798 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 10:06:09,800 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 10:06:09,801 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 10:06:09,803 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 10:06:09,804 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 10:06:09,806 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 14:06:00,178 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 14:06:00,189 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 14:06:00,195 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 14:06:00,203 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 14:06:00,208 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 14:06:00,215 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 14:06:00,221 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 14:06:00,229 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 14:06:00,234 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 14:06:00,240 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 14:06:00,246 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 14:06:00,252 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 14:06:00,258 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 14:06:00,263 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 14:06:00,268 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 14:06:11,360 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 14:06:11,362 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 14:06:11,363 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 14:06:11,365 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 14:06:11,366 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 14:06:11,368 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 14:06:11,370 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 14:06:11,372 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 14:06:11,373 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 14:06:11,375 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 14:06:11,376 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 14:06:11,378 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 14:06:11,379 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 14:06:11,381 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 14:06:11,382 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 14:06:11,493 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 14:06:11,495 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 14:06:11,496 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 14:06:11,497 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 14:06:11,499 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 14:06:11,500 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 14:06:11,501 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 14:06:11,503 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 14:06:11,504 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 14:06:11,506 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 14:06:11,508 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 14:06:11,511 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 14:06:11,514 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 14:06:11,516 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 14:06:11,518 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 14:06:11,538 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\assets\20250410140611_clipboard_20250410100554.png, Prompt: Você é um assistente inteligente utilizado em um chat com IA, que recebe códigos de uma aplicação completa de chat, como você funciona e tudo que 
é programado para fazer no seu contexto.

Seu objetivo é:
- Analisar todo o contexto e aprender com ele.
- Analisar a Pergunta do Usuário.
- Identificar se possui relação com o contexto.
- Se houver relação com o código do contexto responder a pergunta sobre o código sempre seguindo boas práticas:
    - Analisar o código-fonte contido no projeto enviado
    - Identificar melhorias em legibilidade, performance e segurança
    - Sugerir práticas recomendadas de design e arquitetura
    - Indicar partes com dívidas técnicas ou complexidade desnecessária
    - Enviar código modularizado em pequenos arquivos separados por pastas.

Informações adicionais:
- Estrutura do projeto: segue o padrão de pastas iniciando por src/


## Se houver imagem no contexto, identifique se a pergunta é sobre o código do contexto, sobre a imagem ou sobre os dois para elaborar melhorer respostas.
## Se o usuário fizer uma pergunta que não tem relação com imagem ou contexto, use a sua base de conhecimento fora do contexto para responder de forma inteligente.
** Responda em Português-Brasileiro a não ser que o usuário solicite outra língua.**

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.generativeai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: faça code review da minha aplicação.
Assistant: Com certeza! Analisei o código da sua aplicação e preparei algumas sugestões para melhorias em legibilidade, performance, segurança e design.

**1. Estrutura e Modularização:**

*   **Diretórios:** Organize melhor os arquivos dentro de `src`. Crie subdiretórios para `models`, `services`, `handlers`, `utils`, `config`, etc. Isso facilita a localização dos arquivos e a compreensão da estrutura do projeto.
*   **Arquivo de configuração:** Mova a classe `Config` para um arquivo separado (ex: `config.py`) dentro de um diretório `config`.
*   **Funções:** Quebre funções grandes em funções menores e mais focadas. Isso melhora a legibilidade e facilita o teste.

**2. Boas Práticas e Melhorias no Código:**

*   **Rate Limiting:** Implemente o `RateLimiter` nos serviços que consomem a API do Gemini (ex: `ImageProcessor`, `GeminiHandler`). Isso evita sobrecarga na API e garante um uso mais eficiente.
*   **Configurações:** Use a classe `Config` para centralizar todas as configurações da aplicação (ex: chaves de API, modelos, diretórios). Isso facilita a manutenção e a configuração do projeto.
*   **Logging:** Padronize o uso do logger em toda a aplicação. Use os níveis de log apropriados (ex: `info`, `warning`, `error`) para cada tipo de mensagem.
*   **Tratamento de Erros:** Adicione tratamento de erros mais específico em cada função. Isso ajuda a identificar e corrigir problemas mais rapidamente.
*   **Cache:** Utilize o `st.cache_resource` para funções que realizam operações custosas, como a inicialização do `GeminiHandler`. Isso evita a execução repetida dessas operações.
*   **Tipagem:** Utilize tipagem estática (type hints) para melhorar a legibilidade e a segurança do código.
*   **Imagens:** Valide se `uploaded_file` é realmente um arquivo de imagem antes de tentar exibi-lo, evitando erros caso o usuário envie um arquivo de outro tipo.
*   **Prompts:** Crie arquivos de prompt separados para diferentes tarefas (ex: chat, análise de imagem). Isso facilita a manutenção e a personalização dos prompts.

**3. Segurança:**

*   **Chaves de API:** Nunca coloque suas chaves de API diretamente no código. Use variáveis de ambiente ou um arquivo de configuração seguro para armazená-las.
*   **Validação de Input:** Valide todos os inputs do usuário para evitar ataques de injeção de código.

**4. Arquitetura:**

*   **Camadas:** Defina claramente as responsabilidades de cada camada da sua aplicação (ex: apresentação, lógica de negócios, acesso a dados). Isso facilita a manutenção e a evolução do projeto.
*   **Inversão de Dependência:** Use inversão de dependência para desacoplar as diferentes partes da sua aplicação. Isso facilita o teste e a substituição de componentes.
*   **Testes:** Escreva testes unitários e de integração para garantir a qualidade do seu código.

**5. Dívidas Técnicas e Complexidade:**

*   **Funções Longas:** Identifique funções com muitas linhas de código e divida-as em funções menores e mais focadas.
*   **Lógica Duplicada:** Elimine a lógica duplicada criando funções ou classes reutilizáveis.
*   **Comentários:** Adicione comentários para explicar o que o código faz, especialmente em partes complexas.

**Exemplo de Refatoração:**

Aqui está um exemplo de como você pode refatorar a função `process_images` na classe `ImageProcessor`:

```python
# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self._post_process_image(image_name, summary)
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _post_process_image(self, image_name, summary):
        self.document_service.add_image_summary(image_name, summary)
        self.markdown_service.add_image_summary(image_name, summary)
        self.document_service.save_document()
        self.markdown_service.save_markdown()

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history
```

**6. Código Modularizado:**

Para facilitar a visualização e a organização, preparei o código modularizado em arquivos separados:

```
src/
├── config/
│   └── config.py
├── core/
│   ├── handlers/
│   │   └── gemini_handler.py
│   ├── logger_config.py
│   └── rate_limiter.py
├── main.py
├── services/
│   ├── document_service.py
│   ├── gpt_services.py
│   ├── image_processor.py
│   ├── markdown_service.py
│   └── search_files.py
└── utils/
    └── file_utils.py
```

**Observações Finais:**

*   A implementação de todas essas sugestões pode levar tempo, mas o resultado final será um código mais organizado, legível, seguro e fácil de manter.
*   Priorize as melhorias que trazem o maior impacto no seu projeto.
*   Use um sistema de controle de versão (ex: Git) para acompanhar suas mudanças e facilitar a colaboração com outros desenvolvedores.

Se tiver alguma dúvida sobre alguma dessas sugestões, é só perguntar!
User: Extraia o CNPJ dessa imagem.
Assistant: A imagem contém um comprovante de inscrição e de situação cadastral. O CNPJ presente na imagem é:

`33.024.232/0001-39`
User: extraia o número do chamado que começa com  2025
Assistant: O número do chamado que começa com 2025 na imagem é:

`2025040185005611`

User:  Okay, sorry, this is. Okay, so welcome everyone. So our week we call for Brazil at a ca Dow. So for today, we will discuss the following the ARIBA, the production API, the P2P and S2C ticket status updates. And if we have business processes and change management. So for the ARIBA, here are depending the number of pending decisions and open actions. So we have. So for all we have one action and one information and then. For ERP team, we have two actions and two information. So we have a total of six. So let me go to the ARIBA file. Okay, so here are the items that we have in the ARIBA file. So for the first one, we have the domain name change. So for this one, so this is actually extended until January 31, 2026. But yeah, Flavio, I just want to know if we have progress on this one. I need to check and then I get back to you on this. On this sheet, please. Okay, thank you so much. Okay, so. You have included another one. Yes, that's correct. Yeah, so for the next one, we actually have also the TLS 1 point to connection. To. To be transferred to a TLS 1 point. So this is also for your team to check. So the deadline for this one will be on April 30. So do we have also progress on this one or not yet? I believe that I already described my current situation on a column. On the right on the right. You can find my call up. Sorry, here or on the tracker file, be the change. Sorry, let me check. This one. This one. I'm not sure where are the comments that you have. No, I just have my clue comments about the first one in the second and the TLS. So we are already done. The line five. Let me see if I can. Is it. Ah, this one. Okay. Okay, so this is already close. Yes, this. Yeah. So for that one, could you please provide screen shots here if possible? To stretch. Yes, just to show that it is already deployed on your system. As an evidence. Okay. Thank you so much. Okay. Okay, so let me remove this one also. I don't know why this one educated. Okay, so for the next two items is read your cards to the certificate renewal. So this is still for information only once the assertifcates have been released by. So I will be able to provide it to you also. Either in the juror ticket or via. So actually this should not be in progress. Okay. And then for the next one, we have also the. This environment frees for the. Parent real. Or the for the S to see items actually so. This is still ongoing right now. So we are checking internally with the project team when. This project will end. So for now. All the test things for the. Related to a supplier's contracts. Our entries for now. And lastly for the daily monitor and so since Ricardo is not here. Yes, so. Oh, sorry, I did hear. So just so this is a reminder for. That for them to check the. Monitoring file for Brazil. So actually. We are encountering some or we are a cap we have some documents captured in the daily monitoring. So if you could also help with the card to check this capture documents that would be appreciated. I'm not sure if this is the correct. Okay, so this is still on March. Let me just change this one. Okay. Okay. Let me share my screen. Okay, so this is actually the. Monitoring file for Brazil. So right now we only for now we have one so we are still updating the monitoring file. So yeah, if you could help check this one. And see why this PR is having an error in the. Are you beside or in the air beside render. So this is the. What they call this the error message that was encountered. So yeah, if you could check this on the ER beside it would be appreciate. For the other. I don't think we have some captured documents yet. Yeah, so for now we'll be have one. Just this one. So yeah, that's all for the items. So they have questions before move on to our next items. No, please go ahead. Okay, thank you so much. Okay, so for the production KPIs, it's the same as the last week as discussed. So they are the numbers or the figures for the documents for the transactions in harriba and for the care tickets that have been taken by a to an L3. So as for the P2P tickets status update so for the past week we. So we have a result or we handled six tickets for P2P and as for the open items. So for the first one we have the issue with regards to the PR that is in order processing status. So for this one, yeah, the current issue is that this PR is currently in version three and it is showing an error that the version two has been has been stopping ordering. However, we cannot. Reprocess this one on the CIG side, so that's why we raise an SR for this one. Or an Arriba case. So right now we are trying to set up a call with the Arriba vendor as they are also asking for our availability. So once we have established a schedule for the meeting and discuss this with the Arriba vendor. So we will update the care ticket about the discussion. And for the next one, we have the integration issue for this supply. Can I attend to the point one please. So we are pending Arriba vendor get back to us with a viable time slots to have a conversation. So for this one, we so the Accenture team will discuss this first with the SAP Arriba vendor as we have all the details already. So if so we will discuss all the investigations that we had so far in order to help them further investigate this issue. But according to the plan for the city reader, we also talk about the updating of our environment of testing. If I'm not wrong. That the queue us or the. We have plenty to update our environment of testing. And the QA test test environment. The point is I need to go ahead with our S a p requirements that I have shared with you by. Sorry. So I don't know if this point is blocking our evaluation regarding to S a p. This is the point. The one that you have in email or. Yeah, 200 for eight or far nine. S a p requirements. I mean, we'll have on this. Let me see if I can. The line eight. Are you have very an update in this one? Yes, so for this one, this is only in a riba side of love you and not on the other side. Yeah, but. I'm aware about S a p. The point for me is S a p evaluation. So I need to implement new requirements. My S a p environment in production. So I don't know if the test environment. We need to be update in order to go ahead with this S a p new requirements. Yeah, so for this one of lab, you I think we need to. Wait for the synchronization of the data from the production system to the test environment to be finished. So we do implementations of data changes or. To do some. Testing scenarios in the S to see part. Yeah, but when we are when we will be ready. I have mentioned earlier, we will coordinate this with the project team who's doing the project for this project specifically and ask them when this will be finished as the. Other conscious and Bruno is also asking for this one. So we will update you accordingly regarding this one. Okay. Thank you. Okay. Yeah, so let me discuss first the B to B items. Okay, so for the next one, we have the issue with regards to this supplier. Which was raised by a Tadita. So for this one, they are. And the Tadita mentioned in the ticket that they are coordinating with the bank team for the confirmation of the ACM ID for the supplier. And the last update was made to me about this ticket. And the next one is explained to the explain needs to be created, which already exists. And the value one B are one data needs to be partitioned into to these other suppliers. And that after this is done, they will probably you will proceed with the supplier patchy partitioning for a base trace attack. Yes. Yeah. And you also ask to ask to us to check with the bank if the ACM ID of the supplier is a 35 for. Yes. Something. I don't remember everything. This one. Yeah. Yeah. But the informant says that they don't know what they. ACM, ACM ID of the supplier is. And the next case, can we deactivate this ACM ID and position are from. We will see. Yeah, we will see if we can. If we can try to partition it again, because yes, I have showed in the ticket on our discussion tab. There are two suppliers with that are currently existing, which is one having this ACM ID and the other has no ACM ID has no ACM ACM ID mapping yet. So that's why I suggested to ask the bank team if this is their. They don't know. That's. They don't know this. So. This don't they don't know this information. But. We thought that we can deactivate the ACM and partition it hours. Because they don't pass. They don't know this information, which is ACM ID from this supplier. But they are still using this supplier, right? Yeah. I don't know. Don't sure. We we we talked to the bank. So they were they are already know that we ask to you. We are asking to you to activate this ACM. Yeah, I'm not sure if PCM ID can be deactivated since actually this data is coming from the ERP system. So if they don't know. So if you're asking the local team of the bank team, maybe you can also suggest if if they're ERP team knows this data. But yeah, for now, we will see if if the data for the. The data can be mapped to each to one of the suppliers that I have showed I have shown you in the. In the screenshot. That I have provided. Okay. So right now we can deactivate this ACM. We need to with the bank again. Right. Yes. Yes. Yes. Specifically, maybe as their ERP team instead. Because I don't think the local team knows where this data is coming from. But yeah, as far as I'm concerned, this is integrated in that is or this is this data is coming from the ERP system rather. Okay. We're going to check again. And let you. Sure. For now, we will I will do my best to keep the ticket open for now. Okay. Yeah, so I think that's also do you have questions for these two items for the ptp before we move on to SCC items. Okay. Okay. Thank you. Okay. So for the S2C items. So yeah, we have two items here, which is with regards to the SLP template. Adjustment in SLP template and the. So for the S2 items, both of these are currently still in progress due to the synchronization that is happening between the production and test systems. So for the second item. Yeah, so for this one, this for this one, known, known modifications are allowed for now since yeah. We need to wait for this synchronization synchronization to be finished. And I think we have one more item I haven't I wasn't able to add it here. Yeah, so for this request from you, I love you. I believe with regards to this one. So. We will we are still checking this internally. So once we receive a conclusion for this one, we will update you accordingly. Okay. Thank you. Yeah, I will add this to the read the. Sorry, I forgot to add this yet. But yeah, we will check this internet. So let me know when you test environment, it's a very uploaded. Yeah. Yeah, we will get back to you also for that. Okay. Yeah, so that's all from my side. Do you have other questions or concerns that we can address in the call before we end this? No, for my side. No. Are you guys? Me neither. Okay. Thank you. First one, thank you. Thank you, Ray. Thank you, everyone. And yeah, I will send the minutes of the meeting after the call. So yeah, thank you, everyone. And I wish you a great day ahead. Bye.

Traduza para pt-br
2025-04-10 14:06:31,333 [INFO] Resposta da IA (imagem): Okay, aqui está a tradução para português brasileiro do diálogo fornecido:

"Okay, desculpem, é isso. Okay, então bem-vindos a todos. Então, nossa semana chamamos para Brasil na CA Dow. Então, para hoje, vamos discutir o seguinte: o ARIBA, a API de produção, o P2P e atualizações de status de tickets S2C. E se tivermos processos de negócios e gestão de mudanças. Então, para o ARIBA, aqui estão dependendo o número de decisões pendentes e ações em aberto. Então nós temos. Então, para todos nós temos uma ação e uma informação e então. Para a equipe ERP, temos duas ações e duas informações. Então, temos um total de seis. Então, deixe-me ir para o arquivo ARIBA. Okay, então aqui estão os itens que temos no arquivo ARIBA. Então, para o primeiro, temos a mudança de nome de domínio. Então, para este, isso é realmente estendido até 31 de janeiro de 2026. Mas sim, Flávio, eu só quero saber se temos progresso neste. Eu preciso verificar e então eu volto para você neste. Nesta folha, por favor. Okay, muito obrigado. Okay, então. Você incluiu outro. Sim, está correto. Sim, então para o próximo, nós realmente temos também a conexão TLS 1 ponto para. Para. Para ser transferida para um TLS 1 ponto. Então isso também é para sua equipe verificar. Então o prazo para este será em 30 de abril. Então, nós temos também progresso neste ou ainda não? Eu acredito que eu já descrevi minha situação atual em uma coluna. Na direita, na direita. Você pode encontrar minha chamada. Desculpe, aqui ou no arquivo de rastreamento, seja a mudança. Desculpe, deixe-me verificar. Este. Este. Eu não tenho certeza onde estão os comentários que você tem. Não, eu só tenho meus comentários de pistas sobre o primeiro e o segundo e o TLS. Então, já terminamos. A linha cinco. Deixe-me ver se eu posso. É isso. Ah, este. Okay. Okay, então isso já está fechado. Sim, isto. Sim. Então para isso, você poderia por favor fornecer capturas de tela aqui se possível? Para alongar. Sim, apenas para mostrar que já está implantado em seu sistema. Como uma evidência. Okay. Muito obrigado. Okay. Okay, então deixe-me remover este também. Eu não sei por que este educado. Okay, então para os próximos dois itens é ler seus cartões para a renovação do certificado. Então, isto ainda é apenas para informação, uma vez que os assertifcates tenham sido liberados por. Então eu poderei fornecê-lo a você também. Seja no ingresso do júri ou via. Então, na verdade isso não deveria estar em andamento. Okay. E então para o próximo, temos também o. Este ambiente libera para o. Real pai. Ou o para os itens S para ver realmente tão. Isto ainda está em andamento agora. Então estamos verificando internamente com a equipe do projeto quando. Este projeto terminará. Então por agora. Todas as coisas de teste para o. Relacionado aos contratos de um fornecedor. Nossas entradas por enquanto. E, finalmente, para o monitor diário e então, já que Ricardo não está aqui. Sim, então. Oh, desculpe, eu ouvi aqui. Então, apenas então isso é um lembrete para. Que para eles verifiquem o. Arquivo de monitoramento para o Brasil. Então, realmente. Estamos encontrando alguns ou estamos um cap nós temos alguns documentos capturados no monitoramento diário. Então, se você também pudesse ajudar com o cartão para verificar esses documentos capturados, seria apreciado. Não tenho certeza se isto está correto. Okay, então isto ainda está em março. Deixe-me apenas mudar isto. Okay. Okay. Deixe-me compartilhar minha tela. Okay, então este é realmente o. Arquivo de monitoramento para o Brasil. Então agora nós apenas por enquanto temos um, então ainda estamos atualizando o arquivo de monitoramento. Então sim, se você pudesse ajudar a verificar este. E veja por que este RP está tendo um erro no. Você está ao lado ou no ar ao lado render. Então este é o. O que eles chamam isto a mensagem de erro que foi encontrada. Então sim, se você pudesse verificar isso no ER ao lado, seria apreciado. Para os outros. Eu não acho que temos alguns documentos capturados ainda. Sim, então por agora nós teremos um. Apenas este. Então sim, é tudo para os itens. Então eles têm perguntas antes de passarmos para nossos próximos itens. Não, por favor prossiga. Okay, muito obrigado. Okay, então para os KPIs de produção, é o mesmo que na semana passada, conforme discutido. Então eles são os números ou as figuras para os documentos para as transações em harriba e para os bilhetes de atendimento que foram tomados por um para um L3. Então, quanto à atualização do status dos bilhetes de atendimento P2P, então na semana passada nós. Então temos um resultado ou lidamos com seis bilhetes de atendimento para P2P e quanto aos itens em aberto. Então, para o primeiro, temos o problema em relação ao PR que está em estado de processamento de pedidos. Então, para este, sim, o problema atual é que este PR está atualmente na versão três e está mostrando um erro de que a versão dois tem interrompido a encomenda. No entanto, não podemos. Reprocessar este no lado CIG, então é por isso que levantamos um SR para este. Ou um caso Arriba. Então agora estamos tentando agendar uma chamada com o fornecedor Arriba, pois eles também estão perguntando pela nossa disponibilidade. Então, uma vez que tenhamos estabelecido uma programação para a reunião e discutirmos isso com o fornecedor Arriba. Então atualizaremos o bilhete de atendimento sobre a discussão. E para o próximo, temos o problema de integração para este fornecimento. Posso atender ao ponto um por favor. Então, estamos pendentes para que o fornecedor Arriba volte para nós com horários viáveis para ter uma conversa. Então, para este, nós, a equipe Accenture, discutiremos isso primeiro com o fornecedor SAP Arriba, pois já temos todos os detalhes. Então, se assim for, discutiremos todas as investigações que tivemos até agora, a fim de ajudá-los a investigar ainda mais este problema. Mas de acordo com o plano para o leitor da cidade, também falamos sobre a atualização do nosso ambiente de teste. Se eu não estiver enganado. Que o Q us ou o. Temos muito para atualizar nosso ambiente de teste. E o ambiente de teste QA. O ponto é que eu preciso prosseguir com os nossos requisitos S a p que compartilhei com você por. Desculpe. Então, eu não sei se este ponto está bloqueando a nossa avaliação em relação ao S a p. Este é o ponto. Aquele que você tem no e-mail ou. Sim, 200 para oito ou longe nove. Requisitos S a p. Quero dizer, nós teremos neste. Deixe-me ver se eu posso. A linha oito. Você tem alguma atualização nisto? Sim, então, para este, isto é apenas no lado do Ariba do amor você e não no outro lado. Sim, mas. Estou ciente sobre S a p. O ponto para mim é a avaliação S a p. Então eu preciso implementar novos requisitos. Meu ambiente S a p em produção. Então eu não sei se o ambiente de teste. Precisamos ser atualizados para prosseguir com estes novos requisitos S a p. Sim, então, para este amor, você, eu acho que precisamos. Espere pela sincronização dos dados do sistema de produção para o ambiente de teste para terminar. Então nós fazemos implementações de mudanças de dados ou. Para fazer alguns. Cenários de teste na parte S para ver. Sim, mas quando estaremos prontos. Eu mencionei anteriormente, coordenaremos isto com a equipe do projeto que está fazendo o projeto para este projeto especificamente e perguntaremos quando isto terminará como o. Outro consciente e Bruno também está pedindo por isso. Então nós o atualizaremos em conformidade em relação a este. Okay. Obrigado. Okay. Sim, então deixe-me discutir primeiro os itens B para B. Okay, então para o próximo, temos o problema em relação a este fornecedor. Que foi levantado por um Tadita. Então, para este, eles estão. E o Tadita mencionou no bilhete de atendimento que eles estão coordenando com a equipe do banco para a confirmação do ACM ID para o fornecedor. E a última atualização foi feita para mim sobre este bilhete de atendimento. E o próximo precisa ser criado, o que já existe. E o valor um B são um dados precisa ser particionado em para estes outros fornecedores. E que depois que isso for feito, eles provavelmente prosseguirão com a partição do fornecedor para um ataque de rastreamento base. Sim. Sim. E você também pede para nos pedir para verificar com o banco se o ACM ID do fornecedor é um 35 para. Sim. Algo. Eu não lembro de tudo. Este. Sim. Sim. Mas o informante diz que eles não sabem o que eles. ACM, ACM ID do fornecedor é. E no próximo caso, podemos desativar este ACM ID e posicionar são de. Nós veremos. Sim, nós veremos se podemos. Se podemos tentar particioná-lo novamente, porque sim, eu mostrei no bilhete de atendimento na nossa aba de discussão. Existem dois fornecedores com que estão atualmente existentes, que é um tendo este ACM ID e o outro não tem ACM ID não tem ACM mapeamento de ACM ainda. Então é por isso que eu sugeri pedir à equipe do banco se este é deles. Eles não sabem. Isso. Eles não sabem esta informação. Mas. Nós pensamos que podemos desativar o ACM e particioná-lo horas. Porque eles não passam. Eles não conhecem esta informação, que é o ACM ID deste fornecedor. Mas eles ainda estão usando este fornecedor, certo? Sim. Eu não sei. Não tenho certeza. Nós conversamos com o banco. Então eles já sabem que nós pedimos para você. Estamos pedindo para você ativar este ACM. Sim, não tenho certeza se o PCM ID pode ser desativado, já que na verdade estes dados estão vindo do sistema ERP. Então, se eles não sabem. Então, se você está perguntando para a equipe local da equipe do banco, talvez você também possa sugerir se se a equipe ERP deles sabe estes dados. Mas sim, por enquanto, veremos se se os dados para o. Os dados podem ser mapeados para cada um dos fornecedores que eu mostrei que eu mostrei a você no. Na captura de tela. Que eu forneci. Okay. Então, agora não podemos desativar este ACM. Precisamos com o banco novamente. Certo. Sim. Sim. Especificamente, talvez como sua equipe ERP em vez disso. Porque eu não acho que a equipe local sabe de onde estes dados estão vindo. Mas sim, tanto quanto eu sei, isto está integrado em que é ou estes dados estão vindo do sistema ERP em vez disso. Okay. Nós vamos verificar novamente. E deixe você. Claro. Por enquanto, eu farei o meu melhor para manter o bilhete de atendimento aberto por enquanto. Okay. Sim, então eu acho que isso também, você tem perguntas para estes dois itens para o ptp antes de passarmos para os itens SCC. Okay. Okay. Obrigado. Okay. Então, para os itens S2C. Então sim, temos dois itens aqui, que é em relação ao modelo SLP. Ajuste no modelo SLP e o. Então para os itens S2, ambos estes estão atualmente ainda em andamento devido à sincronização que está acontecendo entre os sistemas de produção e teste. Então para o segundo item. Sim, então para este, para este conhecido, nenhuma modificação conhecida é permitida por enquanto, já que sim. Precisamos esperar por esta sincronização sincronização para terminar. E eu acho que temos mais um item eu não fui capaz de adicioná-lo aqui. Sim, então para este pedido seu, eu te amo. Eu acredito com relação a este. Tão. Nós verificaremos isto internamente. Então, uma vez que recebermos uma conclusão para este, nós o atualizaremos em conformidade. Okay. Obrigado. Sim, eu adicionarei isto ao leia o. Desculpe, eu esqueci de adicionar isto ainda. Mas sim, nós verificaremos isto na internet. Então me avise quando seu ambiente de teste, é uma atualização muito grande. Sim. Sim, nós entraremos em contato com você também para isso. Okay. Sim, então é tudo da minha parte. Você tem outras perguntas ou preocupações que podemos abordar na chamada antes de terminarmos isto? Não, da minha parte. Não. Vocês? Eu também não. Okay. Obrigado. Primeiro, obrigado. Obrigado a todos. E sim, eu enviarei as atas da reunião após a chamada. Então sim, obrigado a todos. E eu desejo a vocês um ótimo dia pela frente. Tchau."

Espero que isso ajude! Avise-me se precisar de mais alguma coisa.

2025-04-10 14:06:31,482 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 14:06:31,484 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 14:06:31,486 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 14:06:31,488 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 14:06:31,489 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 14:06:31,491 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 14:06:31,493 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 14:06:31,495 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 14:06:31,496 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 14:06:31,497 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 14:06:31,499 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 14:06:31,501 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 14:06:31,502 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 14:06:31,503 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 14:06:31,504 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 14:10:51,408 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 14:10:51,410 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 14:10:51,412 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 14:10:51,414 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 14:10:51,416 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 14:10:51,417 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 14:10:51,419 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 14:10:51,421 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 14:10:51,423 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 14:10:51,425 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 14:10:51,427 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 14:10:51,429 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 14:10:51,431 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 14:10:51,433 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 14:10:51,435 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 14:10:51,638 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 14:10:51,640 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 14:10:51,641 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 14:10:51,643 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 14:10:51,645 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 14:10:51,647 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 14:10:51,649 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 14:10:51,650 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 14:10:51,653 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 14:10:51,657 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 14:10:51,659 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 14:10:51,661 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 14:10:51,662 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 14:10:51,664 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 14:10:51,666 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 14:10:51,679 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\assets\20250410141051_clipboard_20250410100554.png, Prompt: Você é um assistente inteligente utilizado em um chat com IA, que recebe códigos de uma aplicação completa de chat, como você funciona e tudo que 
é programado para fazer no seu contexto.

Seu objetivo é:
- Analisar todo o contexto e aprender com ele.
- Analisar a Pergunta do Usuário.
- Identificar se possui relação com o contexto.
- Se houver relação com o código do contexto responder a pergunta sobre o código sempre seguindo boas práticas:
    - Analisar o código-fonte contido no projeto enviado
    - Identificar melhorias em legibilidade, performance e segurança
    - Sugerir práticas recomendadas de design e arquitetura
    - Indicar partes com dívidas técnicas ou complexidade desnecessária
    - Enviar código modularizado em pequenos arquivos separados por pastas.

Informações adicionais:
- Estrutura do projeto: segue o padrão de pastas iniciando por src/


## Se houver imagem no contexto, identifique se a pergunta é sobre o código do contexto, sobre a imagem ou sobre os dois para elaborar melhorer respostas.
## Se o usuário fizer uma pergunta que não tem relação com imagem ou contexto, use a sua base de conhecimento fora do contexto para responder de forma inteligente.
** Responda em Português-Brasileiro a não ser que o usuário solicite outra língua.**

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.generativeai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: faça code review da minha aplicação.
Assistant: Com certeza! Analisei o código da sua aplicação e preparei algumas sugestões para melhorias em legibilidade, performance, segurança e design.

**1. Estrutura e Modularização:**

*   **Diretórios:** Organize melhor os arquivos dentro de `src`. Crie subdiretórios para `models`, `services`, `handlers`, `utils`, `config`, etc. Isso facilita a localização dos arquivos e a compreensão da estrutura do projeto.
*   **Arquivo de configuração:** Mova a classe `Config` para um arquivo separado (ex: `config.py`) dentro de um diretório `config`.
*   **Funções:** Quebre funções grandes em funções menores e mais focadas. Isso melhora a legibilidade e facilita o teste.

**2. Boas Práticas e Melhorias no Código:**

*   **Rate Limiting:** Implemente o `RateLimiter` nos serviços que consomem a API do Gemini (ex: `ImageProcessor`, `GeminiHandler`). Isso evita sobrecarga na API e garante um uso mais eficiente.
*   **Configurações:** Use a classe `Config` para centralizar todas as configurações da aplicação (ex: chaves de API, modelos, diretórios). Isso facilita a manutenção e a configuração do projeto.
*   **Logging:** Padronize o uso do logger em toda a aplicação. Use os níveis de log apropriados (ex: `info`, `warning`, `error`) para cada tipo de mensagem.
*   **Tratamento de Erros:** Adicione tratamento de erros mais específico em cada função. Isso ajuda a identificar e corrigir problemas mais rapidamente.
*   **Cache:** Utilize o `st.cache_resource` para funções que realizam operações custosas, como a inicialização do `GeminiHandler`. Isso evita a execução repetida dessas operações.
*   **Tipagem:** Utilize tipagem estática (type hints) para melhorar a legibilidade e a segurança do código.
*   **Imagens:** Valide se `uploaded_file` é realmente um arquivo de imagem antes de tentar exibi-lo, evitando erros caso o usuário envie um arquivo de outro tipo.
*   **Prompts:** Crie arquivos de prompt separados para diferentes tarefas (ex: chat, análise de imagem). Isso facilita a manutenção e a personalização dos prompts.

**3. Segurança:**

*   **Chaves de API:** Nunca coloque suas chaves de API diretamente no código. Use variáveis de ambiente ou um arquivo de configuração seguro para armazená-las.
*   **Validação de Input:** Valide todos os inputs do usuário para evitar ataques de injeção de código.

**4. Arquitetura:**

*   **Camadas:** Defina claramente as responsabilidades de cada camada da sua aplicação (ex: apresentação, lógica de negócios, acesso a dados). Isso facilita a manutenção e a evolução do projeto.
*   **Inversão de Dependência:** Use inversão de dependência para desacoplar as diferentes partes da sua aplicação. Isso facilita o teste e a substituição de componentes.
*   **Testes:** Escreva testes unitários e de integração para garantir a qualidade do seu código.

**5. Dívidas Técnicas e Complexidade:**

*   **Funções Longas:** Identifique funções com muitas linhas de código e divida-as em funções menores e mais focadas.
*   **Lógica Duplicada:** Elimine a lógica duplicada criando funções ou classes reutilizáveis.
*   **Comentários:** Adicione comentários para explicar o que o código faz, especialmente em partes complexas.

**Exemplo de Refatoração:**

Aqui está um exemplo de como você pode refatorar a função `process_images` na classe `ImageProcessor`:

```python
# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self._post_process_image(image_name, summary)
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _post_process_image(self, image_name, summary):
        self.document_service.add_image_summary(image_name, summary)
        self.markdown_service.add_image_summary(image_name, summary)
        self.document_service.save_document()
        self.markdown_service.save_markdown()

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history
```

**6. Código Modularizado:**

Para facilitar a visualização e a organização, preparei o código modularizado em arquivos separados:

```
src/
├── config/
│   └── config.py
├── core/
│   ├── handlers/
│   │   └── gemini_handler.py
│   ├── logger_config.py
│   └── rate_limiter.py
├── main.py
├── services/
│   ├── document_service.py
│   ├── gpt_services.py
│   ├── image_processor.py
│   ├── markdown_service.py
│   └── search_files.py
└── utils/
    └── file_utils.py
```

**Observações Finais:**

*   A implementação de todas essas sugestões pode levar tempo, mas o resultado final será um código mais organizado, legível, seguro e fácil de manter.
*   Priorize as melhorias que trazem o maior impacto no seu projeto.
*   Use um sistema de controle de versão (ex: Git) para acompanhar suas mudanças e facilitar a colaboração com outros desenvolvedores.

Se tiver alguma dúvida sobre alguma dessas sugestões, é só perguntar!
User: Extraia o CNPJ dessa imagem.
Assistant: A imagem contém um comprovante de inscrição e de situação cadastral. O CNPJ presente na imagem é:

`33.024.232/0001-39`
User: extraia o número do chamado que começa com  2025
Assistant: O número do chamado que começa com 2025 na imagem é:

`2025040185005611`

User:  Okay, sorry, this is. Okay, so welcome everyone. So our week we call for Brazil at a ca Dow. So for today, we will discuss the following the ARIBA, the production API, the P2P and S2C ticket status updates. And if we have business processes and change management. So for the ARIBA, here are depending the number of pending decisions and open actions. So we have. So for all we have one action and one information and then. For ERP team, we have two actions and two information. So we have a total of six. So let me go to the ARIBA file. Okay, so here are the items that we have in the ARIBA file. So for the first one, we have the domain name change. So for this one, so this is actually extended until January 31, 2026. But yeah, Flavio, I just want to know if we have progress on this one. I need to check and then I get back to you on this. On this sheet, please. Okay, thank you so much. Okay, so. You have included another one. Yes, that's correct. Yeah, so for the next one, we actually have also the TLS 1 point to connection. To. To be transferred to a TLS 1 point. So this is also for your team to check. So the deadline for this one will be on April 30. So do we have also progress on this one or not yet? I believe that I already described my current situation on a column. On the right on the right. You can find my call up. Sorry, here or on the tracker file, be the change. Sorry, let me check. This one. This one. I'm not sure where are the comments that you have. No, I just have my clue comments about the first one in the second and the TLS. So we are already done. The line five. Let me see if I can. Is it. Ah, this one. Okay. Okay, so this is already close. Yes, this. Yeah. So for that one, could you please provide screen shots here if possible? To stretch. Yes, just to show that it is already deployed on your system. As an evidence. Okay. Thank you so much. Okay. Okay, so let me remove this one also. I don't know why this one educated. Okay, so for the next two items is read your cards to the certificate renewal. So this is still for information only once the assertifcates have been released by. So I will be able to provide it to you also. Either in the juror ticket or via. So actually this should not be in progress. Okay. And then for the next one, we have also the. This environment frees for the. Parent real. Or the for the S to see items actually so. This is still ongoing right now. So we are checking internally with the project team when. This project will end. So for now. All the test things for the. Related to a supplier's contracts. Our entries for now. And lastly for the daily monitor and so since Ricardo is not here. Yes, so. Oh, sorry, I did hear. So just so this is a reminder for. That for them to check the. Monitoring file for Brazil. So actually. We are encountering some or we are a cap we have some documents captured in the daily monitoring. So if you could also help with the card to check this capture documents that would be appreciated. I'm not sure if this is the correct. Okay, so this is still on March. Let me just change this one. Okay. Okay. Let me share my screen. Okay, so this is actually the. Monitoring file for Brazil. So right now we only for now we have one so we are still updating the monitoring file. So yeah, if you could help check this one. And see why this PR is having an error in the. Are you beside or in the air beside render. So this is the. What they call this the error message that was encountered. So yeah, if you could check this on the ER beside it would be appreciate. For the other. I don't think we have some captured documents yet. Yeah, so for now we'll be have one. Just this one. So yeah, that's all for the items. So they have questions before move on to our next items. No, please go ahead. Okay, thank you so much. Okay, so for the production KPIs, it's the same as the last week as discussed. So they are the numbers or the figures for the documents for the transactions in harriba and for the care tickets that have been taken by a to an L3. So as for the P2P tickets status update so for the past week we. So we have a result or we handled six tickets for P2P and as for the open items. So for the first one we have the issue with regards to the PR that is in order processing status. So for this one, yeah, the current issue is that this PR is currently in version three and it is showing an error that the version two has been has been stopping ordering. However, we cannot. Reprocess this one on the CIG side, so that's why we raise an SR for this one. Or an Arriba case. So right now we are trying to set up a call with the Arriba vendor as they are also asking for our availability. So once we have established a schedule for the meeting and discuss this with the Arriba vendor. So we will update the care ticket about the discussion. And for the next one, we have the integration issue for this supply. Can I attend to the point one please. So we are pending Arriba vendor get back to us with a viable time slots to have a conversation. So for this one, we so the Accenture team will discuss this first with the SAP Arriba vendor as we have all the details already. So if so we will discuss all the investigations that we had so far in order to help them further investigate this issue. But according to the plan for the city reader, we also talk about the updating of our environment of testing. If I'm not wrong. That the queue us or the. We have plenty to update our environment of testing. And the QA test test environment. The point is I need to go ahead with our S a p requirements that I have shared with you by. Sorry. So I don't know if this point is blocking our evaluation regarding to S a p. This is the point. The one that you have in email or. Yeah, 200 for eight or far nine. S a p requirements. I mean, we'll have on this. Let me see if I can. The line eight. Are you have very an update in this one? Yes, so for this one, this is only in a riba side of love you and not on the other side. Yeah, but. I'm aware about S a p. The point for me is S a p evaluation. So I need to implement new requirements. My S a p environment in production. So I don't know if the test environment. We need to be update in order to go ahead with this S a p new requirements. Yeah, so for this one of lab, you I think we need to. Wait for the synchronization of the data from the production system to the test environment to be finished. So we do implementations of data changes or. To do some. Testing scenarios in the S to see part. Yeah, but when we are when we will be ready. I have mentioned earlier, we will coordinate this with the project team who's doing the project for this project specifically and ask them when this will be finished as the. Other conscious and Bruno is also asking for this one. So we will update you accordingly regarding this one. Okay. Thank you. Okay. Yeah, so let me discuss first the B to B items. Okay, so for the next one, we have the issue with regards to this supplier. Which was raised by a Tadita. So for this one, they are. And the Tadita mentioned in the ticket that they are coordinating with the bank team for the confirmation of the ACM ID for the supplier. And the last update was made to me about this ticket. And the next one is explained to the explain needs to be created, which already exists. And the value one B are one data needs to be partitioned into to these other suppliers. And that after this is done, they will probably you will proceed with the supplier patchy partitioning for a base trace attack. Yes. Yeah. And you also ask to ask to us to check with the bank if the ACM ID of the supplier is a 35 for. Yes. Something. I don't remember everything. This one. Yeah. Yeah. But the informant says that they don't know what they. ACM, ACM ID of the supplier is. And the next case, can we deactivate this ACM ID and position are from. We will see. Yeah, we will see if we can. If we can try to partition it again, because yes, I have showed in the ticket on our discussion tab. There are two suppliers with that are currently existing, which is one having this ACM ID and the other has no ACM ID has no ACM ACM ID mapping yet. So that's why I suggested to ask the bank team if this is their. They don't know. That's. They don't know this. So. This don't they don't know this information. But. We thought that we can deactivate the ACM and partition it hours. Because they don't pass. They don't know this information, which is ACM ID from this supplier. But they are still using this supplier, right? Yeah. I don't know. Don't sure. We we we talked to the bank. So they were they are already know that we ask to you. We are asking to you to activate this ACM. Yeah, I'm not sure if PCM ID can be deactivated since actually this data is coming from the ERP system. So if they don't know. So if you're asking the local team of the bank team, maybe you can also suggest if if they're ERP team knows this data. But yeah, for now, we will see if if the data for the. The data can be mapped to each to one of the suppliers that I have showed I have shown you in the. In the screenshot. That I have provided. Okay. So right now we can deactivate this ACM. We need to with the bank again. Right. Yes. Yes. Yes. Specifically, maybe as their ERP team instead. Because I don't think the local team knows where this data is coming from. But yeah, as far as I'm concerned, this is integrated in that is or this is this data is coming from the ERP system rather. Okay. We're going to check again. And let you. Sure. For now, we will I will do my best to keep the ticket open for now. Okay. Yeah, so I think that's also do you have questions for these two items for the ptp before we move on to SCC items. Okay. Okay. Thank you. Okay. So for the S2C items. So yeah, we have two items here, which is with regards to the SLP template. Adjustment in SLP template and the. So for the S2 items, both of these are currently still in progress due to the synchronization that is happening between the production and test systems. So for the second item. Yeah, so for this one, this for this one, known, known modifications are allowed for now since yeah. We need to wait for this synchronization synchronization to be finished. And I think we have one more item I haven't I wasn't able to add it here. Yeah, so for this request from you, I love you. I believe with regards to this one. So. We will we are still checking this internally. So once we receive a conclusion for this one, we will update you accordingly. Okay. Thank you. Yeah, I will add this to the read the. Sorry, I forgot to add this yet. But yeah, we will check this internet. So let me know when you test environment, it's a very uploaded. Yeah. Yeah, we will get back to you also for that. Okay. Yeah, so that's all from my side. Do you have other questions or concerns that we can address in the call before we end this? No, for my side. No. Are you guys? Me neither. Okay. Thank you. First one, thank you. Thank you, Ray. Thank you, everyone. And yeah, I will send the minutes of the meeting after the call. So yeah, thank you, everyone. And I wish you a great day ahead. Bye.

Traduza para pt-br
Assistant: Okay, aqui está a tradução para português brasileiro do diálogo fornecido:

"Okay, desculpem, é isso. Okay, então bem-vindos a todos. Então, nossa semana chamamos para Brasil na CA Dow. Então, para hoje, vamos discutir o seguinte: o ARIBA, a API de produção, o P2P e atualizações de status de tickets S2C. E se tivermos processos de negócios e gestão de mudanças. Então, para o ARIBA, aqui estão dependendo o número de decisões pendentes e ações em aberto. Então nós temos. Então, para todos nós temos uma ação e uma informação e então. Para a equipe ERP, temos duas ações e duas informações. Então, temos um total de seis. Então, deixe-me ir para o arquivo ARIBA. Okay, então aqui estão os itens que temos no arquivo ARIBA. Então, para o primeiro, temos a mudança de nome de domínio. Então, para este, isso é realmente estendido até 31 de janeiro de 2026. Mas sim, Flávio, eu só quero saber se temos progresso neste. Eu preciso verificar e então eu volto para você neste. Nesta folha, por favor. Okay, muito obrigado. Okay, então. Você incluiu outro. Sim, está correto. Sim, então para o próximo, nós realmente temos também a conexão TLS 1 ponto para. Para. Para ser transferida para um TLS 1 ponto. Então isso também é para sua equipe verificar. Então o prazo para este será em 30 de abril. Então, nós temos também progresso neste ou ainda não? Eu acredito que eu já descrevi minha situação atual em uma coluna. Na direita, na direita. Você pode encontrar minha chamada. Desculpe, aqui ou no arquivo de rastreamento, seja a mudança. Desculpe, deixe-me verificar. Este. Este. Eu não tenho certeza onde estão os comentários que você tem. Não, eu só tenho meus comentários de pistas sobre o primeiro e o segundo e o TLS. Então, já terminamos. A linha cinco. Deixe-me ver se eu posso. É isso. Ah, este. Okay. Okay, então isso já está fechado. Sim, isto. Sim. Então para isso, você poderia por favor fornecer capturas de tela aqui se possível? Para alongar. Sim, apenas para mostrar que já está implantado em seu sistema. Como uma evidência. Okay. Muito obrigado. Okay. Okay, então deixe-me remover este também. Eu não sei por que este educado. Okay, então para os próximos dois itens é ler seus cartões para a renovação do certificado. Então, isto ainda é apenas para informação, uma vez que os assertifcates tenham sido liberados por. Então eu poderei fornecê-lo a você também. Seja no ingresso do júri ou via. Então, na verdade isso não deveria estar em andamento. Okay. E então para o próximo, temos também o. Este ambiente libera para o. Real pai. Ou o para os itens S para ver realmente tão. Isto ainda está em andamento agora. Então estamos verificando internamente com a equipe do projeto quando. Este projeto terminará. Então por agora. Todas as coisas de teste para o. Relacionado aos contratos de um fornecedor. Nossas entradas por enquanto. E, finalmente, para o monitor diário e então, já que Ricardo não está aqui. Sim, então. Oh, desculpe, eu ouvi aqui. Então, apenas então isso é um lembrete para. Que para eles verifiquem o. Arquivo de monitoramento para o Brasil. Então, realmente. Estamos encontrando alguns ou estamos um cap nós temos alguns documentos capturados no monitoramento diário. Então, se você também pudesse ajudar com o cartão para verificar esses documentos capturados, seria apreciado. Não tenho certeza se isto está correto. Okay, então isto ainda está em março. Deixe-me apenas mudar isto. Okay. Okay. Deixe-me compartilhar minha tela. Okay, então este é realmente o. Arquivo de monitoramento para o Brasil. Então agora nós apenas por enquanto temos um, então ainda estamos atualizando o arquivo de monitoramento. Então sim, se você pudesse ajudar a verificar este. E veja por que este RP está tendo um erro no. Você está ao lado ou no ar ao lado render. Então este é o. O que eles chamam isto a mensagem de erro que foi encontrada. Então sim, se você pudesse verificar isso no ER ao lado, seria apreciado. Para os outros. Eu não acho que temos alguns documentos capturados ainda. Sim, então por agora nós teremos um. Apenas este. Então sim, é tudo para os itens. Então eles têm perguntas antes de passarmos para nossos próximos itens. Não, por favor prossiga. Okay, muito obrigado. Okay, então para os KPIs de produção, é o mesmo que na semana passada, conforme discutido. Então eles são os números ou as figuras para os documentos para as transações em harriba e para os bilhetes de atendimento que foram tomados por um para um L3. Então, quanto à atualização do status dos bilhetes de atendimento P2P, então na semana passada nós. Então temos um resultado ou lidamos com seis bilhetes de atendimento para P2P e quanto aos itens em aberto. Então, para o primeiro, temos o problema em relação ao PR que está em estado de processamento de pedidos. Então, para este, sim, o problema atual é que este PR está atualmente na versão três e está mostrando um erro de que a versão dois tem interrompido a encomenda. No entanto, não podemos. Reprocessar este no lado CIG, então é por isso que levantamos um SR para este. Ou um caso Arriba. Então agora estamos tentando agendar uma chamada com o fornecedor Arriba, pois eles também estão perguntando pela nossa disponibilidade. Então, uma vez que tenhamos estabelecido uma programação para a reunião e discutirmos isso com o fornecedor Arriba. Então atualizaremos o bilhete de atendimento sobre a discussão. E para o próximo, temos o problema de integração para este fornecimento. Posso atender ao ponto um por favor. Então, estamos pendentes para que o fornecedor Arriba volte para nós com horários viáveis para ter uma conversa. Então, para este, nós, a equipe Accenture, discutiremos isso primeiro com o fornecedor SAP Arriba, pois já temos todos os detalhes. Então, se assim for, discutiremos todas as investigações que tivemos até agora, a fim de ajudá-los a investigar ainda mais este problema. Mas de acordo com o plano para o leitor da cidade, também falamos sobre a atualização do nosso ambiente de teste. Se eu não estiver enganado. Que o Q us ou o. Temos muito para atualizar nosso ambiente de teste. E o ambiente de teste QA. O ponto é que eu preciso prosseguir com os nossos requisitos S a p que compartilhei com você por. Desculpe. Então, eu não sei se este ponto está bloqueando a nossa avaliação em relação ao S a p. Este é o ponto. Aquele que você tem no e-mail ou. Sim, 200 para oito ou longe nove. Requisitos S a p. Quero dizer, nós teremos neste. Deixe-me ver se eu posso. A linha oito. Você tem alguma atualização nisto? Sim, então, para este, isto é apenas no lado do Ariba do amor você e não no outro lado. Sim, mas. Estou ciente sobre S a p. O ponto para mim é a avaliação S a p. Então eu preciso implementar novos requisitos. Meu ambiente S a p em produção. Então eu não sei se o ambiente de teste. Precisamos ser atualizados para prosseguir com estes novos requisitos S a p. Sim, então, para este amor, você, eu acho que precisamos. Espere pela sincronização dos dados do sistema de produção para o ambiente de teste para terminar. Então nós fazemos implementações de mudanças de dados ou. Para fazer alguns. Cenários de teste na parte S para ver. Sim, mas quando estaremos prontos. Eu mencionei anteriormente, coordenaremos isto com a equipe do projeto que está fazendo o projeto para este projeto especificamente e perguntaremos quando isto terminará como o. Outro consciente e Bruno também está pedindo por isso. Então nós o atualizaremos em conformidade em relação a este. Okay. Obrigado. Okay. Sim, então deixe-me discutir primeiro os itens B para B. Okay, então para o próximo, temos o problema em relação a este fornecedor. Que foi levantado por um Tadita. Então, para este, eles estão. E o Tadita mencionou no bilhete de atendimento que eles estão coordenando com a equipe do banco para a confirmação do ACM ID para o fornecedor. E a última atualização foi feita para mim sobre este bilhete de atendimento. E o próximo precisa ser criado, o que já existe. E o valor um B são um dados precisa ser particionado em para estes outros fornecedores. E que depois que isso for feito, eles provavelmente prosseguirão com a partição do fornecedor para um ataque de rastreamento base. Sim. Sim. E você também pede para nos pedir para verificar com o banco se o ACM ID do fornecedor é um 35 para. Sim. Algo. Eu não lembro de tudo. Este. Sim. Sim. Mas o informante diz que eles não sabem o que eles. ACM, ACM ID do fornecedor é. E no próximo caso, podemos desativar este ACM ID e posicionar são de. Nós veremos. Sim, nós veremos se podemos. Se podemos tentar particioná-lo novamente, porque sim, eu mostrei no bilhete de atendimento na nossa aba de discussão. Existem dois fornecedores com que estão atualmente existentes, que é um tendo este ACM ID e o outro não tem ACM ID não tem ACM mapeamento de ACM ainda. Então é por isso que eu sugeri pedir à equipe do banco se este é deles. Eles não sabem. Isso. Eles não sabem esta informação. Mas. Nós pensamos que podemos desativar o ACM e particioná-lo horas. Porque eles não passam. Eles não conhecem esta informação, que é o ACM ID deste fornecedor. Mas eles ainda estão usando este fornecedor, certo? Sim. Eu não sei. Não tenho certeza. Nós conversamos com o banco. Então eles já sabem que nós pedimos para você. Estamos pedindo para você ativar este ACM. Sim, não tenho certeza se o PCM ID pode ser desativado, já que na verdade estes dados estão vindo do sistema ERP. Então, se eles não sabem. Então, se você está perguntando para a equipe local da equipe do banco, talvez você também possa sugerir se se a equipe ERP deles sabe estes dados. Mas sim, por enquanto, veremos se se os dados para o. Os dados podem ser mapeados para cada um dos fornecedores que eu mostrei que eu mostrei a você no. Na captura de tela. Que eu forneci. Okay. Então, agora não podemos desativar este ACM. Precisamos com o banco novamente. Certo. Sim. Sim. Especificamente, talvez como sua equipe ERP em vez disso. Porque eu não acho que a equipe local sabe de onde estes dados estão vindo. Mas sim, tanto quanto eu sei, isto está integrado em que é ou estes dados estão vindo do sistema ERP em vez disso. Okay. Nós vamos verificar novamente. E deixe você. Claro. Por enquanto, eu farei o meu melhor para manter o bilhete de atendimento aberto por enquanto. Okay. Sim, então eu acho que isso também, você tem perguntas para estes dois itens para o ptp antes de passarmos para os itens SCC. Okay. Okay. Obrigado. Okay. Então, para os itens S2C. Então sim, temos dois itens aqui, que é em relação ao modelo SLP. Ajuste no modelo SLP e o. Então para os itens S2, ambos estes estão atualmente ainda em andamento devido à sincronização que está acontecendo entre os sistemas de produção e teste. Então para o segundo item. Sim, então para este, para este conhecido, nenhuma modificação conhecida é permitida por enquanto, já que sim. Precisamos esperar por esta sincronização sincronização para terminar. E eu acho que temos mais um item eu não fui capaz de adicioná-lo aqui. Sim, então para este pedido seu, eu te amo. Eu acredito com relação a este. Tão. Nós verificaremos isto internamente. Então, uma vez que recebermos uma conclusão para este, nós o atualizaremos em conformidade. Okay. Obrigado. Sim, eu adicionarei isto ao leia o. Desculpe, eu esqueci de adicionar isto ainda. Mas sim, nós verificaremos isto na internet. Então me avise quando seu ambiente de teste, é uma atualização muito grande. Sim. Sim, nós entraremos em contato com você também para isso. Okay. Sim, então é tudo da minha parte. Você tem outras perguntas ou preocupações que podemos abordar na chamada antes de terminarmos isto? Não, da minha parte. Não. Vocês? Eu também não. Okay. Obrigado. Primeiro, obrigado. Obrigado a todos. E sim, eu enviarei as atas da reunião após a chamada. Então sim, obrigado a todos. E eu desejo a vocês um ótimo dia pela frente. Tchau."

Espero que isso ajude! Avise-me se precisar de mais alguma coisa.

User: Resuma essa reunião com os pontos importantes.
2025-04-10 14:10:56,391 [INFO] Resposta da IA (imagem): Assistant: Claro, aqui está um resumo dos pontos importantes da reunião:

**ARIBA:**

*   **Mudança de Nome de Domínio:** Estendida até 31 de janeiro de 2026. Flávio deve verificar o progresso.
*   **Conexão TLS 1.2:** Prazo para transferência é 30 de abril. Já foi finalizada (linha 5). Solicitaram screenshots como evidência.
*   **Renovação de Certificado:** Aguardando informações para fornecer via ticket ou e-mail.
*   **Ambiente Livre para S2C:** Testes relacionados aos contratos de fornecedores em andamento. Aguardando término do projeto.
*   **Monitoramento Diário:** Verificar o arquivo de monitoramento do Brasil e o erro no PR.

**KPIs de Produção:**

*   Sem alterações em relação à semana anterior.

**Status dos Tickets P2P:**

*   Seis tickets resolvidos.
*   **PR em Processamento de Pedido (Versão 3):** Erro na Versão 2. Aguardando agendamento de reunião com o fornecedor Arriba.
*   **Problema de Integração:** Accenture discutirá com o fornecedor SAP Arriba e atualizará o ambiente de teste.
*   **Fornecedor com Problemas no ACM ID:** Verificar com a equipe do banco (possivelmente a equipe ERP) e desativar o ACM ID.

**Itens S2C:**

*   Ajuste no modelo SLP em andamento, aguardando sincronização entre os sistemas de produção e teste.
*   Solicitação de atualização do ambiente de teste será verificada internamente.

**Geral:**

*   Coordenar a atualização do ambiente de teste com a equipe do projeto.
*   As atas da reunião serão enviadas após a chamada.
2025-04-10 14:10:56,481 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 14:10:56,482 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 14:10:56,485 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 14:10:56,488 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 14:10:56,489 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 14:10:56,491 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 14:10:56,493 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 14:10:56,495 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 14:10:56,497 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 14:10:56,500 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 14:10:56,502 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 14:10:56,504 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 14:10:56,505 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 14:10:56,507 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 14:10:56,509 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 14:13:04,889 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 14:13:04,891 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 14:13:04,892 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 14:13:04,894 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 14:13:04,896 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 14:13:04,898 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 14:13:04,899 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 14:13:04,901 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 14:13:04,902 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 14:13:04,904 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 14:13:04,905 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 14:13:04,907 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 14:13:04,909 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 14:13:04,910 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 14:13:04,911 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 14:13:05,039 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 14:13:05,040 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 14:13:05,042 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 14:13:05,043 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 14:13:05,044 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 14:13:05,046 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 14:13:05,047 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 14:13:05,048 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 14:13:05,050 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 14:13:05,053 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 14:13:05,056 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 14:13:05,058 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 14:13:05,059 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 14:13:05,060 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 14:13:05,062 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 14:13:05,073 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\assets\20250410141305_clipboard_20250410100554.png, Prompt: Você é um assistente inteligente utilizado em um chat com IA, que recebe códigos de uma aplicação completa de chat, como você funciona e tudo que 
é programado para fazer no seu contexto.

Seu objetivo é:
- Analisar todo o contexto e aprender com ele.
- Analisar a Pergunta do Usuário.
- Identificar se possui relação com o contexto.
- Se houver relação com o código do contexto responder a pergunta sobre o código sempre seguindo boas práticas:
    - Analisar o código-fonte contido no projeto enviado
    - Identificar melhorias em legibilidade, performance e segurança
    - Sugerir práticas recomendadas de design e arquitetura
    - Indicar partes com dívidas técnicas ou complexidade desnecessária
    - Enviar código modularizado em pequenos arquivos separados por pastas.

Informações adicionais:
- Estrutura do projeto: segue o padrão de pastas iniciando por src/


## Se houver imagem no contexto, identifique se a pergunta é sobre o código do contexto, sobre a imagem ou sobre os dois para elaborar melhorer respostas.
## Se o usuário fizer uma pergunta que não tem relação com imagem ou contexto, use a sua base de conhecimento fora do contexto para responder de forma inteligente.
** Responda em Português-Brasileiro a não ser que o usuário solicite outra língua.**

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.generativeai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: faça code review da minha aplicação.
Assistant: Com certeza! Analisei o código da sua aplicação e preparei algumas sugestões para melhorias em legibilidade, performance, segurança e design.

**1. Estrutura e Modularização:**

*   **Diretórios:** Organize melhor os arquivos dentro de `src`. Crie subdiretórios para `models`, `services`, `handlers`, `utils`, `config`, etc. Isso facilita a localização dos arquivos e a compreensão da estrutura do projeto.
*   **Arquivo de configuração:** Mova a classe `Config` para um arquivo separado (ex: `config.py`) dentro de um diretório `config`.
*   **Funções:** Quebre funções grandes em funções menores e mais focadas. Isso melhora a legibilidade e facilita o teste.

**2. Boas Práticas e Melhorias no Código:**

*   **Rate Limiting:** Implemente o `RateLimiter` nos serviços que consomem a API do Gemini (ex: `ImageProcessor`, `GeminiHandler`). Isso evita sobrecarga na API e garante um uso mais eficiente.
*   **Configurações:** Use a classe `Config` para centralizar todas as configurações da aplicação (ex: chaves de API, modelos, diretórios). Isso facilita a manutenção e a configuração do projeto.
*   **Logging:** Padronize o uso do logger em toda a aplicação. Use os níveis de log apropriados (ex: `info`, `warning`, `error`) para cada tipo de mensagem.
*   **Tratamento de Erros:** Adicione tratamento de erros mais específico em cada função. Isso ajuda a identificar e corrigir problemas mais rapidamente.
*   **Cache:** Utilize o `st.cache_resource` para funções que realizam operações custosas, como a inicialização do `GeminiHandler`. Isso evita a execução repetida dessas operações.
*   **Tipagem:** Utilize tipagem estática (type hints) para melhorar a legibilidade e a segurança do código.
*   **Imagens:** Valide se `uploaded_file` é realmente um arquivo de imagem antes de tentar exibi-lo, evitando erros caso o usuário envie um arquivo de outro tipo.
*   **Prompts:** Crie arquivos de prompt separados para diferentes tarefas (ex: chat, análise de imagem). Isso facilita a manutenção e a personalização dos prompts.

**3. Segurança:**

*   **Chaves de API:** Nunca coloque suas chaves de API diretamente no código. Use variáveis de ambiente ou um arquivo de configuração seguro para armazená-las.
*   **Validação de Input:** Valide todos os inputs do usuário para evitar ataques de injeção de código.

**4. Arquitetura:**

*   **Camadas:** Defina claramente as responsabilidades de cada camada da sua aplicação (ex: apresentação, lógica de negócios, acesso a dados). Isso facilita a manutenção e a evolução do projeto.
*   **Inversão de Dependência:** Use inversão de dependência para desacoplar as diferentes partes da sua aplicação. Isso facilita o teste e a substituição de componentes.
*   **Testes:** Escreva testes unitários e de integração para garantir a qualidade do seu código.

**5. Dívidas Técnicas e Complexidade:**

*   **Funções Longas:** Identifique funções com muitas linhas de código e divida-as em funções menores e mais focadas.
*   **Lógica Duplicada:** Elimine a lógica duplicada criando funções ou classes reutilizáveis.
*   **Comentários:** Adicione comentários para explicar o que o código faz, especialmente em partes complexas.

**Exemplo de Refatoração:**

Aqui está um exemplo de como você pode refatorar a função `process_images` na classe `ImageProcessor`:

```python
# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self._post_process_image(image_name, summary)
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _post_process_image(self, image_name, summary):
        self.document_service.add_image_summary(image_name, summary)
        self.markdown_service.add_image_summary(image_name, summary)
        self.document_service.save_document()
        self.markdown_service.save_markdown()

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history
```

**6. Código Modularizado:**

Para facilitar a visualização e a organização, preparei o código modularizado em arquivos separados:

```
src/
├── config/
│   └── config.py
├── core/
│   ├── handlers/
│   │   └── gemini_handler.py
│   ├── logger_config.py
│   └── rate_limiter.py
├── main.py
├── services/
│   ├── document_service.py
│   ├── gpt_services.py
│   ├── image_processor.py
│   ├── markdown_service.py
│   └── search_files.py
└── utils/
    └── file_utils.py
```

**Observações Finais:**

*   A implementação de todas essas sugestões pode levar tempo, mas o resultado final será um código mais organizado, legível, seguro e fácil de manter.
*   Priorize as melhorias que trazem o maior impacto no seu projeto.
*   Use um sistema de controle de versão (ex: Git) para acompanhar suas mudanças e facilitar a colaboração com outros desenvolvedores.

Se tiver alguma dúvida sobre alguma dessas sugestões, é só perguntar!
User: Extraia o CNPJ dessa imagem.
Assistant: A imagem contém um comprovante de inscrição e de situação cadastral. O CNPJ presente na imagem é:

`33.024.232/0001-39`
User: extraia o número do chamado que começa com  2025
Assistant: O número do chamado que começa com 2025 na imagem é:

`2025040185005611`

User:  Okay, sorry, this is. Okay, so welcome everyone. So our week we call for Brazil at a ca Dow. So for today, we will discuss the following the ARIBA, the production API, the P2P and S2C ticket status updates. And if we have business processes and change management. So for the ARIBA, here are depending the number of pending decisions and open actions. So we have. So for all we have one action and one information and then. For ERP team, we have two actions and two information. So we have a total of six. So let me go to the ARIBA file. Okay, so here are the items that we have in the ARIBA file. So for the first one, we have the domain name change. So for this one, so this is actually extended until January 31, 2026. But yeah, Flavio, I just want to know if we have progress on this one. I need to check and then I get back to you on this. On this sheet, please. Okay, thank you so much. Okay, so. You have included another one. Yes, that's correct. Yeah, so for the next one, we actually have also the TLS 1 point to connection. To. To be transferred to a TLS 1 point. So this is also for your team to check. So the deadline for this one will be on April 30. So do we have also progress on this one or not yet? I believe that I already described my current situation on a column. On the right on the right. You can find my call up. Sorry, here or on the tracker file, be the change. Sorry, let me check. This one. This one. I'm not sure where are the comments that you have. No, I just have my clue comments about the first one in the second and the TLS. So we are already done. The line five. Let me see if I can. Is it. Ah, this one. Okay. Okay, so this is already close. Yes, this. Yeah. So for that one, could you please provide screen shots here if possible? To stretch. Yes, just to show that it is already deployed on your system. As an evidence. Okay. Thank you so much. Okay. Okay, so let me remove this one also. I don't know why this one educated. Okay, so for the next two items is read your cards to the certificate renewal. So this is still for information only once the assertifcates have been released by. So I will be able to provide it to you also. Either in the juror ticket or via. So actually this should not be in progress. Okay. And then for the next one, we have also the. This environment frees for the. Parent real. Or the for the S to see items actually so. This is still ongoing right now. So we are checking internally with the project team when. This project will end. So for now. All the test things for the. Related to a supplier's contracts. Our entries for now. And lastly for the daily monitor and so since Ricardo is not here. Yes, so. Oh, sorry, I did hear. So just so this is a reminder for. That for them to check the. Monitoring file for Brazil. So actually. We are encountering some or we are a cap we have some documents captured in the daily monitoring. So if you could also help with the card to check this capture documents that would be appreciated. I'm not sure if this is the correct. Okay, so this is still on March. Let me just change this one. Okay. Okay. Let me share my screen. Okay, so this is actually the. Monitoring file for Brazil. So right now we only for now we have one so we are still updating the monitoring file. So yeah, if you could help check this one. And see why this PR is having an error in the. Are you beside or in the air beside render. So this is the. What they call this the error message that was encountered. So yeah, if you could check this on the ER beside it would be appreciate. For the other. I don't think we have some captured documents yet. Yeah, so for now we'll be have one. Just this one. So yeah, that's all for the items. So they have questions before move on to our next items. No, please go ahead. Okay, thank you so much. Okay, so for the production KPIs, it's the same as the last week as discussed. So they are the numbers or the figures for the documents for the transactions in harriba and for the care tickets that have been taken by a to an L3. So as for the P2P tickets status update so for the past week we. So we have a result or we handled six tickets for P2P and as for the open items. So for the first one we have the issue with regards to the PR that is in order processing status. So for this one, yeah, the current issue is that this PR is currently in version three and it is showing an error that the version two has been has been stopping ordering. However, we cannot. Reprocess this one on the CIG side, so that's why we raise an SR for this one. Or an Arriba case. So right now we are trying to set up a call with the Arriba vendor as they are also asking for our availability. So once we have established a schedule for the meeting and discuss this with the Arriba vendor. So we will update the care ticket about the discussion. And for the next one, we have the integration issue for this supply. Can I attend to the point one please. So we are pending Arriba vendor get back to us with a viable time slots to have a conversation. So for this one, we so the Accenture team will discuss this first with the SAP Arriba vendor as we have all the details already. So if so we will discuss all the investigations that we had so far in order to help them further investigate this issue. But according to the plan for the city reader, we also talk about the updating of our environment of testing. If I'm not wrong. That the queue us or the. We have plenty to update our environment of testing. And the QA test test environment. The point is I need to go ahead with our S a p requirements that I have shared with you by. Sorry. So I don't know if this point is blocking our evaluation regarding to S a p. This is the point. The one that you have in email or. Yeah, 200 for eight or far nine. S a p requirements. I mean, we'll have on this. Let me see if I can. The line eight. Are you have very an update in this one? Yes, so for this one, this is only in a riba side of love you and not on the other side. Yeah, but. I'm aware about S a p. The point for me is S a p evaluation. So I need to implement new requirements. My S a p environment in production. So I don't know if the test environment. We need to be update in order to go ahead with this S a p new requirements. Yeah, so for this one of lab, you I think we need to. Wait for the synchronization of the data from the production system to the test environment to be finished. So we do implementations of data changes or. To do some. Testing scenarios in the S to see part. Yeah, but when we are when we will be ready. I have mentioned earlier, we will coordinate this with the project team who's doing the project for this project specifically and ask them when this will be finished as the. Other conscious and Bruno is also asking for this one. So we will update you accordingly regarding this one. Okay. Thank you. Okay. Yeah, so let me discuss first the B to B items. Okay, so for the next one, we have the issue with regards to this supplier. Which was raised by a Tadita. So for this one, they are. And the Tadita mentioned in the ticket that they are coordinating with the bank team for the confirmation of the ACM ID for the supplier. And the last update was made to me about this ticket. And the next one is explained to the explain needs to be created, which already exists. And the value one B are one data needs to be partitioned into to these other suppliers. And that after this is done, they will probably you will proceed with the supplier patchy partitioning for a base trace attack. Yes. Yeah. And you also ask to ask to us to check with the bank if the ACM ID of the supplier is a 35 for. Yes. Something. I don't remember everything. This one. Yeah. Yeah. But the informant says that they don't know what they. ACM, ACM ID of the supplier is. And the next case, can we deactivate this ACM ID and position are from. We will see. Yeah, we will see if we can. If we can try to partition it again, because yes, I have showed in the ticket on our discussion tab. There are two suppliers with that are currently existing, which is one having this ACM ID and the other has no ACM ID has no ACM ACM ID mapping yet. So that's why I suggested to ask the bank team if this is their. They don't know. That's. They don't know this. So. This don't they don't know this information. But. We thought that we can deactivate the ACM and partition it hours. Because they don't pass. They don't know this information, which is ACM ID from this supplier. But they are still using this supplier, right? Yeah. I don't know. Don't sure. We we we talked to the bank. So they were they are already know that we ask to you. We are asking to you to activate this ACM. Yeah, I'm not sure if PCM ID can be deactivated since actually this data is coming from the ERP system. So if they don't know. So if you're asking the local team of the bank team, maybe you can also suggest if if they're ERP team knows this data. But yeah, for now, we will see if if the data for the. The data can be mapped to each to one of the suppliers that I have showed I have shown you in the. In the screenshot. That I have provided. Okay. So right now we can deactivate this ACM. We need to with the bank again. Right. Yes. Yes. Yes. Specifically, maybe as their ERP team instead. Because I don't think the local team knows where this data is coming from. But yeah, as far as I'm concerned, this is integrated in that is or this is this data is coming from the ERP system rather. Okay. We're going to check again. And let you. Sure. For now, we will I will do my best to keep the ticket open for now. Okay. Yeah, so I think that's also do you have questions for these two items for the ptp before we move on to SCC items. Okay. Okay. Thank you. Okay. So for the S2C items. So yeah, we have two items here, which is with regards to the SLP template. Adjustment in SLP template and the. So for the S2 items, both of these are currently still in progress due to the synchronization that is happening between the production and test systems. So for the second item. Yeah, so for this one, this for this one, known, known modifications are allowed for now since yeah. We need to wait for this synchronization synchronization to be finished. And I think we have one more item I haven't I wasn't able to add it here. Yeah, so for this request from you, I love you. I believe with regards to this one. So. We will we are still checking this internally. So once we receive a conclusion for this one, we will update you accordingly. Okay. Thank you. Yeah, I will add this to the read the. Sorry, I forgot to add this yet. But yeah, we will check this internet. So let me know when you test environment, it's a very uploaded. Yeah. Yeah, we will get back to you also for that. Okay. Yeah, so that's all from my side. Do you have other questions or concerns that we can address in the call before we end this? No, for my side. No. Are you guys? Me neither. Okay. Thank you. First one, thank you. Thank you, Ray. Thank you, everyone. And yeah, I will send the minutes of the meeting after the call. So yeah, thank you, everyone. And I wish you a great day ahead. Bye.

Traduza para pt-br
Assistant: Okay, aqui está a tradução para português brasileiro do diálogo fornecido:

"Okay, desculpem, é isso. Okay, então bem-vindos a todos. Então, nossa semana chamamos para Brasil na CA Dow. Então, para hoje, vamos discutir o seguinte: o ARIBA, a API de produção, o P2P e atualizações de status de tickets S2C. E se tivermos processos de negócios e gestão de mudanças. Então, para o ARIBA, aqui estão dependendo o número de decisões pendentes e ações em aberto. Então nós temos. Então, para todos nós temos uma ação e uma informação e então. Para a equipe ERP, temos duas ações e duas informações. Então, temos um total de seis. Então, deixe-me ir para o arquivo ARIBA. Okay, então aqui estão os itens que temos no arquivo ARIBA. Então, para o primeiro, temos a mudança de nome de domínio. Então, para este, isso é realmente estendido até 31 de janeiro de 2026. Mas sim, Flávio, eu só quero saber se temos progresso neste. Eu preciso verificar e então eu volto para você neste. Nesta folha, por favor. Okay, muito obrigado. Okay, então. Você incluiu outro. Sim, está correto. Sim, então para o próximo, nós realmente temos também a conexão TLS 1 ponto para. Para. Para ser transferida para um TLS 1 ponto. Então isso também é para sua equipe verificar. Então o prazo para este será em 30 de abril. Então, nós temos também progresso neste ou ainda não? Eu acredito que eu já descrevi minha situação atual em uma coluna. Na direita, na direita. Você pode encontrar minha chamada. Desculpe, aqui ou no arquivo de rastreamento, seja a mudança. Desculpe, deixe-me verificar. Este. Este. Eu não tenho certeza onde estão os comentários que você tem. Não, eu só tenho meus comentários de pistas sobre o primeiro e o segundo e o TLS. Então, já terminamos. A linha cinco. Deixe-me ver se eu posso. É isso. Ah, este. Okay. Okay, então isso já está fechado. Sim, isto. Sim. Então para isso, você poderia por favor fornecer capturas de tela aqui se possível? Para alongar. Sim, apenas para mostrar que já está implantado em seu sistema. Como uma evidência. Okay. Muito obrigado. Okay. Okay, então deixe-me remover este também. Eu não sei por que este educado. Okay, então para os próximos dois itens é ler seus cartões para a renovação do certificado. Então, isto ainda é apenas para informação, uma vez que os assertifcates tenham sido liberados por. Então eu poderei fornecê-lo a você também. Seja no ingresso do júri ou via. Então, na verdade isso não deveria estar em andamento. Okay. E então para o próximo, temos também o. Este ambiente libera para o. Real pai. Ou o para os itens S para ver realmente tão. Isto ainda está em andamento agora. Então estamos verificando internamente com a equipe do projeto quando. Este projeto terminará. Então por agora. Todas as coisas de teste para o. Relacionado aos contratos de um fornecedor. Nossas entradas por enquanto. E, finalmente, para o monitor diário e então, já que Ricardo não está aqui. Sim, então. Oh, desculpe, eu ouvi aqui. Então, apenas então isso é um lembrete para. Que para eles verifiquem o. Arquivo de monitoramento para o Brasil. Então, realmente. Estamos encontrando alguns ou estamos um cap nós temos alguns documentos capturados no monitoramento diário. Então, se você também pudesse ajudar com o cartão para verificar esses documentos capturados, seria apreciado. Não tenho certeza se isto está correto. Okay, então isto ainda está em março. Deixe-me apenas mudar isto. Okay. Okay. Deixe-me compartilhar minha tela. Okay, então este é realmente o. Arquivo de monitoramento para o Brasil. Então agora nós apenas por enquanto temos um, então ainda estamos atualizando o arquivo de monitoramento. Então sim, se você pudesse ajudar a verificar este. E veja por que este RP está tendo um erro no. Você está ao lado ou no ar ao lado render. Então este é o. O que eles chamam isto a mensagem de erro que foi encontrada. Então sim, se você pudesse verificar isso no ER ao lado, seria apreciado. Para os outros. Eu não acho que temos alguns documentos capturados ainda. Sim, então por agora nós teremos um. Apenas este. Então sim, é tudo para os itens. Então eles têm perguntas antes de passarmos para nossos próximos itens. Não, por favor prossiga. Okay, muito obrigado. Okay, então para os KPIs de produção, é o mesmo que na semana passada, conforme discutido. Então eles são os números ou as figuras para os documentos para as transações em harriba e para os bilhetes de atendimento que foram tomados por um para um L3. Então, quanto à atualização do status dos bilhetes de atendimento P2P, então na semana passada nós. Então temos um resultado ou lidamos com seis bilhetes de atendimento para P2P e quanto aos itens em aberto. Então, para o primeiro, temos o problema em relação ao PR que está em estado de processamento de pedidos. Então, para este, sim, o problema atual é que este PR está atualmente na versão três e está mostrando um erro de que a versão dois tem interrompido a encomenda. No entanto, não podemos. Reprocessar este no lado CIG, então é por isso que levantamos um SR para este. Ou um caso Arriba. Então agora estamos tentando agendar uma chamada com o fornecedor Arriba, pois eles também estão perguntando pela nossa disponibilidade. Então, uma vez que tenhamos estabelecido uma programação para a reunião e discutirmos isso com o fornecedor Arriba. Então atualizaremos o bilhete de atendimento sobre a discussão. E para o próximo, temos o problema de integração para este fornecimento. Posso atender ao ponto um por favor. Então, estamos pendentes para que o fornecedor Arriba volte para nós com horários viáveis para ter uma conversa. Então, para este, nós, a equipe Accenture, discutiremos isso primeiro com o fornecedor SAP Arriba, pois já temos todos os detalhes. Então, se assim for, discutiremos todas as investigações que tivemos até agora, a fim de ajudá-los a investigar ainda mais este problema. Mas de acordo com o plano para o leitor da cidade, também falamos sobre a atualização do nosso ambiente de teste. Se eu não estiver enganado. Que o Q us ou o. Temos muito para atualizar nosso ambiente de teste. E o ambiente de teste QA. O ponto é que eu preciso prosseguir com os nossos requisitos S a p que compartilhei com você por. Desculpe. Então, eu não sei se este ponto está bloqueando a nossa avaliação em relação ao S a p. Este é o ponto. Aquele que você tem no e-mail ou. Sim, 200 para oito ou longe nove. Requisitos S a p. Quero dizer, nós teremos neste. Deixe-me ver se eu posso. A linha oito. Você tem alguma atualização nisto? Sim, então, para este, isto é apenas no lado do Ariba do amor você e não no outro lado. Sim, mas. Estou ciente sobre S a p. O ponto para mim é a avaliação S a p. Então eu preciso implementar novos requisitos. Meu ambiente S a p em produção. Então eu não sei se o ambiente de teste. Precisamos ser atualizados para prosseguir com estes novos requisitos S a p. Sim, então, para este amor, você, eu acho que precisamos. Espere pela sincronização dos dados do sistema de produção para o ambiente de teste para terminar. Então nós fazemos implementações de mudanças de dados ou. Para fazer alguns. Cenários de teste na parte S para ver. Sim, mas quando estaremos prontos. Eu mencionei anteriormente, coordenaremos isto com a equipe do projeto que está fazendo o projeto para este projeto especificamente e perguntaremos quando isto terminará como o. Outro consciente e Bruno também está pedindo por isso. Então nós o atualizaremos em conformidade em relação a este. Okay. Obrigado. Okay. Sim, então deixe-me discutir primeiro os itens B para B. Okay, então para o próximo, temos o problema em relação a este fornecedor. Que foi levantado por um Tadita. Então, para este, eles estão. E o Tadita mencionou no bilhete de atendimento que eles estão coordenando com a equipe do banco para a confirmação do ACM ID para o fornecedor. E a última atualização foi feita para mim sobre este bilhete de atendimento. E o próximo precisa ser criado, o que já existe. E o valor um B são um dados precisa ser particionado em para estes outros fornecedores. E que depois que isso for feito, eles provavelmente prosseguirão com a partição do fornecedor para um ataque de rastreamento base. Sim. Sim. E você também pede para nos pedir para verificar com o banco se o ACM ID do fornecedor é um 35 para. Sim. Algo. Eu não lembro de tudo. Este. Sim. Sim. Mas o informante diz que eles não sabem o que eles. ACM, ACM ID do fornecedor é. E no próximo caso, podemos desativar este ACM ID e posicionar são de. Nós veremos. Sim, nós veremos se podemos. Se podemos tentar particioná-lo novamente, porque sim, eu mostrei no bilhete de atendimento na nossa aba de discussão. Existem dois fornecedores com que estão atualmente existentes, que é um tendo este ACM ID e o outro não tem ACM ID não tem ACM mapeamento de ACM ainda. Então é por isso que eu sugeri pedir à equipe do banco se este é deles. Eles não sabem. Isso. Eles não sabem esta informação. Mas. Nós pensamos que podemos desativar o ACM e particioná-lo horas. Porque eles não passam. Eles não conhecem esta informação, que é o ACM ID deste fornecedor. Mas eles ainda estão usando este fornecedor, certo? Sim. Eu não sei. Não tenho certeza. Nós conversamos com o banco. Então eles já sabem que nós pedimos para você. Estamos pedindo para você ativar este ACM. Sim, não tenho certeza se o PCM ID pode ser desativado, já que na verdade estes dados estão vindo do sistema ERP. Então, se eles não sabem. Então, se você está perguntando para a equipe local da equipe do banco, talvez você também possa sugerir se se a equipe ERP deles sabe estes dados. Mas sim, por enquanto, veremos se se os dados para o. Os dados podem ser mapeados para cada um dos fornecedores que eu mostrei que eu mostrei a você no. Na captura de tela. Que eu forneci. Okay. Então, agora não podemos desativar este ACM. Precisamos com o banco novamente. Certo. Sim. Sim. Especificamente, talvez como sua equipe ERP em vez disso. Porque eu não acho que a equipe local sabe de onde estes dados estão vindo. Mas sim, tanto quanto eu sei, isto está integrado em que é ou estes dados estão vindo do sistema ERP em vez disso. Okay. Nós vamos verificar novamente. E deixe você. Claro. Por enquanto, eu farei o meu melhor para manter o bilhete de atendimento aberto por enquanto. Okay. Sim, então eu acho que isso também, você tem perguntas para estes dois itens para o ptp antes de passarmos para os itens SCC. Okay. Okay. Obrigado. Okay. Então, para os itens S2C. Então sim, temos dois itens aqui, que é em relação ao modelo SLP. Ajuste no modelo SLP e o. Então para os itens S2, ambos estes estão atualmente ainda em andamento devido à sincronização que está acontecendo entre os sistemas de produção e teste. Então para o segundo item. Sim, então para este, para este conhecido, nenhuma modificação conhecida é permitida por enquanto, já que sim. Precisamos esperar por esta sincronização sincronização para terminar. E eu acho que temos mais um item eu não fui capaz de adicioná-lo aqui. Sim, então para este pedido seu, eu te amo. Eu acredito com relação a este. Tão. Nós verificaremos isto internamente. Então, uma vez que recebermos uma conclusão para este, nós o atualizaremos em conformidade. Okay. Obrigado. Sim, eu adicionarei isto ao leia o. Desculpe, eu esqueci de adicionar isto ainda. Mas sim, nós verificaremos isto na internet. Então me avise quando seu ambiente de teste, é uma atualização muito grande. Sim. Sim, nós entraremos em contato com você também para isso. Okay. Sim, então é tudo da minha parte. Você tem outras perguntas ou preocupações que podemos abordar na chamada antes de terminarmos isto? Não, da minha parte. Não. Vocês? Eu também não. Okay. Obrigado. Primeiro, obrigado. Obrigado a todos. E sim, eu enviarei as atas da reunião após a chamada. Então sim, obrigado a todos. E eu desejo a vocês um ótimo dia pela frente. Tchau."

Espero que isso ajude! Avise-me se precisar de mais alguma coisa.

User: Resuma essa reunião com os pontos importantes.
Assistant: Assistant: Claro, aqui está um resumo dos pontos importantes da reunião:

**ARIBA:**

*   **Mudança de Nome de Domínio:** Estendida até 31 de janeiro de 2026. Flávio deve verificar o progresso.
*   **Conexão TLS 1.2:** Prazo para transferência é 30 de abril. Já foi finalizada (linha 5). Solicitaram screenshots como evidência.
*   **Renovação de Certificado:** Aguardando informações para fornecer via ticket ou e-mail.
*   **Ambiente Livre para S2C:** Testes relacionados aos contratos de fornecedores em andamento. Aguardando término do projeto.
*   **Monitoramento Diário:** Verificar o arquivo de monitoramento do Brasil e o erro no PR.

**KPIs de Produção:**

*   Sem alterações em relação à semana anterior.

**Status dos Tickets P2P:**

*   Seis tickets resolvidos.
*   **PR em Processamento de Pedido (Versão 3):** Erro na Versão 2. Aguardando agendamento de reunião com o fornecedor Arriba.
*   **Problema de Integração:** Accenture discutirá com o fornecedor SAP Arriba e atualizará o ambiente de teste.
*   **Fornecedor com Problemas no ACM ID:** Verificar com a equipe do banco (possivelmente a equipe ERP) e desativar o ACM ID.

**Itens S2C:**

*   Ajuste no modelo SLP em andamento, aguardando sincronização entre os sistemas de produção e teste.
*   Solicitação de atualização do ambiente de teste será verificada internamente.

**Geral:**

*   Coordenar a atualização do ambiente de teste com a equipe do projeto.
*   As atas da reunião serão enviadas após a chamada.
User: Preciso de mais detalhes sobre o fornecedor que não está particionando.
2025-04-10 14:13:08,859 [INFO] Resposta da IA (imagem): Assistant: Ok, com base no diálogo, aqui estão os detalhes sobre o fornecedor com problemas de partição:

*   **Problema:** O fornecedor não está sendo particionado corretamente.
*   **Causa:** Existe um problema com o ACM ID (Ariba Cloud Module ID) do fornecedor.
*   **Situação Atual:**
    *   A Tadita está coordenando com a equipe do banco para confirmar o ACM ID.
    *   Já existem dois fornecedores: um com o ACM ID problemático e outro sem nenhum ACM ID.
    *   A equipe do banco (local) não reconhece o ACM ID existente.
*   **Ações:**
    *   Sugeriu-se questionar a equipe ERP do banco sobre o ACM ID, pois esses dados geralmente vêm do sistema ERP.
    *   A equipe está considerando desativar o ACM ID problemático e tentar particionar novamente o fornecedor.
    *   A equipe precisa verificar com o banco novamente (especificamente a equipe ERP) antes de desativar o ACM ID.

Em resumo, o problema parece estar relacionado à identificação e ao gerenciamento do ACM ID do fornecedor no sistema bancário, o que está impedindo a partição correta no Ariba.

2025-04-10 14:13:09,021 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 14:13:09,022 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 14:13:09,024 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 14:13:09,026 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 14:13:09,027 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 14:13:09,028 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 14:13:09,031 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 14:13:09,033 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 14:13:09,034 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 14:13:09,035 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 14:13:09,036 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 14:13:09,037 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 14:13:09,038 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 14:13:09,040 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 14:13:09,041 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 16:48:32,618 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 16:48:32,628 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 16:48:32,637 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 16:48:32,642 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 16:48:32,649 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 16:48:32,657 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 16:48:32,664 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 16:48:32,673 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 16:48:32,680 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 16:48:32,687 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 16:48:32,694 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 16:48:32,702 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 16:48:32,709 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 16:48:32,716 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 16:48:32,723 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 16:48:33,178 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 16:48:33,180 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 16:48:33,182 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 16:48:33,183 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 16:48:33,185 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 16:48:33,186 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 16:48:33,187 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 16:48:33,189 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 16:48:33,191 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 16:48:33,193 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 16:48:33,195 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 16:48:33,198 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 16:48:33,201 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 16:48:33,203 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 16:48:33,205 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 16:48:33,226 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\assets\20250410164833_clipboard_20250410100554.png, Prompt: Você é um assistente inteligente utilizado em um chat com IA, que recebe códigos de uma aplicação completa de chat, como você funciona e tudo que 
é programado para fazer no seu contexto.

Seu objetivo é:
- Analisar todo o contexto e aprender com ele.
- Analisar a Pergunta do Usuário.
- Identificar se possui relação com o contexto.
- Se houver relação com o código do contexto responder a pergunta sobre o código sempre seguindo boas práticas:
    - Analisar o código-fonte contido no projeto enviado
    - Identificar melhorias em legibilidade, performance e segurança
    - Sugerir práticas recomendadas de design e arquitetura
    - Indicar partes com dívidas técnicas ou complexidade desnecessária
    - Enviar código modularizado em pequenos arquivos separados por pastas.

Informações adicionais:
- Estrutura do projeto: segue o padrão de pastas iniciando por src/


## Se houver imagem no contexto, identifique se a pergunta é sobre o código do contexto, sobre a imagem ou sobre os dois para elaborar melhorer respostas.
## Se o usuário fizer uma pergunta que não tem relação com imagem ou contexto, use a sua base de conhecimento fora do contexto para responder de forma inteligente.
** Responda em Português-Brasileiro a não ser que o usuário solicite outra língua.**

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.generativeai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: faça code review da minha aplicação.
Assistant: Com certeza! Analisei o código da sua aplicação e preparei algumas sugestões para melhorias em legibilidade, performance, segurança e design.

**1. Estrutura e Modularização:**

*   **Diretórios:** Organize melhor os arquivos dentro de `src`. Crie subdiretórios para `models`, `services`, `handlers`, `utils`, `config`, etc. Isso facilita a localização dos arquivos e a compreensão da estrutura do projeto.
*   **Arquivo de configuração:** Mova a classe `Config` para um arquivo separado (ex: `config.py`) dentro de um diretório `config`.
*   **Funções:** Quebre funções grandes em funções menores e mais focadas. Isso melhora a legibilidade e facilita o teste.

**2. Boas Práticas e Melhorias no Código:**

*   **Rate Limiting:** Implemente o `RateLimiter` nos serviços que consomem a API do Gemini (ex: `ImageProcessor`, `GeminiHandler`). Isso evita sobrecarga na API e garante um uso mais eficiente.
*   **Configurações:** Use a classe `Config` para centralizar todas as configurações da aplicação (ex: chaves de API, modelos, diretórios). Isso facilita a manutenção e a configuração do projeto.
*   **Logging:** Padronize o uso do logger em toda a aplicação. Use os níveis de log apropriados (ex: `info`, `warning`, `error`) para cada tipo de mensagem.
*   **Tratamento de Erros:** Adicione tratamento de erros mais específico em cada função. Isso ajuda a identificar e corrigir problemas mais rapidamente.
*   **Cache:** Utilize o `st.cache_resource` para funções que realizam operações custosas, como a inicialização do `GeminiHandler`. Isso evita a execução repetida dessas operações.
*   **Tipagem:** Utilize tipagem estática (type hints) para melhorar a legibilidade e a segurança do código.
*   **Imagens:** Valide se `uploaded_file` é realmente um arquivo de imagem antes de tentar exibi-lo, evitando erros caso o usuário envie um arquivo de outro tipo.
*   **Prompts:** Crie arquivos de prompt separados para diferentes tarefas (ex: chat, análise de imagem). Isso facilita a manutenção e a personalização dos prompts.

**3. Segurança:**

*   **Chaves de API:** Nunca coloque suas chaves de API diretamente no código. Use variáveis de ambiente ou um arquivo de configuração seguro para armazená-las.
*   **Validação de Input:** Valide todos os inputs do usuário para evitar ataques de injeção de código.

**4. Arquitetura:**

*   **Camadas:** Defina claramente as responsabilidades de cada camada da sua aplicação (ex: apresentação, lógica de negócios, acesso a dados). Isso facilita a manutenção e a evolução do projeto.
*   **Inversão de Dependência:** Use inversão de dependência para desacoplar as diferentes partes da sua aplicação. Isso facilita o teste e a substituição de componentes.
*   **Testes:** Escreva testes unitários e de integração para garantir a qualidade do seu código.

**5. Dívidas Técnicas e Complexidade:**

*   **Funções Longas:** Identifique funções com muitas linhas de código e divida-as em funções menores e mais focadas.
*   **Lógica Duplicada:** Elimine a lógica duplicada criando funções ou classes reutilizáveis.
*   **Comentários:** Adicione comentários para explicar o que o código faz, especialmente em partes complexas.

**Exemplo de Refatoração:**

Aqui está um exemplo de como você pode refatorar a função `process_images` na classe `ImageProcessor`:

```python
# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self._post_process_image(image_name, summary)
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _post_process_image(self, image_name, summary):
        self.document_service.add_image_summary(image_name, summary)
        self.markdown_service.add_image_summary(image_name, summary)
        self.document_service.save_document()
        self.markdown_service.save_markdown()

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history
```

**6. Código Modularizado:**

Para facilitar a visualização e a organização, preparei o código modularizado em arquivos separados:

```
src/
├── config/
│   └── config.py
├── core/
│   ├── handlers/
│   │   └── gemini_handler.py
│   ├── logger_config.py
│   └── rate_limiter.py
├── main.py
├── services/
│   ├── document_service.py
│   ├── gpt_services.py
│   ├── image_processor.py
│   ├── markdown_service.py
│   └── search_files.py
└── utils/
    └── file_utils.py
```

**Observações Finais:**

*   A implementação de todas essas sugestões pode levar tempo, mas o resultado final será um código mais organizado, legível, seguro e fácil de manter.
*   Priorize as melhorias que trazem o maior impacto no seu projeto.
*   Use um sistema de controle de versão (ex: Git) para acompanhar suas mudanças e facilitar a colaboração com outros desenvolvedores.

Se tiver alguma dúvida sobre alguma dessas sugestões, é só perguntar!
User: Extraia o CNPJ dessa imagem.
Assistant: A imagem contém um comprovante de inscrição e de situação cadastral. O CNPJ presente na imagem é:

`33.024.232/0001-39`
User: extraia o número do chamado que começa com  2025
Assistant: O número do chamado que começa com 2025 na imagem é:

`2025040185005611`

User:  Okay, sorry, this is. Okay, so welcome everyone. So our week we call for Brazil at a ca Dow. So for today, we will discuss the following the ARIBA, the production API, the P2P and S2C ticket status updates. And if we have business processes and change management. So for the ARIBA, here are depending the number of pending decisions and open actions. So we have. So for all we have one action and one information and then. For ERP team, we have two actions and two information. So we have a total of six. So let me go to the ARIBA file. Okay, so here are the items that we have in the ARIBA file. So for the first one, we have the domain name change. So for this one, so this is actually extended until January 31, 2026. But yeah, Flavio, I just want to know if we have progress on this one. I need to check and then I get back to you on this. On this sheet, please. Okay, thank you so much. Okay, so. You have included another one. Yes, that's correct. Yeah, so for the next one, we actually have also the TLS 1 point to connection. To. To be transferred to a TLS 1 point. So this is also for your team to check. So the deadline for this one will be on April 30. So do we have also progress on this one or not yet? I believe that I already described my current situation on a column. On the right on the right. You can find my call up. Sorry, here or on the tracker file, be the change. Sorry, let me check. This one. This one. I'm not sure where are the comments that you have. No, I just have my clue comments about the first one in the second and the TLS. So we are already done. The line five. Let me see if I can. Is it. Ah, this one. Okay. Okay, so this is already close. Yes, this. Yeah. So for that one, could you please provide screen shots here if possible? To stretch. Yes, just to show that it is already deployed on your system. As an evidence. Okay. Thank you so much. Okay. Okay, so let me remove this one also. I don't know why this one educated. Okay, so for the next two items is read your cards to the certificate renewal. So this is still for information only once the assertifcates have been released by. So I will be able to provide it to you also. Either in the juror ticket or via. So actually this should not be in progress. Okay. And then for the next one, we have also the. This environment frees for the. Parent real. Or the for the S to see items actually so. This is still ongoing right now. So we are checking internally with the project team when. This project will end. So for now. All the test things for the. Related to a supplier's contracts. Our entries for now. And lastly for the daily monitor and so since Ricardo is not here. Yes, so. Oh, sorry, I did hear. So just so this is a reminder for. That for them to check the. Monitoring file for Brazil. So actually. We are encountering some or we are a cap we have some documents captured in the daily monitoring. So if you could also help with the card to check this capture documents that would be appreciated. I'm not sure if this is the correct. Okay, so this is still on March. Let me just change this one. Okay. Okay. Let me share my screen. Okay, so this is actually the. Monitoring file for Brazil. So right now we only for now we have one so we are still updating the monitoring file. So yeah, if you could help check this one. And see why this PR is having an error in the. Are you beside or in the air beside render. So this is the. What they call this the error message that was encountered. So yeah, if you could check this on the ER beside it would be appreciate. For the other. I don't think we have some captured documents yet. Yeah, so for now we'll be have one. Just this one. So yeah, that's all for the items. So they have questions before move on to our next items. No, please go ahead. Okay, thank you so much. Okay, so for the production KPIs, it's the same as the last week as discussed. So they are the numbers or the figures for the documents for the transactions in harriba and for the care tickets that have been taken by a to an L3. So as for the P2P tickets status update so for the past week we. So we have a result or we handled six tickets for P2P and as for the open items. So for the first one we have the issue with regards to the PR that is in order processing status. So for this one, yeah, the current issue is that this PR is currently in version three and it is showing an error that the version two has been has been stopping ordering. However, we cannot. Reprocess this one on the CIG side, so that's why we raise an SR for this one. Or an Arriba case. So right now we are trying to set up a call with the Arriba vendor as they are also asking for our availability. So once we have established a schedule for the meeting and discuss this with the Arriba vendor. So we will update the care ticket about the discussion. And for the next one, we have the integration issue for this supply. Can I attend to the point one please. So we are pending Arriba vendor get back to us with a viable time slots to have a conversation. So for this one, we so the Accenture team will discuss this first with the SAP Arriba vendor as we have all the details already. So if so we will discuss all the investigations that we had so far in order to help them further investigate this issue. But according to the plan for the city reader, we also talk about the updating of our environment of testing. If I'm not wrong. That the queue us or the. We have plenty to update our environment of testing. And the QA test test environment. The point is I need to go ahead with our S a p requirements that I have shared with you by. Sorry. So I don't know if this point is blocking our evaluation regarding to S a p. This is the point. The one that you have in email or. Yeah, 200 for eight or far nine. S a p requirements. I mean, we'll have on this. Let me see if I can. The line eight. Are you have very an update in this one? Yes, so for this one, this is only in a riba side of love you and not on the other side. Yeah, but. I'm aware about S a p. The point for me is S a p evaluation. So I need to implement new requirements. My S a p environment in production. So I don't know if the test environment. We need to be update in order to go ahead with this S a p new requirements. Yeah, so for this one of lab, you I think we need to. Wait for the synchronization of the data from the production system to the test environment to be finished. So we do implementations of data changes or. To do some. Testing scenarios in the S to see part. Yeah, but when we are when we will be ready. I have mentioned earlier, we will coordinate this with the project team who's doing the project for this project specifically and ask them when this will be finished as the. Other conscious and Bruno is also asking for this one. So we will update you accordingly regarding this one. Okay. Thank you. Okay. Yeah, so let me discuss first the B to B items. Okay, so for the next one, we have the issue with regards to this supplier. Which was raised by a Tadita. So for this one, they are. And the Tadita mentioned in the ticket that they are coordinating with the bank team for the confirmation of the ACM ID for the supplier. And the last update was made to me about this ticket. And the next one is explained to the explain needs to be created, which already exists. And the value one B are one data needs to be partitioned into to these other suppliers. And that after this is done, they will probably you will proceed with the supplier patchy partitioning for a base trace attack. Yes. Yeah. And you also ask to ask to us to check with the bank if the ACM ID of the supplier is a 35 for. Yes. Something. I don't remember everything. This one. Yeah. Yeah. But the informant says that they don't know what they. ACM, ACM ID of the supplier is. And the next case, can we deactivate this ACM ID and position are from. We will see. Yeah, we will see if we can. If we can try to partition it again, because yes, I have showed in the ticket on our discussion tab. There are two suppliers with that are currently existing, which is one having this ACM ID and the other has no ACM ID has no ACM ACM ID mapping yet. So that's why I suggested to ask the bank team if this is their. They don't know. That's. They don't know this. So. This don't they don't know this information. But. We thought that we can deactivate the ACM and partition it hours. Because they don't pass. They don't know this information, which is ACM ID from this supplier. But they are still using this supplier, right? Yeah. I don't know. Don't sure. We we we talked to the bank. So they were they are already know that we ask to you. We are asking to you to activate this ACM. Yeah, I'm not sure if PCM ID can be deactivated since actually this data is coming from the ERP system. So if they don't know. So if you're asking the local team of the bank team, maybe you can also suggest if if they're ERP team knows this data. But yeah, for now, we will see if if the data for the. The data can be mapped to each to one of the suppliers that I have showed I have shown you in the. In the screenshot. That I have provided. Okay. So right now we can deactivate this ACM. We need to with the bank again. Right. Yes. Yes. Yes. Specifically, maybe as their ERP team instead. Because I don't think the local team knows where this data is coming from. But yeah, as far as I'm concerned, this is integrated in that is or this is this data is coming from the ERP system rather. Okay. We're going to check again. And let you. Sure. For now, we will I will do my best to keep the ticket open for now. Okay. Yeah, so I think that's also do you have questions for these two items for the ptp before we move on to SCC items. Okay. Okay. Thank you. Okay. So for the S2C items. So yeah, we have two items here, which is with regards to the SLP template. Adjustment in SLP template and the. So for the S2 items, both of these are currently still in progress due to the synchronization that is happening between the production and test systems. So for the second item. Yeah, so for this one, this for this one, known, known modifications are allowed for now since yeah. We need to wait for this synchronization synchronization to be finished. And I think we have one more item I haven't I wasn't able to add it here. Yeah, so for this request from you, I love you. I believe with regards to this one. So. We will we are still checking this internally. So once we receive a conclusion for this one, we will update you accordingly. Okay. Thank you. Yeah, I will add this to the read the. Sorry, I forgot to add this yet. But yeah, we will check this internet. So let me know when you test environment, it's a very uploaded. Yeah. Yeah, we will get back to you also for that. Okay. Yeah, so that's all from my side. Do you have other questions or concerns that we can address in the call before we end this? No, for my side. No. Are you guys? Me neither. Okay. Thank you. First one, thank you. Thank you, Ray. Thank you, everyone. And yeah, I will send the minutes of the meeting after the call. So yeah, thank you, everyone. And I wish you a great day ahead. Bye.

Traduza para pt-br
Assistant: Okay, aqui está a tradução para português brasileiro do diálogo fornecido:

"Okay, desculpem, é isso. Okay, então bem-vindos a todos. Então, nossa semana chamamos para Brasil na CA Dow. Então, para hoje, vamos discutir o seguinte: o ARIBA, a API de produção, o P2P e atualizações de status de tickets S2C. E se tivermos processos de negócios e gestão de mudanças. Então, para o ARIBA, aqui estão dependendo o número de decisões pendentes e ações em aberto. Então nós temos. Então, para todos nós temos uma ação e uma informação e então. Para a equipe ERP, temos duas ações e duas informações. Então, temos um total de seis. Então, deixe-me ir para o arquivo ARIBA. Okay, então aqui estão os itens que temos no arquivo ARIBA. Então, para o primeiro, temos a mudança de nome de domínio. Então, para este, isso é realmente estendido até 31 de janeiro de 2026. Mas sim, Flávio, eu só quero saber se temos progresso neste. Eu preciso verificar e então eu volto para você neste. Nesta folha, por favor. Okay, muito obrigado. Okay, então. Você incluiu outro. Sim, está correto. Sim, então para o próximo, nós realmente temos também a conexão TLS 1 ponto para. Para. Para ser transferida para um TLS 1 ponto. Então isso também é para sua equipe verificar. Então o prazo para este será em 30 de abril. Então, nós temos também progresso neste ou ainda não? Eu acredito que eu já descrevi minha situação atual em uma coluna. Na direita, na direita. Você pode encontrar minha chamada. Desculpe, aqui ou no arquivo de rastreamento, seja a mudança. Desculpe, deixe-me verificar. Este. Este. Eu não tenho certeza onde estão os comentários que você tem. Não, eu só tenho meus comentários de pistas sobre o primeiro e o segundo e o TLS. Então, já terminamos. A linha cinco. Deixe-me ver se eu posso. É isso. Ah, este. Okay. Okay, então isso já está fechado. Sim, isto. Sim. Então para isso, você poderia por favor fornecer capturas de tela aqui se possível? Para alongar. Sim, apenas para mostrar que já está implantado em seu sistema. Como uma evidência. Okay. Muito obrigado. Okay. Okay, então deixe-me remover este também. Eu não sei por que este educado. Okay, então para os próximos dois itens é ler seus cartões para a renovação do certificado. Então, isto ainda é apenas para informação, uma vez que os assertifcates tenham sido liberados por. Então eu poderei fornecê-lo a você também. Seja no ingresso do júri ou via. Então, na verdade isso não deveria estar em andamento. Okay. E então para o próximo, temos também o. Este ambiente libera para o. Real pai. Ou o para os itens S para ver realmente tão. Isto ainda está em andamento agora. Então estamos verificando internamente com a equipe do projeto quando. Este projeto terminará. Então por agora. Todas as coisas de teste para o. Relacionado aos contratos de um fornecedor. Nossas entradas por enquanto. E, finalmente, para o monitor diário e então, já que Ricardo não está aqui. Sim, então. Oh, desculpe, eu ouvi aqui. Então, apenas então isso é um lembrete para. Que para eles verifiquem o. Arquivo de monitoramento para o Brasil. Então, realmente. Estamos encontrando alguns ou estamos um cap nós temos alguns documentos capturados no monitoramento diário. Então, se você também pudesse ajudar com o cartão para verificar esses documentos capturados, seria apreciado. Não tenho certeza se isto está correto. Okay, então isto ainda está em março. Deixe-me apenas mudar isto. Okay. Okay. Deixe-me compartilhar minha tela. Okay, então este é realmente o. Arquivo de monitoramento para o Brasil. Então agora nós apenas por enquanto temos um, então ainda estamos atualizando o arquivo de monitoramento. Então sim, se você pudesse ajudar a verificar este. E veja por que este RP está tendo um erro no. Você está ao lado ou no ar ao lado render. Então este é o. O que eles chamam isto a mensagem de erro que foi encontrada. Então sim, se você pudesse verificar isso no ER ao lado, seria apreciado. Para os outros. Eu não acho que temos alguns documentos capturados ainda. Sim, então por agora nós teremos um. Apenas este. Então sim, é tudo para os itens. Então eles têm perguntas antes de passarmos para nossos próximos itens. Não, por favor prossiga. Okay, muito obrigado. Okay, então para os KPIs de produção, é o mesmo que na semana passada, conforme discutido. Então eles são os números ou as figuras para os documentos para as transações em harriba e para os bilhetes de atendimento que foram tomados por um para um L3. Então, quanto à atualização do status dos bilhetes de atendimento P2P, então na semana passada nós. Então temos um resultado ou lidamos com seis bilhetes de atendimento para P2P e quanto aos itens em aberto. Então, para o primeiro, temos o problema em relação ao PR que está em estado de processamento de pedidos. Então, para este, sim, o problema atual é que este PR está atualmente na versão três e está mostrando um erro de que a versão dois tem interrompido a encomenda. No entanto, não podemos. Reprocessar este no lado CIG, então é por isso que levantamos um SR para este. Ou um caso Arriba. Então agora estamos tentando agendar uma chamada com o fornecedor Arriba, pois eles também estão perguntando pela nossa disponibilidade. Então, uma vez que tenhamos estabelecido uma programação para a reunião e discutirmos isso com o fornecedor Arriba. Então atualizaremos o bilhete de atendimento sobre a discussão. E para o próximo, temos o problema de integração para este fornecimento. Posso atender ao ponto um por favor. Então, estamos pendentes para que o fornecedor Arriba volte para nós com horários viáveis para ter uma conversa. Então, para este, nós, a equipe Accenture, discutiremos isso primeiro com o fornecedor SAP Arriba, pois já temos todos os detalhes. Então, se assim for, discutiremos todas as investigações que tivemos até agora, a fim de ajudá-los a investigar ainda mais este problema. Mas de acordo com o plano para o leitor da cidade, também falamos sobre a atualização do nosso ambiente de teste. Se eu não estiver enganado. Que o Q us ou o. Temos muito para atualizar nosso ambiente de teste. E o ambiente de teste QA. O ponto é que eu preciso prosseguir com os nossos requisitos S a p que compartilhei com você por. Desculpe. Então, eu não sei se este ponto está bloqueando a nossa avaliação em relação ao S a p. Este é o ponto. Aquele que você tem no e-mail ou. Sim, 200 para oito ou longe nove. Requisitos S a p. Quero dizer, nós teremos neste. Deixe-me ver se eu posso. A linha oito. Você tem alguma atualização nisto? Sim, então, para este, isto é apenas no lado do Ariba do amor você e não no outro lado. Sim, mas. Estou ciente sobre S a p. O ponto para mim é a avaliação S a p. Então eu preciso implementar novos requisitos. Meu ambiente S a p em produção. Então eu não sei se o ambiente de teste. Precisamos ser atualizados para prosseguir com estes novos requisitos S a p. Sim, então, para este amor, você, eu acho que precisamos. Espere pela sincronização dos dados do sistema de produção para o ambiente de teste para terminar. Então nós fazemos implementações de mudanças de dados ou. Para fazer alguns. Cenários de teste na parte S para ver. Sim, mas quando estaremos prontos. Eu mencionei anteriormente, coordenaremos isto com a equipe do projeto que está fazendo o projeto para este projeto especificamente e perguntaremos quando isto terminará como o. Outro consciente e Bruno também está pedindo por isso. Então nós o atualizaremos em conformidade em relação a este. Okay. Obrigado. Okay. Sim, então deixe-me discutir primeiro os itens B para B. Okay, então para o próximo, temos o problema em relação a este fornecedor. Que foi levantado por um Tadita. Então, para este, eles estão. E o Tadita mencionou no bilhete de atendimento que eles estão coordenando com a equipe do banco para a confirmação do ACM ID para o fornecedor. E a última atualização foi feita para mim sobre este bilhete de atendimento. E o próximo precisa ser criado, o que já existe. E o valor um B são um dados precisa ser particionado em para estes outros fornecedores. E que depois que isso for feito, eles provavelmente prosseguirão com a partição do fornecedor para um ataque de rastreamento base. Sim. Sim. E você também pede para nos pedir para verificar com o banco se o ACM ID do fornecedor é um 35 para. Sim. Algo. Eu não lembro de tudo. Este. Sim. Sim. Mas o informante diz que eles não sabem o que eles. ACM, ACM ID do fornecedor é. E no próximo caso, podemos desativar este ACM ID e posicionar são de. Nós veremos. Sim, nós veremos se podemos. Se podemos tentar particioná-lo novamente, porque sim, eu mostrei no bilhete de atendimento na nossa aba de discussão. Existem dois fornecedores com que estão atualmente existentes, que é um tendo este ACM ID e o outro não tem ACM ID não tem ACM mapeamento de ACM ainda. Então é por isso que eu sugeri pedir à equipe do banco se este é deles. Eles não sabem. Isso. Eles não sabem esta informação. Mas. Nós pensamos que podemos desativar o ACM e particioná-lo horas. Porque eles não passam. Eles não conhecem esta informação, que é o ACM ID deste fornecedor. Mas eles ainda estão usando este fornecedor, certo? Sim. Eu não sei. Não tenho certeza. Nós conversamos com o banco. Então eles já sabem que nós pedimos para você. Estamos pedindo para você ativar este ACM. Sim, não tenho certeza se o PCM ID pode ser desativado, já que na verdade estes dados estão vindo do sistema ERP. Então, se eles não sabem. Então, se você está perguntando para a equipe local da equipe do banco, talvez você também possa sugerir se se a equipe ERP deles sabe estes dados. Mas sim, por enquanto, veremos se se os dados para o. Os dados podem ser mapeados para cada um dos fornecedores que eu mostrei que eu mostrei a você no. Na captura de tela. Que eu forneci. Okay. Então, agora não podemos desativar este ACM. Precisamos com o banco novamente. Certo. Sim. Sim. Especificamente, talvez como sua equipe ERP em vez disso. Porque eu não acho que a equipe local sabe de onde estes dados estão vindo. Mas sim, tanto quanto eu sei, isto está integrado em que é ou estes dados estão vindo do sistema ERP em vez disso. Okay. Nós vamos verificar novamente. E deixe você. Claro. Por enquanto, eu farei o meu melhor para manter o bilhete de atendimento aberto por enquanto. Okay. Sim, então eu acho que isso também, você tem perguntas para estes dois itens para o ptp antes de passarmos para os itens SCC. Okay. Okay. Obrigado. Okay. Então, para os itens S2C. Então sim, temos dois itens aqui, que é em relação ao modelo SLP. Ajuste no modelo SLP e o. Então para os itens S2, ambos estes estão atualmente ainda em andamento devido à sincronização que está acontecendo entre os sistemas de produção e teste. Então para o segundo item. Sim, então para este, para este conhecido, nenhuma modificação conhecida é permitida por enquanto, já que sim. Precisamos esperar por esta sincronização sincronização para terminar. E eu acho que temos mais um item eu não fui capaz de adicioná-lo aqui. Sim, então para este pedido seu, eu te amo. Eu acredito com relação a este. Tão. Nós verificaremos isto internamente. Então, uma vez que recebermos uma conclusão para este, nós o atualizaremos em conformidade. Okay. Obrigado. Sim, eu adicionarei isto ao leia o. Desculpe, eu esqueci de adicionar isto ainda. Mas sim, nós verificaremos isto na internet. Então me avise quando seu ambiente de teste, é uma atualização muito grande. Sim. Sim, nós entraremos em contato com você também para isso. Okay. Sim, então é tudo da minha parte. Você tem outras perguntas ou preocupações que podemos abordar na chamada antes de terminarmos isto? Não, da minha parte. Não. Vocês? Eu também não. Okay. Obrigado. Primeiro, obrigado. Obrigado a todos. E sim, eu enviarei as atas da reunião após a chamada. Então sim, obrigado a todos. E eu desejo a vocês um ótimo dia pela frente. Tchau."

Espero que isso ajude! Avise-me se precisar de mais alguma coisa.

User: Resuma essa reunião com os pontos importantes.
Assistant: Assistant: Claro, aqui está um resumo dos pontos importantes da reunião:

**ARIBA:**

*   **Mudança de Nome de Domínio:** Estendida até 31 de janeiro de 2026. Flávio deve verificar o progresso.
*   **Conexão TLS 1.2:** Prazo para transferência é 30 de abril. Já foi finalizada (linha 5). Solicitaram screenshots como evidência.
*   **Renovação de Certificado:** Aguardando informações para fornecer via ticket ou e-mail.
*   **Ambiente Livre para S2C:** Testes relacionados aos contratos de fornecedores em andamento. Aguardando término do projeto.
*   **Monitoramento Diário:** Verificar o arquivo de monitoramento do Brasil e o erro no PR.

**KPIs de Produção:**

*   Sem alterações em relação à semana anterior.

**Status dos Tickets P2P:**

*   Seis tickets resolvidos.
*   **PR em Processamento de Pedido (Versão 3):** Erro na Versão 2. Aguardando agendamento de reunião com o fornecedor Arriba.
*   **Problema de Integração:** Accenture discutirá com o fornecedor SAP Arriba e atualizará o ambiente de teste.
*   **Fornecedor com Problemas no ACM ID:** Verificar com a equipe do banco (possivelmente a equipe ERP) e desativar o ACM ID.

**Itens S2C:**

*   Ajuste no modelo SLP em andamento, aguardando sincronização entre os sistemas de produção e teste.
*   Solicitação de atualização do ambiente de teste será verificada internamente.

**Geral:**

*   Coordenar a atualização do ambiente de teste com a equipe do projeto.
*   As atas da reunião serão enviadas após a chamada.
User: Preciso de mais detalhes sobre o fornecedor que não está particionando.
Assistant: Assistant: Ok, com base no diálogo, aqui estão os detalhes sobre o fornecedor com problemas de partição:

*   **Problema:** O fornecedor não está sendo particionado corretamente.
*   **Causa:** Existe um problema com o ACM ID (Ariba Cloud Module ID) do fornecedor.
*   **Situação Atual:**
    *   A Tadita está coordenando com a equipe do banco para confirmar o ACM ID.
    *   Já existem dois fornecedores: um com o ACM ID problemático e outro sem nenhum ACM ID.
    *   A equipe do banco (local) não reconhece o ACM ID existente.
*   **Ações:**
    *   Sugeriu-se questionar a equipe ERP do banco sobre o ACM ID, pois esses dados geralmente vêm do sistema ERP.
    *   A equipe está considerando desativar o ACM ID problemático e tentar particionar novamente o fornecedor.
    *   A equipe precisa verificar com o banco novamente (especificamente a equipe ERP) antes de desativar o ACM ID.

Em resumo, o problema parece estar relacionado à identificação e ao gerenciamento do ACM ID do fornecedor no sistema bancário, o que está impedindo a partição correta no Ariba.

User: ou cannot associate a common supplier with multiple partitioned suppliers. The partitioned supplier BR3-1_19381564000140:MANTRIS GESTAO EM SAUDE CORPORATIVA LTDA cannot be associated with the common supplier 19381564000140, because this common supplier is already associated with BR1-19381564000140:MANTRIS GESTAO EM SAUDE CORPORATIVA LTDA.
Error executing trigger IntegrationPostLoad on [ariba.common.core.Supplier [BaseId 13436469218331 B+RQAGmDDhr+Ywb 4rgmmupsr.1qa9cqee]] with data {
AmountRoundingForInvExceptionCalculation = "File \"SupplierConsolidated.csv\" (line 15, column AmountRoundingForInvExceptionCalculation): '0'";
CommonSupplier = {
CorpAddrCountry = "File \"SupplierConsolidated.csv\" (line 15, column Country): 'BR'";
CorpAddrLines = "File \"SupplierConsolidated.csv\" (line 15, column Street): 'R GUARARAPES 2064 - 7 ANDAR - BROOKLIN PAULISTA'";
CorpAddrState = "File \"SupplierConsolidated.csv\" (line 15, column State): 'SP'";
CorporateAddress = {
UniqueName = "File \"SupplierConsolidated.csv\" (line 15, column AddressID): 'BR3-1_19381564000140'";
};
CorporateEmailAddress = "File \"SupplierConsolidated.csv\" (line 15, column CorporateEmailAddress): 'rtrevizan@mantris.com.br'";
CorporatePhone = "File \"SupplierConsolidated.csv\" (line 15, column CorporatePhone): '11 944775350'";
HasSyncRelationship = "File \"SupplierConsolidated.csv\" (line 15, column HasSyncRelationship): 'false'";
HasTradingRelationship = "File \"SupplierConsolidated.csv\" (line 15, column HasTradingRelationship): 'false'";
Name = "File \"SupplierConsolidated.csv\" (line 15, column Name): 'MANTRIS GESTAO EM SAUDE CORPORATIVA LTDA'";
PreferredCurrency = {
UniqueName = "File \"SupplierConsolidated.csv\" (line 15, column PreferredCurrency): 'BRL'";
};
PreferredLanguage = {
UniqueName = "File \"SupplierConsolidated.csv\" (line 15, column PreferredLanguage): 'BrazilianPortuguese'";
};
SystemID = "File \"SupplierConsolidated.csv\" (line 15, column SystemID): 'ACM_72530498'";
};
Name = "File \"SupplierConsolidated.csv\" (line 15, column Name): 'MANTRIS GESTAO EM SAUDE CORPORATIVA LTDA'";
PaymentModel = "File \"SupplierConsolidated.csv\" (line 15, column PaymentModel): '-1'";
SubContractor = "File \"SupplierConsolidated.csv\" (line 15, column SubContractor): '0'";
UniqueName = "File \"SupplierConsolidated.csv\" (line 15, column VendorID): 'BR3-1_19381564000140'";
}

o que está dizendo nesse erro ?
2025-04-10 16:48:40,524 [INFO] Resposta da IA (imagem): Assistant: O erro indica que você está tentando associar um fornecedor comum a múltiplos fornecedores particionados, o que não é permitido no sistema Ariba.

**Detalhes do Erro:**

*   **Mensagem principal:** "Você não pode associar um fornecedor comum com múltiplos fornecedores particionados."
*   **Fornecedor particionado com problema:** `BR3-1_19381564000140:MANTRIS GESTAO EM SAUDE CORPORATIVA LTDA`
*   **Fornecedor comum:** `19381564000140`
*   **Fornecedor particionado já associado ao fornecedor comum:** `BR1-19381564000140:MANTRIS GESTAO EM SAUDE CORPORATIVA LTDA`

**Interpretação:**

O sistema já tem o fornecedor comum `19381564000140` associado ao fornecedor particionado `BR1-19381564000140:MANTRIS GESTAO EM SAUDE CORPORATIVA LTDA`. Você está tentando associar o mesmo fornecedor comum ao fornecedor particionado `BR3-1_19381564000140:MANTRIS GESTAO EM SAUDE CORPORATIVA LTDA`, o que causa o erro.

**Possíveis Causas:**

*   **Configuração Incorreta:** Pode haver um erro na forma como os fornecedores estão configurados no sistema Ariba.
*   **Dados Duplicados:** Possivelmente há dados duplicados ou conflitantes na importação do arquivo `SupplierConsolidated.csv`.
*   **Lógica de Particionamento Inadequada:** A lógica de particionamento pode não estar funcionando corretamente, resultando em tentativas de associações inválidas.

**Informações Adicionais do Log:**

O log também fornece informações sobre os dados do fornecedor que estão sendo processados a partir do arquivo `SupplierConsolidated.csv`, incluindo:

*   Nome do fornecedor: `MANTRIS GESTAO EM SAUDE CORPORATIVA LTDA`
*   Endereço: `R GUARARAPES 2064 - 7 ANDAR - BROOKLIN PAULISTA`
*   País: `BR`
*   Estado: `SP`
*   E-mail: `rtrevizan@mantris.com.br`
*   Telefone: `11 944775350`
*   ID do sistema: `ACM_72530498`

**Próximos Passos:**

1.  **Verifique a configuração dos fornecedores:** No sistema Ariba, examine a configuração dos fornecedores `BR3-1_19381564000140`, `BR1-19381564000140` e `19381564000140` para identificar quaisquer erros.
2.  **Analise o arquivo `SupplierConsolidated.csv`:** Verifique a linha 15 (onde o erro foi detectado) para garantir que os dados do fornecedor estejam corretos e consistentes.
3.  **Revise a lógica de particionamento:** Certifique-se de que a lógica de particionamento está configurada corretamente para evitar associações inválidas.
4.  **Remova ou corrija a associação existente:** Se a associação entre o fornecedor comum e o fornecedor particionado `BR1-19381564000140` estiver incorreta, remova-a ou corrija-a antes de tentar associar o fornecedor comum a `BR3-1_19381564000140`.

Lembre-se de que a solução exata dependerá da sua configuração específica do Ariba e da sua lógica de particionamento.
2025-04-10 16:48:40,608 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 16:48:40,609 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 16:48:40,611 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 16:48:40,613 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 16:48:40,614 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 16:48:40,616 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 16:48:40,617 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 16:48:40,619 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 16:48:40,621 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 16:48:40,622 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 16:48:40,624 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 16:48:40,625 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 16:48:40,627 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 16:48:40,628 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 16:48:40,629 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 17:09:16,465 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 17:09:16,467 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 17:09:16,469 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 17:09:16,470 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 17:09:16,472 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 17:09:16,473 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 17:09:16,475 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 17:09:16,476 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 17:09:16,477 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 17:09:16,479 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 17:09:16,480 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 17:09:16,481 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 17:09:16,483 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 17:09:16,484 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 17:09:16,486 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 17:09:16,629 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 17:09:16,631 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 17:09:16,633 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 17:09:16,635 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 17:09:16,637 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 17:09:16,639 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 17:09:16,641 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 17:09:16,643 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 17:09:16,645 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 17:09:16,647 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 17:09:16,648 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 17:09:16,650 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 17:09:16,652 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 17:09:16,653 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 17:09:16,655 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 17:09:23,896 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 17:09:23,897 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 17:09:23,898 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 17:09:23,900 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 17:09:23,902 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 17:09:23,904 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 17:09:23,905 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 17:09:23,907 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 17:09:23,908 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 17:09:23,910 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 17:09:23,911 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 17:09:23,912 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 17:09:23,914 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 17:09:23,915 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 17:09:23,917 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 17:09:24,074 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 17:09:24,075 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 17:09:24,077 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 17:09:24,078 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 17:09:24,080 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 17:09:24,081 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 17:09:24,082 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 17:09:24,084 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 17:09:24,086 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 17:09:24,089 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 17:09:24,092 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 17:09:24,094 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 17:09:24,096 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 17:09:24,097 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 17:09:24,099 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 17:09:24,118 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\assets\20250410170924_clipboard_20250410170916.png, Prompt: Você é um assistente inteligente utilizado em um chat com IA, que recebe códigos de uma aplicação completa de chat, como você funciona e tudo que 
é programado para fazer no seu contexto.

Seu objetivo é:
- Analisar todo o contexto e aprender com ele.
- Analisar a Pergunta do Usuário.
- Identificar se possui relação com o contexto.
- Se houver relação com o código do contexto responder a pergunta sobre o código sempre seguindo boas práticas:
    - Analisar o código-fonte contido no projeto enviado
    - Identificar melhorias em legibilidade, performance e segurança
    - Sugerir práticas recomendadas de design e arquitetura
    - Indicar partes com dívidas técnicas ou complexidade desnecessária
    - Enviar código modularizado em pequenos arquivos separados por pastas.

Informações adicionais:
- Estrutura do projeto: segue o padrão de pastas iniciando por src/


## Se houver imagem no contexto, identifique se a pergunta é sobre o código do contexto, sobre a imagem ou sobre os dois para elaborar melhorer respostas.
## Se o usuário fizer uma pergunta que não tem relação com imagem ou contexto, use a sua base de conhecimento fora do contexto para responder de forma inteligente.
** Responda em Português-Brasileiro a não ser que o usuário solicite outra língua.**

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.generativeai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: faça code review da minha aplicação.
Assistant: Com certeza! Analisei o código da sua aplicação e preparei algumas sugestões para melhorias em legibilidade, performance, segurança e design.

**1. Estrutura e Modularização:**

*   **Diretórios:** Organize melhor os arquivos dentro de `src`. Crie subdiretórios para `models`, `services`, `handlers`, `utils`, `config`, etc. Isso facilita a localização dos arquivos e a compreensão da estrutura do projeto.
*   **Arquivo de configuração:** Mova a classe `Config` para um arquivo separado (ex: `config.py`) dentro de um diretório `config`.
*   **Funções:** Quebre funções grandes em funções menores e mais focadas. Isso melhora a legibilidade e facilita o teste.

**2. Boas Práticas e Melhorias no Código:**

*   **Rate Limiting:** Implemente o `RateLimiter` nos serviços que consomem a API do Gemini (ex: `ImageProcessor`, `GeminiHandler`). Isso evita sobrecarga na API e garante um uso mais eficiente.
*   **Configurações:** Use a classe `Config` para centralizar todas as configurações da aplicação (ex: chaves de API, modelos, diretórios). Isso facilita a manutenção e a configuração do projeto.
*   **Logging:** Padronize o uso do logger em toda a aplicação. Use os níveis de log apropriados (ex: `info`, `warning`, `error`) para cada tipo de mensagem.
*   **Tratamento de Erros:** Adicione tratamento de erros mais específico em cada função. Isso ajuda a identificar e corrigir problemas mais rapidamente.
*   **Cache:** Utilize o `st.cache_resource` para funções que realizam operações custosas, como a inicialização do `GeminiHandler`. Isso evita a execução repetida dessas operações.
*   **Tipagem:** Utilize tipagem estática (type hints) para melhorar a legibilidade e a segurança do código.
*   **Imagens:** Valide se `uploaded_file` é realmente um arquivo de imagem antes de tentar exibi-lo, evitando erros caso o usuário envie um arquivo de outro tipo.
*   **Prompts:** Crie arquivos de prompt separados para diferentes tarefas (ex: chat, análise de imagem). Isso facilita a manutenção e a personalização dos prompts.

**3. Segurança:**

*   **Chaves de API:** Nunca coloque suas chaves de API diretamente no código. Use variáveis de ambiente ou um arquivo de configuração seguro para armazená-las.
*   **Validação de Input:** Valide todos os inputs do usuário para evitar ataques de injeção de código.

**4. Arquitetura:**

*   **Camadas:** Defina claramente as responsabilidades de cada camada da sua aplicação (ex: apresentação, lógica de negócios, acesso a dados). Isso facilita a manutenção e a evolução do projeto.
*   **Inversão de Dependência:** Use inversão de dependência para desacoplar as diferentes partes da sua aplicação. Isso facilita o teste e a substituição de componentes.
*   **Testes:** Escreva testes unitários e de integração para garantir a qualidade do seu código.

**5. Dívidas Técnicas e Complexidade:**

*   **Funções Longas:** Identifique funções com muitas linhas de código e divida-as em funções menores e mais focadas.
*   **Lógica Duplicada:** Elimine a lógica duplicada criando funções ou classes reutilizáveis.
*   **Comentários:** Adicione comentários para explicar o que o código faz, especialmente em partes complexas.

**Exemplo de Refatoração:**

Aqui está um exemplo de como você pode refatorar a função `process_images` na classe `ImageProcessor`:

```python
# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self._post_process_image(image_name, summary)
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _post_process_image(self, image_name, summary):
        self.document_service.add_image_summary(image_name, summary)
        self.markdown_service.add_image_summary(image_name, summary)
        self.document_service.save_document()
        self.markdown_service.save_markdown()

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history
```

**6. Código Modularizado:**

Para facilitar a visualização e a organização, preparei o código modularizado em arquivos separados:

```
src/
├── config/
│   └── config.py
├── core/
│   ├── handlers/
│   │   └── gemini_handler.py
│   ├── logger_config.py
│   └── rate_limiter.py
├── main.py
├── services/
│   ├── document_service.py
│   ├── gpt_services.py
│   ├── image_processor.py
│   ├── markdown_service.py
│   └── search_files.py
└── utils/
    └── file_utils.py
```

**Observações Finais:**

*   A implementação de todas essas sugestões pode levar tempo, mas o resultado final será um código mais organizado, legível, seguro e fácil de manter.
*   Priorize as melhorias que trazem o maior impacto no seu projeto.
*   Use um sistema de controle de versão (ex: Git) para acompanhar suas mudanças e facilitar a colaboração com outros desenvolvedores.

Se tiver alguma dúvida sobre alguma dessas sugestões, é só perguntar!
User: Extraia o CNPJ dessa imagem.
Assistant: A imagem contém um comprovante de inscrição e de situação cadastral. O CNPJ presente na imagem é:

`33.024.232/0001-39`
User: extraia o número do chamado que começa com  2025
Assistant: O número do chamado que começa com 2025 na imagem é:

`2025040185005611`

User:  Okay, sorry, this is. Okay, so welcome everyone. So our week we call for Brazil at a ca Dow. So for today, we will discuss the following the ARIBA, the production API, the P2P and S2C ticket status updates. And if we have business processes and change management. So for the ARIBA, here are depending the number of pending decisions and open actions. So we have. So for all we have one action and one information and then. For ERP team, we have two actions and two information. So we have a total of six. So let me go to the ARIBA file. Okay, so here are the items that we have in the ARIBA file. So for the first one, we have the domain name change. So for this one, so this is actually extended until January 31, 2026. But yeah, Flavio, I just want to know if we have progress on this one. I need to check and then I get back to you on this. On this sheet, please. Okay, thank you so much. Okay, so. You have included another one. Yes, that's correct. Yeah, so for the next one, we actually have also the TLS 1 point to connection. To. To be transferred to a TLS 1 point. So this is also for your team to check. So the deadline for this one will be on April 30. So do we have also progress on this one or not yet? I believe that I already described my current situation on a column. On the right on the right. You can find my call up. Sorry, here or on the tracker file, be the change. Sorry, let me check. This one. This one. I'm not sure where are the comments that you have. No, I just have my clue comments about the first one in the second and the TLS. So we are already done. The line five. Let me see if I can. Is it. Ah, this one. Okay. Okay, so this is already close. Yes, this. Yeah. So for that one, could you please provide screen shots here if possible? To stretch. Yes, just to show that it is already deployed on your system. As an evidence. Okay. Thank you so much. Okay. Okay, so let me remove this one also. I don't know why this one educated. Okay, so for the next two items is read your cards to the certificate renewal. So this is still for information only once the assertifcates have been released by. So I will be able to provide it to you also. Either in the juror ticket or via. So actually this should not be in progress. Okay. And then for the next one, we have also the. This environment frees for the. Parent real. Or the for the S to see items actually so. This is still ongoing right now. So we are checking internally with the project team when. This project will end. So for now. All the test things for the. Related to a supplier's contracts. Our entries for now. And lastly for the daily monitor and so since Ricardo is not here. Yes, so. Oh, sorry, I did hear. So just so this is a reminder for. That for them to check the. Monitoring file for Brazil. So actually. We are encountering some or we are a cap we have some documents captured in the daily monitoring. So if you could also help with the card to check this capture documents that would be appreciated. I'm not sure if this is the correct. Okay, so this is still on March. Let me just change this one. Okay. Okay. Let me share my screen. Okay, so this is actually the. Monitoring file for Brazil. So right now we only for now we have one so we are still updating the monitoring file. So yeah, if you could help check this one. And see why this PR is having an error in the. Are you beside or in the air beside render. So this is the. What they call this the error message that was encountered. So yeah, if you could check this on the ER beside it would be appreciate. For the other. I don't think we have some captured documents yet. Yeah, so for now we'll be have one. Just this one. So yeah, that's all for the items. So they have questions before move on to our next items. No, please go ahead. Okay, thank you so much. Okay, so for the production KPIs, it's the same as the last week as discussed. So they are the numbers or the figures for the documents for the transactions in harriba and for the care tickets that have been taken by a to an L3. So as for the P2P tickets status update so for the past week we. So we have a result or we handled six tickets for P2P and as for the open items. So for the first one we have the issue with regards to the PR that is in order processing status. So for this one, yeah, the current issue is that this PR is currently in version three and it is showing an error that the version two has been has been stopping ordering. However, we cannot. Reprocess this one on the CIG side, so that's why we raise an SR for this one. Or an Arriba case. So right now we are trying to set up a call with the Arriba vendor as they are also asking for our availability. So once we have established a schedule for the meeting and discuss this with the Arriba vendor. So we will update the care ticket about the discussion. And for the next one, we have the integration issue for this supply. Can I attend to the point one please. So we are pending Arriba vendor get back to us with a viable time slots to have a conversation. So for this one, we so the Accenture team will discuss this first with the SAP Arriba vendor as we have all the details already. So if so we will discuss all the investigations that we had so far in order to help them further investigate this issue. But according to the plan for the city reader, we also talk about the updating of our environment of testing. If I'm not wrong. That the queue us or the. We have plenty to update our environment of testing. And the QA test test environment. The point is I need to go ahead with our S a p requirements that I have shared with you by. Sorry. So I don't know if this point is blocking our evaluation regarding to S a p. This is the point. The one that you have in email or. Yeah, 200 for eight or far nine. S a p requirements. I mean, we'll have on this. Let me see if I can. The line eight. Are you have very an update in this one? Yes, so for this one, this is only in a riba side of love you and not on the other side. Yeah, but. I'm aware about S a p. The point for me is S a p evaluation. So I need to implement new requirements. My S a p environment in production. So I don't know if the test environment. We need to be update in order to go ahead with this S a p new requirements. Yeah, so for this one of lab, you I think we need to. Wait for the synchronization of the data from the production system to the test environment to be finished. So we do implementations of data changes or. To do some. Testing scenarios in the S to see part. Yeah, but when we are when we will be ready. I have mentioned earlier, we will coordinate this with the project team who's doing the project for this project specifically and ask them when this will be finished as the. Other conscious and Bruno is also asking for this one. So we will update you accordingly regarding this one. Okay. Thank you. Okay. Yeah, so let me discuss first the B to B items. Okay, so for the next one, we have the issue with regards to this supplier. Which was raised by a Tadita. So for this one, they are. And the Tadita mentioned in the ticket that they are coordinating with the bank team for the confirmation of the ACM ID for the supplier. And the last update was made to me about this ticket. And the next one is explained to the explain needs to be created, which already exists. And the value one B are one data needs to be partitioned into to these other suppliers. And that after this is done, they will probably you will proceed with the supplier patchy partitioning for a base trace attack. Yes. Yeah. And you also ask to ask to us to check with the bank if the ACM ID of the supplier is a 35 for. Yes. Something. I don't remember everything. This one. Yeah. Yeah. But the informant says that they don't know what they. ACM, ACM ID of the supplier is. And the next case, can we deactivate this ACM ID and position are from. We will see. Yeah, we will see if we can. If we can try to partition it again, because yes, I have showed in the ticket on our discussion tab. There are two suppliers with that are currently existing, which is one having this ACM ID and the other has no ACM ID has no ACM ACM ID mapping yet. So that's why I suggested to ask the bank team if this is their. They don't know. That's. They don't know this. So. This don't they don't know this information. But. We thought that we can deactivate the ACM and partition it hours. Because they don't pass. They don't know this information, which is ACM ID from this supplier. But they are still using this supplier, right? Yeah. I don't know. Don't sure. We we we talked to the bank. So they were they are already know that we ask to you. We are asking to you to activate this ACM. Yeah, I'm not sure if PCM ID can be deactivated since actually this data is coming from the ERP system. So if they don't know. So if you're asking the local team of the bank team, maybe you can also suggest if if they're ERP team knows this data. But yeah, for now, we will see if if the data for the. The data can be mapped to each to one of the suppliers that I have showed I have shown you in the. In the screenshot. That I have provided. Okay. So right now we can deactivate this ACM. We need to with the bank again. Right. Yes. Yes. Yes. Specifically, maybe as their ERP team instead. Because I don't think the local team knows where this data is coming from. But yeah, as far as I'm concerned, this is integrated in that is or this is this data is coming from the ERP system rather. Okay. We're going to check again. And let you. Sure. For now, we will I will do my best to keep the ticket open for now. Okay. Yeah, so I think that's also do you have questions for these two items for the ptp before we move on to SCC items. Okay. Okay. Thank you. Okay. So for the S2C items. So yeah, we have two items here, which is with regards to the SLP template. Adjustment in SLP template and the. So for the S2 items, both of these are currently still in progress due to the synchronization that is happening between the production and test systems. So for the second item. Yeah, so for this one, this for this one, known, known modifications are allowed for now since yeah. We need to wait for this synchronization synchronization to be finished. And I think we have one more item I haven't I wasn't able to add it here. Yeah, so for this request from you, I love you. I believe with regards to this one. So. We will we are still checking this internally. So once we receive a conclusion for this one, we will update you accordingly. Okay. Thank you. Yeah, I will add this to the read the. Sorry, I forgot to add this yet. But yeah, we will check this internet. So let me know when you test environment, it's a very uploaded. Yeah. Yeah, we will get back to you also for that. Okay. Yeah, so that's all from my side. Do you have other questions or concerns that we can address in the call before we end this? No, for my side. No. Are you guys? Me neither. Okay. Thank you. First one, thank you. Thank you, Ray. Thank you, everyone. And yeah, I will send the minutes of the meeting after the call. So yeah, thank you, everyone. And I wish you a great day ahead. Bye.

Traduza para pt-br
Assistant: Okay, aqui está a tradução para português brasileiro do diálogo fornecido:

"Okay, desculpem, é isso. Okay, então bem-vindos a todos. Então, nossa semana chamamos para Brasil na CA Dow. Então, para hoje, vamos discutir o seguinte: o ARIBA, a API de produção, o P2P e atualizações de status de tickets S2C. E se tivermos processos de negócios e gestão de mudanças. Então, para o ARIBA, aqui estão dependendo o número de decisões pendentes e ações em aberto. Então nós temos. Então, para todos nós temos uma ação e uma informação e então. Para a equipe ERP, temos duas ações e duas informações. Então, temos um total de seis. Então, deixe-me ir para o arquivo ARIBA. Okay, então aqui estão os itens que temos no arquivo ARIBA. Então, para o primeiro, temos a mudança de nome de domínio. Então, para este, isso é realmente estendido até 31 de janeiro de 2026. Mas sim, Flávio, eu só quero saber se temos progresso neste. Eu preciso verificar e então eu volto para você neste. Nesta folha, por favor. Okay, muito obrigado. Okay, então. Você incluiu outro. Sim, está correto. Sim, então para o próximo, nós realmente temos também a conexão TLS 1 ponto para. Para. Para ser transferida para um TLS 1 ponto. Então isso também é para sua equipe verificar. Então o prazo para este será em 30 de abril. Então, nós temos também progresso neste ou ainda não? Eu acredito que eu já descrevi minha situação atual em uma coluna. Na direita, na direita. Você pode encontrar minha chamada. Desculpe, aqui ou no arquivo de rastreamento, seja a mudança. Desculpe, deixe-me verificar. Este. Este. Eu não tenho certeza onde estão os comentários que você tem. Não, eu só tenho meus comentários de pistas sobre o primeiro e o segundo e o TLS. Então, já terminamos. A linha cinco. Deixe-me ver se eu posso. É isso. Ah, este. Okay. Okay, então isso já está fechado. Sim, isto. Sim. Então para isso, você poderia por favor fornecer capturas de tela aqui se possível? Para alongar. Sim, apenas para mostrar que já está implantado em seu sistema. Como uma evidência. Okay. Muito obrigado. Okay. Okay, então deixe-me remover este também. Eu não sei por que este educado. Okay, então para os próximos dois itens é ler seus cartões para a renovação do certificado. Então, isto ainda é apenas para informação, uma vez que os assertifcates tenham sido liberados por. Então eu poderei fornecê-lo a você também. Seja no ingresso do júri ou via. Então, na verdade isso não deveria estar em andamento. Okay. E então para o próximo, temos também o. Este ambiente libera para o. Real pai. Ou o para os itens S para ver realmente tão. Isto ainda está em andamento agora. Então estamos verificando internamente com a equipe do projeto quando. Este projeto terminará. Então por agora. Todas as coisas de teste para o. Relacionado aos contratos de um fornecedor. Nossas entradas por enquanto. E, finalmente, para o monitor diário e então, já que Ricardo não está aqui. Sim, então. Oh, desculpe, eu ouvi aqui. Então, apenas então isso é um lembrete para. Que para eles verifiquem o. Arquivo de monitoramento para o Brasil. Então, realmente. Estamos encontrando alguns ou estamos um cap nós temos alguns documentos capturados no monitoramento diário. Então, se você também pudesse ajudar com o cartão para verificar esses documentos capturados, seria apreciado. Não tenho certeza se isto está correto. Okay, então isto ainda está em março. Deixe-me apenas mudar isto. Okay. Okay. Deixe-me compartilhar minha tela. Okay, então este é realmente o. Arquivo de monitoramento para o Brasil. Então agora nós apenas por enquanto temos um, então ainda estamos atualizando o arquivo de monitoramento. Então sim, se você pudesse ajudar a verificar este. E veja por que este RP está tendo um erro no. Você está ao lado ou no ar ao lado render. Então este é o. O que eles chamam isto a mensagem de erro que foi encontrada. Então sim, se você pudesse verificar isso no ER ao lado, seria apreciado. Para os outros. Eu não acho que temos alguns documentos capturados ainda. Sim, então por agora nós teremos um. Apenas este. Então sim, é tudo para os itens. Então eles têm perguntas antes de passarmos para nossos próximos itens. Não, por favor prossiga. Okay, muito obrigado. Okay, então para os KPIs de produção, é o mesmo que na semana passada, conforme discutido. Então eles são os números ou as figuras para os documentos para as transações em harriba e para os bilhetes de atendimento que foram tomados por um para um L3. Então, quanto à atualização do status dos bilhetes de atendimento P2P, então na semana passada nós. Então temos um resultado ou lidamos com seis bilhetes de atendimento para P2P e quanto aos itens em aberto. Então, para o primeiro, temos o problema em relação ao PR que está em estado de processamento de pedidos. Então, para este, sim, o problema atual é que este PR está atualmente na versão três e está mostrando um erro de que a versão dois tem interrompido a encomenda. No entanto, não podemos. Reprocessar este no lado CIG, então é por isso que levantamos um SR para este. Ou um caso Arriba. Então agora estamos tentando agendar uma chamada com o fornecedor Arriba, pois eles também estão perguntando pela nossa disponibilidade. Então, uma vez que tenhamos estabelecido uma programação para a reunião e discutirmos isso com o fornecedor Arriba. Então atualizaremos o bilhete de atendimento sobre a discussão. E para o próximo, temos o problema de integração para este fornecimento. Posso atender ao ponto um por favor. Então, estamos pendentes para que o fornecedor Arriba volte para nós com horários viáveis para ter uma conversa. Então, para este, nós, a equipe Accenture, discutiremos isso primeiro com o fornecedor SAP Arriba, pois já temos todos os detalhes. Então, se assim for, discutiremos todas as investigações que tivemos até agora, a fim de ajudá-los a investigar ainda mais este problema. Mas de acordo com o plano para o leitor da cidade, também falamos sobre a atualização do nosso ambiente de teste. Se eu não estiver enganado. Que o Q us ou o. Temos muito para atualizar nosso ambiente de teste. E o ambiente de teste QA. O ponto é que eu preciso prosseguir com os nossos requisitos S a p que compartilhei com você por. Desculpe. Então, eu não sei se este ponto está bloqueando a nossa avaliação em relação ao S a p. Este é o ponto. Aquele que você tem no e-mail ou. Sim, 200 para oito ou longe nove. Requisitos S a p. Quero dizer, nós teremos neste. Deixe-me ver se eu posso. A linha oito. Você tem alguma atualização nisto? Sim, então, para este, isto é apenas no lado do Ariba do amor você e não no outro lado. Sim, mas. Estou ciente sobre S a p. O ponto para mim é a avaliação S a p. Então eu preciso implementar novos requisitos. Meu ambiente S a p em produção. Então eu não sei se o ambiente de teste. Precisamos ser atualizados para prosseguir com estes novos requisitos S a p. Sim, então, para este amor, você, eu acho que precisamos. Espere pela sincronização dos dados do sistema de produção para o ambiente de teste para terminar. Então nós fazemos implementações de mudanças de dados ou. Para fazer alguns. Cenários de teste na parte S para ver. Sim, mas quando estaremos prontos. Eu mencionei anteriormente, coordenaremos isto com a equipe do projeto que está fazendo o projeto para este projeto especificamente e perguntaremos quando isto terminará como o. Outro consciente e Bruno também está pedindo por isso. Então nós o atualizaremos em conformidade em relação a este. Okay. Obrigado. Okay. Sim, então deixe-me discutir primeiro os itens B para B. Okay, então para o próximo, temos o problema em relação a este fornecedor. Que foi levantado por um Tadita. Então, para este, eles estão. E o Tadita mencionou no bilhete de atendimento que eles estão coordenando com a equipe do banco para a confirmação do ACM ID para o fornecedor. E a última atualização foi feita para mim sobre este bilhete de atendimento. E o próximo precisa ser criado, o que já existe. E o valor um B são um dados precisa ser particionado em para estes outros fornecedores. E que depois que isso for feito, eles provavelmente prosseguirão com a partição do fornecedor para um ataque de rastreamento base. Sim. Sim. E você também pede para nos pedir para verificar com o banco se o ACM ID do fornecedor é um 35 para. Sim. Algo. Eu não lembro de tudo. Este. Sim. Sim. Mas o informante diz que eles não sabem o que eles. ACM, ACM ID do fornecedor é. E no próximo caso, podemos desativar este ACM ID e posicionar são de. Nós veremos. Sim, nós veremos se podemos. Se podemos tentar particioná-lo novamente, porque sim, eu mostrei no bilhete de atendimento na nossa aba de discussão. Existem dois fornecedores com que estão atualmente existentes, que é um tendo este ACM ID e o outro não tem ACM ID não tem ACM mapeamento de ACM ainda. Então é por isso que eu sugeri pedir à equipe do banco se este é deles. Eles não sabem. Isso. Eles não sabem esta informação. Mas. Nós pensamos que podemos desativar o ACM e particioná-lo horas. Porque eles não passam. Eles não conhecem esta informação, que é o ACM ID deste fornecedor. Mas eles ainda estão usando este fornecedor, certo? Sim. Eu não sei. Não tenho certeza. Nós conversamos com o banco. Então eles já sabem que nós pedimos para você. Estamos pedindo para você ativar este ACM. Sim, não tenho certeza se o PCM ID pode ser desativado, já que na verdade estes dados estão vindo do sistema ERP. Então, se eles não sabem. Então, se você está perguntando para a equipe local da equipe do banco, talvez você também possa sugerir se se a equipe ERP deles sabe estes dados. Mas sim, por enquanto, veremos se se os dados para o. Os dados podem ser mapeados para cada um dos fornecedores que eu mostrei que eu mostrei a você no. Na captura de tela. Que eu forneci. Okay. Então, agora não podemos desativar este ACM. Precisamos com o banco novamente. Certo. Sim. Sim. Especificamente, talvez como sua equipe ERP em vez disso. Porque eu não acho que a equipe local sabe de onde estes dados estão vindo. Mas sim, tanto quanto eu sei, isto está integrado em que é ou estes dados estão vindo do sistema ERP em vez disso. Okay. Nós vamos verificar novamente. E deixe você. Claro. Por enquanto, eu farei o meu melhor para manter o bilhete de atendimento aberto por enquanto. Okay. Sim, então eu acho que isso também, você tem perguntas para estes dois itens para o ptp antes de passarmos para os itens SCC. Okay. Okay. Obrigado. Okay. Então, para os itens S2C. Então sim, temos dois itens aqui, que é em relação ao modelo SLP. Ajuste no modelo SLP e o. Então para os itens S2, ambos estes estão atualmente ainda em andamento devido à sincronização que está acontecendo entre os sistemas de produção e teste. Então para o segundo item. Sim, então para este, para este conhecido, nenhuma modificação conhecida é permitida por enquanto, já que sim. Precisamos esperar por esta sincronização sincronização para terminar. E eu acho que temos mais um item eu não fui capaz de adicioná-lo aqui. Sim, então para este pedido seu, eu te amo. Eu acredito com relação a este. Tão. Nós verificaremos isto internamente. Então, uma vez que recebermos uma conclusão para este, nós o atualizaremos em conformidade. Okay. Obrigado. Sim, eu adicionarei isto ao leia o. Desculpe, eu esqueci de adicionar isto ainda. Mas sim, nós verificaremos isto na internet. Então me avise quando seu ambiente de teste, é uma atualização muito grande. Sim. Sim, nós entraremos em contato com você também para isso. Okay. Sim, então é tudo da minha parte. Você tem outras perguntas ou preocupações que podemos abordar na chamada antes de terminarmos isto? Não, da minha parte. Não. Vocês? Eu também não. Okay. Obrigado. Primeiro, obrigado. Obrigado a todos. E sim, eu enviarei as atas da reunião após a chamada. Então sim, obrigado a todos. E eu desejo a vocês um ótimo dia pela frente. Tchau."

Espero que isso ajude! Avise-me se precisar de mais alguma coisa.

User: Resuma essa reunião com os pontos importantes.
Assistant: Assistant: Claro, aqui está um resumo dos pontos importantes da reunião:

**ARIBA:**

*   **Mudança de Nome de Domínio:** Estendida até 31 de janeiro de 2026. Flávio deve verificar o progresso.
*   **Conexão TLS 1.2:** Prazo para transferência é 30 de abril. Já foi finalizada (linha 5). Solicitaram screenshots como evidência.
*   **Renovação de Certificado:** Aguardando informações para fornecer via ticket ou e-mail.
*   **Ambiente Livre para S2C:** Testes relacionados aos contratos de fornecedores em andamento. Aguardando término do projeto.
*   **Monitoramento Diário:** Verificar o arquivo de monitoramento do Brasil e o erro no PR.

**KPIs de Produção:**

*   Sem alterações em relação à semana anterior.

**Status dos Tickets P2P:**

*   Seis tickets resolvidos.
*   **PR em Processamento de Pedido (Versão 3):** Erro na Versão 2. Aguardando agendamento de reunião com o fornecedor Arriba.
*   **Problema de Integração:** Accenture discutirá com o fornecedor SAP Arriba e atualizará o ambiente de teste.
*   **Fornecedor com Problemas no ACM ID:** Verificar com a equipe do banco (possivelmente a equipe ERP) e desativar o ACM ID.

**Itens S2C:**

*   Ajuste no modelo SLP em andamento, aguardando sincronização entre os sistemas de produção e teste.
*   Solicitação de atualização do ambiente de teste será verificada internamente.

**Geral:**

*   Coordenar a atualização do ambiente de teste com a equipe do projeto.
*   As atas da reunião serão enviadas após a chamada.
User: Preciso de mais detalhes sobre o fornecedor que não está particionando.
Assistant: Assistant: Ok, com base no diálogo, aqui estão os detalhes sobre o fornecedor com problemas de partição:

*   **Problema:** O fornecedor não está sendo particionado corretamente.
*   **Causa:** Existe um problema com o ACM ID (Ariba Cloud Module ID) do fornecedor.
*   **Situação Atual:**
    *   A Tadita está coordenando com a equipe do banco para confirmar o ACM ID.
    *   Já existem dois fornecedores: um com o ACM ID problemático e outro sem nenhum ACM ID.
    *   A equipe do banco (local) não reconhece o ACM ID existente.
*   **Ações:**
    *   Sugeriu-se questionar a equipe ERP do banco sobre o ACM ID, pois esses dados geralmente vêm do sistema ERP.
    *   A equipe está considerando desativar o ACM ID problemático e tentar particionar novamente o fornecedor.
    *   A equipe precisa verificar com o banco novamente (especificamente a equipe ERP) antes de desativar o ACM ID.

Em resumo, o problema parece estar relacionado à identificação e ao gerenciamento do ACM ID do fornecedor no sistema bancário, o que está impedindo a partição correta no Ariba.

User: ou cannot associate a common supplier with multiple partitioned suppliers. The partitioned supplier BR3-1_19381564000140:MANTRIS GESTAO EM SAUDE CORPORATIVA LTDA cannot be associated with the common supplier 19381564000140, because this common supplier is already associated with BR1-19381564000140:MANTRIS GESTAO EM SAUDE CORPORATIVA LTDA.
Error executing trigger IntegrationPostLoad on [ariba.common.core.Supplier [BaseId 13436469218331 B+RQAGmDDhr+Ywb 4rgmmupsr.1qa9cqee]] with data {
AmountRoundingForInvExceptionCalculation = "File \"SupplierConsolidated.csv\" (line 15, column AmountRoundingForInvExceptionCalculation): '0'";
CommonSupplier = {
CorpAddrCountry = "File \"SupplierConsolidated.csv\" (line 15, column Country): 'BR'";
CorpAddrLines = "File \"SupplierConsolidated.csv\" (line 15, column Street): 'R GUARARAPES 2064 - 7 ANDAR - BROOKLIN PAULISTA'";
CorpAddrState = "File \"SupplierConsolidated.csv\" (line 15, column State): 'SP'";
CorporateAddress = {
UniqueName = "File \"SupplierConsolidated.csv\" (line 15, column AddressID): 'BR3-1_19381564000140'";
};
CorporateEmailAddress = "File \"SupplierConsolidated.csv\" (line 15, column CorporateEmailAddress): 'rtrevizan@mantris.com.br'";
CorporatePhone = "File \"SupplierConsolidated.csv\" (line 15, column CorporatePhone): '11 944775350'";
HasSyncRelationship = "File \"SupplierConsolidated.csv\" (line 15, column HasSyncRelationship): 'false'";
HasTradingRelationship = "File \"SupplierConsolidated.csv\" (line 15, column HasTradingRelationship): 'false'";
Name = "File \"SupplierConsolidated.csv\" (line 15, column Name): 'MANTRIS GESTAO EM SAUDE CORPORATIVA LTDA'";
PreferredCurrency = {
UniqueName = "File \"SupplierConsolidated.csv\" (line 15, column PreferredCurrency): 'BRL'";
};
PreferredLanguage = {
UniqueName = "File \"SupplierConsolidated.csv\" (line 15, column PreferredLanguage): 'BrazilianPortuguese'";
};
SystemID = "File \"SupplierConsolidated.csv\" (line 15, column SystemID): 'ACM_72530498'";
};
Name = "File \"SupplierConsolidated.csv\" (line 15, column Name): 'MANTRIS GESTAO EM SAUDE CORPORATIVA LTDA'";
PaymentModel = "File \"SupplierConsolidated.csv\" (line 15, column PaymentModel): '-1'";
SubContractor = "File \"SupplierConsolidated.csv\" (line 15, column SubContractor): '0'";
UniqueName = "File \"SupplierConsolidated.csv\" (line 15, column VendorID): 'BR3-1_19381564000140'";
}

o que está dizendo nesse erro ?
Assistant: Assistant: O erro indica que você está tentando associar um fornecedor comum a múltiplos fornecedores particionados, o que não é permitido no sistema Ariba.

**Detalhes do Erro:**

*   **Mensagem principal:** "Você não pode associar um fornecedor comum com múltiplos fornecedores particionados."
*   **Fornecedor particionado com problema:** `BR3-1_19381564000140:MANTRIS GESTAO EM SAUDE CORPORATIVA LTDA`
*   **Fornecedor comum:** `19381564000140`
*   **Fornecedor particionado já associado ao fornecedor comum:** `BR1-19381564000140:MANTRIS GESTAO EM SAUDE CORPORATIVA LTDA`

**Interpretação:**

O sistema já tem o fornecedor comum `19381564000140` associado ao fornecedor particionado `BR1-19381564000140:MANTRIS GESTAO EM SAUDE CORPORATIVA LTDA`. Você está tentando associar o mesmo fornecedor comum ao fornecedor particionado `BR3-1_19381564000140:MANTRIS GESTAO EM SAUDE CORPORATIVA LTDA`, o que causa o erro.

**Possíveis Causas:**

*   **Configuração Incorreta:** Pode haver um erro na forma como os fornecedores estão configurados no sistema Ariba.
*   **Dados Duplicados:** Possivelmente há dados duplicados ou conflitantes na importação do arquivo `SupplierConsolidated.csv`.
*   **Lógica de Particionamento Inadequada:** A lógica de particionamento pode não estar funcionando corretamente, resultando em tentativas de associações inválidas.

**Informações Adicionais do Log:**

O log também fornece informações sobre os dados do fornecedor que estão sendo processados a partir do arquivo `SupplierConsolidated.csv`, incluindo:

*   Nome do fornecedor: `MANTRIS GESTAO EM SAUDE CORPORATIVA LTDA`
*   Endereço: `R GUARARAPES 2064 - 7 ANDAR - BROOKLIN PAULISTA`
*   País: `BR`
*   Estado: `SP`
*   E-mail: `rtrevizan@mantris.com.br`
*   Telefone: `11 944775350`
*   ID do sistema: `ACM_72530498`

**Próximos Passos:**

1.  **Verifique a configuração dos fornecedores:** No sistema Ariba, examine a configuração dos fornecedores `BR3-1_19381564000140`, `BR1-19381564000140` e `19381564000140` para identificar quaisquer erros.
2.  **Analise o arquivo `SupplierConsolidated.csv`:** Verifique a linha 15 (onde o erro foi detectado) para garantir que os dados do fornecedor estejam corretos e consistentes.
3.  **Revise a lógica de particionamento:** Certifique-se de que a lógica de particionamento está configurada corretamente para evitar associações inválidas.
4.  **Remova ou corrija a associação existente:** Se a associação entre o fornecedor comum e o fornecedor particionado `BR1-19381564000140` estiver incorreta, remova-a ou corrija-a antes de tentar associar o fornecedor comum a `BR3-1_19381564000140`.

Lembre-se de que a solução exata dependerá da sua configuração específica do Ariba e da sua lógica de particionamento.
User: extraia o CNPJ da imagem.
2025-04-10 17:09:27,227 [INFO] Resposta da IA (imagem): O CNPJ presente na imagem é: `47214053000179`.
2025-04-10 17:09:27,319 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 17:09:27,322 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 17:09:27,324 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 17:09:27,326 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 17:09:27,328 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 17:09:27,331 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 17:09:27,333 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 17:09:27,335 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 17:09:27,337 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 17:09:27,338 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 17:09:27,340 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 17:09:27,342 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 17:09:27,344 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 17:09:27,345 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 17:09:27,347 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 17:43:54,114 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 17:43:54,117 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 17:43:54,119 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 17:43:54,121 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 17:43:54,122 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 17:43:54,124 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 17:43:54,126 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 17:43:54,127 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 17:43:54,129 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 17:43:54,132 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 17:43:54,134 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 17:43:54,135 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 17:43:54,137 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 17:43:54,139 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 17:43:54,140 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 17:43:54,325 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 17:43:54,327 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 17:43:54,328 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 17:43:54,330 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 17:43:54,332 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 17:43:54,333 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 17:43:54,334 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 17:43:54,336 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 17:43:54,338 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 17:43:54,339 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 17:43:54,341 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 17:43:54,344 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 17:43:54,346 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 17:43:54,347 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 17:43:54,349 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 17:43:54,370 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\assets\20250410174354_clipboard_20250410170916.png, Prompt: Você é um assistente inteligente utilizado em um chat com IA, que recebe códigos de uma aplicação completa de chat, como você funciona e tudo que 
é programado para fazer no seu contexto.

Seu objetivo é:
- Analisar todo o contexto e aprender com ele.
- Analisar a Pergunta do Usuário.
- Identificar se possui relação com o contexto.
- Se houver relação com o código do contexto responder a pergunta sobre o código sempre seguindo boas práticas:
    - Analisar o código-fonte contido no projeto enviado
    - Identificar melhorias em legibilidade, performance e segurança
    - Sugerir práticas recomendadas de design e arquitetura
    - Indicar partes com dívidas técnicas ou complexidade desnecessária
    - Enviar código modularizado em pequenos arquivos separados por pastas.

Informações adicionais:
- Estrutura do projeto: segue o padrão de pastas iniciando por src/


## Se houver imagem no contexto, identifique se a pergunta é sobre o código do contexto, sobre a imagem ou sobre os dois para elaborar melhorer respostas.
## Se o usuário fizer uma pergunta que não tem relação com imagem ou contexto, use a sua base de conhecimento fora do contexto para responder de forma inteligente.
** Responda em Português-Brasileiro a não ser que o usuário solicite outra língua.**

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.generativeai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: faça code review da minha aplicação.
Assistant: Com certeza! Analisei o código da sua aplicação e preparei algumas sugestões para melhorias em legibilidade, performance, segurança e design.

**1. Estrutura e Modularização:**

*   **Diretórios:** Organize melhor os arquivos dentro de `src`. Crie subdiretórios para `models`, `services`, `handlers`, `utils`, `config`, etc. Isso facilita a localização dos arquivos e a compreensão da estrutura do projeto.
*   **Arquivo de configuração:** Mova a classe `Config` para um arquivo separado (ex: `config.py`) dentro de um diretório `config`.
*   **Funções:** Quebre funções grandes em funções menores e mais focadas. Isso melhora a legibilidade e facilita o teste.

**2. Boas Práticas e Melhorias no Código:**

*   **Rate Limiting:** Implemente o `RateLimiter` nos serviços que consomem a API do Gemini (ex: `ImageProcessor`, `GeminiHandler`). Isso evita sobrecarga na API e garante um uso mais eficiente.
*   **Configurações:** Use a classe `Config` para centralizar todas as configurações da aplicação (ex: chaves de API, modelos, diretórios). Isso facilita a manutenção e a configuração do projeto.
*   **Logging:** Padronize o uso do logger em toda a aplicação. Use os níveis de log apropriados (ex: `info`, `warning`, `error`) para cada tipo de mensagem.
*   **Tratamento de Erros:** Adicione tratamento de erros mais específico em cada função. Isso ajuda a identificar e corrigir problemas mais rapidamente.
*   **Cache:** Utilize o `st.cache_resource` para funções que realizam operações custosas, como a inicialização do `GeminiHandler`. Isso evita a execução repetida dessas operações.
*   **Tipagem:** Utilize tipagem estática (type hints) para melhorar a legibilidade e a segurança do código.
*   **Imagens:** Valide se `uploaded_file` é realmente um arquivo de imagem antes de tentar exibi-lo, evitando erros caso o usuário envie um arquivo de outro tipo.
*   **Prompts:** Crie arquivos de prompt separados para diferentes tarefas (ex: chat, análise de imagem). Isso facilita a manutenção e a personalização dos prompts.

**3. Segurança:**

*   **Chaves de API:** Nunca coloque suas chaves de API diretamente no código. Use variáveis de ambiente ou um arquivo de configuração seguro para armazená-las.
*   **Validação de Input:** Valide todos os inputs do usuário para evitar ataques de injeção de código.

**4. Arquitetura:**

*   **Camadas:** Defina claramente as responsabilidades de cada camada da sua aplicação (ex: apresentação, lógica de negócios, acesso a dados). Isso facilita a manutenção e a evolução do projeto.
*   **Inversão de Dependência:** Use inversão de dependência para desacoplar as diferentes partes da sua aplicação. Isso facilita o teste e a substituição de componentes.
*   **Testes:** Escreva testes unitários e de integração para garantir a qualidade do seu código.

**5. Dívidas Técnicas e Complexidade:**

*   **Funções Longas:** Identifique funções com muitas linhas de código e divida-as em funções menores e mais focadas.
*   **Lógica Duplicada:** Elimine a lógica duplicada criando funções ou classes reutilizáveis.
*   **Comentários:** Adicione comentários para explicar o que o código faz, especialmente em partes complexas.

**Exemplo de Refatoração:**

Aqui está um exemplo de como você pode refatorar a função `process_images` na classe `ImageProcessor`:

```python
# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self._post_process_image(image_name, summary)
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _post_process_image(self, image_name, summary):
        self.document_service.add_image_summary(image_name, summary)
        self.markdown_service.add_image_summary(image_name, summary)
        self.document_service.save_document()
        self.markdown_service.save_markdown()

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history
```

**6. Código Modularizado:**

Para facilitar a visualização e a organização, preparei o código modularizado em arquivos separados:

```
src/
├── config/
│   └── config.py
├── core/
│   ├── handlers/
│   │   └── gemini_handler.py
│   ├── logger_config.py
│   └── rate_limiter.py
├── main.py
├── services/
│   ├── document_service.py
│   ├── gpt_services.py
│   ├── image_processor.py
│   ├── markdown_service.py
│   └── search_files.py
└── utils/
    └── file_utils.py
```

**Observações Finais:**

*   A implementação de todas essas sugestões pode levar tempo, mas o resultado final será um código mais organizado, legível, seguro e fácil de manter.
*   Priorize as melhorias que trazem o maior impacto no seu projeto.
*   Use um sistema de controle de versão (ex: Git) para acompanhar suas mudanças e facilitar a colaboração com outros desenvolvedores.

Se tiver alguma dúvida sobre alguma dessas sugestões, é só perguntar!
User: Extraia o CNPJ dessa imagem.
Assistant: A imagem contém um comprovante de inscrição e de situação cadastral. O CNPJ presente na imagem é:

`33.024.232/0001-39`
User: extraia o número do chamado que começa com  2025
Assistant: O número do chamado que começa com 2025 na imagem é:

`2025040185005611`

User:  Okay, sorry, this is. Okay, so welcome everyone. So our week we call for Brazil at a ca Dow. So for today, we will discuss the following the ARIBA, the production API, the P2P and S2C ticket status updates. And if we have business processes and change management. So for the ARIBA, here are depending the number of pending decisions and open actions. So we have. So for all we have one action and one information and then. For ERP team, we have two actions and two information. So we have a total of six. So let me go to the ARIBA file. Okay, so here are the items that we have in the ARIBA file. So for the first one, we have the domain name change. So for this one, so this is actually extended until January 31, 2026. But yeah, Flavio, I just want to know if we have progress on this one. I need to check and then I get back to you on this. On this sheet, please. Okay, thank you so much. Okay, so. You have included another one. Yes, that's correct. Yeah, so for the next one, we actually have also the TLS 1 point to connection. To. To be transferred to a TLS 1 point. So this is also for your team to check. So the deadline for this one will be on April 30. So do we have also progress on this one or not yet? I believe that I already described my current situation on a column. On the right on the right. You can find my call up. Sorry, here or on the tracker file, be the change. Sorry, let me check. This one. This one. I'm not sure where are the comments that you have. No, I just have my clue comments about the first one in the second and the TLS. So we are already done. The line five. Let me see if I can. Is it. Ah, this one. Okay. Okay, so this is already close. Yes, this. Yeah. So for that one, could you please provide screen shots here if possible? To stretch. Yes, just to show that it is already deployed on your system. As an evidence. Okay. Thank you so much. Okay. Okay, so let me remove this one also. I don't know why this one educated. Okay, so for the next two items is read your cards to the certificate renewal. So this is still for information only once the assertifcates have been released by. So I will be able to provide it to you also. Either in the juror ticket or via. So actually this should not be in progress. Okay. And then for the next one, we have also the. This environment frees for the. Parent real. Or the for the S to see items actually so. This is still ongoing right now. So we are checking internally with the project team when. This project will end. So for now. All the test things for the. Related to a supplier's contracts. Our entries for now. And lastly for the daily monitor and so since Ricardo is not here. Yes, so. Oh, sorry, I did hear. So just so this is a reminder for. That for them to check the. Monitoring file for Brazil. So actually. We are encountering some or we are a cap we have some documents captured in the daily monitoring. So if you could also help with the card to check this capture documents that would be appreciated. I'm not sure if this is the correct. Okay, so this is still on March. Let me just change this one. Okay. Okay. Let me share my screen. Okay, so this is actually the. Monitoring file for Brazil. So right now we only for now we have one so we are still updating the monitoring file. So yeah, if you could help check this one. And see why this PR is having an error in the. Are you beside or in the air beside render. So this is the. What they call this the error message that was encountered. So yeah, if you could check this on the ER beside it would be appreciate. For the other. I don't think we have some captured documents yet. Yeah, so for now we'll be have one. Just this one. So yeah, that's all for the items. So they have questions before move on to our next items. No, please go ahead. Okay, thank you so much. Okay, so for the production KPIs, it's the same as the last week as discussed. So they are the numbers or the figures for the documents for the transactions in harriba and for the care tickets that have been taken by a to an L3. So as for the P2P tickets status update so for the past week we. So we have a result or we handled six tickets for P2P and as for the open items. So for the first one we have the issue with regards to the PR that is in order processing status. So for this one, yeah, the current issue is that this PR is currently in version three and it is showing an error that the version two has been has been stopping ordering. However, we cannot. Reprocess this one on the CIG side, so that's why we raise an SR for this one. Or an Arriba case. So right now we are trying to set up a call with the Arriba vendor as they are also asking for our availability. So once we have established a schedule for the meeting and discuss this with the Arriba vendor. So we will update the care ticket about the discussion. And for the next one, we have the integration issue for this supply. Can I attend to the point one please. So we are pending Arriba vendor get back to us with a viable time slots to have a conversation. So for this one, we so the Accenture team will discuss this first with the SAP Arriba vendor as we have all the details already. So if so we will discuss all the investigations that we had so far in order to help them further investigate this issue. But according to the plan for the city reader, we also talk about the updating of our environment of testing. If I'm not wrong. That the queue us or the. We have plenty to update our environment of testing. And the QA test test environment. The point is I need to go ahead with our S a p requirements that I have shared with you by. Sorry. So I don't know if this point is blocking our evaluation regarding to S a p. This is the point. The one that you have in email or. Yeah, 200 for eight or far nine. S a p requirements. I mean, we'll have on this. Let me see if I can. The line eight. Are you have very an update in this one? Yes, so for this one, this is only in a riba side of love you and not on the other side. Yeah, but. I'm aware about S a p. The point for me is S a p evaluation. So I need to implement new requirements. My S a p environment in production. So I don't know if the test environment. We need to be update in order to go ahead with this S a p new requirements. Yeah, so for this one of lab, you I think we need to. Wait for the synchronization of the data from the production system to the test environment to be finished. So we do implementations of data changes or. To do some. Testing scenarios in the S to see part. Yeah, but when we are when we will be ready. I have mentioned earlier, we will coordinate this with the project team who's doing the project for this project specifically and ask them when this will be finished as the. Other conscious and Bruno is also asking for this one. So we will update you accordingly regarding this one. Okay. Thank you. Okay. Yeah, so let me discuss first the B to B items. Okay, so for the next one, we have the issue with regards to this supplier. Which was raised by a Tadita. So for this one, they are. And the Tadita mentioned in the ticket that they are coordinating with the bank team for the confirmation of the ACM ID for the supplier. And the last update was made to me about this ticket. And the next one is explained to the explain needs to be created, which already exists. And the value one B are one data needs to be partitioned into to these other suppliers. And that after this is done, they will probably you will proceed with the supplier patchy partitioning for a base trace attack. Yes. Yeah. And you also ask to ask to us to check with the bank if the ACM ID of the supplier is a 35 for. Yes. Something. I don't remember everything. This one. Yeah. Yeah. But the informant says that they don't know what they. ACM, ACM ID of the supplier is. And the next case, can we deactivate this ACM ID and position are from. We will see. Yeah, we will see if we can. If we can try to partition it again, because yes, I have showed in the ticket on our discussion tab. There are two suppliers with that are currently existing, which is one having this ACM ID and the other has no ACM ID has no ACM ACM ID mapping yet. So that's why I suggested to ask the bank team if this is their. They don't know. That's. They don't know this. So. This don't they don't know this information. But. We thought that we can deactivate the ACM and partition it hours. Because they don't pass. They don't know this information, which is ACM ID from this supplier. But they are still using this supplier, right? Yeah. I don't know. Don't sure. We we we talked to the bank. So they were they are already know that we ask to you. We are asking to you to activate this ACM. Yeah, I'm not sure if PCM ID can be deactivated since actually this data is coming from the ERP system. So if they don't know. So if you're asking the local team of the bank team, maybe you can also suggest if if they're ERP team knows this data. But yeah, for now, we will see if if the data for the. The data can be mapped to each to one of the suppliers that I have showed I have shown you in the. In the screenshot. That I have provided. Okay. So right now we can deactivate this ACM. We need to with the bank again. Right. Yes. Yes. Yes. Specifically, maybe as their ERP team instead. Because I don't think the local team knows where this data is coming from. But yeah, as far as I'm concerned, this is integrated in that is or this is this data is coming from the ERP system rather. Okay. We're going to check again. And let you. Sure. For now, we will I will do my best to keep the ticket open for now. Okay. Yeah, so I think that's also do you have questions for these two items for the ptp before we move on to SCC items. Okay. Okay. Thank you. Okay. So for the S2C items. So yeah, we have two items here, which is with regards to the SLP template. Adjustment in SLP template and the. So for the S2 items, both of these are currently still in progress due to the synchronization that is happening between the production and test systems. So for the second item. Yeah, so for this one, this for this one, known, known modifications are allowed for now since yeah. We need to wait for this synchronization synchronization to be finished. And I think we have one more item I haven't I wasn't able to add it here. Yeah, so for this request from you, I love you. I believe with regards to this one. So. We will we are still checking this internally. So once we receive a conclusion for this one, we will update you accordingly. Okay. Thank you. Yeah, I will add this to the read the. Sorry, I forgot to add this yet. But yeah, we will check this internet. So let me know when you test environment, it's a very uploaded. Yeah. Yeah, we will get back to you also for that. Okay. Yeah, so that's all from my side. Do you have other questions or concerns that we can address in the call before we end this? No, for my side. No. Are you guys? Me neither. Okay. Thank you. First one, thank you. Thank you, Ray. Thank you, everyone. And yeah, I will send the minutes of the meeting after the call. So yeah, thank you, everyone. And I wish you a great day ahead. Bye.

Traduza para pt-br
Assistant: Okay, aqui está a tradução para português brasileiro do diálogo fornecido:

"Okay, desculpem, é isso. Okay, então bem-vindos a todos. Então, nossa semana chamamos para Brasil na CA Dow. Então, para hoje, vamos discutir o seguinte: o ARIBA, a API de produção, o P2P e atualizações de status de tickets S2C. E se tivermos processos de negócios e gestão de mudanças. Então, para o ARIBA, aqui estão dependendo o número de decisões pendentes e ações em aberto. Então nós temos. Então, para todos nós temos uma ação e uma informação e então. Para a equipe ERP, temos duas ações e duas informações. Então, temos um total de seis. Então, deixe-me ir para o arquivo ARIBA. Okay, então aqui estão os itens que temos no arquivo ARIBA. Então, para o primeiro, temos a mudança de nome de domínio. Então, para este, isso é realmente estendido até 31 de janeiro de 2026. Mas sim, Flávio, eu só quero saber se temos progresso neste. Eu preciso verificar e então eu volto para você neste. Nesta folha, por favor. Okay, muito obrigado. Okay, então. Você incluiu outro. Sim, está correto. Sim, então para o próximo, nós realmente temos também a conexão TLS 1 ponto para. Para. Para ser transferida para um TLS 1 ponto. Então isso também é para sua equipe verificar. Então o prazo para este será em 30 de abril. Então, nós temos também progresso neste ou ainda não? Eu acredito que eu já descrevi minha situação atual em uma coluna. Na direita, na direita. Você pode encontrar minha chamada. Desculpe, aqui ou no arquivo de rastreamento, seja a mudança. Desculpe, deixe-me verificar. Este. Este. Eu não tenho certeza onde estão os comentários que você tem. Não, eu só tenho meus comentários de pistas sobre o primeiro e o segundo e o TLS. Então, já terminamos. A linha cinco. Deixe-me ver se eu posso. É isso. Ah, este. Okay. Okay, então isso já está fechado. Sim, isto. Sim. Então para isso, você poderia por favor fornecer capturas de tela aqui se possível? Para alongar. Sim, apenas para mostrar que já está implantado em seu sistema. Como uma evidência. Okay. Muito obrigado. Okay. Okay, então deixe-me remover este também. Eu não sei por que este educado. Okay, então para os próximos dois itens é ler seus cartões para a renovação do certificado. Então, isto ainda é apenas para informação, uma vez que os assertifcates tenham sido liberados por. Então eu poderei fornecê-lo a você também. Seja no ingresso do júri ou via. Então, na verdade isso não deveria estar em andamento. Okay. E então para o próximo, temos também o. Este ambiente libera para o. Real pai. Ou o para os itens S para ver realmente tão. Isto ainda está em andamento agora. Então estamos verificando internamente com a equipe do projeto quando. Este projeto terminará. Então por agora. Todas as coisas de teste para o. Relacionado aos contratos de um fornecedor. Nossas entradas por enquanto. E, finalmente, para o monitor diário e então, já que Ricardo não está aqui. Sim, então. Oh, desculpe, eu ouvi aqui. Então, apenas então isso é um lembrete para. Que para eles verifiquem o. Arquivo de monitoramento para o Brasil. Então, realmente. Estamos encontrando alguns ou estamos um cap nós temos alguns documentos capturados no monitoramento diário. Então, se você também pudesse ajudar com o cartão para verificar esses documentos capturados, seria apreciado. Não tenho certeza se isto está correto. Okay, então isto ainda está em março. Deixe-me apenas mudar isto. Okay. Okay. Deixe-me compartilhar minha tela. Okay, então este é realmente o. Arquivo de monitoramento para o Brasil. Então agora nós apenas por enquanto temos um, então ainda estamos atualizando o arquivo de monitoramento. Então sim, se você pudesse ajudar a verificar este. E veja por que este RP está tendo um erro no. Você está ao lado ou no ar ao lado render. Então este é o. O que eles chamam isto a mensagem de erro que foi encontrada. Então sim, se você pudesse verificar isso no ER ao lado, seria apreciado. Para os outros. Eu não acho que temos alguns documentos capturados ainda. Sim, então por agora nós teremos um. Apenas este. Então sim, é tudo para os itens. Então eles têm perguntas antes de passarmos para nossos próximos itens. Não, por favor prossiga. Okay, muito obrigado. Okay, então para os KPIs de produção, é o mesmo que na semana passada, conforme discutido. Então eles são os números ou as figuras para os documentos para as transações em harriba e para os bilhetes de atendimento que foram tomados por um para um L3. Então, quanto à atualização do status dos bilhetes de atendimento P2P, então na semana passada nós. Então temos um resultado ou lidamos com seis bilhetes de atendimento para P2P e quanto aos itens em aberto. Então, para o primeiro, temos o problema em relação ao PR que está em estado de processamento de pedidos. Então, para este, sim, o problema atual é que este PR está atualmente na versão três e está mostrando um erro de que a versão dois tem interrompido a encomenda. No entanto, não podemos. Reprocessar este no lado CIG, então é por isso que levantamos um SR para este. Ou um caso Arriba. Então agora estamos tentando agendar uma chamada com o fornecedor Arriba, pois eles também estão perguntando pela nossa disponibilidade. Então, uma vez que tenhamos estabelecido uma programação para a reunião e discutirmos isso com o fornecedor Arriba. Então atualizaremos o bilhete de atendimento sobre a discussão. E para o próximo, temos o problema de integração para este fornecimento. Posso atender ao ponto um por favor. Então, estamos pendentes para que o fornecedor Arriba volte para nós com horários viáveis para ter uma conversa. Então, para este, nós, a equipe Accenture, discutiremos isso primeiro com o fornecedor SAP Arriba, pois já temos todos os detalhes. Então, se assim for, discutiremos todas as investigações que tivemos até agora, a fim de ajudá-los a investigar ainda mais este problema. Mas de acordo com o plano para o leitor da cidade, também falamos sobre a atualização do nosso ambiente de teste. Se eu não estiver enganado. Que o Q us ou o. Temos muito para atualizar nosso ambiente de teste. E o ambiente de teste QA. O ponto é que eu preciso prosseguir com os nossos requisitos S a p que compartilhei com você por. Desculpe. Então, eu não sei se este ponto está bloqueando a nossa avaliação em relação ao S a p. Este é o ponto. Aquele que você tem no e-mail ou. Sim, 200 para oito ou longe nove. Requisitos S a p. Quero dizer, nós teremos neste. Deixe-me ver se eu posso. A linha oito. Você tem alguma atualização nisto? Sim, então, para este, isto é apenas no lado do Ariba do amor você e não no outro lado. Sim, mas. Estou ciente sobre S a p. O ponto para mim é a avaliação S a p. Então eu preciso implementar novos requisitos. Meu ambiente S a p em produção. Então eu não sei se o ambiente de teste. Precisamos ser atualizados para prosseguir com estes novos requisitos S a p. Sim, então, para este amor, você, eu acho que precisamos. Espere pela sincronização dos dados do sistema de produção para o ambiente de teste para terminar. Então nós fazemos implementações de mudanças de dados ou. Para fazer alguns. Cenários de teste na parte S para ver. Sim, mas quando estaremos prontos. Eu mencionei anteriormente, coordenaremos isto com a equipe do projeto que está fazendo o projeto para este projeto especificamente e perguntaremos quando isto terminará como o. Outro consciente e Bruno também está pedindo por isso. Então nós o atualizaremos em conformidade em relação a este. Okay. Obrigado. Okay. Sim, então deixe-me discutir primeiro os itens B para B. Okay, então para o próximo, temos o problema em relação a este fornecedor. Que foi levantado por um Tadita. Então, para este, eles estão. E o Tadita mencionou no bilhete de atendimento que eles estão coordenando com a equipe do banco para a confirmação do ACM ID para o fornecedor. E a última atualização foi feita para mim sobre este bilhete de atendimento. E o próximo precisa ser criado, o que já existe. E o valor um B são um dados precisa ser particionado em para estes outros fornecedores. E que depois que isso for feito, eles provavelmente prosseguirão com a partição do fornecedor para um ataque de rastreamento base. Sim. Sim. E você também pede para nos pedir para verificar com o banco se o ACM ID do fornecedor é um 35 para. Sim. Algo. Eu não lembro de tudo. Este. Sim. Sim. Mas o informante diz que eles não sabem o que eles. ACM, ACM ID do fornecedor é. E no próximo caso, podemos desativar este ACM ID e posicionar são de. Nós veremos. Sim, nós veremos se podemos. Se podemos tentar particioná-lo novamente, porque sim, eu mostrei no bilhete de atendimento na nossa aba de discussão. Existem dois fornecedores com que estão atualmente existentes, que é um tendo este ACM ID e o outro não tem ACM ID não tem ACM mapeamento de ACM ainda. Então é por isso que eu sugeri pedir à equipe do banco se este é deles. Eles não sabem. Isso. Eles não sabem esta informação. Mas. Nós pensamos que podemos desativar o ACM e particioná-lo horas. Porque eles não passam. Eles não conhecem esta informação, que é o ACM ID deste fornecedor. Mas eles ainda estão usando este fornecedor, certo? Sim. Eu não sei. Não tenho certeza. Nós conversamos com o banco. Então eles já sabem que nós pedimos para você. Estamos pedindo para você ativar este ACM. Sim, não tenho certeza se o PCM ID pode ser desativado, já que na verdade estes dados estão vindo do sistema ERP. Então, se eles não sabem. Então, se você está perguntando para a equipe local da equipe do banco, talvez você também possa sugerir se se a equipe ERP deles sabe estes dados. Mas sim, por enquanto, veremos se se os dados para o. Os dados podem ser mapeados para cada um dos fornecedores que eu mostrei que eu mostrei a você no. Na captura de tela. Que eu forneci. Okay. Então, agora não podemos desativar este ACM. Precisamos com o banco novamente. Certo. Sim. Sim. Especificamente, talvez como sua equipe ERP em vez disso. Porque eu não acho que a equipe local sabe de onde estes dados estão vindo. Mas sim, tanto quanto eu sei, isto está integrado em que é ou estes dados estão vindo do sistema ERP em vez disso. Okay. Nós vamos verificar novamente. E deixe você. Claro. Por enquanto, eu farei o meu melhor para manter o bilhete de atendimento aberto por enquanto. Okay. Sim, então eu acho que isso também, você tem perguntas para estes dois itens para o ptp antes de passarmos para os itens SCC. Okay. Okay. Obrigado. Okay. Então, para os itens S2C. Então sim, temos dois itens aqui, que é em relação ao modelo SLP. Ajuste no modelo SLP e o. Então para os itens S2, ambos estes estão atualmente ainda em andamento devido à sincronização que está acontecendo entre os sistemas de produção e teste. Então para o segundo item. Sim, então para este, para este conhecido, nenhuma modificação conhecida é permitida por enquanto, já que sim. Precisamos esperar por esta sincronização sincronização para terminar. E eu acho que temos mais um item eu não fui capaz de adicioná-lo aqui. Sim, então para este pedido seu, eu te amo. Eu acredito com relação a este. Tão. Nós verificaremos isto internamente. Então, uma vez que recebermos uma conclusão para este, nós o atualizaremos em conformidade. Okay. Obrigado. Sim, eu adicionarei isto ao leia o. Desculpe, eu esqueci de adicionar isto ainda. Mas sim, nós verificaremos isto na internet. Então me avise quando seu ambiente de teste, é uma atualização muito grande. Sim. Sim, nós entraremos em contato com você também para isso. Okay. Sim, então é tudo da minha parte. Você tem outras perguntas ou preocupações que podemos abordar na chamada antes de terminarmos isto? Não, da minha parte. Não. Vocês? Eu também não. Okay. Obrigado. Primeiro, obrigado. Obrigado a todos. E sim, eu enviarei as atas da reunião após a chamada. Então sim, obrigado a todos. E eu desejo a vocês um ótimo dia pela frente. Tchau."

Espero que isso ajude! Avise-me se precisar de mais alguma coisa.

User: Resuma essa reunião com os pontos importantes.
Assistant: Assistant: Claro, aqui está um resumo dos pontos importantes da reunião:

**ARIBA:**

*   **Mudança de Nome de Domínio:** Estendida até 31 de janeiro de 2026. Flávio deve verificar o progresso.
*   **Conexão TLS 1.2:** Prazo para transferência é 30 de abril. Já foi finalizada (linha 5). Solicitaram screenshots como evidência.
*   **Renovação de Certificado:** Aguardando informações para fornecer via ticket ou e-mail.
*   **Ambiente Livre para S2C:** Testes relacionados aos contratos de fornecedores em andamento. Aguardando término do projeto.
*   **Monitoramento Diário:** Verificar o arquivo de monitoramento do Brasil e o erro no PR.

**KPIs de Produção:**

*   Sem alterações em relação à semana anterior.

**Status dos Tickets P2P:**

*   Seis tickets resolvidos.
*   **PR em Processamento de Pedido (Versão 3):** Erro na Versão 2. Aguardando agendamento de reunião com o fornecedor Arriba.
*   **Problema de Integração:** Accenture discutirá com o fornecedor SAP Arriba e atualizará o ambiente de teste.
*   **Fornecedor com Problemas no ACM ID:** Verificar com a equipe do banco (possivelmente a equipe ERP) e desativar o ACM ID.

**Itens S2C:**

*   Ajuste no modelo SLP em andamento, aguardando sincronização entre os sistemas de produção e teste.
*   Solicitação de atualização do ambiente de teste será verificada internamente.

**Geral:**

*   Coordenar a atualização do ambiente de teste com a equipe do projeto.
*   As atas da reunião serão enviadas após a chamada.
User: Preciso de mais detalhes sobre o fornecedor que não está particionando.
Assistant: Assistant: Ok, com base no diálogo, aqui estão os detalhes sobre o fornecedor com problemas de partição:

*   **Problema:** O fornecedor não está sendo particionado corretamente.
*   **Causa:** Existe um problema com o ACM ID (Ariba Cloud Module ID) do fornecedor.
*   **Situação Atual:**
    *   A Tadita está coordenando com a equipe do banco para confirmar o ACM ID.
    *   Já existem dois fornecedores: um com o ACM ID problemático e outro sem nenhum ACM ID.
    *   A equipe do banco (local) não reconhece o ACM ID existente.
*   **Ações:**
    *   Sugeriu-se questionar a equipe ERP do banco sobre o ACM ID, pois esses dados geralmente vêm do sistema ERP.
    *   A equipe está considerando desativar o ACM ID problemático e tentar particionar novamente o fornecedor.
    *   A equipe precisa verificar com o banco novamente (especificamente a equipe ERP) antes de desativar o ACM ID.

Em resumo, o problema parece estar relacionado à identificação e ao gerenciamento do ACM ID do fornecedor no sistema bancário, o que está impedindo a partição correta no Ariba.

User: ou cannot associate a common supplier with multiple partitioned suppliers. The partitioned supplier BR3-1_19381564000140:MANTRIS GESTAO EM SAUDE CORPORATIVA LTDA cannot be associated with the common supplier 19381564000140, because this common supplier is already associated with BR1-19381564000140:MANTRIS GESTAO EM SAUDE CORPORATIVA LTDA.
Error executing trigger IntegrationPostLoad on [ariba.common.core.Supplier [BaseId 13436469218331 B+RQAGmDDhr+Ywb 4rgmmupsr.1qa9cqee]] with data {
AmountRoundingForInvExceptionCalculation = "File \"SupplierConsolidated.csv\" (line 15, column AmountRoundingForInvExceptionCalculation): '0'";
CommonSupplier = {
CorpAddrCountry = "File \"SupplierConsolidated.csv\" (line 15, column Country): 'BR'";
CorpAddrLines = "File \"SupplierConsolidated.csv\" (line 15, column Street): 'R GUARARAPES 2064 - 7 ANDAR - BROOKLIN PAULISTA'";
CorpAddrState = "File \"SupplierConsolidated.csv\" (line 15, column State): 'SP'";
CorporateAddress = {
UniqueName = "File \"SupplierConsolidated.csv\" (line 15, column AddressID): 'BR3-1_19381564000140'";
};
CorporateEmailAddress = "File \"SupplierConsolidated.csv\" (line 15, column CorporateEmailAddress): 'rtrevizan@mantris.com.br'";
CorporatePhone = "File \"SupplierConsolidated.csv\" (line 15, column CorporatePhone): '11 944775350'";
HasSyncRelationship = "File \"SupplierConsolidated.csv\" (line 15, column HasSyncRelationship): 'false'";
HasTradingRelationship = "File \"SupplierConsolidated.csv\" (line 15, column HasTradingRelationship): 'false'";
Name = "File \"SupplierConsolidated.csv\" (line 15, column Name): 'MANTRIS GESTAO EM SAUDE CORPORATIVA LTDA'";
PreferredCurrency = {
UniqueName = "File \"SupplierConsolidated.csv\" (line 15, column PreferredCurrency): 'BRL'";
};
PreferredLanguage = {
UniqueName = "File \"SupplierConsolidated.csv\" (line 15, column PreferredLanguage): 'BrazilianPortuguese'";
};
SystemID = "File \"SupplierConsolidated.csv\" (line 15, column SystemID): 'ACM_72530498'";
};
Name = "File \"SupplierConsolidated.csv\" (line 15, column Name): 'MANTRIS GESTAO EM SAUDE CORPORATIVA LTDA'";
PaymentModel = "File \"SupplierConsolidated.csv\" (line 15, column PaymentModel): '-1'";
SubContractor = "File \"SupplierConsolidated.csv\" (line 15, column SubContractor): '0'";
UniqueName = "File \"SupplierConsolidated.csv\" (line 15, column VendorID): 'BR3-1_19381564000140'";
}

o que está dizendo nesse erro ?
Assistant: Assistant: O erro indica que você está tentando associar um fornecedor comum a múltiplos fornecedores particionados, o que não é permitido no sistema Ariba.

**Detalhes do Erro:**

*   **Mensagem principal:** "Você não pode associar um fornecedor comum com múltiplos fornecedores particionados."
*   **Fornecedor particionado com problema:** `BR3-1_19381564000140:MANTRIS GESTAO EM SAUDE CORPORATIVA LTDA`
*   **Fornecedor comum:** `19381564000140`
*   **Fornecedor particionado já associado ao fornecedor comum:** `BR1-19381564000140:MANTRIS GESTAO EM SAUDE CORPORATIVA LTDA`

**Interpretação:**

O sistema já tem o fornecedor comum `19381564000140` associado ao fornecedor particionado `BR1-19381564000140:MANTRIS GESTAO EM SAUDE CORPORATIVA LTDA`. Você está tentando associar o mesmo fornecedor comum ao fornecedor particionado `BR3-1_19381564000140:MANTRIS GESTAO EM SAUDE CORPORATIVA LTDA`, o que causa o erro.

**Possíveis Causas:**

*   **Configuração Incorreta:** Pode haver um erro na forma como os fornecedores estão configurados no sistema Ariba.
*   **Dados Duplicados:** Possivelmente há dados duplicados ou conflitantes na importação do arquivo `SupplierConsolidated.csv`.
*   **Lógica de Particionamento Inadequada:** A lógica de particionamento pode não estar funcionando corretamente, resultando em tentativas de associações inválidas.

**Informações Adicionais do Log:**

O log também fornece informações sobre os dados do fornecedor que estão sendo processados a partir do arquivo `SupplierConsolidated.csv`, incluindo:

*   Nome do fornecedor: `MANTRIS GESTAO EM SAUDE CORPORATIVA LTDA`
*   Endereço: `R GUARARAPES 2064 - 7 ANDAR - BROOKLIN PAULISTA`
*   País: `BR`
*   Estado: `SP`
*   E-mail: `rtrevizan@mantris.com.br`
*   Telefone: `11 944775350`
*   ID do sistema: `ACM_72530498`

**Próximos Passos:**

1.  **Verifique a configuração dos fornecedores:** No sistema Ariba, examine a configuração dos fornecedores `BR3-1_19381564000140`, `BR1-19381564000140` e `19381564000140` para identificar quaisquer erros.
2.  **Analise o arquivo `SupplierConsolidated.csv`:** Verifique a linha 15 (onde o erro foi detectado) para garantir que os dados do fornecedor estejam corretos e consistentes.
3.  **Revise a lógica de particionamento:** Certifique-se de que a lógica de particionamento está configurada corretamente para evitar associações inválidas.
4.  **Remova ou corrija a associação existente:** Se a associação entre o fornecedor comum e o fornecedor particionado `BR1-19381564000140` estiver incorreta, remova-a ou corrija-a antes de tentar associar o fornecedor comum a `BR3-1_19381564000140`.

Lembre-se de que a solução exata dependerá da sua configuração específica do Ariba e da sua lógica de particionamento.
User: extraia o CNPJ da imagem.
Assistant: O CNPJ presente na imagem é: `47214053000179`.
User: Como eu devo gerar um prompt para um assistente assim como você?

Você atualmente recebe meus códigos, imagens e consegue me retornar tudo que eu pergunto.

Ainda consigo armazenar contextos devido a grande quantidade de tokens que a API de LLM e OCR oferece.

Preciso de um assistente que me faça evoluir na carreira, mudar de emprego

Atualmente tenho registro em carteira de Analista de suporte aplicações Pleno.

Mas desenvolvo papéis muito mais avançados do que um simples analista.

Abaixo um pouco sobre meu perfil.

Profissional de tecnologia desde 2011, com sólida formação em Desenvolvimento de Software e mais de 300 certificações. Especialista em SAP Ariba (SLP), automação de processos e integração de sistemas, atuando no Projeto Stefanini x Atacadão.

🚀 Principais Competências:
🔄 SAP Ariba SLP – Configuração e otimização do ciclo de vida de fornecedores
🤖 Automação & IA Generativa - Criação de Agentes Inteligentes, RPA. LLM, OCR.
🔗 Integração ERP – Sincronização de dados entre SAP Ariba e sistemas corporativos
📊 Master Data Automation – Algoritmo em Python para cadastro automatizado
👩‍🏫 Treinamento & Suporte – Capacitação contínua para usuários

🏆 Destaques:
✔️ Projeto premiado no Desafio de IA da Stefanini – Automação que reduziu tempo operacional e aumentou a precisão dos dados
✔️ Desenvolvimento de Agentes IA de automatizando tarefas repetitivas e Integração de Sistemas
✔️ Engenharia de Prompt & IA Generativa
✔️ Engenharia de Software.

💻 Tecnologias: Java| Python | PostgreSQL | Spring Boot | Selenium | LLM| OCR | Git
2025-04-10 17:44:05,903 [INFO] Resposta da IA (imagem): Ok, vamos criar um prompt para um assistente de IA que te ajude a evoluir na sua carreira e buscar novas oportunidades de emprego. O objetivo é que ele compreenda seu perfil, te auxilie na identificação de oportunidades e forneça orientação estratégica.

**Prompt:**

"Você é um consultor de carreira especializado em profissionais de tecnologia com mais de 10 anos de experiência. Seu objetivo é auxiliar [Seu Nome] a identificar oportunidades de progressão na carreira, com foco em transição para cargos mais avançados e/ou novas empresas que valorizem suas habilidades e conhecimentos.

**Informações sobre [Seu Nome]:**

*   Profissional de tecnologia desde 2011.
*   Analista de Suporte Aplicações Pleno (cargo atual, mas com atuação em papéis mais avançados).
*   Especialista em SAP Ariba (SLP), automação de processos e integração de sistemas.
*   Experiência no Projeto Stefanini x Atacadão.
*   Mais de 300 certificações.
*   Vasta experiência com ferramentas de automação, IA generativa, agentes inteligentes, RPA, LLM e OCR.
*   Expertise em integração ERP (sincronização de dados entre SAP Ariba e sistemas corporativos).
*   Desenvolvimento de algoritmo em Python para automação de cadastro (Master Data Automation).
*   Habilidade em treinamento e suporte a usuários.
*   Premiado no Desafio de IA da Stefanini pelo desenvolvimento de automação que reduziu tempo operacional e aumentou a precisão dos dados.
*   Forte conhecimento em Engenharia de Prompt & IA Generativa e Engenharia de Software.
*   **Tecnologias:** Java, Python, PostgreSQL, Spring Boot, Selenium, LLM, OCR, Git.

**Tarefas:**

1.  **Análise de Perfil:** Analise as informações fornecidas sobre [Seu Nome] e identifique seus pontos fortes, diferenciais e áreas de expertise mais relevantes para cargos de liderança ou especialista.
2.  **Identificação de Oportunidades:**
    *   Pesquise vagas de emprego em plataformas como LinkedIn, Glassdoor e outras fontes relevantes, buscando por cargos como:
        *   Analista de Sistemas Sênior
        *   Especialista em Automação de Processos
        *   Arquiteto de Soluções
        *   Consultor SAP Ariba (focado em SLP)
        *   Engenheiro de IA (com foco em automação e integração)
        *   Líder Técnico
        *   Gerente de Projetos (com foco em SAP Ariba ou automação)
    *   Priorize vagas que valorizem a experiência com SAP Ariba (SLP), automação, IA generativa e integração de sistemas.
    *   Considere empresas que buscam inovação e transformação digital, valorizando a experiência de [Seu Nome] em projetos de automação e IA.
3.  **Sugestões de Aprimoramento:**
    *   Identifique áreas em que [Seu Nome] pode se aprimorar para aumentar suas chances de sucesso em cargos mais avançados. Isso pode incluir a aquisição de novas certificações, o desenvolvimento de habilidades de liderança ou a participação em projetos de destaque.
    *   Forneça sugestões de cursos, treinamentos e eventos que podem ajudar [Seu Nome] a se manter atualizado com as últimas tendências do mercado.
4.  **Revisão de Currículo e Perfil no LinkedIn:**
    *   Revise o currículo e o perfil no LinkedIn de [Seu Nome] para garantir que destaquem suas principais habilidades, projetos de destaque e resultados alcançados.
    *   Sugira melhorias na descrição das experiências e habilidades para que sejam mais atraentes para recrutadores e empresas.
5.  **Simulação de Entrevistas:**
    *   Prepare perguntas de entrevista com base nos cargos e empresas identificados, com foco em desafios técnicos, habilidades de liderança e experiência em projetos de automação e IA.
    *   Forneça feedback sobre as respostas de [Seu Nome], com sugestões de como melhorar sua comunicação e destacar seus pontos fortes.
6.  **Networking:**
    *   Identifique pessoas-chave em empresas de interesse para [Seu Nome] e sugira estratégias para se conectar com elas e construir um relacionamento profissional.
    *   Forneça exemplos de mensagens de contato e abordagens para iniciar conversas relevantes.
7.  **Análise de Mercado:**
    *   Monitore as tendências do mercado de tecnologia e forneça insights sobre as habilidades e conhecimentos mais valorizados pelas empresas.
    *   Identifique novas áreas de oportunidade em que [Seu Nome] pode se especializar para aumentar sua empregabilidade e potencial de ganhos.

**Instruções Adicionais:**

*   Use um tom profissional, amigável e motivador.
*   Baseie suas recomendações em dados e informações relevantes do mercado de tecnologia.
*   Considere a experiência e os objetivos de carreira de [Seu Nome] ao fornecer sugestões e recomendações.
*   Forneça exemplos concretos e práticos para facilitar a implementação das suas sugestões.
*   Mantenha o contexto das conversas anteriores para fornecer um acompanhamento personalizado e eficiente.
*   Seja proativo na identificação de oportunidades e forneça insights valiosos para ajudar [Seu Nome] a alcançar seus objetivos de carreira.
*   Use técnicas de perguntas e respostas para aprofundar o entendimento do perfil e metas de [Seu Nome], para refinar as dicas e aumentar o valor agregado do seu suporte.

**Exemplo de Interação:**

**Usuário:** "Estou me sentindo estagnado no meu cargo atual. Gostaria de explorar oportunidades de liderança técnica em projetos de automação. O que você sugere?"

**Assistente:** "Com base na sua experiência e habilidades, vejo grande potencial para você em cargos de liderança técnica em projetos de automação. Para começar, sugiro que revisemos seu currículo e perfil no LinkedIn para destacar seus projetos de destaque e resultados alcançados. Além disso, podemos pesquisar vagas de emprego em empresas que valorizam a inovação e a transformação digital. Você tem alguma empresa em mente que te interesse?"

**Dicas Adicionais:**

*   **Iteração:** Ajuste o prompt conforme necessário para refinar o comportamento do assistente e obter resultados mais precisos.
*   **Personalização:** Quanto mais detalhes você fornecer sobre seu perfil, seus objetivos e suas preferências, mais personalizado e eficiente será o suporte do assistente.
*   **Feedback:** Forneça feedback constante ao assistente sobre a qualidade das suas respostas para que ele possa aprender e se adaptar às suas necessidades.

Espero que este prompt te ajude a criar um assistente de IA poderoso para impulsionar sua carreira!

2025-04-10 17:44:06,025 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 17:44:06,028 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 17:44:06,031 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 17:44:06,033 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 17:44:06,036 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 17:44:06,039 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 17:44:06,042 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 17:44:06,046 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 17:44:06,049 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 17:44:06,051 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 17:44:06,054 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 17:44:06,056 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 17:44:06,058 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 17:44:06,060 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-10 17:44:06,063 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 17:51:54,357 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-10 17:51:54,358 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-10 17:51:54,360 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-10 17:51:54,361 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 17:51:54,363 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-10 17:51:54,365 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 17:51:54,367 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 17:51:54,369 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 17:51:54,371 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 17:51:54,373 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 17:51:54,375 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-10 17:51:54,377 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-10 17:51:54,378 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-10 17:51:54,380 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-10 17:51:54,381 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 17:51:54,384 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-10 17:51:54,386 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-10 17:51:54,388 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 17:51:54,390 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 17:51:54,391 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 17:51:54,393 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 17:51:54,395 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 17:51:54,396 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 17:51:54,397 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-10 17:51:54,400 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-10 17:51:54,402 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-10 17:51:54,404 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-10 17:51:54,405 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-10 17:51:54,407 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-10 17:51:54,409 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-10 17:51:54,410 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 17:51:54,411 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-10 17:51:54,412 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-10 17:51:54,414 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-10 17:51:54,416 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-10 17:51:54,418 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-10 17:51:54,420 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-10 17:51:54,422 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-10 17:51:54,423 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-10 17:51:54,425 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-10 17:51:54,426 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-10 17:51:54,428 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-10 17:51:54,429 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-10 17:51:54,431 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-10 17:51:54,432 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-10 17:51:54,435 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-10 17:51:54,437 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-10 17:51:54,439 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-10 17:51:54,440 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-10 17:51:54,441 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-10 17:51:54,443 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-10 17:51:54,445 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-10 17:51:54,446 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-10 17:51:54,448 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-10 17:51:55,238 [INFO] Modelo Gemini 'gemini-2.0-flash-exp' inicializado com sucesso.
2025-04-10 17:53:29,161 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-10 17:53:29,163 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-10 17:53:29,165 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-10 17:53:29,166 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 17:53:29,168 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-10 17:53:29,169 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 17:53:29,171 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 17:53:29,173 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 17:53:29,174 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 17:53:29,177 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 17:53:29,179 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-10 17:53:29,183 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-10 17:53:29,187 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-10 17:53:29,189 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-10 17:53:29,190 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 17:53:29,194 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-10 17:53:29,196 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-10 17:53:29,199 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 17:53:29,201 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 17:53:29,202 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 17:53:29,204 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 17:53:29,205 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 17:53:29,207 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 17:53:29,210 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-10 17:53:29,212 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-10 17:53:29,215 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-10 17:53:29,217 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-10 17:53:29,218 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-10 17:53:29,220 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-10 17:53:29,222 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-10 17:53:29,224 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 17:53:29,226 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-10 17:53:29,228 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-10 17:53:29,231 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-10 17:53:29,233 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-10 17:53:29,234 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-10 17:53:29,236 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-10 17:53:29,237 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-10 17:53:29,239 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-10 17:53:29,241 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-10 17:53:29,243 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-10 17:53:29,245 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-10 17:53:29,248 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-10 17:53:29,249 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-10 17:53:29,251 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-10 17:53:29,252 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-10 17:53:29,254 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-10 17:53:29,255 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-10 17:53:29,257 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-10 17:53:29,260 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-10 17:53:29,261 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-10 17:53:29,263 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-10 17:53:29,265 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-10 17:53:29,267 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-10 17:53:29,388 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-10 17:53:29,389 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-10 17:53:29,391 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-10 17:53:29,393 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 17:53:29,396 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-10 17:53:29,399 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 17:53:29,402 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 17:53:29,404 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 17:53:29,405 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 17:53:29,407 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 17:53:29,409 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-10 17:53:29,412 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-10 17:53:29,415 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-10 17:53:29,417 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-10 17:53:29,419 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 17:53:29,420 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-10 17:53:29,422 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-10 17:53:29,424 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 17:53:29,427 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 17:53:29,429 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 17:53:29,432 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 17:53:29,434 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 17:53:29,436 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 17:53:29,439 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-10 17:53:29,444 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-10 17:53:29,448 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-10 17:53:29,451 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-10 17:53:29,453 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-10 17:53:29,455 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-10 17:53:29,457 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-10 17:53:29,460 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 17:53:29,462 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-10 17:53:29,466 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-10 17:53:29,468 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-10 17:53:29,469 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-10 17:53:29,472 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-10 17:53:29,474 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-10 17:53:29,476 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-10 17:53:29,478 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-10 17:53:29,480 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-10 17:53:29,482 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-10 17:53:29,484 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-10 17:53:29,485 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-10 17:53:29,486 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-10 17:53:29,488 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-10 17:53:29,490 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-10 17:53:29,492 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-10 17:53:29,494 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-10 17:53:29,497 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-10 17:53:29,498 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-10 17:53:29,500 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-10 17:53:29,502 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-10 17:53:29,503 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-10 17:53:29,505 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-10 17:53:29,511 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente inteligente utilizado em um chat com IA, que recebe códigos de uma aplicação completa de chat, como você funciona e tudo que 
é programado para fazer no seu contexto.

Seu objetivo é:
- Analisar todo o contexto e aprender com ele.
- Analisar a Pergunta do Usuário.
- Identificar se possui relação com o contexto.
- Se houver relação com o código do contexto responder a pergunta sobre o código sempre seguindo boas práticas:
    - Analisar o código-fonte contido no projeto enviado
    - Identificar melhorias em legibilidade, performance e segurança
    - Sugerir práticas recomendadas de design e arquitetura
    - Indicar partes com dívidas técnicas ou complexidade desnecessária
    - Enviar código modularizado em pequenos arquivos separados por pastas.

Informações adicionais:
- Estrutura do projeto: segue o padrão de pastas iniciando por src/


## Se houver imagem no contexto, identifique se a pergunta é sobre o código do contexto, sobre a imagem ou sobre os dois para elaborar melhorer respostas.
## Se o usuário fizer uma pergunta que não tem relação com imagem ou contexto, use a sua base de conhecimento fora do contexto para responder de forma inteligente.
** Responda em Português-Brasileiro a não ser que o usuário solicite outra língua.**

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diretório atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Definição dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Criação dos diretórios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configuração do layout do Streamlit
st.set_page_config(
    page_title="Análise de Ações",
    layout="wide",
    page_icon="📈"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscilações

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Período', 'Oscilação'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("📊 Dashboard de Análise de Ações")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma ação", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"📌 Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("🔍 Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descrição', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa ação.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("💰 Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("📉 Oscilações")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("📈 Tabela Geral de Ações")
        st.dataframe(self.acoes)

# Execução
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para análise fundamentalista de ações da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from config.common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"Não foram encontrados arquivos .txt no diretório {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conteúdo do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conteúdo HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conteúdo HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se está correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # Configuração de vídeos
    video_config = VideoConfig(root_path=root_path)
    video_processor = VideoToAudioConverter(video_config=video_config)
    video_processor.process_videos()
    
    # # Configuração de áudios
    audio_config = AudioConfig(root_path=root_path)
    audio_processor = AudioToConverter(audio_config=audio_config)
    audio_processor.process_audio_files()
    
    # # Processamento de transcrições e envio de embeddings
    # embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    # embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diretório
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diretório.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do último arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conteúdo do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimensões do embedding:", embedding.shape)

# Caminho do diretório de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o último embedding
load_and_print_embedding(embedding_directory)


# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_whisper.py

from transformers import pipeline
from common_paths import CommonPaths

commmom_paths = CommonPaths()

# Função para ler a transcrição de um arquivo
def read_transcription(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()
    return lines

# Função para converter a pontuação em estrelas
def score_to_stars(score):
    if score >= 0.75:
        return 5
    elif score >= 0.55:
        return 4
    elif score >= 0.35:
        return 3
    elif score >= 0.15:
        return 2
    else:
        return 1

# Inicializar o pipeline de análise de sentimento
sentiment_pipeline = pipeline("sentiment-analysis", model="neuralmind/bert-base-portuguese-cased")

# Caminho para o arquivo de transcrição
file_path = commmom_paths.TRANSCRIPTION_OUTPUT_PATH / 'feedback.txt'

# Ler a transcrição
lines = read_transcription(file_path)

# Analisar sentimento para cada feedback
for line in lines:
    sentiment = sentiment_pipeline(line.strip())
    score = sentiment[0]['score']
    stars = score_to_stars(score)
    
    # Ajuste manual para feedbacks com aspectos positivos e negativos
    if any(word in line.lower() for word in ["mas", "porém", "no entanto"]):
        stars = max(1, stars - 1)  # Reduz uma estrela se houver contradição
    
    # Ajuste adicional baseado em palavras-chave
    positive_keywords = ["excelente", "ótimo", "perfeito", "rápido", "eficiente", "satisfeito", "clara", "eficaz", "bom trabalho"]
    negative_keywords = ["ruim", "péssimo", "horrível", "poderia", "não resolveu", "insatisfatória", "confusa", "inadequado", "lento", "não houve seguimento", "não atendeu", "não funcionou", "demorado"]
    
    positive_count = sum(word in line.lower() for word in positive_keywords)
    negative_count = sum(word in line.lower() for word in negative_keywords)
    
    # Ajuste de estrelas baseado na contagem de palavras-chave
    if positive_count > negative_count:
        stars = min(5, stars + 1)
    elif negative_count > positive_count:
        stars = max(1, stars - 1)
    
    print(f"Texto: {line.strip()}")
    print(f"Sentimento: {stars} estrelas")
    print(f"Pontuação: {score:.2f}")
    print('---')


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configuração de transcrições e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diretório onde está procurando as transcrições
        print(f"Diretório de entrada das transcrições: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcrição no diretório de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcrição encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo não encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcrição do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} está vazio.")
                    return

            # Gerar o embedding da transcrição
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcrição e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de áudio
    with sr.Microphone() as source:
        print("Diga algo em português...")

        while True:
            try:
                # Escuta o áudio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {texto_portugues}")

                # Traduz o texto para o inglês
                traducao = translator.translate(texto_portugues)
                print(f"Tradução para o inglês: {traducao}")

            except sr.UnknownValueError:
                print("Não foi possível entender o áudio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usuário")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configurações de áudio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de áudio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em português...")

try:
    audio_buffer = []

    while True:
        # Lê dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o áudio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o áudio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradução
            print(f"Tradução para o inglês: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usuário")

    # Fecha o stream de áudio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O vídeo {video_path} não contém áudio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de vídeo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de vídeo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de vídeo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Conversão de vídeo para áudio concluída!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configurações da API
handler = genai('gemini-1.5-flash')

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Função para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conteúdo com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares únicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcrição obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' não é uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuniões: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de reuniões: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|tópicos da|pontos (relevantes|principais)|análise)\b.*\b(reunião|última (reunião|conversa|sessão))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como análise de reunião.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padrão.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que são muito similares ao comando atual para evitar redundância.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padrão: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # Métodos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de análise de reunião.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conteúdo usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inválida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar variáveis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conteúdo com base no prompt, contexto e reunião"""
        try:
            # Supondo que a API espera um dicionário com os parâmetros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisição para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar variáveis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Criação da conclusão do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que você acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["faça um resumo da última reunião.", "tópicos da última reunião", "resuma a última reunião", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Você é responsável por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - Não acrescentar título nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informações acima e forneça uma solução detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Você é responsável analisar com detalhes a reunião de [str_meeting], e fornecer uma longa estória sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precisão da resposta.
            - Não acrescente título nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e faça uma pesquisa online para uma resposta mais precisa das questões de [str_text]
        - Não acrescente título nas respostas.
        
        ------
        [context]: Regras Básicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("Áudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum áudio detectado.")
                return None
            except sr.UnknownValueError:
                print("Não foi possível entender o áudio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configurações da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicionário
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta não está no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API: {e}")
        return None

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: Observe esse código e identifique pontos que posso melhorar .
2025-04-10 17:53:53,696 [INFO] Resposta da IA (texto): Com certeza! Analisei o código e encontrei algumas áreas com oportunidades de melhoria em termos de legibilidade, organização e otimização. Abaixo, apresento as sugestões e um exemplo de como você pode refatorar seu código:

**1. Modularização e Organização:**

*   **Problema:** O arquivo `chat_streamlit.py` está muito grande e contém várias responsabilidades (gerenciamento da interface Streamlit, interação com o Gemini, manipulação de imagens, etc.).
*   **Solução:** Divida o código em módulos menores, cada um com uma responsabilidade bem definida. Isso facilita a manutenção, o teste e a reutilização do código.

**2. Gerenciamento de Estado:**

*   **Problema:** O uso excessivo de `st.session_state` pode tornar o código difícil de entender e depurar.
*   **Solução:** Agrupe as variáveis de estado relacionadas em classes ou dicionários e use funções para atualizar o estado de forma consistente.

**3. Tratamento de Erros:**

*   **Problema:** Os blocos `try...except` estão muito genéricos, o que dificulta a identificação e o tratamento de erros específicos.
*   **Solução:** Use exceções mais específicas e forneça mensagens de erro claras e informativas.

**4. Performance:**

*   **Problema:** A leitura de todos os arquivos Python no início do script pode ser lenta, especialmente em projetos grandes.
*   **Solução:** Use um cache para armazenar o conteúdo dos arquivos Python e atualize o cache apenas quando os arquivos forem modificados.

**5. Segurança:**

*   **Problema:** A chave da API do Gemini está armazenada no arquivo `.env`. Embora isso seja melhor do que incluir a chave diretamente no código, ainda não é ideal.
*   **Solução:** Use um sistema de gerenciamento de segredos mais robusto para armazenar e acessar a chave da API.

**6. Rate Limiting:**

*   **Problema:** A lógica de rate limiting está implementada diretamente no `chat_streamlit.py`.
*   **Solução:** Mova a lógica de rate limiting para uma classe separada e use-a em todos os lugares onde você interage com o Gemini.

**Exemplo de Refatoração:**

Aqui está um exemplo de como você pode refatorar o código `chat_streamlit.py` para melhorar a modularização e a organização:

**`src/chat_app/components/sidebar.py`**

```python
# src/chat_app/components/sidebar.py
import streamlit as st

def sidebar():
    with st.sidebar:
        st.title("Chat IA Inteligente")

        # Seção de geração de imagem
        st.markdown("### Gerar Imagem")
        image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
        if st.button("Gerar Imagem"):   
            if image_prompt:
                generated_image = generate_image(image_prompt)

                if generated_image:
                    st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                    st.session_state.generated_image = None #Limpa para não exibir em cima

                    st.rerun()
            else:
                st.warning("Por favor, digite um prompt para gerar a imagem.")

        # Seção de imagens (sempre visível)
        st.markdown("### Adicionar Imagem (Opcional)")
        st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

        # Layout em duas colunas para os botões de imagem
        col1, col2 = st.columns(2)

        with col1:
            # Botão para verificar a área de transferência
            if st.button("📋 Colar", use_container_width=True):
                if on_paste_click():
                    st.success("Imagem colada!")
                    st.rerun()
                else:
                    st.warning("Nada encontrado.")

        with col2:
            # Botão para limpar a imagem atual (se houver)
            if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
                if st.button("🗑️ Limpar", use_container_width=True):
                    clear_all_images()
                    st.rerun()
            else:
                # Placeholder para manter o layout alinhado
                st.write("")

        # Uploader de imagem com chave dinâmica
        uploaded_file = st.file_uploader(
            "📷 Ou faça upload de imagem",
            type=["png", "jpg", "jpeg"],
            label_visibility="visible",
            key=st.session_state.file_uploader_key
        )

        # Atualiza o estado da imagem quando um arquivo é carregado
        if uploaded_file:
            st.session_state.uploaded_image = uploaded_file
            on_file_upload()
            st.success("Imagem carregada!")

        # Exibe a imagem selecionada na barra lateral
        if st.session_state.clipboard_image_preview:
            st.image(st.session_state.clipboard_image_preview, use_container_width=True)
            st.caption("Imagem da área de transferência")
        elif st.session_state.uploaded_image:
            st.image(st.session_state.uploaded_image, use_container_width=True)
            st.caption("Imagem carregada")

        st.markdown("---")

        # Botão para limpar o histórico de conversa
        if st.button("🧹 Limpar conversa", use_container_width=True):
            st.session_state.messages = []
            st.rerun()

        st.caption("Desenvolvido com Streamlit e Gemini AI")
```

**`src/chat_app/components/chat_history.py`**

```python
# src/chat_app/components/chat_history.py
import streamlit as st

def chat_history():
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            # Se houver imagem, exiba-a (se armazenada)
            if message.get("image"):
                st.image(message["image"], use_container_width=True)
            # Exibe o conteúdo da mensagem (texto)
            st.markdown(message["content"])
```

**`src/chat_app/utils/image_utils.py`**

```python
# src/chat_app/utils/image_utils.py
from PIL import Image
import io
from datetime import datetime
import os
from config.config import Config

def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

def save_image(image_data):
    os.makedirs(Config.ASSETS_DIR, exist_ok=True)
    img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
    img_path = os.path.join(Config.ASSETS_DIR, img_name)
    with open(img_path, "wb") as f:
        f.write(image_data.getbuffer())
    with Image.open(img_path) as img:
        img_display = img.copy()
    return img_path, img_display
```

**`src/chat_app/app.py`**

```python
# src/chat_app/app.py
import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python
from chat_app.components.sidebar import sidebar
from chat_app.components.chat_history import chat_history
from chat_app.utils.image_utils import check_clipboard, save_image

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        img_path, img_display = save_image(image_data)

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
sidebar()

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
chat_history()

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)
```

**Próximos Passos:**

1.  **Implemente as Refatorações:** Comece movendo o código para os arquivos sugeridos.
2.  **Teste:** Após cada refatoração, execute testes para garantir que nada foi quebrado.
3.  **Continue Refatorando:** Continue movendo o código e adaptando as sugestões aqui mencionadas.

Ao seguir essas sugestões, você tornará seu código mais limpo, organizado e fácil de manter.
2025-04-10 17:53:53,823 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-10 17:53:53,824 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-10 17:53:53,826 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-10 17:53:53,828 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 17:53:53,830 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-10 17:53:53,832 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 17:53:53,834 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 17:53:53,836 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 17:53:53,837 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 17:53:53,839 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 17:53:53,840 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-10 17:53:53,841 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-10 17:53:53,842 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-10 17:53:53,844 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-10 17:53:53,846 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 17:53:53,848 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-10 17:53:53,851 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-10 17:53:53,852 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 17:53:53,854 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 17:53:53,855 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 17:53:53,857 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 17:53:53,858 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 17:53:53,860 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 17:53:53,862 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-10 17:53:53,864 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-10 17:53:53,867 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-10 17:53:53,868 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-10 17:53:53,870 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-10 17:53:53,871 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-10 17:53:53,873 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-10 17:53:53,874 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 17:53:53,876 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-10 17:53:53,877 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-10 17:53:53,880 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-10 17:53:53,882 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-10 17:53:53,884 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-10 17:53:53,885 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-10 17:53:53,886 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-10 17:53:53,888 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-10 17:53:53,890 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-10 17:53:53,891 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-10 17:53:53,893 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-10 17:53:53,894 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-10 17:53:53,897 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-10 17:53:53,899 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-10 17:53:53,900 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-10 17:53:53,902 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-10 17:53:53,903 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-10 17:53:53,904 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-10 17:53:53,906 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-10 17:53:53,907 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-10 17:53:53,908 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-10 17:53:53,910 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-10 17:53:53,912 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-10 17:59:51,184 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-10 17:59:51,186 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-10 17:59:51,187 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-10 17:59:51,188 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-10 17:59:51,189 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-10 17:59:51,190 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-10 17:59:51,192 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-10 17:59:51,193 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-10 17:59:51,195 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-10 17:59:51,196 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-10 17:59:51,198 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-10 17:59:51,201 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-10 17:59:51,202 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-10 17:59:51,930 [INFO] Modelo Gemini 'gemini-2.0-flash-exp' inicializado com sucesso.
2025-04-10 18:00:51,391 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-10 18:00:51,392 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-10 18:00:51,393 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-10 18:00:51,394 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-10 18:00:51,396 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-10 18:00:51,397 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-10 18:00:51,399 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-10 18:00:51,400 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-10 18:00:51,401 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-10 18:00:51,403 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-10 18:00:51,405 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-10 18:00:51,408 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-10 18:00:51,409 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-10 18:00:51,410 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-10 18:00:51,412 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-10 18:00:51,414 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-10 18:00:51,415 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-10 18:00:51,416 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-10 18:00:51,418 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-10 18:00:51,419 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-10 18:00:51,421 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-10 18:00:51,423 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 18:00:51,425 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-10 18:00:51,427 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-10 18:00:51,428 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-10 18:00:51,430 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-10 18:00:51,431 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-10 18:00:51,433 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-10 18:00:51,434 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-10 18:00:51,435 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-10 18:00:51,437 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-10 18:00:51,439 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-10 18:00:51,441 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-10 18:00:51,443 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-10 18:00:51,444 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-10 18:00:51,445 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-10 18:00:51,447 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-10 18:00:51,448 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-10 18:00:51,449 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-10 18:00:51,450 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-10 18:00:51,452 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-10 18:00:51,454 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-10 18:00:51,455 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-10 18:00:51,458 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-10 18:00:51,459 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-10 18:00:51,461 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-10 18:00:51,462 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-10 18:00:51,463 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-10 18:00:51,465 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-10 18:00:51,466 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-10 18:00:51,467 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-10 18:00:51,469 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-10 18:00:51,471 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-10 18:00:51,473 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-10 19:43:20,540 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-10 19:43:20,547 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-10 19:43:20,552 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-10 19:43:20,558 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-10 19:43:20,566 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-10 19:43:20,573 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-10 19:43:20,579 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-10 19:43:20,584 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-10 19:43:20,589 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-10 19:43:20,596 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-10 19:43:20,602 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-10 19:43:20,607 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-10 19:43:20,614 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-10 19:43:20,619 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-10 19:43:20,624 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-10 19:43:20,630 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-10 19:43:20,634 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-10 19:43:20,635 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-10 19:43:20,640 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-10 19:43:20,647 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-10 19:43:20,654 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-10 19:43:20,661 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 19:43:20,667 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-10 19:43:20,672 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-10 19:43:20,679 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-10 19:43:20,685 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-10 19:43:20,689 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-10 19:43:20,695 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-10 19:43:20,700 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-10 19:43:20,705 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-10 19:43:20,711 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-10 19:43:20,716 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-10 19:43:20,721 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-10 19:43:20,727 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-10 19:43:20,733 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-10 19:43:20,739 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-10 19:43:20,744 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-10 19:43:20,745 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-10 19:43:20,751 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-10 19:43:20,756 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-10 19:43:20,763 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-10 19:43:20,769 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-10 19:43:20,773 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-10 19:43:20,778 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-10 19:43:20,783 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-10 19:43:20,788 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-10 19:43:20,793 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-10 19:43:20,798 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-10 19:43:20,803 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-10 19:43:20,809 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-10 19:43:20,814 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-10 19:43:20,821 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-10 19:43:20,827 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-10 19:43:20,836 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-10 19:43:21,126 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-10 19:43:21,131 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-10 19:43:21,133 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-10 19:43:21,135 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-10 19:43:21,137 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-10 19:43:21,139 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-10 19:43:21,141 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-10 19:43:21,143 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-10 19:43:21,146 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-10 19:43:21,148 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-10 19:43:21,150 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-10 19:43:21,151 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-10 19:43:21,153 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-10 19:43:21,154 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-10 19:43:21,156 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-10 19:43:21,157 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-10 19:43:21,159 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-10 19:43:21,161 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-10 19:43:21,164 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-10 19:43:21,166 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-10 19:43:21,168 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-10 19:43:21,170 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 19:43:21,171 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-10 19:43:21,174 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-10 19:43:21,176 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-10 19:43:21,224 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-10 19:43:21,232 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-10 19:43:21,234 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-10 19:43:21,239 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-10 19:43:21,249 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-10 19:43:21,255 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-10 19:43:21,257 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-10 19:43:21,259 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-10 19:43:21,262 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-10 19:43:21,264 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-10 19:43:21,266 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-10 19:43:21,268 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-10 19:43:21,271 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-10 19:43:21,278 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-10 19:43:21,282 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-10 19:43:21,285 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-10 19:43:21,287 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-10 19:43:21,288 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-10 19:43:21,289 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-10 19:43:21,291 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-10 19:43:21,293 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-10 19:43:21,296 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-10 19:43:21,298 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-10 19:43:21,299 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-10 19:43:21,300 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-10 19:43:21,302 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-10 19:43:21,303 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-10 19:43:21,305 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-10 19:43:21,306 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-10 19:43:21,309 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diretório atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Definição dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Criação dos diretórios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configuração do layout do Streamlit
st.set_page_config(
    page_title="Análise de Ações",
    layout="wide",
    page_icon="📈"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscilações

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Período', 'Oscilação'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("📊 Dashboard de Análise de Ações")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma ação", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"📌 Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("🔍 Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descrição', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa ação.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("💰 Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("📉 Oscilações")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("📈 Tabela Geral de Ações")
        st.dataframe(self.acoes)

# Execução
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para análise fundamentalista de ações da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from config.common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"Não foram encontrados arquivos .txt no diretório {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conteúdo do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conteúdo HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conteúdo HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se está correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # Configuração de vídeos
    video_config = VideoConfig(root_path=root_path)
    video_processor = VideoToAudioConverter(video_config=video_config)
    video_processor.process_videos()
    
    # # Configuração de áudios
    audio_config = AudioConfig(root_path=root_path)
    audio_processor = AudioToConverter(audio_config=audio_config)
    audio_processor.process_audio_files()
    
    # # Processamento de transcrições e envio de embeddings
    # embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    # embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diretório
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diretório.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do último arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conteúdo do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimensões do embedding:", embedding.shape)

# Caminho do diretório de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o último embedding
load_and_print_embedding(embedding_directory)


# test_whisper.py

from transformers import pipeline
from common_paths import CommonPaths

commmom_paths = CommonPaths()

# Função para ler a transcrição de um arquivo
def read_transcription(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()
    return lines

# Função para converter a pontuação em estrelas
def score_to_stars(score):
    if score >= 0.75:
        return 5
    elif score >= 0.55:
        return 4
    elif score >= 0.35:
        return 3
    elif score >= 0.15:
        return 2
    else:
        return 1

# Inicializar o pipeline de análise de sentimento
sentiment_pipeline = pipeline("sentiment-analysis", model="neuralmind/bert-base-portuguese-cased")

# Caminho para o arquivo de transcrição
file_path = commmom_paths.TRANSCRIPTION_OUTPUT_PATH / 'feedback.txt'

# Ler a transcrição
lines = read_transcription(file_path)

# Analisar sentimento para cada feedback
for line in lines:
    sentiment = sentiment_pipeline(line.strip())
    score = sentiment[0]['score']
    stars = score_to_stars(score)
    
    # Ajuste manual para feedbacks com aspectos positivos e negativos
    if any(word in line.lower() for word in ["mas", "porém", "no entanto"]):
        stars = max(1, stars - 1)  # Reduz uma estrela se houver contradição
    
    # Ajuste adicional baseado em palavras-chave
    positive_keywords = ["excelente", "ótimo", "perfeito", "rápido", "eficiente", "satisfeito", "clara", "eficaz", "bom trabalho"]
    negative_keywords = ["ruim", "péssimo", "horrível", "poderia", "não resolveu", "insatisfatória", "confusa", "inadequado", "lento", "não houve seguimento", "não atendeu", "não funcionou", "demorado"]
    
    positive_count = sum(word in line.lower() for word in positive_keywords)
    negative_count = sum(word in line.lower() for word in negative_keywords)
    
    # Ajuste de estrelas baseado na contagem de palavras-chave
    if positive_count > negative_count:
        stars = min(5, stars + 1)
    elif negative_count > positive_count:
        stars = max(1, stars - 1)
    
    print(f"Texto: {line.strip()}")
    print(f"Sentimento: {stars} estrelas")
    print(f"Pontuação: {score:.2f}")
    print('---')


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configuração de transcrições e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diretório onde está procurando as transcrições
        print(f"Diretório de entrada das transcrições: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcrição no diretório de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcrição encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo não encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcrição do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} está vazio.")
                    return

            # Gerar o embedding da transcrição
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcrição e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de áudio
    with sr.Microphone() as source:
        print("Diga algo em português...")

        while True:
            try:
                # Escuta o áudio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {texto_portugues}")

                # Traduz o texto para o inglês
                traducao = translator.translate(texto_portugues)
                print(f"Tradução para o inglês: {traducao}")

            except sr.UnknownValueError:
                print("Não foi possível entender o áudio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usuário")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configurações de áudio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de áudio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em português...")

try:
    audio_buffer = []

    while True:
        # Lê dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o áudio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o áudio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradução
            print(f"Tradução para o inglês: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usuário")

    # Fecha o stream de áudio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O vídeo {video_path} não contém áudio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de vídeo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de vídeo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de vídeo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Conversão de vídeo para áudio concluída!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configurações da API
handler = genai('gemini-1.5-flash')

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Função para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conteúdo com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares únicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcrição obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' não é uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuniões: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de reuniões: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|tópicos da|pontos (relevantes|principais)|análise)\b.*\b(reunião|última (reunião|conversa|sessão))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como análise de reunião.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padrão.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que são muito similares ao comando atual para evitar redundância.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padrão: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # Métodos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de análise de reunião.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conteúdo usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inválida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar variáveis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conteúdo com base no prompt, contexto e reunião"""
        try:
            # Supondo que a API espera um dicionário com os parâmetros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisição para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar variáveis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Criação da conclusão do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que você acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["faça um resumo da última reunião.", "tópicos da última reunião", "resuma a última reunião", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Você é responsável por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - Não acrescentar título nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informações acima e forneça uma solução detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Você é responsável analisar com detalhes a reunião de [str_meeting], e fornecer uma longa estória sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precisão da resposta.
            - Não acrescente título nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e faça uma pesquisa online para uma resposta mais precisa das questões de [str_text]
        - Não acrescente título nas respostas.
        
        ------
        [context]: Regras Básicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("Áudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum áudio detectado.")
                return None
            except sr.UnknownValueError:
                print("Não foi possível entender o áudio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configurações da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicionário
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta não está no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API: {e}")
        return None

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: Como posso integrar todas as aplicações para funcionar em uma única MAIN.py ?

Streamlit. Chat de voz. converter vídeo para audio e audio para texto e depois salvar no banco de dados.
2025-04-10 19:43:31,253 [INFO] Resposta da IA (texto): Para integrar todas as aplicações em um único `main.py`, você precisa modularizar seu código existente e usar uma estrutura central que coordene a execução das diferentes partes (Streamlit chat de voz, conversão de vídeo para áudio, áudio para texto e salvamento no banco de dados).

Aqui está um esboço de como você pode abordar isso, com ênfase na organização e coordenação das tarefas:

```python
# main.py

import streamlit as st
import os
from pathlib import Path

# Importar as funções e classes de cada módulo
from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from voice_assistent.class_voice_assistent.main import MainApp
from voice_assistent.class_voice_assistent.gpt_communication.gemini_gpt import GenerativeModelHandler
from transcriptions.transcriptions_config import TranscriptionConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from config.config import Config
from core.rate_limiter import RateLimiter

# 1. Configuração Inicial
PROJECT_ROOT = Path(__file__).resolve().parent
root_path = str(PROJECT_ROOT)
print(f"Root path: {root_path}")

Config.ensure_directories()

# 2. Funções de Processamento
def process_videos():
    video_config = VideoConfig(root_path=root_path)
    video_processor = VideoToAudioConverter(video_config=video_config)
    video_processor.process_videos()

def process_audio():
    audio_config = AudioConfig(root_path=root_path)
    audio_processor = AudioToConverter(audio_config=audio_config)
    audio_processor.process_audio_files()

def process_transcriptions_and_embeddings(api_url):
    transcription_config = TranscriptionConfig(root_path=root_path)
    embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

# 3. Interface Streamlit
def main():
    st.title("Aplicações Integradas")

    # 3.1. Sidebar com opções
    app_mode = st.sidebar.selectbox(
        "Escolha a aplicação:",
        ["Chat de Voz", "Converter Vídeo para Áudio", "Converter Áudio para Texto", "Enviar Embeddings para o Banco de Dados"]
    )

    # 3.2. Lógica para cada aplicação
    if app_mode == "Chat de Voz":
        st.header("Chat de Voz")
        model = GenerativeModelHandler('gemini-1.5-flash')
        app = MainApp(model)
        app.run()

    elif app_mode == "Converter Vídeo para Áudio":
        st.header("Converter Vídeo para Áudio")
        if st.button("Converter Vídeos"):
            process_videos()
            st.success("Vídeos convertidos para áudio com sucesso!")

    elif app_mode == "Converter Áudio para Texto":
        st.header("Converter Áudio para Texto")
        if st.button("Converter Áudios"):
            process_audio()
            st.success("Áudios convertidos para texto com sucesso!")

    elif app_mode == "Enviar Embeddings para o Banco de Dados":
        st.header("Enviar Embeddings para o Banco de Dados")
        api_url = st.text_input("URL da API para enviar as transcrições:", "http://localhost:8081/api/meetings/transcriptions")

        if st.button("Enviar Embeddings"):
            process_transcriptions_and_embeddings(api_url)
            st.success("Embeddings enviados para o banco de dados com sucesso!")

# 4. Execução Principal
if __name__ == "__main__":
    main()
```

### Estrutura do Projeto

-   **`main.py`**: Este é o ponto de entrada. Ele usa Streamlit para criar uma interface que permite selecionar qual parte da aplicação executar.

-   **`video_to_audio/`**, **`audio_to_text/`**, **`text_to_embedding/`**, **`voice_assistent/`**: Esses diretórios contêm o código existente de cada parte da sua aplicação.

### Passos para Implementar

1.  **Modularização:**

    *   Certifique-se de que cada parte da sua aplicação (conversão de vídeo, conversão de áudio, chat de voz) esteja bem encapsulada em funções ou classes reutilizáveis.
    *   Mova essas funções e classes para os respectivos módulos (arquivos `.py` dentro de seus diretórios).

2.  **Integração no `main.py`:**

    *   Importe as funções e classes dos módulos.
    *   Use Streamlit para criar uma interface que permita ao usuário escolher qual função executar.
    *   Implemente botões ou seletores para acionar as diferentes partes da aplicação.

3.  **Gerenciamento de Dependências:**

    *   Certifique-se de que todas as dependências necessárias para cada parte da aplicação estejam instaladas.  Considere usar um arquivo `requirements.txt` para listar todas as dependências.

4.  **Testes:**

    *   Teste cada parte da aplicação individualmente para garantir que funcionem corretamente.
    *   Teste a integração geral para garantir que as diferentes partes possam ser executadas em sequência ou individualmente, conforme necessário.

### Pontos Chave

*   **Modularização:** Dividir o código em módulos facilita a manutenção e reutilização.
*   **Interface Streamlit:** Streamlit permite criar uma interface web simples para interagir com sua aplicação.
*   **Gerenciamento de Estado:** Use `st.session_state` para manter o estado entre as execuções da aplicação Streamlit (por exemplo, se um vídeo foi convertido ou não).
*   **Tratamento de Erros:** Implemente tratamento de erros para lidar com falhas em qualquer parte da aplicação e fornecer feedback útil ao usuário.
*   **Configuração:** Utilize arquivos de configuração (como o seu `config.py`) para armazenar caminhos de arquivos, URLs de API e outras configurações que podem precisar ser ajustadas.

Seguindo esses passos, você deve conseguir integrar todas as suas aplicações em um único script `main.py` com uma interface Streamlit para facilitar o uso e a demonstração.

2025-04-10 19:43:31,356 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-10 19:43:31,358 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-10 19:43:31,359 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-10 19:43:31,362 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-10 19:43:31,364 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-10 19:43:31,365 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-10 19:43:31,367 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-10 19:43:31,369 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-10 19:43:31,372 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-10 19:43:31,374 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-10 19:43:31,376 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-10 19:43:31,379 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-10 19:43:31,382 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-10 19:43:31,384 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-10 19:43:31,386 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-10 19:43:31,387 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-10 19:43:31,389 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-10 19:43:31,390 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-10 19:43:31,392 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-10 19:43:31,393 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-10 19:43:31,395 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-10 19:43:31,397 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 19:43:31,400 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-10 19:43:31,401 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-10 19:43:31,403 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-10 19:43:31,405 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-10 19:43:31,406 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-10 19:43:31,408 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-10 19:43:31,409 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-10 19:43:31,412 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-10 19:43:31,414 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-10 19:43:31,416 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-10 19:43:31,418 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-10 19:43:31,419 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-10 19:43:31,421 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-10 19:43:31,422 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-10 19:43:31,424 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-10 19:43:31,425 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-10 19:43:31,426 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-10 19:43:31,428 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-10 19:43:31,430 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-10 19:43:31,433 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-10 19:43:31,434 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-10 19:43:31,436 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-10 19:43:31,437 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-10 19:43:31,439 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-10 19:43:31,440 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-10 19:43:31,441 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-10 19:43:31,443 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-10 19:43:31,445 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-10 19:43:31,447 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-10 19:43:31,451 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-10 19:43:31,452 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-10 19:43:31,454 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-10 19:46:09,713 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-10 19:46:09,715 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-10 19:46:09,716 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-10 19:46:09,718 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-10 19:46:09,720 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-10 19:46:09,722 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-10 19:46:09,724 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-10 19:46:09,726 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-10 19:46:09,728 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-10 19:46:09,730 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-10 19:46:09,732 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-10 19:46:09,733 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-10 19:46:09,735 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-10 19:46:09,737 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-10 19:46:09,739 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-10 19:46:09,740 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-10 19:46:09,742 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-10 19:46:09,744 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-10 19:46:09,747 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-10 19:46:09,749 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-10 19:46:09,751 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-10 19:46:09,753 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 19:46:09,755 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-10 19:46:09,757 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-10 19:46:09,758 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-10 19:46:09,761 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-10 19:46:09,764 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-10 19:46:09,766 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-10 19:46:09,768 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-10 19:46:09,771 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-10 19:46:09,772 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-10 19:46:09,774 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-10 19:46:09,775 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-10 19:46:09,778 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-10 19:46:09,780 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-10 19:46:09,782 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-10 19:46:09,784 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-10 19:46:09,785 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-10 19:46:09,786 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-10 19:46:09,788 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-10 19:46:09,790 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-10 19:46:09,792 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-10 19:46:09,794 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-10 19:46:09,797 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-10 19:46:09,799 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-10 19:46:09,801 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-10 19:46:09,803 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-10 19:46:09,804 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-10 19:46:09,806 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-10 19:46:09,808 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-10 19:46:09,810 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-10 19:46:09,813 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-10 19:46:09,815 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-10 19:46:09,817 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-10 19:46:09,946 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-10 19:46:09,948 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-10 19:46:09,949 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-10 19:46:09,952 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-10 19:46:09,954 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-10 19:46:09,956 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-10 19:46:09,957 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-10 19:46:09,959 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-10 19:46:09,962 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-10 19:46:09,964 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-10 19:46:09,967 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-10 19:46:09,969 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-10 19:46:09,971 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-10 19:46:09,972 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-10 19:46:09,974 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-10 19:46:09,976 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-10 19:46:09,978 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-10 19:46:09,980 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-10 19:46:09,983 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-10 19:46:09,986 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-10 19:46:09,988 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-10 19:46:09,990 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 19:46:09,991 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-10 19:46:09,993 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-10 19:46:09,995 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-10 19:46:09,996 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-10 19:46:09,999 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-10 19:46:10,000 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-10 19:46:10,002 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-10 19:46:10,003 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-10 19:46:10,005 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-10 19:46:10,010 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-10 19:46:10,019 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-10 19:46:10,021 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-10 19:46:10,024 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-10 19:46:10,025 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-10 19:46:10,027 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-10 19:46:10,028 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-10 19:46:10,029 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-10 19:46:10,032 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-10 19:46:10,033 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-10 19:46:10,035 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-10 19:46:10,036 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-10 19:46:10,038 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-10 19:46:10,039 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-10 19:46:10,041 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-10 19:46:10,042 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-10 19:46:10,044 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-10 19:46:10,047 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-10 19:46:10,049 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-10 19:46:10,051 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-10 19:46:10,053 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-10 19:46:10,054 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-10 19:46:10,056 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-10 19:46:10,061 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diretório atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Definição dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Criação dos diretórios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configuração do layout do Streamlit
st.set_page_config(
    page_title="Análise de Ações",
    layout="wide",
    page_icon="📈"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscilações

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Período', 'Oscilação'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("📊 Dashboard de Análise de Ações")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma ação", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"📌 Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("🔍 Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descrição', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa ação.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("💰 Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("📉 Oscilações")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("📈 Tabela Geral de Ações")
        st.dataframe(self.acoes)

# Execução
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para análise fundamentalista de ações da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from config.common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"Não foram encontrados arquivos .txt no diretório {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conteúdo do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conteúdo HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conteúdo HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se está correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # Configuração de vídeos
    video_config = VideoConfig(root_path=root_path)
    video_processor = VideoToAudioConverter(video_config=video_config)
    video_processor.process_videos()
    
    # # Configuração de áudios
    audio_config = AudioConfig(root_path=root_path)
    audio_processor = AudioToConverter(audio_config=audio_config)
    audio_processor.process_audio_files()
    
    # # Processamento de transcrições e envio de embeddings
    # embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    # embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diretório
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diretório.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do último arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conteúdo do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimensões do embedding:", embedding.shape)

# Caminho do diretório de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o último embedding
load_and_print_embedding(embedding_directory)


# test_whisper.py

from transformers import pipeline
from common_paths import CommonPaths

commmom_paths = CommonPaths()

# Função para ler a transcrição de um arquivo
def read_transcription(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()
    return lines

# Função para converter a pontuação em estrelas
def score_to_stars(score):
    if score >= 0.75:
        return 5
    elif score >= 0.55:
        return 4
    elif score >= 0.35:
        return 3
    elif score >= 0.15:
        return 2
    else:
        return 1

# Inicializar o pipeline de análise de sentimento
sentiment_pipeline = pipeline("sentiment-analysis", model="neuralmind/bert-base-portuguese-cased")

# Caminho para o arquivo de transcrição
file_path = commmom_paths.TRANSCRIPTION_OUTPUT_PATH / 'feedback.txt'

# Ler a transcrição
lines = read_transcription(file_path)

# Analisar sentimento para cada feedback
for line in lines:
    sentiment = sentiment_pipeline(line.strip())
    score = sentiment[0]['score']
    stars = score_to_stars(score)
    
    # Ajuste manual para feedbacks com aspectos positivos e negativos
    if any(word in line.lower() for word in ["mas", "porém", "no entanto"]):
        stars = max(1, stars - 1)  # Reduz uma estrela se houver contradição
    
    # Ajuste adicional baseado em palavras-chave
    positive_keywords = ["excelente", "ótimo", "perfeito", "rápido", "eficiente", "satisfeito", "clara", "eficaz", "bom trabalho"]
    negative_keywords = ["ruim", "péssimo", "horrível", "poderia", "não resolveu", "insatisfatória", "confusa", "inadequado", "lento", "não houve seguimento", "não atendeu", "não funcionou", "demorado"]
    
    positive_count = sum(word in line.lower() for word in positive_keywords)
    negative_count = sum(word in line.lower() for word in negative_keywords)
    
    # Ajuste de estrelas baseado na contagem de palavras-chave
    if positive_count > negative_count:
        stars = min(5, stars + 1)
    elif negative_count > positive_count:
        stars = max(1, stars - 1)
    
    print(f"Texto: {line.strip()}")
    print(f"Sentimento: {stars} estrelas")
    print(f"Pontuação: {score:.2f}")
    print('---')


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configuração de transcrições e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diretório onde está procurando as transcrições
        print(f"Diretório de entrada das transcrições: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcrição no diretório de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcrição encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo não encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcrição do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} está vazio.")
                    return

            # Gerar o embedding da transcrição
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcrição e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de áudio
    with sr.Microphone() as source:
        print("Diga algo em português...")

        while True:
            try:
                # Escuta o áudio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {texto_portugues}")

                # Traduz o texto para o inglês
                traducao = translator.translate(texto_portugues)
                print(f"Tradução para o inglês: {traducao}")

            except sr.UnknownValueError:
                print("Não foi possível entender o áudio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usuário")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configurações de áudio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de áudio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em português...")

try:
    audio_buffer = []

    while True:
        # Lê dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o áudio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o áudio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradução
            print(f"Tradução para o inglês: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usuário")

    # Fecha o stream de áudio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O vídeo {video_path} não contém áudio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de vídeo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de vídeo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de vídeo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Conversão de vídeo para áudio concluída!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configurações da API
handler = genai('gemini-1.5-flash')

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Função para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conteúdo com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares únicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcrição obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' não é uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuniões: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de reuniões: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|tópicos da|pontos (relevantes|principais)|análise)\b.*\b(reunião|última (reunião|conversa|sessão))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como análise de reunião.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padrão.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que são muito similares ao comando atual para evitar redundância.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padrão: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # Métodos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de análise de reunião.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conteúdo usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inválida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar variáveis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conteúdo com base no prompt, contexto e reunião"""
        try:
            # Supondo que a API espera um dicionário com os parâmetros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisição para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar variáveis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Criação da conclusão do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que você acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["faça um resumo da última reunião.", "tópicos da última reunião", "resuma a última reunião", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Você é responsável por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - Não acrescentar título nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informações acima e forneça uma solução detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Você é responsável analisar com detalhes a reunião de [str_meeting], e fornecer uma longa estória sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precisão da resposta.
            - Não acrescente título nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e faça uma pesquisa online para uma resposta mais precisa das questões de [str_text]
        - Não acrescente título nas respostas.
        
        ------
        [context]: Regras Básicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("Áudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum áudio detectado.")
                return None
            except sr.UnknownValueError:
                print("Não foi possível entender o áudio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configurações da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicionário
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta não está no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API: {e}")
        return None

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: Como posso integrar todas as aplicações para funcionar em uma única MAIN.py ?

Streamlit. Chat de voz. converter vídeo para audio e audio para texto e depois salvar no banco de dados.
Assistant: Para integrar todas as aplicações em um único `main.py`, você precisa modularizar seu código existente e usar uma estrutura central que coordene a execução das diferentes partes (Streamlit chat de voz, conversão de vídeo para áudio, áudio para texto e salvamento no banco de dados).

Aqui está um esboço de como você pode abordar isso, com ênfase na organização e coordenação das tarefas:

```python
# main.py

import streamlit as st
import os
from pathlib import Path

# Importar as funções e classes de cada módulo
from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from voice_assistent.class_voice_assistent.main import MainApp
from voice_assistent.class_voice_assistent.gpt_communication.gemini_gpt import GenerativeModelHandler
from transcriptions.transcriptions_config import TranscriptionConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from config.config import Config
from core.rate_limiter import RateLimiter

# 1. Configuração Inicial
PROJECT_ROOT = Path(__file__).resolve().parent
root_path = str(PROJECT_ROOT)
print(f"Root path: {root_path}")

Config.ensure_directories()

# 2. Funções de Processamento
def process_videos():
    video_config = VideoConfig(root_path=root_path)
    video_processor = VideoToAudioConverter(video_config=video_config)
    video_processor.process_videos()

def process_audio():
    audio_config = AudioConfig(root_path=root_path)
    audio_processor = AudioToConverter(audio_config=audio_config)
    audio_processor.process_audio_files()

def process_transcriptions_and_embeddings(api_url):
    transcription_config = TranscriptionConfig(root_path=root_path)
    embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

# 3. Interface Streamlit
def main():
    st.title("Aplicações Integradas")

    # 3.1. Sidebar com opções
    app_mode = st.sidebar.selectbox(
        "Escolha a aplicação:",
        ["Chat de Voz", "Converter Vídeo para Áudio", "Converter Áudio para Texto", "Enviar Embeddings para o Banco de Dados"]
    )

    # 3.2. Lógica para cada aplicação
    if app_mode == "Chat de Voz":
        st.header("Chat de Voz")
        model = GenerativeModelHandler('gemini-1.5-flash')
        app = MainApp(model)
        app.run()

    elif app_mode == "Converter Vídeo para Áudio":
        st.header("Converter Vídeo para Áudio")
        if st.button("Converter Vídeos"):
            process_videos()
            st.success("Vídeos convertidos para áudio com sucesso!")

    elif app_mode == "Converter Áudio para Texto":
        st.header("Converter Áudio para Texto")
        if st.button("Converter Áudios"):
            process_audio()
            st.success("Áudios convertidos para texto com sucesso!")

    elif app_mode == "Enviar Embeddings para o Banco de Dados":
        st.header("Enviar Embeddings para o Banco de Dados")
        api_url = st.text_input("URL da API para enviar as transcrições:", "http://localhost:8081/api/meetings/transcriptions")

        if st.button("Enviar Embeddings"):
            process_transcriptions_and_embeddings(api_url)
            st.success("Embeddings enviados para o banco de dados com sucesso!")

# 4. Execução Principal
if __name__ == "__main__":
    main()
```

### Estrutura do Projeto

-   **`main.py`**: Este é o ponto de entrada. Ele usa Streamlit para criar uma interface que permite selecionar qual parte da aplicação executar.

-   **`video_to_audio/`**, **`audio_to_text/`**, **`text_to_embedding/`**, **`voice_assistent/`**: Esses diretórios contêm o código existente de cada parte da sua aplicação.

### Passos para Implementar

1.  **Modularização:**

    *   Certifique-se de que cada parte da sua aplicação (conversão de vídeo, conversão de áudio, chat de voz) esteja bem encapsulada em funções ou classes reutilizáveis.
    *   Mova essas funções e classes para os respectivos módulos (arquivos `.py` dentro de seus diretórios).

2.  **Integração no `main.py`:**

    *   Importe as funções e classes dos módulos.
    *   Use Streamlit para criar uma interface que permita ao usuário escolher qual função executar.
    *   Implemente botões ou seletores para acionar as diferentes partes da aplicação.

3.  **Gerenciamento de Dependências:**

    *   Certifique-se de que todas as dependências necessárias para cada parte da aplicação estejam instaladas.  Considere usar um arquivo `requirements.txt` para listar todas as dependências.

4.  **Testes:**

    *   Teste cada parte da aplicação individualmente para garantir que funcionem corretamente.
    *   Teste a integração geral para garantir que as diferentes partes possam ser executadas em sequência ou individualmente, conforme necessário.

### Pontos Chave

*   **Modularização:** Dividir o código em módulos facilita a manutenção e reutilização.
*   **Interface Streamlit:** Streamlit permite criar uma interface web simples para interagir com sua aplicação.
*   **Gerenciamento de Estado:** Use `st.session_state` para manter o estado entre as execuções da aplicação Streamlit (por exemplo, se um vídeo foi convertido ou não).
*   **Tratamento de Erros:** Implemente tratamento de erros para lidar com falhas em qualquer parte da aplicação e fornecer feedback útil ao usuário.
*   **Configuração:** Utilize arquivos de configuração (como o seu `config.py`) para armazenar caminhos de arquivos, URLs de API e outras configurações que podem precisar ser ajustadas.

Seguindo esses passos, você deve conseguir integrar todas as suas aplicações em um único script `main.py` com uma interface Streamlit para facilitar o uso e a demonstração.

User: observe em src/chat_streamlit que já tenho uma aplicação que funciona como chat e consegue gerar imagem e ler imagem com OCR... atualmente só funciona por text .... quero colocar via comando de voz tbm  com a parte da aplicação que usa Speech Recoginition
2025-04-10 19:46:17,968 [INFO] Resposta da IA (texto): Ok, para integrar o comando de voz ao seu `chat_streamlit.py`, você precisará adaptar o código existente para incluir a funcionalidade de reconhecimento de voz e direcionar a entrada de voz para as mesmas funções que já processam o texto. Aqui está um guia passo a passo de como fazer isso:

**1. Importar as Bibliotecas Necessárias:**

Adicione as seguintes importações ao topo do seu arquivo `chat_streamlit.py`:

```python
import speech_recognition as sr
import threading
```

**2. Função para Capturar Comando de Voz:**

Crie uma função para capturar o comando de voz usando `speech_recognition`. Esta função será executada em uma thread separada para evitar bloquear a interface do Streamlit.

```python
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        st.write("Diga algo...")  # Mensagem na interface
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            command = recognizer.recognize_google(audio, language='pt-BR')
            st.write(f"Você disse: {command}")  # Mostra o comando reconhecido
            return command
        except sr.WaitTimeoutError:
            st.write("Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            st.write("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            st.write(f"Erro ao solicitar resultados: {e}")
            return None
```

**3. Adicionar um Botão para Ativar o Reconhecimento de Voz:**

Adicione um botão na barra lateral para iniciar a captura de voz. Esse botão iniciará uma thread que executa a função `capture_voice_command`.

```python
with st.sidebar:
    # ... (seu código existente da barra lateral)

    if st.button("🎤 Usar Comando de Voz"):
        st.session_state.voice_command = None  # Limpa o comando anterior
        st.session_state.voice_thread = threading.Thread(target=lambda: setattr(st.session_state, 'voice_command', capture_voice_command()))
        st.session_state.voice_thread.start()
```

**4. Modificar a Lógica de Processamento de Mensagens:**

Modifique a lógica de processamento de mensagens para verificar se há um comando de voz capturado. Se houver, use esse comando como entrada para a função `process_message`.

```python
# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # ... (seu código existente para determinar o placeholder)

    # Verifica se um comando de voz foi capturado
    if 'voice_command' in st.session_state and st.session_state.voice_command:
        user_input = st.session_state.voice_command
        process_message(user_input, current_image)
        st.session_state.voice_command = None  # Limpa o comando para não repetir

    else:
        user_input = st.chat_input(placeholder)

        if user_input:
            # Processa a mensagem com a imagem (se houver) ou apenas texto
            process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)
```

**Explicação das Modificações:**

*   **`capture_voice_command()`:** Esta função captura o áudio do microfone, tenta reconhecer a fala e retorna o comando de texto. Ela também exibe mensagens de status na interface do Streamlit.
*   **Botão "🎤 Usar Comando de Voz":** Quando este botão é clicado, ele cria e inicia uma nova thread que executa a função `capture_voice_command`.  O resultado do comando de voz é armazenado em `st.session_state.voice_command`.
*   **Lógica de Processamento Modificada:** A seção de entrada de texto agora verifica se `st.session_state.voice_command` contém um valor. Se sim, ele usa esse valor como a entrada do usuário e chama `process_message`. Depois, ele limpa `st.session_state.voice_command` para evitar que o comando seja processado novamente.
*   **Thread:** Usar uma thread para a captura de voz é crucial porque `recognizer.listen()` é um método bloqueante. Sem uma thread, a interface do Streamlit ficaria congelada enquanto o áudio é capturado.

**Pontos Adicionais:**

*   **Permissões do Microfone:** Certifique-se de que seu navegador tenha permissão para acessar o microfone.
*   **Thread Safety:** Tenha cuidado com a segurança de threads ao modificar `st.session_state`. Streamlit não é inerentemente thread-safe, então evite operações simultâneas que possam causar problemas. Usar `setattr` como mostrado é uma maneira segura de atualizar o estado.
*   **Feedback Visual:** Forneça feedback visual ao usuário para que saiba quando o sistema está gravando, processando ou aguardando um comando de voz.

Com estas modificações, seu `chat_streamlit.py` deve ser capaz de capturar comandos de voz e processá-los da mesma forma que os comandos de texto. Isso adicionará a funcionalidade de controle por voz que você deseja.

2025-04-10 19:46:18,074 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-10 19:46:18,075 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-10 19:46:18,077 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-10 19:46:18,079 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-10 19:46:18,081 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-10 19:46:18,083 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-10 19:46:18,085 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-10 19:46:18,087 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-10 19:46:18,089 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-10 19:46:18,091 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-10 19:46:18,093 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-10 19:46:18,095 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-10 19:46:18,098 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-10 19:46:18,100 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-10 19:46:18,102 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-10 19:46:18,104 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-10 19:46:18,106 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-10 19:46:18,107 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-10 19:46:18,109 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-10 19:46:18,111 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-10 19:46:18,114 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-10 19:46:18,117 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 19:46:18,119 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-10 19:46:18,121 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-10 19:46:18,123 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-10 19:46:18,126 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-10 19:46:18,128 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-10 19:46:18,131 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-10 19:46:18,134 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-10 19:46:18,136 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-10 19:46:18,138 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-10 19:46:18,140 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-10 19:46:18,142 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-10 19:46:18,144 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-10 19:46:18,147 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-10 19:46:18,150 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-10 19:46:18,152 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-10 19:46:18,153 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-10 19:46:18,155 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-10 19:46:18,157 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-10 19:46:18,159 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-10 19:46:18,162 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-10 19:46:18,165 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-10 19:46:18,167 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-10 19:46:18,169 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-10 19:46:18,171 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-10 19:46:18,173 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-10 19:46:18,175 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-10 19:46:18,178 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-10 19:46:18,180 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-10 19:46:18,183 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-10 19:46:18,185 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-10 19:46:18,187 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-10 19:46:18,189 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-10 19:53:12,131 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-10 19:53:12,133 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-10 19:53:12,135 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-10 19:53:12,136 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-10 19:53:12,138 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-10 19:53:12,140 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-10 19:53:12,142 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-10 19:53:12,144 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-10 19:53:12,146 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-10 19:53:12,147 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-10 19:53:12,149 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-10 19:53:12,151 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-10 19:53:12,153 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-10 19:53:12,156 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-10 19:53:12,158 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-10 19:53:12,161 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-10 19:53:12,163 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-10 19:53:12,164 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-10 19:53:12,165 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-10 19:53:12,167 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-10 19:53:12,168 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-10 19:53:12,169 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 19:53:12,171 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-10 19:53:12,176 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-10 19:53:12,178 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-10 19:53:12,179 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-10 19:53:12,180 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-10 19:53:12,182 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-10 19:53:12,183 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-10 19:53:12,184 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-10 19:53:12,186 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-10 19:53:12,188 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-10 19:53:12,190 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-10 19:53:12,192 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-10 19:53:12,193 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-10 19:53:12,194 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-10 19:53:12,196 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-10 19:53:12,197 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-10 19:53:12,198 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-10 19:53:12,200 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-10 19:53:12,202 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-10 19:53:12,204 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-10 19:53:12,207 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-10 19:53:12,210 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-10 19:53:12,211 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-10 19:53:12,213 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-10 19:53:12,216 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-10 19:53:12,217 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-10 19:53:12,219 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-10 19:53:12,222 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-10 19:53:12,225 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-10 19:53:12,226 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-10 19:53:12,228 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-10 19:53:12,230 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-10 19:53:12,353 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-10 19:53:12,355 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-10 19:53:12,356 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-10 19:53:12,358 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-10 19:53:12,360 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-10 19:53:12,362 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-10 19:53:12,364 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-10 19:53:12,365 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-10 19:53:12,367 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-10 19:53:12,369 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-10 19:53:12,371 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-10 19:53:12,373 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-10 19:53:12,375 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-10 19:53:12,377 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-10 19:53:12,379 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-10 19:53:12,381 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-10 19:53:12,382 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-10 19:53:12,383 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-10 19:53:12,384 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-10 19:53:12,386 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-10 19:53:12,388 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-10 19:53:12,391 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 19:53:12,392 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-10 19:53:12,394 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-10 19:53:12,395 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-10 19:53:12,397 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-10 19:53:12,402 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-10 19:53:12,409 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-10 19:53:12,411 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-10 19:53:12,414 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-10 19:53:12,416 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-10 19:53:12,417 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-10 19:53:12,419 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-10 19:53:12,421 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-10 19:53:12,423 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-10 19:53:12,424 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-10 19:53:12,426 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-10 19:53:12,427 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-10 19:53:12,428 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-10 19:53:12,429 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-10 19:53:12,431 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-10 19:53:12,432 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-10 19:53:12,433 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-10 19:53:12,435 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-10 19:53:12,436 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-10 19:53:12,438 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-10 19:53:12,440 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-10 19:53:12,442 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-10 19:53:12,443 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-10 19:53:12,444 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-10 19:53:12,445 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-10 19:53:12,447 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-10 19:53:12,448 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-10 19:53:12,450 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-10 19:53:12,452 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diretório atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Definição dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Criação dos diretórios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configuração do layout do Streamlit
st.set_page_config(
    page_title="Análise de Ações",
    layout="wide",
    page_icon="📈"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscilações

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Período', 'Oscilação'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("📊 Dashboard de Análise de Ações")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma ação", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"📌 Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("🔍 Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descrição', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa ação.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("💰 Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("📉 Oscilações")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("📈 Tabela Geral de Ações")
        st.dataframe(self.acoes)

# Execução
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para análise fundamentalista de ações da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from config.common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"Não foram encontrados arquivos .txt no diretório {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conteúdo do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conteúdo HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conteúdo HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se está correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # Configuração de vídeos
    video_config = VideoConfig(root_path=root_path)
    video_processor = VideoToAudioConverter(video_config=video_config)
    video_processor.process_videos()
    
    # # Configuração de áudios
    audio_config = AudioConfig(root_path=root_path)
    audio_processor = AudioToConverter(audio_config=audio_config)
    audio_processor.process_audio_files()
    
    # # Processamento de transcrições e envio de embeddings
    # embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    # embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diretório
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diretório.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do último arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conteúdo do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimensões do embedding:", embedding.shape)

# Caminho do diretório de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o último embedding
load_and_print_embedding(embedding_directory)


# test_whisper.py

from transformers import pipeline
from common_paths import CommonPaths

commmom_paths = CommonPaths()

# Função para ler a transcrição de um arquivo
def read_transcription(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()
    return lines

# Função para converter a pontuação em estrelas
def score_to_stars(score):
    if score >= 0.75:
        return 5
    elif score >= 0.55:
        return 4
    elif score >= 0.35:
        return 3
    elif score >= 0.15:
        return 2
    else:
        return 1

# Inicializar o pipeline de análise de sentimento
sentiment_pipeline = pipeline("sentiment-analysis", model="neuralmind/bert-base-portuguese-cased")

# Caminho para o arquivo de transcrição
file_path = commmom_paths.TRANSCRIPTION_OUTPUT_PATH / 'feedback.txt'

# Ler a transcrição
lines = read_transcription(file_path)

# Analisar sentimento para cada feedback
for line in lines:
    sentiment = sentiment_pipeline(line.strip())
    score = sentiment[0]['score']
    stars = score_to_stars(score)
    
    # Ajuste manual para feedbacks com aspectos positivos e negativos
    if any(word in line.lower() for word in ["mas", "porém", "no entanto"]):
        stars = max(1, stars - 1)  # Reduz uma estrela se houver contradição
    
    # Ajuste adicional baseado em palavras-chave
    positive_keywords = ["excelente", "ótimo", "perfeito", "rápido", "eficiente", "satisfeito", "clara", "eficaz", "bom trabalho"]
    negative_keywords = ["ruim", "péssimo", "horrível", "poderia", "não resolveu", "insatisfatória", "confusa", "inadequado", "lento", "não houve seguimento", "não atendeu", "não funcionou", "demorado"]
    
    positive_count = sum(word in line.lower() for word in positive_keywords)
    negative_count = sum(word in line.lower() for word in negative_keywords)
    
    # Ajuste de estrelas baseado na contagem de palavras-chave
    if positive_count > negative_count:
        stars = min(5, stars + 1)
    elif negative_count > positive_count:
        stars = max(1, stars - 1)
    
    print(f"Texto: {line.strip()}")
    print(f"Sentimento: {stars} estrelas")
    print(f"Pontuação: {score:.2f}")
    print('---')


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configuração de transcrições e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diretório onde está procurando as transcrições
        print(f"Diretório de entrada das transcrições: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcrição no diretório de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcrição encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo não encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcrição do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} está vazio.")
                    return

            # Gerar o embedding da transcrição
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcrição e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de áudio
    with sr.Microphone() as source:
        print("Diga algo em português...")

        while True:
            try:
                # Escuta o áudio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {texto_portugues}")

                # Traduz o texto para o inglês
                traducao = translator.translate(texto_portugues)
                print(f"Tradução para o inglês: {traducao}")

            except sr.UnknownValueError:
                print("Não foi possível entender o áudio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usuário")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configurações de áudio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de áudio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em português...")

try:
    audio_buffer = []

    while True:
        # Lê dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o áudio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o áudio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradução
            print(f"Tradução para o inglês: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usuário")

    # Fecha o stream de áudio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O vídeo {video_path} não contém áudio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de vídeo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de vídeo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de vídeo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Conversão de vídeo para áudio concluída!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configurações da API
handler = genai('gemini-1.5-flash')

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Função para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conteúdo com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares únicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcrição obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' não é uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuniões: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de reuniões: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|tópicos da|pontos (relevantes|principais)|análise)\b.*\b(reunião|última (reunião|conversa|sessão))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como análise de reunião.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padrão.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que são muito similares ao comando atual para evitar redundância.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padrão: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # Métodos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de análise de reunião.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conteúdo usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inválida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar variáveis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conteúdo com base no prompt, contexto e reunião"""
        try:
            # Supondo que a API espera um dicionário com os parâmetros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisição para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar variáveis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Criação da conclusão do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que você acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["faça um resumo da última reunião.", "tópicos da última reunião", "resuma a última reunião", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Você é responsável por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - Não acrescentar título nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informações acima e forneça uma solução detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Você é responsável analisar com detalhes a reunião de [str_meeting], e fornecer uma longa estória sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precisão da resposta.
            - Não acrescente título nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e faça uma pesquisa online para uma resposta mais precisa das questões de [str_text]
        - Não acrescente título nas respostas.
        
        ------
        [context]: Regras Básicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("Áudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum áudio detectado.")
                return None
            except sr.UnknownValueError:
                print("Não foi possível entender o áudio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configurações da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicionário
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta não está no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API: {e}")
        return None

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: Como posso integrar todas as aplicações para funcionar em uma única MAIN.py ?

Streamlit. Chat de voz. converter vídeo para audio e audio para texto e depois salvar no banco de dados.
Assistant: Para integrar todas as aplicações em um único `main.py`, você precisa modularizar seu código existente e usar uma estrutura central que coordene a execução das diferentes partes (Streamlit chat de voz, conversão de vídeo para áudio, áudio para texto e salvamento no banco de dados).

Aqui está um esboço de como você pode abordar isso, com ênfase na organização e coordenação das tarefas:

```python
# main.py

import streamlit as st
import os
from pathlib import Path

# Importar as funções e classes de cada módulo
from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from voice_assistent.class_voice_assistent.main import MainApp
from voice_assistent.class_voice_assistent.gpt_communication.gemini_gpt import GenerativeModelHandler
from transcriptions.transcriptions_config import TranscriptionConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from config.config import Config
from core.rate_limiter import RateLimiter

# 1. Configuração Inicial
PROJECT_ROOT = Path(__file__).resolve().parent
root_path = str(PROJECT_ROOT)
print(f"Root path: {root_path}")

Config.ensure_directories()

# 2. Funções de Processamento
def process_videos():
    video_config = VideoConfig(root_path=root_path)
    video_processor = VideoToAudioConverter(video_config=video_config)
    video_processor.process_videos()

def process_audio():
    audio_config = AudioConfig(root_path=root_path)
    audio_processor = AudioToConverter(audio_config=audio_config)
    audio_processor.process_audio_files()

def process_transcriptions_and_embeddings(api_url):
    transcription_config = TranscriptionConfig(root_path=root_path)
    embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

# 3. Interface Streamlit
def main():
    st.title("Aplicações Integradas")

    # 3.1. Sidebar com opções
    app_mode = st.sidebar.selectbox(
        "Escolha a aplicação:",
        ["Chat de Voz", "Converter Vídeo para Áudio", "Converter Áudio para Texto", "Enviar Embeddings para o Banco de Dados"]
    )

    # 3.2. Lógica para cada aplicação
    if app_mode == "Chat de Voz":
        st.header("Chat de Voz")
        model = GenerativeModelHandler('gemini-1.5-flash')
        app = MainApp(model)
        app.run()

    elif app_mode == "Converter Vídeo para Áudio":
        st.header("Converter Vídeo para Áudio")
        if st.button("Converter Vídeos"):
            process_videos()
            st.success("Vídeos convertidos para áudio com sucesso!")

    elif app_mode == "Converter Áudio para Texto":
        st.header("Converter Áudio para Texto")
        if st.button("Converter Áudios"):
            process_audio()
            st.success("Áudios convertidos para texto com sucesso!")

    elif app_mode == "Enviar Embeddings para o Banco de Dados":
        st.header("Enviar Embeddings para o Banco de Dados")
        api_url = st.text_input("URL da API para enviar as transcrições:", "http://localhost:8081/api/meetings/transcriptions")

        if st.button("Enviar Embeddings"):
            process_transcriptions_and_embeddings(api_url)
            st.success("Embeddings enviados para o banco de dados com sucesso!")

# 4. Execução Principal
if __name__ == "__main__":
    main()
```

### Estrutura do Projeto

-   **`main.py`**: Este é o ponto de entrada. Ele usa Streamlit para criar uma interface que permite selecionar qual parte da aplicação executar.

-   **`video_to_audio/`**, **`audio_to_text/`**, **`text_to_embedding/`**, **`voice_assistent/`**: Esses diretórios contêm o código existente de cada parte da sua aplicação.

### Passos para Implementar

1.  **Modularização:**

    *   Certifique-se de que cada parte da sua aplicação (conversão de vídeo, conversão de áudio, chat de voz) esteja bem encapsulada em funções ou classes reutilizáveis.
    *   Mova essas funções e classes para os respectivos módulos (arquivos `.py` dentro de seus diretórios).

2.  **Integração no `main.py`:**

    *   Importe as funções e classes dos módulos.
    *   Use Streamlit para criar uma interface que permita ao usuário escolher qual função executar.
    *   Implemente botões ou seletores para acionar as diferentes partes da aplicação.

3.  **Gerenciamento de Dependências:**

    *   Certifique-se de que todas as dependências necessárias para cada parte da aplicação estejam instaladas.  Considere usar um arquivo `requirements.txt` para listar todas as dependências.

4.  **Testes:**

    *   Teste cada parte da aplicação individualmente para garantir que funcionem corretamente.
    *   Teste a integração geral para garantir que as diferentes partes possam ser executadas em sequência ou individualmente, conforme necessário.

### Pontos Chave

*   **Modularização:** Dividir o código em módulos facilita a manutenção e reutilização.
*   **Interface Streamlit:** Streamlit permite criar uma interface web simples para interagir com sua aplicação.
*   **Gerenciamento de Estado:** Use `st.session_state` para manter o estado entre as execuções da aplicação Streamlit (por exemplo, se um vídeo foi convertido ou não).
*   **Tratamento de Erros:** Implemente tratamento de erros para lidar com falhas em qualquer parte da aplicação e fornecer feedback útil ao usuário.
*   **Configuração:** Utilize arquivos de configuração (como o seu `config.py`) para armazenar caminhos de arquivos, URLs de API e outras configurações que podem precisar ser ajustadas.

Seguindo esses passos, você deve conseguir integrar todas as suas aplicações em um único script `main.py` com uma interface Streamlit para facilitar o uso e a demonstração.

User: observe em src/chat_streamlit que já tenho uma aplicação que funciona como chat e consegue gerar imagem e ler imagem com OCR... atualmente só funciona por text .... quero colocar via comando de voz tbm  com a parte da aplicação que usa Speech Recoginition
Assistant: Ok, para integrar o comando de voz ao seu `chat_streamlit.py`, você precisará adaptar o código existente para incluir a funcionalidade de reconhecimento de voz e direcionar a entrada de voz para as mesmas funções que já processam o texto. Aqui está um guia passo a passo de como fazer isso:

**1. Importar as Bibliotecas Necessárias:**

Adicione as seguintes importações ao topo do seu arquivo `chat_streamlit.py`:

```python
import speech_recognition as sr
import threading
```

**2. Função para Capturar Comando de Voz:**

Crie uma função para capturar o comando de voz usando `speech_recognition`. Esta função será executada em uma thread separada para evitar bloquear a interface do Streamlit.

```python
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        st.write("Diga algo...")  # Mensagem na interface
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            command = recognizer.recognize_google(audio, language='pt-BR')
            st.write(f"Você disse: {command}")  # Mostra o comando reconhecido
            return command
        except sr.WaitTimeoutError:
            st.write("Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            st.write("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            st.write(f"Erro ao solicitar resultados: {e}")
            return None
```

**3. Adicionar um Botão para Ativar o Reconhecimento de Voz:**

Adicione um botão na barra lateral para iniciar a captura de voz. Esse botão iniciará uma thread que executa a função `capture_voice_command`.

```python
with st.sidebar:
    # ... (seu código existente da barra lateral)

    if st.button("🎤 Usar Comando de Voz"):
        st.session_state.voice_command = None  # Limpa o comando anterior
        st.session_state.voice_thread = threading.Thread(target=lambda: setattr(st.session_state, 'voice_command', capture_voice_command()))
        st.session_state.voice_thread.start()
```

**4. Modificar a Lógica de Processamento de Mensagens:**

Modifique a lógica de processamento de mensagens para verificar se há um comando de voz capturado. Se houver, use esse comando como entrada para a função `process_message`.

```python
# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # ... (seu código existente para determinar o placeholder)

    # Verifica se um comando de voz foi capturado
    if 'voice_command' in st.session_state and st.session_state.voice_command:
        user_input = st.session_state.voice_command
        process_message(user_input, current_image)
        st.session_state.voice_command = None  # Limpa o comando para não repetir

    else:
        user_input = st.chat_input(placeholder)

        if user_input:
            # Processa a mensagem com a imagem (se houver) ou apenas texto
            process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)
```

**Explicação das Modificações:**

*   **`capture_voice_command()`:** Esta função captura o áudio do microfone, tenta reconhecer a fala e retorna o comando de texto. Ela também exibe mensagens de status na interface do Streamlit.
*   **Botão "🎤 Usar Comando de Voz":** Quando este botão é clicado, ele cria e inicia uma nova thread que executa a função `capture_voice_command`.  O resultado do comando de voz é armazenado em `st.session_state.voice_command`.
*   **Lógica de Processamento Modificada:** A seção de entrada de texto agora verifica se `st.session_state.voice_command` contém um valor. Se sim, ele usa esse valor como a entrada do usuário e chama `process_message`. Depois, ele limpa `st.session_state.voice_command` para evitar que o comando seja processado novamente.
*   **Thread:** Usar uma thread para a captura de voz é crucial porque `recognizer.listen()` é um método bloqueante. Sem uma thread, a interface do Streamlit ficaria congelada enquanto o áudio é capturado.

**Pontos Adicionais:**

*   **Permissões do Microfone:** Certifique-se de que seu navegador tenha permissão para acessar o microfone.
*   **Thread Safety:** Tenha cuidado com a segurança de threads ao modificar `st.session_state`. Streamlit não é inerentemente thread-safe, então evite operações simultâneas que possam causar problemas. Usar `setattr` como mostrado é uma maneira segura de atualizar o estado.
*   **Feedback Visual:** Forneça feedback visual ao usuário para que saiba quando o sistema está gravando, processando ou aguardando um comando de voz.

Com estas modificações, seu `chat_streamlit.py` deve ser capaz de capturar comandos de voz e processá-los da mesma forma que os comandos de texto. Isso adicionará a funcionalidade de controle por voz que você deseja.

User: eu ja tenho uma aplicação de comando de voz dentro da pasta voice_assistent.
2025-04-10 19:53:29,067 [INFO] Resposta da IA (texto): Entendi. Em vez de criar uma nova função de captura de voz, podemos integrar a sua aplicação existente `voice_assistent` ao `chat_streamlit.py`. Isso envolve principalmente reutilizar as funções de captura e interpretação de comandos da sua aplicação `voice_assistent` existente e integrá-las na interface do Streamlit.

Aqui estão os passos detalhados:

**1. Importar as Funções Necessárias de `voice_assistent`:**

No topo do seu arquivo `chat_streamlit.py`, importe as funções que você precisa de `voice_assistent.main`:

```python
from voice_assistent.class_voice_assistent.main import MainApp
from voice_assistent.class_voice_assistent.gpt_communication.gemini_gpt import GenerativeModelHandler
import threading
import streamlit as st
```

**2. Inicializar a Aplicação `voice_assistent`:**

Você precisará inicializar a aplicação `voice_assistent` uma vez, preferencialmente fora do loop principal do Streamlit, para que não seja reinicializada a cada interação.

```python
# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Inicializa a aplicação de voz
@st.cache_resource
def get_voice_app():
    model = GenerativeModelHandler('gemini-1.5-flash')
    return MainApp(model)

voice_app = get_voice_app()
```

**3. Modificar o Botão de Comando de Voz:**

Modifique o botão na barra lateral para usar a função de captura de voz da sua aplicação `voice_assistent`. Em vez de criar uma nova thread, você chamará a função `handle_command` da sua `voice_app` diretamente.

```python
with st.sidebar:
    # ... (seu código existente da barra lateral)

    if st.button("🎤 Usar Comando de Voz"):
        st.session_state.voice_command = None  # Limpa o comando anterior
        st.session_state.voice_thread = threading.Thread(target=lambda: setattr(st.session_state, 'voice_command', voice_app.handle_command(voice_app.voice_handler.capture_voice_command())))
        st.session_state.voice_thread.start()
```

**4. Modificar a Lógica de Processamento de Mensagens:**

Modifique a lógica de processamento de mensagens para verificar se há um comando de voz capturado. Se houver, use esse comando como entrada para a função `process_message`.

```python
# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # ... (seu código existente para determinar o placeholder)

    # Verifica se um comando de voz foi capturado
    if 'voice_command' in st.session_state and st.session_state.voice_command:
        user_input = st.session_state.voice_command
        process_message(user_input, current_image)
        st.session_state.voice_command = None  # Limpa o comando para não repetir

    else:
        user_input = st.chat_input(placeholder)

        if user_input:
            # Processa a mensagem com a imagem (se houver) ou apenas texto
            process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)
```

**Explicação das Modificações:**

*   **Importação de `MainApp`:** Importe a classe `MainApp` da sua aplicação `voice_assistent`.
*   **Inicialização de `MainApp`:** Inicialize a aplicação `MainApp` usando o modelo `GenerativeModelHandler`. Isso deve ser feito apenas uma vez.
*   **Botão "🎤 Usar Comando de Voz":** Quando este botão é clicado, ele chama `voice_app.run()` para iniciar a captura de voz.
*   **Lógica de Processamento Modificada:** A seção de entrada de texto agora verifica se `st.session_state.voice_command` contém um valor. Se sim, ele usa esse valor como a entrada do usuário e chama `process_message`. Depois, ele limpa `st.session_state.voice_command` para evitar que o comando seja processado novamente.

**Considerações Importantes:**

*   **Cache:** Para garantir que a `MainApp` e o `GenerativeModelHandler` sejam inicializados apenas uma vez, use `@st.cache_resource`. Isso evita a reinicialização a cada interação.
*   **`handle_command`:** A função `handle_command` deve retornar o texto da resposta. Este texto é então usado como entrada para `process_message`.
*   **State Management:** O uso de `st.session_state` permite manter o estado entre as execuções da aplicação Streamlit.

Com estas modificações, seu `chat_streamlit.py` deve ser capaz de capturar comandos de voz usando a aplicação `voice_assistent` e processá-los da mesma forma que os comandos de texto. Isso adicionará a funcionalidade de controle por voz que você deseja.

2025-04-10 19:53:29,154 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-10 19:53:29,157 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-10 19:53:29,159 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-10 19:53:29,161 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-10 19:53:29,163 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-10 19:53:29,164 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-10 19:53:29,166 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-10 19:53:29,167 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-10 19:53:29,169 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-10 19:53:29,170 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-10 19:53:29,172 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-10 19:53:29,174 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-10 19:53:29,176 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-10 19:53:29,178 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-10 19:53:29,179 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-10 19:53:29,180 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-10 19:53:29,181 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-10 19:53:29,183 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-10 19:53:29,184 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-10 19:53:29,186 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-10 19:53:29,187 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-10 19:53:29,189 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 19:53:29,192 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-10 19:53:29,195 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-10 19:53:29,223 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-10 19:53:29,225 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-10 19:53:29,226 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-10 19:53:29,228 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-10 19:53:29,233 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-10 19:53:29,238 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-10 19:53:29,241 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-10 19:53:29,242 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-10 19:53:29,244 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-10 19:53:29,245 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-10 19:53:29,247 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-10 19:53:29,248 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-10 19:53:29,249 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-10 19:53:29,250 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-10 19:53:29,251 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-10 19:53:29,252 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-10 19:53:29,253 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-10 19:53:29,255 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-10 19:53:29,256 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-10 19:53:29,258 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-10 19:53:29,259 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-10 19:53:29,260 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-10 19:53:29,262 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-10 19:53:29,263 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-10 19:53:29,264 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-10 19:53:29,265 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-10 19:53:29,266 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-10 19:53:29,268 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-10 19:53:29,269 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-10 19:53:29,270 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-10 20:12:56,290 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-10 20:12:56,291 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-10 20:12:56,293 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-10 20:12:56,295 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-10 20:12:56,296 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-10 20:12:56,299 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-10 20:12:56,300 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-10 20:12:56,301 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-10 20:12:56,303 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-10 20:12:56,304 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-10 20:12:56,306 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-10 20:12:56,307 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-10 20:12:56,308 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-10 20:12:56,310 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-10 20:12:56,313 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-10 20:12:56,315 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-10 20:12:56,316 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-10 20:12:56,317 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-10 20:12:56,319 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-10 20:12:56,320 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-10 20:12:56,323 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-10 20:12:56,324 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 20:12:56,327 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-10 20:12:56,330 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-10 20:12:56,331 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-10 20:12:56,333 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-10 20:12:56,334 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-10 20:12:56,336 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-10 20:12:56,338 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-10 20:12:56,341 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-10 20:12:56,345 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-10 20:12:56,347 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-10 20:12:56,349 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-10 20:12:56,350 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-10 20:12:56,351 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-10 20:12:56,353 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-10 20:12:56,354 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-10 20:12:56,355 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-10 20:12:56,357 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-10 20:12:56,358 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-10 20:12:56,360 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-10 20:12:56,362 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-10 20:12:56,364 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-10 20:12:56,365 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-10 20:12:56,366 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-10 20:12:56,368 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-10 20:12:56,370 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-10 20:12:56,371 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-10 20:12:56,373 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-10 20:12:56,376 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-10 20:12:56,379 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-10 20:12:56,380 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-10 20:12:56,382 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-10 20:12:56,384 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-10 20:12:56,508 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-10 20:12:56,510 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-10 20:12:56,511 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-10 20:12:56,512 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-10 20:12:56,514 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-10 20:12:56,516 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-10 20:12:56,518 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-10 20:12:56,519 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-10 20:12:56,521 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-10 20:12:56,522 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-10 20:12:56,524 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-10 20:12:56,526 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-10 20:12:56,530 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-10 20:12:56,532 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-10 20:12:56,534 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-10 20:12:56,536 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-10 20:12:56,538 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-10 20:12:56,539 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-10 20:12:56,541 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-10 20:12:56,543 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-10 20:12:56,545 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-10 20:12:56,548 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 20:12:56,549 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-10 20:12:56,551 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-10 20:12:56,552 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-10 20:12:56,554 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-10 20:12:56,556 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-10 20:12:56,557 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-10 20:12:56,559 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-10 20:12:56,561 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-10 20:12:56,563 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-10 20:12:56,565 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-10 20:12:56,566 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-10 20:12:56,568 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-10 20:12:56,569 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-10 20:12:56,570 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-10 20:12:56,571 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-10 20:12:56,572 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-10 20:12:56,574 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-10 20:12:56,575 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-10 20:12:56,577 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-10 20:12:56,580 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-10 20:12:56,582 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-10 20:12:56,583 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-10 20:12:56,584 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-10 20:12:56,585 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-10 20:12:56,587 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-10 20:12:56,588 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-10 20:12:56,589 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-10 20:12:56,590 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-10 20:12:56,591 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-10 20:12:56,593 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-10 20:12:56,595 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-10 20:12:56,596 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-10 20:12:56,598 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diretório atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Definição dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Criação dos diretórios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configuração do layout do Streamlit
st.set_page_config(
    page_title="Análise de Ações",
    layout="wide",
    page_icon="📈"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscilações

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Período', 'Oscilação'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("📊 Dashboard de Análise de Ações")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma ação", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"📌 Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("🔍 Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descrição', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa ação.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("💰 Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("📉 Oscilações")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("📈 Tabela Geral de Ações")
        st.dataframe(self.acoes)

# Execução
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para análise fundamentalista de ações da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from config.common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"Não foram encontrados arquivos .txt no diretório {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conteúdo do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conteúdo HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conteúdo HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se está correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # Configuração de vídeos
    video_config = VideoConfig(root_path=root_path)
    video_processor = VideoToAudioConverter(video_config=video_config)
    video_processor.process_videos()
    
    # # Configuração de áudios
    audio_config = AudioConfig(root_path=root_path)
    audio_processor = AudioToConverter(audio_config=audio_config)
    audio_processor.process_audio_files()
    
    # # Processamento de transcrições e envio de embeddings
    # embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    # embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diretório
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diretório.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do último arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conteúdo do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimensões do embedding:", embedding.shape)

# Caminho do diretório de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o último embedding
load_and_print_embedding(embedding_directory)


# test_whisper.py

from transformers import pipeline
from common_paths import CommonPaths

commmom_paths = CommonPaths()

# Função para ler a transcrição de um arquivo
def read_transcription(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()
    return lines

# Função para converter a pontuação em estrelas
def score_to_stars(score):
    if score >= 0.75:
        return 5
    elif score >= 0.55:
        return 4
    elif score >= 0.35:
        return 3
    elif score >= 0.15:
        return 2
    else:
        return 1

# Inicializar o pipeline de análise de sentimento
sentiment_pipeline = pipeline("sentiment-analysis", model="neuralmind/bert-base-portuguese-cased")

# Caminho para o arquivo de transcrição
file_path = commmom_paths.TRANSCRIPTION_OUTPUT_PATH / 'feedback.txt'

# Ler a transcrição
lines = read_transcription(file_path)

# Analisar sentimento para cada feedback
for line in lines:
    sentiment = sentiment_pipeline(line.strip())
    score = sentiment[0]['score']
    stars = score_to_stars(score)
    
    # Ajuste manual para feedbacks com aspectos positivos e negativos
    if any(word in line.lower() for word in ["mas", "porém", "no entanto"]):
        stars = max(1, stars - 1)  # Reduz uma estrela se houver contradição
    
    # Ajuste adicional baseado em palavras-chave
    positive_keywords = ["excelente", "ótimo", "perfeito", "rápido", "eficiente", "satisfeito", "clara", "eficaz", "bom trabalho"]
    negative_keywords = ["ruim", "péssimo", "horrível", "poderia", "não resolveu", "insatisfatória", "confusa", "inadequado", "lento", "não houve seguimento", "não atendeu", "não funcionou", "demorado"]
    
    positive_count = sum(word in line.lower() for word in positive_keywords)
    negative_count = sum(word in line.lower() for word in negative_keywords)
    
    # Ajuste de estrelas baseado na contagem de palavras-chave
    if positive_count > negative_count:
        stars = min(5, stars + 1)
    elif negative_count > positive_count:
        stars = max(1, stars - 1)
    
    print(f"Texto: {line.strip()}")
    print(f"Sentimento: {stars} estrelas")
    print(f"Pontuação: {score:.2f}")
    print('---')


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configuração de transcrições e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diretório onde está procurando as transcrições
        print(f"Diretório de entrada das transcrições: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcrição no diretório de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcrição encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo não encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcrição do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} está vazio.")
                    return

            # Gerar o embedding da transcrição
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcrição e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de áudio
    with sr.Microphone() as source:
        print("Diga algo em português...")

        while True:
            try:
                # Escuta o áudio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {texto_portugues}")

                # Traduz o texto para o inglês
                traducao = translator.translate(texto_portugues)
                print(f"Tradução para o inglês: {traducao}")

            except sr.UnknownValueError:
                print("Não foi possível entender o áudio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usuário")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configurações de áudio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de áudio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em português...")

try:
    audio_buffer = []

    while True:
        # Lê dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o áudio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o áudio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradução
            print(f"Tradução para o inglês: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usuário")

    # Fecha o stream de áudio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O vídeo {video_path} não contém áudio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de vídeo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de vídeo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de vídeo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Conversão de vídeo para áudio concluída!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configurações da API
handler = genai('gemini-1.5-flash')

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Função para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conteúdo com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares únicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcrição obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' não é uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuniões: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de reuniões: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|tópicos da|pontos (relevantes|principais)|análise)\b.*\b(reunião|última (reunião|conversa|sessão))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como análise de reunião.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padrão.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que são muito similares ao comando atual para evitar redundância.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padrão: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # Métodos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de análise de reunião.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conteúdo usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inválida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar variáveis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conteúdo com base no prompt, contexto e reunião"""
        try:
            # Supondo que a API espera um dicionário com os parâmetros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisição para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar variáveis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Criação da conclusão do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que você acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["faça um resumo da última reunião.", "tópicos da última reunião", "resuma a última reunião", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Você é responsável por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - Não acrescentar título nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informações acima e forneça uma solução detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Você é responsável analisar com detalhes a reunião de [str_meeting], e fornecer uma longa estória sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precisão da resposta.
            - Não acrescente título nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e faça uma pesquisa online para uma resposta mais precisa das questões de [str_text]
        - Não acrescente título nas respostas.
        
        ------
        [context]: Regras Básicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("Áudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum áudio detectado.")
                return None
            except sr.UnknownValueError:
                print("Não foi possível entender o áudio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configurações da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicionário
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta não está no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API: {e}")
        return None

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: Como posso integrar todas as aplicações para funcionar em uma única MAIN.py ?

Streamlit. Chat de voz. converter vídeo para audio e audio para texto e depois salvar no banco de dados.
Assistant: Para integrar todas as aplicações em um único `main.py`, você precisa modularizar seu código existente e usar uma estrutura central que coordene a execução das diferentes partes (Streamlit chat de voz, conversão de vídeo para áudio, áudio para texto e salvamento no banco de dados).

Aqui está um esboço de como você pode abordar isso, com ênfase na organização e coordenação das tarefas:

```python
# main.py

import streamlit as st
import os
from pathlib import Path

# Importar as funções e classes de cada módulo
from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from voice_assistent.class_voice_assistent.main import MainApp
from voice_assistent.class_voice_assistent.gpt_communication.gemini_gpt import GenerativeModelHandler
from transcriptions.transcriptions_config import TranscriptionConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from config.config import Config
from core.rate_limiter import RateLimiter

# 1. Configuração Inicial
PROJECT_ROOT = Path(__file__).resolve().parent
root_path = str(PROJECT_ROOT)
print(f"Root path: {root_path}")

Config.ensure_directories()

# 2. Funções de Processamento
def process_videos():
    video_config = VideoConfig(root_path=root_path)
    video_processor = VideoToAudioConverter(video_config=video_config)
    video_processor.process_videos()

def process_audio():
    audio_config = AudioConfig(root_path=root_path)
    audio_processor = AudioToConverter(audio_config=audio_config)
    audio_processor.process_audio_files()

def process_transcriptions_and_embeddings(api_url):
    transcription_config = TranscriptionConfig(root_path=root_path)
    embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

# 3. Interface Streamlit
def main():
    st.title("Aplicações Integradas")

    # 3.1. Sidebar com opções
    app_mode = st.sidebar.selectbox(
        "Escolha a aplicação:",
        ["Chat de Voz", "Converter Vídeo para Áudio", "Converter Áudio para Texto", "Enviar Embeddings para o Banco de Dados"]
    )

    # 3.2. Lógica para cada aplicação
    if app_mode == "Chat de Voz":
        st.header("Chat de Voz")
        model = GenerativeModelHandler('gemini-1.5-flash')
        app = MainApp(model)
        app.run()

    elif app_mode == "Converter Vídeo para Áudio":
        st.header("Converter Vídeo para Áudio")
        if st.button("Converter Vídeos"):
            process_videos()
            st.success("Vídeos convertidos para áudio com sucesso!")

    elif app_mode == "Converter Áudio para Texto":
        st.header("Converter Áudio para Texto")
        if st.button("Converter Áudios"):
            process_audio()
            st.success("Áudios convertidos para texto com sucesso!")

    elif app_mode == "Enviar Embeddings para o Banco de Dados":
        st.header("Enviar Embeddings para o Banco de Dados")
        api_url = st.text_input("URL da API para enviar as transcrições:", "http://localhost:8081/api/meetings/transcriptions")

        if st.button("Enviar Embeddings"):
            process_transcriptions_and_embeddings(api_url)
            st.success("Embeddings enviados para o banco de dados com sucesso!")

# 4. Execução Principal
if __name__ == "__main__":
    main()
```

### Estrutura do Projeto

-   **`main.py`**: Este é o ponto de entrada. Ele usa Streamlit para criar uma interface que permite selecionar qual parte da aplicação executar.

-   **`video_to_audio/`**, **`audio_to_text/`**, **`text_to_embedding/`**, **`voice_assistent/`**: Esses diretórios contêm o código existente de cada parte da sua aplicação.

### Passos para Implementar

1.  **Modularização:**

    *   Certifique-se de que cada parte da sua aplicação (conversão de vídeo, conversão de áudio, chat de voz) esteja bem encapsulada em funções ou classes reutilizáveis.
    *   Mova essas funções e classes para os respectivos módulos (arquivos `.py` dentro de seus diretórios).

2.  **Integração no `main.py`:**

    *   Importe as funções e classes dos módulos.
    *   Use Streamlit para criar uma interface que permita ao usuário escolher qual função executar.
    *   Implemente botões ou seletores para acionar as diferentes partes da aplicação.

3.  **Gerenciamento de Dependências:**

    *   Certifique-se de que todas as dependências necessárias para cada parte da aplicação estejam instaladas.  Considere usar um arquivo `requirements.txt` para listar todas as dependências.

4.  **Testes:**

    *   Teste cada parte da aplicação individualmente para garantir que funcionem corretamente.
    *   Teste a integração geral para garantir que as diferentes partes possam ser executadas em sequência ou individualmente, conforme necessário.

### Pontos Chave

*   **Modularização:** Dividir o código em módulos facilita a manutenção e reutilização.
*   **Interface Streamlit:** Streamlit permite criar uma interface web simples para interagir com sua aplicação.
*   **Gerenciamento de Estado:** Use `st.session_state` para manter o estado entre as execuções da aplicação Streamlit (por exemplo, se um vídeo foi convertido ou não).
*   **Tratamento de Erros:** Implemente tratamento de erros para lidar com falhas em qualquer parte da aplicação e fornecer feedback útil ao usuário.
*   **Configuração:** Utilize arquivos de configuração (como o seu `config.py`) para armazenar caminhos de arquivos, URLs de API e outras configurações que podem precisar ser ajustadas.

Seguindo esses passos, você deve conseguir integrar todas as suas aplicações em um único script `main.py` com uma interface Streamlit para facilitar o uso e a demonstração.

User: observe em src/chat_streamlit que já tenho uma aplicação que funciona como chat e consegue gerar imagem e ler imagem com OCR... atualmente só funciona por text .... quero colocar via comando de voz tbm  com a parte da aplicação que usa Speech Recoginition
Assistant: Ok, para integrar o comando de voz ao seu `chat_streamlit.py`, você precisará adaptar o código existente para incluir a funcionalidade de reconhecimento de voz e direcionar a entrada de voz para as mesmas funções que já processam o texto. Aqui está um guia passo a passo de como fazer isso:

**1. Importar as Bibliotecas Necessárias:**

Adicione as seguintes importações ao topo do seu arquivo `chat_streamlit.py`:

```python
import speech_recognition as sr
import threading
```

**2. Função para Capturar Comando de Voz:**

Crie uma função para capturar o comando de voz usando `speech_recognition`. Esta função será executada em uma thread separada para evitar bloquear a interface do Streamlit.

```python
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        st.write("Diga algo...")  # Mensagem na interface
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            command = recognizer.recognize_google(audio, language='pt-BR')
            st.write(f"Você disse: {command}")  # Mostra o comando reconhecido
            return command
        except sr.WaitTimeoutError:
            st.write("Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            st.write("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            st.write(f"Erro ao solicitar resultados: {e}")
            return None
```

**3. Adicionar um Botão para Ativar o Reconhecimento de Voz:**

Adicione um botão na barra lateral para iniciar a captura de voz. Esse botão iniciará uma thread que executa a função `capture_voice_command`.

```python
with st.sidebar:
    # ... (seu código existente da barra lateral)

    if st.button("🎤 Usar Comando de Voz"):
        st.session_state.voice_command = None  # Limpa o comando anterior
        st.session_state.voice_thread = threading.Thread(target=lambda: setattr(st.session_state, 'voice_command', capture_voice_command()))
        st.session_state.voice_thread.start()
```

**4. Modificar a Lógica de Processamento de Mensagens:**

Modifique a lógica de processamento de mensagens para verificar se há um comando de voz capturado. Se houver, use esse comando como entrada para a função `process_message`.

```python
# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # ... (seu código existente para determinar o placeholder)

    # Verifica se um comando de voz foi capturado
    if 'voice_command' in st.session_state and st.session_state.voice_command:
        user_input = st.session_state.voice_command
        process_message(user_input, current_image)
        st.session_state.voice_command = None  # Limpa o comando para não repetir

    else:
        user_input = st.chat_input(placeholder)

        if user_input:
            # Processa a mensagem com a imagem (se houver) ou apenas texto
            process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)
```

**Explicação das Modificações:**

*   **`capture_voice_command()`:** Esta função captura o áudio do microfone, tenta reconhecer a fala e retorna o comando de texto. Ela também exibe mensagens de status na interface do Streamlit.
*   **Botão "🎤 Usar Comando de Voz":** Quando este botão é clicado, ele cria e inicia uma nova thread que executa a função `capture_voice_command`.  O resultado do comando de voz é armazenado em `st.session_state.voice_command`.
*   **Lógica de Processamento Modificada:** A seção de entrada de texto agora verifica se `st.session_state.voice_command` contém um valor. Se sim, ele usa esse valor como a entrada do usuário e chama `process_message`. Depois, ele limpa `st.session_state.voice_command` para evitar que o comando seja processado novamente.
*   **Thread:** Usar uma thread para a captura de voz é crucial porque `recognizer.listen()` é um método bloqueante. Sem uma thread, a interface do Streamlit ficaria congelada enquanto o áudio é capturado.

**Pontos Adicionais:**

*   **Permissões do Microfone:** Certifique-se de que seu navegador tenha permissão para acessar o microfone.
*   **Thread Safety:** Tenha cuidado com a segurança de threads ao modificar `st.session_state`. Streamlit não é inerentemente thread-safe, então evite operações simultâneas que possam causar problemas. Usar `setattr` como mostrado é uma maneira segura de atualizar o estado.
*   **Feedback Visual:** Forneça feedback visual ao usuário para que saiba quando o sistema está gravando, processando ou aguardando um comando de voz.

Com estas modificações, seu `chat_streamlit.py` deve ser capaz de capturar comandos de voz e processá-los da mesma forma que os comandos de texto. Isso adicionará a funcionalidade de controle por voz que você deseja.

User: eu ja tenho uma aplicação de comando de voz dentro da pasta voice_assistent.
Assistant: Entendi. Em vez de criar uma nova função de captura de voz, podemos integrar a sua aplicação existente `voice_assistent` ao `chat_streamlit.py`. Isso envolve principalmente reutilizar as funções de captura e interpretação de comandos da sua aplicação `voice_assistent` existente e integrá-las na interface do Streamlit.

Aqui estão os passos detalhados:

**1. Importar as Funções Necessárias de `voice_assistent`:**

No topo do seu arquivo `chat_streamlit.py`, importe as funções que você precisa de `voice_assistent.main`:

```python
from voice_assistent.class_voice_assistent.main import MainApp
from voice_assistent.class_voice_assistent.gpt_communication.gemini_gpt import GenerativeModelHandler
import threading
import streamlit as st
```

**2. Inicializar a Aplicação `voice_assistent`:**

Você precisará inicializar a aplicação `voice_assistent` uma vez, preferencialmente fora do loop principal do Streamlit, para que não seja reinicializada a cada interação.

```python
# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Inicializa a aplicação de voz
@st.cache_resource
def get_voice_app():
    model = GenerativeModelHandler('gemini-1.5-flash')
    return MainApp(model)

voice_app = get_voice_app()
```

**3. Modificar o Botão de Comando de Voz:**

Modifique o botão na barra lateral para usar a função de captura de voz da sua aplicação `voice_assistent`. Em vez de criar uma nova thread, você chamará a função `handle_command` da sua `voice_app` diretamente.

```python
with st.sidebar:
    # ... (seu código existente da barra lateral)

    if st.button("🎤 Usar Comando de Voz"):
        st.session_state.voice_command = None  # Limpa o comando anterior
        st.session_state.voice_thread = threading.Thread(target=lambda: setattr(st.session_state, 'voice_command', voice_app.handle_command(voice_app.voice_handler.capture_voice_command())))
        st.session_state.voice_thread.start()
```

**4. Modificar a Lógica de Processamento de Mensagens:**

Modifique a lógica de processamento de mensagens para verificar se há um comando de voz capturado. Se houver, use esse comando como entrada para a função `process_message`.

```python
# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # ... (seu código existente para determinar o placeholder)

    # Verifica se um comando de voz foi capturado
    if 'voice_command' in st.session_state and st.session_state.voice_command:
        user_input = st.session_state.voice_command
        process_message(user_input, current_image)
        st.session_state.voice_command = None  # Limpa o comando para não repetir

    else:
        user_input = st.chat_input(placeholder)

        if user_input:
            # Processa a mensagem com a imagem (se houver) ou apenas texto
            process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)
```

**Explicação das Modificações:**

*   **Importação de `MainApp`:** Importe a classe `MainApp` da sua aplicação `voice_assistent`.
*   **Inicialização de `MainApp`:** Inicialize a aplicação `MainApp` usando o modelo `GenerativeModelHandler`. Isso deve ser feito apenas uma vez.
*   **Botão "🎤 Usar Comando de Voz":** Quando este botão é clicado, ele chama `voice_app.run()` para iniciar a captura de voz.
*   **Lógica de Processamento Modificada:** A seção de entrada de texto agora verifica se `st.session_state.voice_command` contém um valor. Se sim, ele usa esse valor como a entrada do usuário e chama `process_message`. Depois, ele limpa `st.session_state.voice_command` para evitar que o comando seja processado novamente.

**Considerações Importantes:**

*   **Cache:** Para garantir que a `MainApp` e o `GenerativeModelHandler` sejam inicializados apenas uma vez, use `@st.cache_resource`. Isso evita a reinicialização a cada interação.
*   **`handle_command`:** A função `handle_command` deve retornar o texto da resposta. Este texto é então usado como entrada para `process_message`.
*   **State Management:** O uso de `st.session_state` permite manter o estado entre as execuções da aplicação Streamlit.

Com estas modificações, seu `chat_streamlit.py` deve ser capaz de capturar comandos de voz usando a aplicação `voice_assistent` e processá-los da mesma forma que os comandos de texto. Isso adicionará a funcionalidade de controle por voz que você deseja.

User: refatore a minha aplicação inteira com as boas práticas de POO e SOLID.
2025-04-10 20:13:35,252 [INFO] Resposta da IA (texto): ```python
# app_config\app_config.py
from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()

    def get_root_path(self):
        return str(self.ROOT_PATH)

    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py
from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None, transcription_config=None):
        super().__init__(root_path)
        self.transcription_config = transcription_config if transcription_config else TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = self.transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])

    def get_audio_input_path(self):
        return self.AUDIO_INPUT_PATH

    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# audio_to_text\audio_to_text.py
import whisper
from audio_to_text.audio_config.audio_config import AudioConfig
from abc import ABC, abstractmethod
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class IAudioProcessor(ABC):
    @abstractmethod
    def process_audio_files(self):
        pass

class AudioToConverter(IAudioProcessor):
    def __init__(self, audio_config: AudioConfig, whisper_model_name="base"):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.get_audio_input_path()
        self.TRANSCRIPTION_INPUT_PATH = audio_config.get_transcription_input_path()
        self.model = whisper.load_model(whisper_model_name)
        self.logger = logging.getLogger(__name__)

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            self.logger.info(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                self.logger.info(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, self.model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            self.logger.info(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            self.logger.error(f"Erro ao processar o arquivo {audio_file_path}: {e}", exc_info=True)

# chat_app\chat_streamlit.py
import streamlit as st
import time
from datetime import datetime
from PIL import Image
import os
import io
from dotenv import load_dotenv
import threading  # Importa o módulo threading
from google import genai
from google.genai import types

# Core Components
from chat_app.config.config import Config
from chat_app.core.handlers.gemini_handler import GeminiHandler
from chat_app.core.rate_limiter import RateLimiter

# Services
from chat_app.services.search_files import ler_todos_arquivos_python

# Submodules (Separadas para melhor organização)
from chat_app.modules.image_handling import ImageHandler
from chat_app.modules.voice_input import VoiceInputHandler
from chat_app.modules.message_handling import MessageHandler
from chat_app.modules.image_generation import ImageGenerator


class ChatApp:
    def __init__(self):
        load_dotenv()  # Carrega as variáveis de ambiente
        Config.ensure_directories()  # Garante que os diretórios necessários existam

        self.rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

        self.gemini_handler = GeminiHandler("gemini-2.0-flash-exp")  # Inicializa GeminiHandler
        self.image_handler = ImageHandler()
        self.voice_input_handler = VoiceInputHandler()
        self.message_handler = MessageHandler()
        self.image_generator = ImageGenerator()

        self.initialize_session_state()
        self.load_chat_prompt()

    def initialize_session_state(self):
        """Inicializa os estados do session_state."""
        default_states = {
            "messages": [],
            "processing": False,
            "uploaded_image": None,
            "clipboard_image_preview": None,
            "clipboard_image_file": None,
            "last_message_time": 0,
            "file_uploader_key": "uploader_0",
            "generated_image": None,
            "image_prompt": None,
            "voice_command": None  # Estado para armazenar comandos de voz
        }

        for key, default_value in default_states.items():
            if key not in st.session_state:
                st.session_state[key] = default_value

    def load_chat_prompt(self):
        """Carrega o prompt do chat e adiciona o contexto do código fonte."""
        codigo_fonte = ler_todos_arquivos_python()
        self.chat_prompt = f"{self.load_prompt_from_file()}\n\nContexto:\n\n{codigo_fonte}"

    def load_prompt_from_file(self):
        """Carrega o prompt base do arquivo."""
        try:
            with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
                return file.read().strip()
        except FileNotFoundError:
            return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

    def process_message(self, user_input, image_data=None, generated_image=None):
        """Processa a mensagem do usuário (com ou sem imagem)."""
        st.session_state.processing = True  # Marca como processando para bloquear novos inputs
        st.session_state.current_prompt = user_input
        st.session_state.current_image = image_data
        st.session_state.current_generated_image = generated_image

        st.rerun()  # Força a reexecução para atualizar a UI e mostrar o indicador de processamento

    def execute_processing(self):
        """Executa o processamento da mensagem."""
        user_input = st.session_state.current_prompt
        image_data = st.session_state.current_image
        generated_image = st.session_state.current_generated_image

        self.rate_limiter.wait_for_slot()  # Garante que não exceda o limite de requisições

        current_time = time.time()
        time_since_last_message = current_time - st.session_state.last_message_time
        wait_time = max(0, 2 - time_since_last_message)
        time.sleep(wait_time)

        st.session_state.last_message_time = time.time()

        img_path = None
        img_display = None

        if image_data:
            img_path, img_display = self.image_handler.save_uploaded_image(image_data)
            st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
        elif generated_image:
            img_path = self.image_handler.save_generated_image(generated_image)
            st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
        else:
            st.session_state.messages.append({"role": "user", "content": user_input})

        self.message_handler.limit_message_history(st.session_state.messages)

        full_prompt = self.chat_prompt + "\n\n"  # Start with the base prompt
        full_prompt = self.message_handler.build_full_prompt(full_prompt, st.session_state.messages, user_input)

        try:
            if img_path:
                response = self.gemini_handler.generate_content(img_path, full_prompt)
            else:
                response = self.gemini_handler.generate_content(None, full_prompt)
        except Exception as e:
            response = f"❌ Erro ao gerar resposta: {str(e)}"

        st.session_state.messages.append({"role": "assistant", "content": response})
        self.message_handler.limit_message_history(st.session_state.messages)

        if img_path and os.path.exists(img_path):
            os.remove(img_path)

        st.session_state.processing = False
        st.session_state.current_prompt = None
        st.session_state.current_image = None
        st.session_state.current_generated_image = None

    def run(self):
        """Função principal para rodar a aplicação Streamlit."""
        # Executa o processamento se estiver na fila
        if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
            self.execute_processing()
            st.rerun()

        # Configuração da barra lateral
        with st.sidebar:
            st.title("Chat IA Inteligente")

            # Seção de geração de imagem
            st.markdown("### Gerar Imagem")
            image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
            if st.button("Gerar Imagem"):
                if image_prompt:
                    generated_image = self.image_generator.generate_image(image_prompt)
                    if generated_image:
                        st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                        st.session_state.generated_image = None  # Limpa para não exibir em cima

                        st.rerun()
                else:
                    st.warning("Por favor, digite um prompt para gerar a imagem.")

            # Seção de imagens (sempre visível)
            st.markdown("### Adicionar Imagem (Opcional)")
            st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

            # Layout em duas colunas para os botões de imagem
            col1, col2 = st.columns(2)

            with col1:
                # Botão para verificar a área de transferência
                if st.button("📋 Colar", use_container_width=True):
                    if self.image_handler.on_paste_click():
                        st.success("Imagem colada!")
                        st.rerun()
                    else:
                        st.warning("Nada encontrado.")

            with col2:
                # Botão para limpar a imagem atual (se houver)
                if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
                    if st.button("🗑️ Limpar", use_container_width=True):
                        self.image_handler.clear_all_images()
                        st.rerun()
                else:
                    # Placeholder para manter o layout alinhado
                    st.write("")

            # Uploader de imagem com chave dinâmica
            uploaded_file = st.file_uploader(
                "📷 Ou faça upload de imagem",
                type=["png", "jpg", "jpeg"],
                label_visibility="visible",
                key=st.session_state.file_uploader_key
            )

            # Atualiza o estado da imagem quando um arquivo é carregado
            if uploaded_file:
                st.session_state.uploaded_image = uploaded_file
                self.image_handler.on_file_upload()
                st.success("Imagem carregada!")

            # Exibe a imagem selecionada na barra lateral
            self.image_handler.display_selected_image()

            st.markdown("---")

             # Adiciona botão para comando de voz
            if st.button("🎤 Usar Comando de Voz"):
                st.session_state.voice_thread = threading.Thread(target=self.voice_input_handler.capture_voice_command)
                st.session_state.voice_thread.start()

            # Botão para limpar o histórico de conversa
            if st.button("🧹 Limpar conversa", use_container_width=True):
                st.session_state.messages = []
                st.rerun()

            st.caption("Desenvolvido com Streamlit e Gemini AI")

        # Exibição do histórico de mensagens
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                # Se houver imagem, exiba-a (se armazenada)
                if message.get("image"):
                    st.image(message["image"], use_container_width=True)
                # Exibe o conteúdo da mensagem (texto)
                st.markdown(message["content"])

        # Adiciona indicador de digitação quando estiver processando
        if st.session_state.processing:
            with st.chat_message("assistant"):
                st.markdown("Gerando resposta...")

        # Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
        if not st.session_state.processing:
            # Verifica se há uma imagem disponível
            current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

            # Adapta o placeholder com base na presença de imagem
            if current_image:
                placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
            else:
                placeholder = "Digite sua mensagem..."

            # Verifica se há um comando de voz
            if st.session_state.get('voice_command'):
                user_input = st.session_state.voice_command
                st.session_state.voice_command = None  # Limpa o comando
                self.process_message(user_input, current_image)
            else:
                user_input = st.chat_input(placeholder)

                if user_input:
                    # Processa a mensagem com a imagem (se houver) ou apenas texto
                    self.process_message(user_input, current_image)
        else:
            st.chat_input("Aguarde o processamento...", disabled=True)


if __name__ == "__main__":
    app = ChatApp()
    app.run()

# chat_app\config\config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent

    ASSETS_DIR = BASE_DIR / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"

    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"

    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"

    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"

    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}

    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR,
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py
from chat_app.services.gpt_services import GenerativeModelHandler
from chat_app.core.logger_config import logger
from chat_app.core.rate_limiter import RateLimiter

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py
import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py
import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0]
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py
from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from chat_app.config.config import Config
import os
from chat_app.core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from chat_app.core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py
import os
import time
import shutil
import json
from chat_app.config.config import Config
from chat_app.services.gpt_services import GenerativeModelHandler
from chat_app.services.document_service import DocumentService
from chat_app.services.markdown_service import MarkdownService
from chat_app.utils.file_utils import list_images
from chat_app.core.logger_config import logger
from chat_app.core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(
2025-04-10 20:13:35,444 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-10 20:13:35,446 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-10 20:13:35,449 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-10 20:13:35,452 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-10 20:13:35,455 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-10 20:13:35,457 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-10 20:13:35,460 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-10 20:13:35,462 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-10 20:13:35,464 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-10 20:13:35,467 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-10 20:13:35,468 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-10 20:13:35,469 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-10 20:13:35,471 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-10 20:13:35,472 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-10 20:13:35,474 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-10 20:13:35,475 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-10 20:13:35,476 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-10 20:13:35,478 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-10 20:13:35,480 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-10 20:13:35,482 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-10 20:13:35,484 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-10 20:13:35,486 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 20:13:35,487 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-10 20:13:35,489 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-10 20:13:35,491 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-10 20:13:35,492 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-10 20:13:35,494 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-10 20:13:35,497 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-10 20:13:35,499 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-10 20:13:35,501 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-10 20:13:35,503 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-10 20:13:35,505 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-10 20:13:35,507 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-10 20:13:35,508 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-10 20:13:35,510 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-10 20:13:35,512 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-10 20:13:35,513 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-10 20:13:35,515 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-10 20:13:35,517 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-10 20:13:35,519 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-10 20:13:35,521 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-10 20:13:35,522 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-10 20:13:35,524 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-10 20:13:35,526 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-10 20:13:35,528 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-10 20:13:35,529 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-10 20:13:35,532 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-10 20:13:35,534 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-10 20:13:35,535 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-10 20:13:35,536 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-10 20:13:35,537 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-10 20:13:35,539 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-10 20:13:35,540 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-10 20:13:35,542 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-10 20:16:50,210 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-10 20:16:50,211 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-10 20:16:50,213 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-10 20:16:50,214 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-10 20:16:50,216 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-10 20:16:50,218 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-10 20:16:50,220 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-10 20:16:50,222 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-10 20:16:50,223 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-10 20:16:50,225 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-10 20:16:50,226 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-10 20:16:50,228 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-10 20:16:50,230 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-10 20:16:50,231 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-10 20:16:50,233 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-10 20:16:50,236 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-10 20:16:50,238 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-10 20:16:50,239 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-10 20:16:50,240 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-10 20:16:50,241 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-10 20:16:50,243 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-10 20:16:50,245 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 20:16:50,261 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-10 20:16:50,263 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-10 20:16:50,267 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-10 20:16:50,275 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-10 20:16:50,276 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-10 20:16:50,277 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-10 20:16:50,278 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-10 20:16:50,280 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-10 20:16:50,281 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-10 20:16:50,282 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-10 20:16:50,284 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-10 20:16:50,286 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-10 20:16:50,288 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-10 20:16:50,290 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-10 20:16:50,291 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-10 20:16:50,292 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-10 20:16:50,294 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-10 20:16:50,295 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-10 20:16:50,297 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-10 20:16:50,298 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-10 20:16:50,300 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-10 20:16:50,303 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-10 20:16:50,305 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-10 20:16:50,307 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-10 20:16:50,309 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-10 20:16:50,311 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-10 20:16:50,313 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-10 20:16:50,315 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-10 20:16:50,317 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-10 20:16:50,320 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-10 20:16:50,323 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-10 20:16:50,325 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-10 20:16:50,448 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-10 20:16:50,452 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-10 20:16:50,455 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-10 20:16:50,458 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-10 20:16:50,462 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-10 20:16:50,464 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-10 20:16:50,466 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-10 20:16:50,468 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-10 20:16:50,470 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-10 20:16:50,472 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-10 20:16:50,474 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-10 20:16:50,476 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-10 20:16:50,477 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-10 20:16:50,479 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-10 20:16:50,480 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-10 20:16:50,482 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-10 20:16:50,484 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-10 20:16:50,486 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-10 20:16:50,488 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-10 20:16:50,490 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-10 20:16:50,492 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-10 20:16:50,493 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 20:16:50,495 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-10 20:16:50,496 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-10 20:16:50,499 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-10 20:16:50,501 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-10 20:16:50,503 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-10 20:16:50,505 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-10 20:16:50,507 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-10 20:16:50,508 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-10 20:16:50,509 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-10 20:16:50,511 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-10 20:16:50,512 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-10 20:16:50,514 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-10 20:16:50,515 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-10 20:16:50,517 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-10 20:16:50,520 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-10 20:16:50,522 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-10 20:16:50,524 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-10 20:16:50,525 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-10 20:16:50,526 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-10 20:16:50,528 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-10 20:16:50,529 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-10 20:16:50,530 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-10 20:16:50,532 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-10 20:16:50,534 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-10 20:16:50,536 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-10 20:16:50,538 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-10 20:16:50,540 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-10 20:16:50,542 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-10 20:16:50,543 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-10 20:16:50,544 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-10 20:16:50,546 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-10 20:16:50,547 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-10 20:16:50,551 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diretório atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Definição dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Criação dos diretórios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configuração do layout do Streamlit
st.set_page_config(
    page_title="Análise de Ações",
    layout="wide",
    page_icon="📈"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscilações

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Período', 'Oscilação'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("📊 Dashboard de Análise de Ações")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma ação", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"📌 Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("🔍 Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descrição', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa ação.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("💰 Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("📉 Oscilações")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("📈 Tabela Geral de Ações")
        st.dataframe(self.acoes)

# Execução
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para análise fundamentalista de ações da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from config.common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"Não foram encontrados arquivos .txt no diretório {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conteúdo do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conteúdo HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conteúdo HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se está correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # Configuração de vídeos
    video_config = VideoConfig(root_path=root_path)
    video_processor = VideoToAudioConverter(video_config=video_config)
    video_processor.process_videos()
    
    # # Configuração de áudios
    audio_config = AudioConfig(root_path=root_path)
    audio_processor = AudioToConverter(audio_config=audio_config)
    audio_processor.process_audio_files()
    
    # # Processamento de transcrições e envio de embeddings
    # embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    # embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diretório
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diretório.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do último arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conteúdo do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimensões do embedding:", embedding.shape)

# Caminho do diretório de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o último embedding
load_and_print_embedding(embedding_directory)


# test_whisper.py

from transformers import pipeline
from common_paths import CommonPaths

commmom_paths = CommonPaths()

# Função para ler a transcrição de um arquivo
def read_transcription(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()
    return lines

# Função para converter a pontuação em estrelas
def score_to_stars(score):
    if score >= 0.75:
        return 5
    elif score >= 0.55:
        return 4
    elif score >= 0.35:
        return 3
    elif score >= 0.15:
        return 2
    else:
        return 1

# Inicializar o pipeline de análise de sentimento
sentiment_pipeline = pipeline("sentiment-analysis", model="neuralmind/bert-base-portuguese-cased")

# Caminho para o arquivo de transcrição
file_path = commmom_paths.TRANSCRIPTION_OUTPUT_PATH / 'feedback.txt'

# Ler a transcrição
lines = read_transcription(file_path)

# Analisar sentimento para cada feedback
for line in lines:
    sentiment = sentiment_pipeline(line.strip())
    score = sentiment[0]['score']
    stars = score_to_stars(score)
    
    # Ajuste manual para feedbacks com aspectos positivos e negativos
    if any(word in line.lower() for word in ["mas", "porém", "no entanto"]):
        stars = max(1, stars - 1)  # Reduz uma estrela se houver contradição
    
    # Ajuste adicional baseado em palavras-chave
    positive_keywords = ["excelente", "ótimo", "perfeito", "rápido", "eficiente", "satisfeito", "clara", "eficaz", "bom trabalho"]
    negative_keywords = ["ruim", "péssimo", "horrível", "poderia", "não resolveu", "insatisfatória", "confusa", "inadequado", "lento", "não houve seguimento", "não atendeu", "não funcionou", "demorado"]
    
    positive_count = sum(word in line.lower() for word in positive_keywords)
    negative_count = sum(word in line.lower() for word in negative_keywords)
    
    # Ajuste de estrelas baseado na contagem de palavras-chave
    if positive_count > negative_count:
        stars = min(5, stars + 1)
    elif negative_count > positive_count:
        stars = max(1, stars - 1)
    
    print(f"Texto: {line.strip()}")
    print(f"Sentimento: {stars} estrelas")
    print(f"Pontuação: {score:.2f}")
    print('---')


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configuração de transcrições e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diretório onde está procurando as transcrições
        print(f"Diretório de entrada das transcrições: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcrição no diretório de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcrição encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo não encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcrição do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} está vazio.")
                    return

            # Gerar o embedding da transcrição
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcrição e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de áudio
    with sr.Microphone() as source:
        print("Diga algo em português...")

        while True:
            try:
                # Escuta o áudio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {texto_portugues}")

                # Traduz o texto para o inglês
                traducao = translator.translate(texto_portugues)
                print(f"Tradução para o inglês: {traducao}")

            except sr.UnknownValueError:
                print("Não foi possível entender o áudio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usuário")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configurações de áudio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de áudio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em português...")

try:
    audio_buffer = []

    while True:
        # Lê dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o áudio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o áudio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradução
            print(f"Tradução para o inglês: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usuário")

    # Fecha o stream de áudio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O vídeo {video_path} não contém áudio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de vídeo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de vídeo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de vídeo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Conversão de vídeo para áudio concluída!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configurações da API
handler = genai('gemini-1.5-flash')

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Função para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conteúdo com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares únicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcrição obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' não é uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuniões: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de reuniões: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|tópicos da|pontos (relevantes|principais)|análise)\b.*\b(reunião|última (reunião|conversa|sessão))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como análise de reunião.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padrão.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que são muito similares ao comando atual para evitar redundância.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padrão: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # Métodos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de análise de reunião.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conteúdo usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inválida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar variáveis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conteúdo com base no prompt, contexto e reunião"""
        try:
            # Supondo que a API espera um dicionário com os parâmetros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisição para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar variáveis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Criação da conclusão do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que você acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["faça um resumo da última reunião.", "tópicos da última reunião", "resuma a última reunião", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Você é responsável por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - Não acrescentar título nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informações acima e forneça uma solução detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Você é responsável analisar com detalhes a reunião de [str_meeting], e fornecer uma longa estória sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precisão da resposta.
            - Não acrescente título nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e faça uma pesquisa online para uma resposta mais precisa das questões de [str_text]
        - Não acrescente título nas respostas.
        
        ------
        [context]: Regras Básicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("Áudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum áudio detectado.")
                return None
            except sr.UnknownValueError:
                print("Não foi possível entender o áudio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configurações da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicionário
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta não está no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API: {e}")
        return None

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: Como posso integrar todas as aplicações para funcionar em uma única MAIN.py ?

Streamlit. Chat de voz. converter vídeo para audio e audio para texto e depois salvar no banco de dados.
Assistant: Para integrar todas as aplicações em um único `main.py`, você precisa modularizar seu código existente e usar uma estrutura central que coordene a execução das diferentes partes (Streamlit chat de voz, conversão de vídeo para áudio, áudio para texto e salvamento no banco de dados).

Aqui está um esboço de como você pode abordar isso, com ênfase na organização e coordenação das tarefas:

```python
# main.py

import streamlit as st
import os
from pathlib import Path

# Importar as funções e classes de cada módulo
from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from voice_assistent.class_voice_assistent.main import MainApp
from voice_assistent.class_voice_assistent.gpt_communication.gemini_gpt import GenerativeModelHandler
from transcriptions.transcriptions_config import TranscriptionConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from config.config import Config
from core.rate_limiter import RateLimiter

# 1. Configuração Inicial
PROJECT_ROOT = Path(__file__).resolve().parent
root_path = str(PROJECT_ROOT)
print(f"Root path: {root_path}")

Config.ensure_directories()

# 2. Funções de Processamento
def process_videos():
    video_config = VideoConfig(root_path=root_path)
    video_processor = VideoToAudioConverter(video_config=video_config)
    video_processor.process_videos()

def process_audio():
    audio_config = AudioConfig(root_path=root_path)
    audio_processor = AudioToConverter(audio_config=audio_config)
    audio_processor.process_audio_files()

def process_transcriptions_and_embeddings(api_url):
    transcription_config = TranscriptionConfig(root_path=root_path)
    embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

# 3. Interface Streamlit
def main():
    st.title("Aplicações Integradas")

    # 3.1. Sidebar com opções
    app_mode = st.sidebar.selectbox(
        "Escolha a aplicação:",
        ["Chat de Voz", "Converter Vídeo para Áudio", "Converter Áudio para Texto", "Enviar Embeddings para o Banco de Dados"]
    )

    # 3.2. Lógica para cada aplicação
    if app_mode == "Chat de Voz":
        st.header("Chat de Voz")
        model = GenerativeModelHandler('gemini-1.5-flash')
        app = MainApp(model)
        app.run()

    elif app_mode == "Converter Vídeo para Áudio":
        st.header("Converter Vídeo para Áudio")
        if st.button("Converter Vídeos"):
            process_videos()
            st.success("Vídeos convertidos para áudio com sucesso!")

    elif app_mode == "Converter Áudio para Texto":
        st.header("Converter Áudio para Texto")
        if st.button("Converter Áudios"):
            process_audio()
            st.success("Áudios convertidos para texto com sucesso!")

    elif app_mode == "Enviar Embeddings para o Banco de Dados":
        st.header("Enviar Embeddings para o Banco de Dados")
        api_url = st.text_input("URL da API para enviar as transcrições:", "http://localhost:8081/api/meetings/transcriptions")

        if st.button("Enviar Embeddings"):
            process_transcriptions_and_embeddings(api_url)
            st.success("Embeddings enviados para o banco de dados com sucesso!")

# 4. Execução Principal
if __name__ == "__main__":
    main()
```

### Estrutura do Projeto

-   **`main.py`**: Este é o ponto de entrada. Ele usa Streamlit para criar uma interface que permite selecionar qual parte da aplicação executar.

-   **`video_to_audio/`**, **`audio_to_text/`**, **`text_to_embedding/`**, **`voice_assistent/`**: Esses diretórios contêm o código existente de cada parte da sua aplicação.

### Passos para Implementar

1.  **Modularização:**

    *   Certifique-se de que cada parte da sua aplicação (conversão de vídeo, conversão de áudio, chat de voz) esteja bem encapsulada em funções ou classes reutilizáveis.
    *   Mova essas funções e classes para os respectivos módulos (arquivos `.py` dentro de seus diretórios).

2.  **Integração no `main.py`:**

    *   Importe as funções e classes dos módulos.
    *   Use Streamlit para criar uma interface que permita ao usuário escolher qual função executar.
    *   Implemente botões ou seletores para acionar as diferentes partes da aplicação.

3.  **Gerenciamento de Dependências:**

    *   Certifique-se de que todas as dependências necessárias para cada parte da aplicação estejam instaladas.  Considere usar um arquivo `requirements.txt` para listar todas as dependências.

4.  **Testes:**

    *   Teste cada parte da aplicação individualmente para garantir que funcionem corretamente.
    *   Teste a integração geral para garantir que as diferentes partes possam ser executadas em sequência ou individualmente, conforme necessário.

### Pontos Chave

*   **Modularização:** Dividir o código em módulos facilita a manutenção e reutilização.
*   **Interface Streamlit:** Streamlit permite criar uma interface web simples para interagir com sua aplicação.
*   **Gerenciamento de Estado:** Use `st.session_state` para manter o estado entre as execuções da aplicação Streamlit (por exemplo, se um vídeo foi convertido ou não).
*   **Tratamento de Erros:** Implemente tratamento de erros para lidar com falhas em qualquer parte da aplicação e fornecer feedback útil ao usuário.
*   **Configuração:** Utilize arquivos de configuração (como o seu `config.py`) para armazenar caminhos de arquivos, URLs de API e outras configurações que podem precisar ser ajustadas.

Seguindo esses passos, você deve conseguir integrar todas as suas aplicações em um único script `main.py` com uma interface Streamlit para facilitar o uso e a demonstração.

User: observe em src/chat_streamlit que já tenho uma aplicação que funciona como chat e consegue gerar imagem e ler imagem com OCR... atualmente só funciona por text .... quero colocar via comando de voz tbm  com a parte da aplicação que usa Speech Recoginition
Assistant: Ok, para integrar o comando de voz ao seu `chat_streamlit.py`, você precisará adaptar o código existente para incluir a funcionalidade de reconhecimento de voz e direcionar a entrada de voz para as mesmas funções que já processam o texto. Aqui está um guia passo a passo de como fazer isso:

**1. Importar as Bibliotecas Necessárias:**

Adicione as seguintes importações ao topo do seu arquivo `chat_streamlit.py`:

```python
import speech_recognition as sr
import threading
```

**2. Função para Capturar Comando de Voz:**

Crie uma função para capturar o comando de voz usando `speech_recognition`. Esta função será executada em uma thread separada para evitar bloquear a interface do Streamlit.

```python
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        st.write("Diga algo...")  # Mensagem na interface
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            command = recognizer.recognize_google(audio, language='pt-BR')
            st.write(f"Você disse: {command}")  # Mostra o comando reconhecido
            return command
        except sr.WaitTimeoutError:
            st.write("Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            st.write("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            st.write(f"Erro ao solicitar resultados: {e}")
            return None
```

**3. Adicionar um Botão para Ativar o Reconhecimento de Voz:**

Adicione um botão na barra lateral para iniciar a captura de voz. Esse botão iniciará uma thread que executa a função `capture_voice_command`.

```python
with st.sidebar:
    # ... (seu código existente da barra lateral)

    if st.button("🎤 Usar Comando de Voz"):
        st.session_state.voice_command = None  # Limpa o comando anterior
        st.session_state.voice_thread = threading.Thread(target=lambda: setattr(st.session_state, 'voice_command', capture_voice_command()))
        st.session_state.voice_thread.start()
```

**4. Modificar a Lógica de Processamento de Mensagens:**

Modifique a lógica de processamento de mensagens para verificar se há um comando de voz capturado. Se houver, use esse comando como entrada para a função `process_message`.

```python
# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # ... (seu código existente para determinar o placeholder)

    # Verifica se um comando de voz foi capturado
    if 'voice_command' in st.session_state and st.session_state.voice_command:
        user_input = st.session_state.voice_command
        process_message(user_input, current_image)
        st.session_state.voice_command = None  # Limpa o comando para não repetir

    else:
        user_input = st.chat_input(placeholder)

        if user_input:
            # Processa a mensagem com a imagem (se houver) ou apenas texto
            process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)
```

**Explicação das Modificações:**

*   **`capture_voice_command()`:** Esta função captura o áudio do microfone, tenta reconhecer a fala e retorna o comando de texto. Ela também exibe mensagens de status na interface do Streamlit.
*   **Botão "🎤 Usar Comando de Voz":** Quando este botão é clicado, ele cria e inicia uma nova thread que executa a função `capture_voice_command`.  O resultado do comando de voz é armazenado em `st.session_state.voice_command`.
*   **Lógica de Processamento Modificada:** A seção de entrada de texto agora verifica se `st.session_state.voice_command` contém um valor. Se sim, ele usa esse valor como a entrada do usuário e chama `process_message`. Depois, ele limpa `st.session_state.voice_command` para evitar que o comando seja processado novamente.
*   **Thread:** Usar uma thread para a captura de voz é crucial porque `recognizer.listen()` é um método bloqueante. Sem uma thread, a interface do Streamlit ficaria congelada enquanto o áudio é capturado.

**Pontos Adicionais:**

*   **Permissões do Microfone:** Certifique-se de que seu navegador tenha permissão para acessar o microfone.
*   **Thread Safety:** Tenha cuidado com a segurança de threads ao modificar `st.session_state`. Streamlit não é inerentemente thread-safe, então evite operações simultâneas que possam causar problemas. Usar `setattr` como mostrado é uma maneira segura de atualizar o estado.
*   **Feedback Visual:** Forneça feedback visual ao usuário para que saiba quando o sistema está gravando, processando ou aguardando um comando de voz.

Com estas modificações, seu `chat_streamlit.py` deve ser capaz de capturar comandos de voz e processá-los da mesma forma que os comandos de texto. Isso adicionará a funcionalidade de controle por voz que você deseja.

User: eu ja tenho uma aplicação de comando de voz dentro da pasta voice_assistent.
Assistant: Entendi. Em vez de criar uma nova função de captura de voz, podemos integrar a sua aplicação existente `voice_assistent` ao `chat_streamlit.py`. Isso envolve principalmente reutilizar as funções de captura e interpretação de comandos da sua aplicação `voice_assistent` existente e integrá-las na interface do Streamlit.

Aqui estão os passos detalhados:

**1. Importar as Funções Necessárias de `voice_assistent`:**

No topo do seu arquivo `chat_streamlit.py`, importe as funções que você precisa de `voice_assistent.main`:

```python
from voice_assistent.class_voice_assistent.main import MainApp
from voice_assistent.class_voice_assistent.gpt_communication.gemini_gpt import GenerativeModelHandler
import threading
import streamlit as st
```

**2. Inicializar a Aplicação `voice_assistent`:**

Você precisará inicializar a aplicação `voice_assistent` uma vez, preferencialmente fora do loop principal do Streamlit, para que não seja reinicializada a cada interação.

```python
# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Inicializa a aplicação de voz
@st.cache_resource
def get_voice_app():
    model = GenerativeModelHandler('gemini-1.5-flash')
    return MainApp(model)

voice_app = get_voice_app()
```

**3. Modificar o Botão de Comando de Voz:**

Modifique o botão na barra lateral para usar a função de captura de voz da sua aplicação `voice_assistent`. Em vez de criar uma nova thread, você chamará a função `handle_command` da sua `voice_app` diretamente.

```python
with st.sidebar:
    # ... (seu código existente da barra lateral)

    if st.button("🎤 Usar Comando de Voz"):
        st.session_state.voice_command = None  # Limpa o comando anterior
        st.session_state.voice_thread = threading.Thread(target=lambda: setattr(st.session_state, 'voice_command', voice_app.handle_command(voice_app.voice_handler.capture_voice_command())))
        st.session_state.voice_thread.start()
```

**4. Modificar a Lógica de Processamento de Mensagens:**

Modifique a lógica de processamento de mensagens para verificar se há um comando de voz capturado. Se houver, use esse comando como entrada para a função `process_message`.

```python
# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # ... (seu código existente para determinar o placeholder)

    # Verifica se um comando de voz foi capturado
    if 'voice_command' in st.session_state and st.session_state.voice_command:
        user_input = st.session_state.voice_command
        process_message(user_input, current_image)
        st.session_state.voice_command = None  # Limpa o comando para não repetir

    else:
        user_input = st.chat_input(placeholder)

        if user_input:
            # Processa a mensagem com a imagem (se houver) ou apenas texto
            process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)
```

**Explicação das Modificações:**

*   **Importação de `MainApp`:** Importe a classe `MainApp` da sua aplicação `voice_assistent`.
*   **Inicialização de `MainApp`:** Inicialize a aplicação `MainApp` usando o modelo `GenerativeModelHandler`. Isso deve ser feito apenas uma vez.
*   **Botão "🎤 Usar Comando de Voz":** Quando este botão é clicado, ele chama `voice_app.run()` para iniciar a captura de voz.
*   **Lógica de Processamento Modificada:** A seção de entrada de texto agora verifica se `st.session_state.voice_command` contém um valor. Se sim, ele usa esse valor como a entrada do usuário e chama `process_message`. Depois, ele limpa `st.session_state.voice_command` para evitar que o comando seja processado novamente.

**Considerações Importantes:**

*   **Cache:** Para garantir que a `MainApp` e o `GenerativeModelHandler` sejam inicializados apenas uma vez, use `@st.cache_resource`. Isso evita a reinicialização a cada interação.
*   **`handle_command`:** A função `handle_command` deve retornar o texto da resposta. Este texto é então usado como entrada para `process_message`.
*   **State Management:** O uso de `st.session_state` permite manter o estado entre as execuções da aplicação Streamlit.

Com estas modificações, seu `chat_streamlit.py` deve ser capaz de capturar comandos de voz usando a aplicação `voice_assistent` e processá-los da mesma forma que os comandos de texto. Isso adicionará a funcionalidade de controle por voz que você deseja.

User: refatore a minha aplicação inteira com as boas práticas de POO e SOLID.
Assistant: ```python
# app_config\app_config.py
from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()

    def get_root_path(self):
        return str(self.ROOT_PATH)

    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py
from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None, transcription_config=None):
        super().__init__(root_path)
        self.transcription_config = transcription_config if transcription_config else TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = self.transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])

    def get_audio_input_path(self):
        return self.AUDIO_INPUT_PATH

    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# audio_to_text\audio_to_text.py
import whisper
from audio_to_text.audio_config.audio_config import AudioConfig
from abc import ABC, abstractmethod
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class IAudioProcessor(ABC):
    @abstractmethod
    def process_audio_files(self):
        pass

class AudioToConverter(IAudioProcessor):
    def __init__(self, audio_config: AudioConfig, whisper_model_name="base"):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.get_audio_input_path()
        self.TRANSCRIPTION_INPUT_PATH = audio_config.get_transcription_input_path()
        self.model = whisper.load_model(whisper_model_name)
        self.logger = logging.getLogger(__name__)

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            self.logger.info(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                self.logger.info(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, self.model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            self.logger.info(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            self.logger.error(f"Erro ao processar o arquivo {audio_file_path}: {e}", exc_info=True)

# chat_app\chat_streamlit.py
import streamlit as st
import time
from datetime import datetime
from PIL import Image
import os
import io
from dotenv import load_dotenv
import threading  # Importa o módulo threading
from google import genai
from google.genai import types

# Core Components
from chat_app.config.config import Config
from chat_app.core.handlers.gemini_handler import GeminiHandler
from chat_app.core.rate_limiter import RateLimiter

# Services
from chat_app.services.search_files import ler_todos_arquivos_python

# Submodules (Separadas para melhor organização)
from chat_app.modules.image_handling import ImageHandler
from chat_app.modules.voice_input import VoiceInputHandler
from chat_app.modules.message_handling import MessageHandler
from chat_app.modules.image_generation import ImageGenerator


class ChatApp:
    def __init__(self):
        load_dotenv()  # Carrega as variáveis de ambiente
        Config.ensure_directories()  # Garante que os diretórios necessários existam

        self.rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

        self.gemini_handler = GeminiHandler("gemini-2.0-flash-exp")  # Inicializa GeminiHandler
        self.image_handler = ImageHandler()
        self.voice_input_handler = VoiceInputHandler()
        self.message_handler = MessageHandler()
        self.image_generator = ImageGenerator()

        self.initialize_session_state()
        self.load_chat_prompt()

    def initialize_session_state(self):
        """Inicializa os estados do session_state."""
        default_states = {
            "messages": [],
            "processing": False,
            "uploaded_image": None,
            "clipboard_image_preview": None,
            "clipboard_image_file": None,
            "last_message_time": 0,
            "file_uploader_key": "uploader_0",
            "generated_image": None,
            "image_prompt": None,
            "voice_command": None  # Estado para armazenar comandos de voz
        }

        for key, default_value in default_states.items():
            if key not in st.session_state:
                st.session_state[key] = default_value

    def load_chat_prompt(self):
        """Carrega o prompt do chat e adiciona o contexto do código fonte."""
        codigo_fonte = ler_todos_arquivos_python()
        self.chat_prompt = f"{self.load_prompt_from_file()}\n\nContexto:\n\n{codigo_fonte}"

    def load_prompt_from_file(self):
        """Carrega o prompt base do arquivo."""
        try:
            with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
                return file.read().strip()
        except FileNotFoundError:
            return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

    def process_message(self, user_input, image_data=None, generated_image=None):
        """Processa a mensagem do usuário (com ou sem imagem)."""
        st.session_state.processing = True  # Marca como processando para bloquear novos inputs
        st.session_state.current_prompt = user_input
        st.session_state.current_image = image_data
        st.session_state.current_generated_image = generated_image

        st.rerun()  # Força a reexecução para atualizar a UI e mostrar o indicador de processamento

    def execute_processing(self):
        """Executa o processamento da mensagem."""
        user_input = st.session_state.current_prompt
        image_data = st.session_state.current_image
        generated_image = st.session_state.current_generated_image

        self.rate_limiter.wait_for_slot()  # Garante que não exceda o limite de requisições

        current_time = time.time()
        time_since_last_message = current_time - st.session_state.last_message_time
        wait_time = max(0, 2 - time_since_last_message)
        time.sleep(wait_time)

        st.session_state.last_message_time = time.time()

        img_path = None
        img_display = None

        if image_data:
            img_path, img_display = self.image_handler.save_uploaded_image(image_data)
            st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
        elif generated_image:
            img_path = self.image_handler.save_generated_image(generated_image)
            st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
        else:
            st.session_state.messages.append({"role": "user", "content": user_input})

        self.message_handler.limit_message_history(st.session_state.messages)

        full_prompt = self.chat_prompt + "\n\n"  # Start with the base prompt
        full_prompt = self.message_handler.build_full_prompt(full_prompt, st.session_state.messages, user_input)

        try:
            if img_path:
                response = self.gemini_handler.generate_content(img_path, full_prompt)
            else:
                response = self.gemini_handler.generate_content(None, full_prompt)
        except Exception as e:
            response = f"❌ Erro ao gerar resposta: {str(e)}"

        st.session_state.messages.append({"role": "assistant", "content": response})
        self.message_handler.limit_message_history(st.session_state.messages)

        if img_path and os.path.exists(img_path):
            os.remove(img_path)

        st.session_state.processing = False
        st.session_state.current_prompt = None
        st.session_state.current_image = None
        st.session_state.current_generated_image = None

    def run(self):
        """Função principal para rodar a aplicação Streamlit."""
        # Executa o processamento se estiver na fila
        if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
            self.execute_processing()
            st.rerun()

        # Configuração da barra lateral
        with st.sidebar:
            st.title("Chat IA Inteligente")

            # Seção de geração de imagem
            st.markdown("### Gerar Imagem")
            image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
            if st.button("Gerar Imagem"):
                if image_prompt:
                    generated_image = self.image_generator.generate_image(image_prompt)
                    if generated_image:
                        st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                        st.session_state.generated_image = None  # Limpa para não exibir em cima

                        st.rerun()
                else:
                    st.warning("Por favor, digite um prompt para gerar a imagem.")

            # Seção de imagens (sempre visível)
            st.markdown("### Adicionar Imagem (Opcional)")
            st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

            # Layout em duas colunas para os botões de imagem
            col1, col2 = st.columns(2)

            with col1:
                # Botão para verificar a área de transferência
                if st.button("📋 Colar", use_container_width=True):
                    if self.image_handler.on_paste_click():
                        st.success("Imagem colada!")
                        st.rerun()
                    else:
                        st.warning("Nada encontrado.")

            with col2:
                # Botão para limpar a imagem atual (se houver)
                if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
                    if st.button("🗑️ Limpar", use_container_width=True):
                        self.image_handler.clear_all_images()
                        st.rerun()
                else:
                    # Placeholder para manter o layout alinhado
                    st.write("")

            # Uploader de imagem com chave dinâmica
            uploaded_file = st.file_uploader(
                "📷 Ou faça upload de imagem",
                type=["png", "jpg", "jpeg"],
                label_visibility="visible",
                key=st.session_state.file_uploader_key
            )

            # Atualiza o estado da imagem quando um arquivo é carregado
            if uploaded_file:
                st.session_state.uploaded_image = uploaded_file
                self.image_handler.on_file_upload()
                st.success("Imagem carregada!")

            # Exibe a imagem selecionada na barra lateral
            self.image_handler.display_selected_image()

            st.markdown("---")

             # Adiciona botão para comando de voz
            if st.button("🎤 Usar Comando de Voz"):
                st.session_state.voice_thread = threading.Thread(target=self.voice_input_handler.capture_voice_command)
                st.session_state.voice_thread.start()

            # Botão para limpar o histórico de conversa
            if st.button("🧹 Limpar conversa", use_container_width=True):
                st.session_state.messages = []
                st.rerun()

            st.caption("Desenvolvido com Streamlit e Gemini AI")

        # Exibição do histórico de mensagens
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                # Se houver imagem, exiba-a (se armazenada)
                if message.get("image"):
                    st.image(message["image"], use_container_width=True)
                # Exibe o conteúdo da mensagem (texto)
                st.markdown(message["content"])

        # Adiciona indicador de digitação quando estiver processando
        if st.session_state.processing:
            with st.chat_message("assistant"):
                st.markdown("Gerando resposta...")

        # Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
        if not st.session_state.processing:
            # Verifica se há uma imagem disponível
            current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

            # Adapta o placeholder com base na presença de imagem
            if current_image:
                placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
            else:
                placeholder = "Digite sua mensagem..."

            # Verifica se há um comando de voz
            if st.session_state.get('voice_command'):
                user_input = st.session_state.voice_command
                st.session_state.voice_command = None  # Limpa o comando
                self.process_message(user_input, current_image)
            else:
                user_input = st.chat_input(placeholder)

                if user_input:
                    # Processa a mensagem com a imagem (se houver) ou apenas texto
                    self.process_message(user_input, current_image)
        else:
            st.chat_input("Aguarde o processamento...", disabled=True)


if __name__ == "__main__":
    app = ChatApp()
    app.run()

# chat_app\config\config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent

    ASSETS_DIR = BASE_DIR / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"

    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"

    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"

    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"

    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}

    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR,
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py
from chat_app.services.gpt_services import GenerativeModelHandler
from chat_app.core.logger_config import logger
from chat_app.core.rate_limiter import RateLimiter

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py
import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py
import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0]
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py
from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from chat_app.config.config import Config
import os
from chat_app.core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from chat_app.core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py
import os
import time
import shutil
import json
from chat_app.config.config import Config
from chat_app.services.gpt_services import GenerativeModelHandler
from chat_app.services.document_service import DocumentService
from chat_app.services.markdown_service import MarkdownService
from chat_app.utils.file_utils import list_images
from chat_app.core.logger_config import logger
from chat_app.core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(
User: Preciso garantir que seja enviado todo o código e preciso eliminar partes redundantes e dispensáveis sem perder funcionalidades.
2025-04-10 20:17:18,218 [INFO] Resposta da IA (texto): Ok, aqui está a refatoração do código com boas práticas de POO e SOLID, eliminando redundâncias e garantindo que todas as funcionalidades sejam mantidas. O código foi estruturado para melhorar a organização e facilitar a manutenção.

```python
# app_config\app_config.py
from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()

    def get_root_path(self):
        return str(self.ROOT_PATH)

    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py
from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None, transcription_config=None):
        super().__init__(root_path)
        self.transcription_config = transcription_config if transcription_config else TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = self.transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])

    def get_audio_input_path(self):
        return self.AUDIO_INPUT_PATH

    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# audio_to_text\audio_to_text.py
import whisper
from audio_to_text.audio_config.audio_config import AudioConfig
from abc import ABC, abstractmethod
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class IAudioProcessor(ABC):
    @abstractmethod
    def process_audio_files(self):
        pass

class AudioToConverter(IAudioProcessor):
    def __init__(self, audio_config: AudioConfig, whisper_model_name="base"):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.get_audio_input_path()
        self.TRANSCRIPTION_INPUT_PATH = audio_config.get_transcription_input_path()
        self.model = whisper.load_model(whisper_model_name)
        self.logger = logging.getLogger(__name__)

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            self.logger.info(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                self.logger.info(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, self.model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            self.logger.info(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            self.logger.error(f"Erro ao processar o arquivo {audio_file_path}: {e}", exc_info=True)

# chat_app\chat_streamlit.py
import streamlit as st
import time
from datetime import datetime
from PIL import Image
import os
import io
from dotenv import load_dotenv
import threading  # Importa o módulo threading
from google import genai
from google.genai import types

# Core Components
from chat_app.config.config import Config
from chat_app.core.handlers.gemini_handler import GeminiHandler
from chat_app.core.rate_limiter import RateLimiter

# Services
from chat_app.services.search_files import ler_todos_arquivos_python

# Submodules (Separadas para melhor organização)
from chat_app.modules.image_handling import ImageHandler
from chat_app.modules.voice_input import VoiceInputHandler
from chat_app.modules.message_handling import MessageHandler
from chat_app.modules.image_generation import ImageGenerator


class ChatApp:
    def __init__(self):
        load_dotenv()  # Carrega as variáveis de ambiente
        Config.ensure_directories()  # Garante que os diretórios necessários existam

        self.rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

        self.gemini_handler = GeminiHandler("gemini-2.0-flash-exp")  # Inicializa GeminiHandler
        self.image_handler = ImageHandler(self)  # Passa a instância de ChatApp
        self.voice_input_handler = VoiceInputHandler(self)  # Passa a instância de ChatApp
        self.message_handler = MessageHandler()
        self.image_generator = ImageGenerator()

        self.initialize_session_state()
        self.load_chat_prompt()

    def initialize_session_state(self):
        """Inicializa os estados do session_state."""
        default_states = {
            "messages": [],
            "processing": False,
            "uploaded_image": None,
            "clipboard_image_preview": None,
            "clipboard_image_file": None,
            "last_message_time": 0,
            "file_uploader_key": "uploader_0",
            "generated_image": None,
            "image_prompt": None,
            "voice_command": None  # Estado para armazenar comandos de voz
        }

        for key, default_value in default_states.items():
            if key not in st.session_state:
                st.session_state[key] = default_value

    def load_chat_prompt(self):
        """Carrega o prompt do chat e adiciona o contexto do código fonte."""
        codigo_fonte = ler_todos_arquivos_python()
        self.chat_prompt = f"{self.load_prompt_from_file()}\n\nContexto:\n\n{codigo_fonte}"

    def load_prompt_from_file(self):
        """Carrega o prompt base do arquivo."""
        try:
            with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
                return file.read().strip()
        except FileNotFoundError:
            return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

    def process_message(self, user_input, image_data=None, generated_image=None):
        """Processa a mensagem do usuário (com ou sem imagem)."""
        st.session_state.processing = True  # Marca como processando para bloquear novos inputs
        st.session_state.current_prompt = user_input
        st.session_state.current_image = image_data
        st.session_state.current_generated_image = generated_image

        st.rerun()  # Força a reexecução para atualizar a UI e mostrar o indicador de processamento

    def execute_processing(self):
        """Executa o processamento da mensagem."""
        user_input = st.session_state.current_prompt
        image_data = st.session_state.current_image
        generated_image = st.session_state.current_generated_image

        self.rate_limiter.wait_for_slot()  # Garante que não exceda o limite de requisições

        current_time = time.time()
        time_since_last_message = current_time - st.session_state.last_message_time
        wait_time = max(0, 2 - time_since_last_message)
        time.sleep(wait_time)

        st.session_state.last_message_time = time.time()

        img_path = None
        img_display = None

        if image_data:
            img_path, img_display = self.image_handler.save_uploaded_image(image_data)
            st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
        elif generated_image:
            img_path = self.image_handler.save_generated_image(generated_image)
            st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
        else:
            st.session_state.messages.append({"role": "user", "content": user_input})

        self.message_handler.limit_message_history(st.session_state.messages)

        full_prompt = self.chat_prompt + "\n\n"  # Start with the base prompt
        full_prompt = self.message_handler.build_full_prompt(full_prompt, st.session_state.messages, user_input)

        try:
            if img_path:
                response = self.gemini_handler.generate_content(img_path, full_prompt)
            else:
                response = self.gemini_handler.generate_content(None, full_prompt)
        except Exception as e:
            response = f"❌ Erro ao gerar resposta: {str(e)}"

        st.session_state.messages.append({"role": "assistant", "content": response})
        self.message_handler.limit_message_history(st.session_state.messages)

        if img_path and os.path.exists(img_path):
            os.remove(img_path)

        st.session_state.processing = False
        st.session_state.current_prompt = None
        st.session_state.current_image = None
        st.session_state.current_generated_image = None

    def run(self):
        """Função principal para rodar a aplicação Streamlit."""
        # Executa o processamento se estiver na fila
        if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
            self.execute_processing()
            st.rerun()

        # Configuração da barra lateral
        with st.sidebar:
            st.title("Chat IA Inteligente")

            # Seção de geração de imagem
            st.markdown("### Gerar Imagem")
            image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
            if st.button("Gerar Imagem"):
                if image_prompt:
                    generated_image = self.image_generator.generate_image(image_prompt)
                    if generated_image:
                        st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                        st.session_state.generated_image = None  # Limpa para não exibir em cima

                        st.rerun()
                else:
                    st.warning("Por favor, digite um prompt para gerar a imagem.")

            # Seção de imagens (sempre visível)
            st.markdown("### Adicionar Imagem (Opcional)")
            st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

            # Layout em duas colunas para os botões de imagem
            col1, col2 = st.columns(2)

            with col1:
                # Botão para verificar a área de transferência
                if st.button("📋 Colar", use_container_width=True):
                    if self.image_handler.on_paste_click():
                        st.success("Imagem colada!")
                        st.rerun()
                    else:
                        st.warning("Nada encontrado.")

            with col2:
                # Botão para limpar a imagem atual (se houver)
                if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
                    if st.button("🗑️ Limpar", use_container_width=True):
                        self.image_handler.clear_all_images()
                        st.rerun()
                else:
                    # Placeholder para manter o layout alinhado
                    st.write("")

            # Uploader de imagem com chave dinâmica
            uploaded_file = st.file_uploader(
                "📷 Ou faça upload de imagem",
                type=["png", "jpg", "jpeg"],
                label_visibility="visible",
                key=st.session_state.file_uploader_key
            )

            # Atualiza o estado da imagem quando um arquivo é carregado
            if uploaded_file:
                st.session_state.uploaded_image = uploaded_file
                self.image_handler.on_file_upload()
                st.success("Imagem carregada!")

            # Exibe a imagem selecionada na barra lateral
            self.image_handler.display_selected_image()

            st.markdown("---")

            # Adiciona botão para comando de voz
            if st.button("🎤 Usar Comando de Voz"):
                st.session_state.voice_thread = threading.Thread(target=self.voice_input_handler.capture_voice_command)
                st.session_state.voice_thread.start()

            # Botão para limpar o histórico de conversa
            if st.button("🧹 Limpar conversa", use_container_width=True):
                st.session_state.messages = []
                st.rerun()

            st.caption("Desenvolvido com Streamlit e Gemini AI")

        # Exibição do histórico de mensagens
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                # Se houver imagem, exiba-a (se armazenada)
                if message.get("image"):
                    st.image(message["image"], use_container_width=True)
                # Exibe o conteúdo da mensagem (texto)
                st.markdown(message["content"])

        # Adiciona indicador de digitação quando estiver processando
        if st.session_state.processing:
            with st.chat_message("assistant"):
                st.markdown("Gerando resposta...")

        # Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
        if not st.session_state.processing:
            # Verifica se há uma imagem disponível
            current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

            # Adapta o placeholder com base na presença de imagem
            if current_image:
                placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
            else:
                placeholder = "Digite sua mensagem..."

            # Verifica se há um comando de voz
            if st.session_state.get('voice_command'):
                user_input = st.session_state.voice_command
                st.session_state.voice_command = None  # Limpa o comando
                self.process_message(user_input, current_image)
            else:
                user_input = st.chat_input(placeholder)

                if user_input:
                    # Processa a mensagem com a imagem (se houver) ou apenas texto
                    self.process_message(user_input, current_image)
        else:
            st.chat_input("Aguarde o processamento...", disabled=True)


if __name__ == "__main__":
    app = ChatApp()
    app.run()

# chat_app\config\config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent

    ASSETS_DIR = BASE_DIR / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"

    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"

    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"

    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"

    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}

    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR,
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py
from chat_app.services.gpt_services import GenerativeModelHandler
from chat_app.core.logger_config import logger
from chat_app.core.rate_limiter import RateLimiter

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py
import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py
import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0]
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py
from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from chat_app.config.config import Config
import os
from chat_app.core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from chat_app.core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py
import os
import time
import shutil
import json
from chat_app.config.config import Config
from chat_app.services.gpt_services import GenerativeModelHandler
from chat_app.services.document_service import DocumentService
from chat_app.services.markdown_service import MarkdownService
from chat_app.utils.file_utils import list_images
from chat_app.core.logger_config import logger
from chat_app.core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response
2025-04-10 20:17:18,453 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-10 20:17:18,455 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-10 20:17:18,456 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-10 20:17:18,458 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-10 20:17:18,461 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-10 20:17:18,463 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-10 20:17:18,465 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-10 20:17:18,467 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-10 20:17:18,470 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-10 20:17:18,472 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-10 20:17:18,474 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-10 20:17:18,476 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-10 20:17:18,478 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-10 20:17:18,480 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-10 20:17:18,481 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-10 20:17:18,483 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-10 20:17:18,485 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-10 20:17:18,487 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-10 20:17:18,489 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-10 20:17:18,492 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-10 20:17:18,494 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-10 20:17:18,495 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 20:17:18,498 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-10 20:17:18,500 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-10 20:17:18,503 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-10 20:17:18,506 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-10 20:17:18,508 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-10 20:17:18,511 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-10 20:17:18,513 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-10 20:17:18,516 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-10 20:17:18,519 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-10 20:17:18,522 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-10 20:17:18,525 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-10 20:17:18,527 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-10 20:17:18,529 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-10 20:17:18,532 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-10 20:17:18,533 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-10 20:17:18,535 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-10 20:17:18,537 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-10 20:17:18,540 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-10 20:17:18,542 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-10 20:17:18,544 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-10 20:17:18,546 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-10 20:17:18,548 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-10 20:17:18,550 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-10 20:17:18,552 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-10 20:17:18,554 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-10 20:17:18,557 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-10 20:17:18,559 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-10 20:17:18,560 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-10 20:17:18,562 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-10 20:17:18,564 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-10 20:17:18,566 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-10 20:17:18,568 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-10 20:20:56,602 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-10 20:20:56,604 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-10 20:20:56,606 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-10 20:20:56,607 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-10 20:20:56,609 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-10 20:20:56,611 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-10 20:20:56,612 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-10 20:20:56,614 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-10 20:20:56,616 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-10 20:20:56,618 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-10 20:20:56,621 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-10 20:20:56,622 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-10 20:20:56,625 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-10 20:20:56,627 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-10 20:20:56,629 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-10 20:20:56,631 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-10 20:20:56,632 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-10 20:20:56,633 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-10 20:20:56,635 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-10 20:20:56,636 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-10 20:20:56,638 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-10 20:20:56,639 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 20:20:56,640 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-10 20:20:56,642 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-10 20:20:56,644 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-10 20:20:56,646 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-10 20:20:56,648 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-10 20:20:56,649 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-10 20:20:56,650 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-10 20:20:56,652 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-10 20:20:56,654 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-10 20:20:56,656 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-10 20:20:56,657 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-10 20:20:56,660 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-10 20:20:56,663 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-10 20:20:56,665 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-10 20:20:56,667 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-10 20:20:56,669 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-10 20:20:56,671 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-10 20:20:56,672 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-10 20:20:56,673 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-10 20:20:56,674 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-10 20:20:56,676 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-10 20:20:56,678 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-10 20:20:56,680 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-10 20:20:56,681 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-10 20:20:56,683 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-10 20:20:56,684 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-10 20:20:56,686 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-10 20:20:56,687 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-10 20:20:56,688 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-10 20:20:56,690 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-10 20:20:56,693 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-10 20:20:56,695 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-10 20:20:56,821 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-10 20:20:56,825 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-10 20:20:56,828 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-10 20:20:56,831 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-10 20:20:56,834 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-10 20:20:56,836 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-10 20:20:56,837 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-10 20:20:56,839 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-10 20:20:56,840 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-10 20:20:56,843 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-10 20:20:56,845 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-10 20:20:56,847 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-10 20:20:56,848 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-10 20:20:56,849 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-10 20:20:56,851 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-10 20:20:56,853 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-10 20:20:56,854 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-10 20:20:56,855 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-10 20:20:56,857 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-10 20:20:56,859 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-10 20:20:56,861 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-10 20:20:56,863 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 20:20:56,864 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-10 20:20:56,866 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-10 20:20:56,868 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-10 20:20:56,869 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-10 20:20:56,871 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-10 20:20:56,873 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-10 20:20:56,874 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-10 20:20:56,876 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-10 20:20:56,878 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-10 20:20:56,880 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-10 20:20:56,882 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-10 20:20:56,883 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-10 20:20:56,885 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-10 20:20:56,887 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-10 20:20:56,888 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-10 20:20:56,889 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-10 20:20:56,890 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-10 20:20:56,899 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-10 20:20:56,904 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-10 20:20:56,910 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-10 20:20:56,913 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-10 20:20:56,916 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-10 20:20:56,919 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-10 20:20:56,920 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-10 20:20:56,922 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-10 20:20:56,923 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-10 20:20:56,924 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-10 20:20:56,926 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-10 20:20:56,928 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-10 20:20:56,929 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-10 20:20:56,931 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-10 20:20:56,932 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-10 20:20:56,935 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diretório atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Definição dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Criação dos diretórios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configuração do layout do Streamlit
st.set_page_config(
    page_title="Análise de Ações",
    layout="wide",
    page_icon="📈"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscilações

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Período', 'Oscilação'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("📊 Dashboard de Análise de Ações")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma ação", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"📌 Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("🔍 Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descrição', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa ação.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("💰 Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("📉 Oscilações")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("📈 Tabela Geral de Ações")
        st.dataframe(self.acoes)

# Execução
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para análise fundamentalista de ações da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from config.common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"Não foram encontrados arquivos .txt no diretório {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conteúdo do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conteúdo HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conteúdo HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se está correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # Configuração de vídeos
    video_config = VideoConfig(root_path=root_path)
    video_processor = VideoToAudioConverter(video_config=video_config)
    video_processor.process_videos()
    
    # # Configuração de áudios
    audio_config = AudioConfig(root_path=root_path)
    audio_processor = AudioToConverter(audio_config=audio_config)
    audio_processor.process_audio_files()
    
    # # Processamento de transcrições e envio de embeddings
    # embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    # embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diretório
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diretório.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do último arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conteúdo do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimensões do embedding:", embedding.shape)

# Caminho do diretório de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o último embedding
load_and_print_embedding(embedding_directory)


# test_whisper.py

from transformers import pipeline
from common_paths import CommonPaths

commmom_paths = CommonPaths()

# Função para ler a transcrição de um arquivo
def read_transcription(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()
    return lines

# Função para converter a pontuação em estrelas
def score_to_stars(score):
    if score >= 0.75:
        return 5
    elif score >= 0.55:
        return 4
    elif score >= 0.35:
        return 3
    elif score >= 0.15:
        return 2
    else:
        return 1

# Inicializar o pipeline de análise de sentimento
sentiment_pipeline = pipeline("sentiment-analysis", model="neuralmind/bert-base-portuguese-cased")

# Caminho para o arquivo de transcrição
file_path = commmom_paths.TRANSCRIPTION_OUTPUT_PATH / 'feedback.txt'

# Ler a transcrição
lines = read_transcription(file_path)

# Analisar sentimento para cada feedback
for line in lines:
    sentiment = sentiment_pipeline(line.strip())
    score = sentiment[0]['score']
    stars = score_to_stars(score)
    
    # Ajuste manual para feedbacks com aspectos positivos e negativos
    if any(word in line.lower() for word in ["mas", "porém", "no entanto"]):
        stars = max(1, stars - 1)  # Reduz uma estrela se houver contradição
    
    # Ajuste adicional baseado em palavras-chave
    positive_keywords = ["excelente", "ótimo", "perfeito", "rápido", "eficiente", "satisfeito", "clara", "eficaz", "bom trabalho"]
    negative_keywords = ["ruim", "péssimo", "horrível", "poderia", "não resolveu", "insatisfatória", "confusa", "inadequado", "lento", "não houve seguimento", "não atendeu", "não funcionou", "demorado"]
    
    positive_count = sum(word in line.lower() for word in positive_keywords)
    negative_count = sum(word in line.lower() for word in negative_keywords)
    
    # Ajuste de estrelas baseado na contagem de palavras-chave
    if positive_count > negative_count:
        stars = min(5, stars + 1)
    elif negative_count > positive_count:
        stars = max(1, stars - 1)
    
    print(f"Texto: {line.strip()}")
    print(f"Sentimento: {stars} estrelas")
    print(f"Pontuação: {score:.2f}")
    print('---')


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configuração de transcrições e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diretório onde está procurando as transcrições
        print(f"Diretório de entrada das transcrições: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcrição no diretório de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcrição encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo não encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcrição do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} está vazio.")
                    return

            # Gerar o embedding da transcrição
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcrição e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de áudio
    with sr.Microphone() as source:
        print("Diga algo em português...")

        while True:
            try:
                # Escuta o áudio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {texto_portugues}")

                # Traduz o texto para o inglês
                traducao = translator.translate(texto_portugues)
                print(f"Tradução para o inglês: {traducao}")

            except sr.UnknownValueError:
                print("Não foi possível entender o áudio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usuário")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configurações de áudio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de áudio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em português...")

try:
    audio_buffer = []

    while True:
        # Lê dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o áudio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o áudio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradução
            print(f"Tradução para o inglês: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usuário")

    # Fecha o stream de áudio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O vídeo {video_path} não contém áudio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de vídeo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de vídeo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de vídeo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Conversão de vídeo para áudio concluída!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configurações da API
handler = genai('gemini-1.5-flash')

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Função para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conteúdo com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares únicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcrição obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' não é uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuniões: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de reuniões: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|tópicos da|pontos (relevantes|principais)|análise)\b.*\b(reunião|última (reunião|conversa|sessão))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como análise de reunião.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padrão.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que são muito similares ao comando atual para evitar redundância.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padrão: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # Métodos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de análise de reunião.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conteúdo usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inválida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar variáveis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conteúdo com base no prompt, contexto e reunião"""
        try:
            # Supondo que a API espera um dicionário com os parâmetros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisição para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar variáveis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Criação da conclusão do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que você acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["faça um resumo da última reunião.", "tópicos da última reunião", "resuma a última reunião", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Você é responsável por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - Não acrescentar título nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informações acima e forneça uma solução detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Você é responsável analisar com detalhes a reunião de [str_meeting], e fornecer uma longa estória sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precisão da resposta.
            - Não acrescente título nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e faça uma pesquisa online para uma resposta mais precisa das questões de [str_text]
        - Não acrescente título nas respostas.
        
        ------
        [context]: Regras Básicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("Áudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum áudio detectado.")
                return None
            except sr.UnknownValueError:
                print("Não foi possível entender o áudio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configurações da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicionário
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta não está no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API: {e}")
        return None

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: Como posso integrar todas as aplicações para funcionar em uma única MAIN.py ?

Streamlit. Chat de voz. converter vídeo para audio e audio para texto e depois salvar no banco de dados.
Assistant: Para integrar todas as aplicações em um único `main.py`, você precisa modularizar seu código existente e usar uma estrutura central que coordene a execução das diferentes partes (Streamlit chat de voz, conversão de vídeo para áudio, áudio para texto e salvamento no banco de dados).

Aqui está um esboço de como você pode abordar isso, com ênfase na organização e coordenação das tarefas:

```python
# main.py

import streamlit as st
import os
from pathlib import Path

# Importar as funções e classes de cada módulo
from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from voice_assistent.class_voice_assistent.main import MainApp
from voice_assistent.class_voice_assistent.gpt_communication.gemini_gpt import GenerativeModelHandler
from transcriptions.transcriptions_config import TranscriptionConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from config.config import Config
from core.rate_limiter import RateLimiter

# 1. Configuração Inicial
PROJECT_ROOT = Path(__file__).resolve().parent
root_path = str(PROJECT_ROOT)
print(f"Root path: {root_path}")

Config.ensure_directories()

# 2. Funções de Processamento
def process_videos():
    video_config = VideoConfig(root_path=root_path)
    video_processor = VideoToAudioConverter(video_config=video_config)
    video_processor.process_videos()

def process_audio():
    audio_config = AudioConfig(root_path=root_path)
    audio_processor = AudioToConverter(audio_config=audio_config)
    audio_processor.process_audio_files()

def process_transcriptions_and_embeddings(api_url):
    transcription_config = TranscriptionConfig(root_path=root_path)
    embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())
    embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    embedding_processor_wrapper.process_transcriptions()

# 3. Interface Streamlit
def main():
    st.title("Aplicações Integradas")

    # 3.1. Sidebar com opções
    app_mode = st.sidebar.selectbox(
        "Escolha a aplicação:",
        ["Chat de Voz", "Converter Vídeo para Áudio", "Converter Áudio para Texto", "Enviar Embeddings para o Banco de Dados"]
    )

    # 3.2. Lógica para cada aplicação
    if app_mode == "Chat de Voz":
        st.header("Chat de Voz")
        model = GenerativeModelHandler('gemini-1.5-flash')
        app = MainApp(model)
        app.run()

    elif app_mode == "Converter Vídeo para Áudio":
        st.header("Converter Vídeo para Áudio")
        if st.button("Converter Vídeos"):
            process_videos()
            st.success("Vídeos convertidos para áudio com sucesso!")

    elif app_mode == "Converter Áudio para Texto":
        st.header("Converter Áudio para Texto")
        if st.button("Converter Áudios"):
            process_audio()
            st.success("Áudios convertidos para texto com sucesso!")

    elif app_mode == "Enviar Embeddings para o Banco de Dados":
        st.header("Enviar Embeddings para o Banco de Dados")
        api_url = st.text_input("URL da API para enviar as transcrições:", "http://localhost:8081/api/meetings/transcriptions")

        if st.button("Enviar Embeddings"):
            process_transcriptions_and_embeddings(api_url)
            st.success("Embeddings enviados para o banco de dados com sucesso!")

# 4. Execução Principal
if __name__ == "__main__":
    main()
```

### Estrutura do Projeto

-   **`main.py`**: Este é o ponto de entrada. Ele usa Streamlit para criar uma interface que permite selecionar qual parte da aplicação executar.

-   **`video_to_audio/`**, **`audio_to_text/`**, **`text_to_embedding/`**, **`voice_assistent/`**: Esses diretórios contêm o código existente de cada parte da sua aplicação.

### Passos para Implementar

1.  **Modularização:**

    *   Certifique-se de que cada parte da sua aplicação (conversão de vídeo, conversão de áudio, chat de voz) esteja bem encapsulada em funções ou classes reutilizáveis.
    *   Mova essas funções e classes para os respectivos módulos (arquivos `.py` dentro de seus diretórios).

2.  **Integração no `main.py`:**

    *   Importe as funções e classes dos módulos.
    *   Use Streamlit para criar uma interface que permita ao usuário escolher qual função executar.
    *   Implemente botões ou seletores para acionar as diferentes partes da aplicação.

3.  **Gerenciamento de Dependências:**

    *   Certifique-se de que todas as dependências necessárias para cada parte da aplicação estejam instaladas.  Considere usar um arquivo `requirements.txt` para listar todas as dependências.

4.  **Testes:**

    *   Teste cada parte da aplicação individualmente para garantir que funcionem corretamente.
    *   Teste a integração geral para garantir que as diferentes partes possam ser executadas em sequência ou individualmente, conforme necessário.

### Pontos Chave

*   **Modularização:** Dividir o código em módulos facilita a manutenção e reutilização.
*   **Interface Streamlit:** Streamlit permite criar uma interface web simples para interagir com sua aplicação.
*   **Gerenciamento de Estado:** Use `st.session_state` para manter o estado entre as execuções da aplicação Streamlit (por exemplo, se um vídeo foi convertido ou não).
*   **Tratamento de Erros:** Implemente tratamento de erros para lidar com falhas em qualquer parte da aplicação e fornecer feedback útil ao usuário.
*   **Configuração:** Utilize arquivos de configuração (como o seu `config.py`) para armazenar caminhos de arquivos, URLs de API e outras configurações que podem precisar ser ajustadas.

Seguindo esses passos, você deve conseguir integrar todas as suas aplicações em um único script `main.py` com uma interface Streamlit para facilitar o uso e a demonstração.

User: observe em src/chat_streamlit que já tenho uma aplicação que funciona como chat e consegue gerar imagem e ler imagem com OCR... atualmente só funciona por text .... quero colocar via comando de voz tbm  com a parte da aplicação que usa Speech Recoginition
Assistant: Ok, para integrar o comando de voz ao seu `chat_streamlit.py`, você precisará adaptar o código existente para incluir a funcionalidade de reconhecimento de voz e direcionar a entrada de voz para as mesmas funções que já processam o texto. Aqui está um guia passo a passo de como fazer isso:

**1. Importar as Bibliotecas Necessárias:**

Adicione as seguintes importações ao topo do seu arquivo `chat_streamlit.py`:

```python
import speech_recognition as sr
import threading
```

**2. Função para Capturar Comando de Voz:**

Crie uma função para capturar o comando de voz usando `speech_recognition`. Esta função será executada em uma thread separada para evitar bloquear a interface do Streamlit.

```python
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        st.write("Diga algo...")  # Mensagem na interface
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            command = recognizer.recognize_google(audio, language='pt-BR')
            st.write(f"Você disse: {command}")  # Mostra o comando reconhecido
            return command
        except sr.WaitTimeoutError:
            st.write("Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            st.write("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            st.write(f"Erro ao solicitar resultados: {e}")
            return None
```

**3. Adicionar um Botão para Ativar o Reconhecimento de Voz:**

Adicione um botão na barra lateral para iniciar a captura de voz. Esse botão iniciará uma thread que executa a função `capture_voice_command`.

```python
with st.sidebar:
    # ... (seu código existente da barra lateral)

    if st.button("🎤 Usar Comando de Voz"):
        st.session_state.voice_command = None  # Limpa o comando anterior
        st.session_state.voice_thread = threading.Thread(target=lambda: setattr(st.session_state, 'voice_command', capture_voice_command()))
        st.session_state.voice_thread.start()
```

**4. Modificar a Lógica de Processamento de Mensagens:**

Modifique a lógica de processamento de mensagens para verificar se há um comando de voz capturado. Se houver, use esse comando como entrada para a função `process_message`.

```python
# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # ... (seu código existente para determinar o placeholder)

    # Verifica se um comando de voz foi capturado
    if 'voice_command' in st.session_state and st.session_state.voice_command:
        user_input = st.session_state.voice_command
        process_message(user_input, current_image)
        st.session_state.voice_command = None  # Limpa o comando para não repetir

    else:
        user_input = st.chat_input(placeholder)

        if user_input:
            # Processa a mensagem com a imagem (se houver) ou apenas texto
            process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)
```

**Explicação das Modificações:**

*   **`capture_voice_command()`:** Esta função captura o áudio do microfone, tenta reconhecer a fala e retorna o comando de texto. Ela também exibe mensagens de status na interface do Streamlit.
*   **Botão "🎤 Usar Comando de Voz":** Quando este botão é clicado, ele cria e inicia uma nova thread que executa a função `capture_voice_command`.  O resultado do comando de voz é armazenado em `st.session_state.voice_command`.
*   **Lógica de Processamento Modificada:** A seção de entrada de texto agora verifica se `st.session_state.voice_command` contém um valor. Se sim, ele usa esse valor como a entrada do usuário e chama `process_message`. Depois, ele limpa `st.session_state.voice_command` para evitar que o comando seja processado novamente.
*   **Thread:** Usar uma thread para a captura de voz é crucial porque `recognizer.listen()` é um método bloqueante. Sem uma thread, a interface do Streamlit ficaria congelada enquanto o áudio é capturado.

**Pontos Adicionais:**

*   **Permissões do Microfone:** Certifique-se de que seu navegador tenha permissão para acessar o microfone.
*   **Thread Safety:** Tenha cuidado com a segurança de threads ao modificar `st.session_state`. Streamlit não é inerentemente thread-safe, então evite operações simultâneas que possam causar problemas. Usar `setattr` como mostrado é uma maneira segura de atualizar o estado.
*   **Feedback Visual:** Forneça feedback visual ao usuário para que saiba quando o sistema está gravando, processando ou aguardando um comando de voz.

Com estas modificações, seu `chat_streamlit.py` deve ser capaz de capturar comandos de voz e processá-los da mesma forma que os comandos de texto. Isso adicionará a funcionalidade de controle por voz que você deseja.

User: eu ja tenho uma aplicação de comando de voz dentro da pasta voice_assistent.
Assistant: Entendi. Em vez de criar uma nova função de captura de voz, podemos integrar a sua aplicação existente `voice_assistent` ao `chat_streamlit.py`. Isso envolve principalmente reutilizar as funções de captura e interpretação de comandos da sua aplicação `voice_assistent` existente e integrá-las na interface do Streamlit.

Aqui estão os passos detalhados:

**1. Importar as Funções Necessárias de `voice_assistent`:**

No topo do seu arquivo `chat_streamlit.py`, importe as funções que você precisa de `voice_assistent.main`:

```python
from voice_assistent.class_voice_assistent.main import MainApp
from voice_assistent.class_voice_assistent.gpt_communication.gemini_gpt import GenerativeModelHandler
import threading
import streamlit as st
```

**2. Inicializar a Aplicação `voice_assistent`:**

Você precisará inicializar a aplicação `voice_assistent` uma vez, preferencialmente fora do loop principal do Streamlit, para que não seja reinicializada a cada interação.

```python
# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Inicializa a aplicação de voz
@st.cache_resource
def get_voice_app():
    model = GenerativeModelHandler('gemini-1.5-flash')
    return MainApp(model)

voice_app = get_voice_app()
```

**3. Modificar o Botão de Comando de Voz:**

Modifique o botão na barra lateral para usar a função de captura de voz da sua aplicação `voice_assistent`. Em vez de criar uma nova thread, você chamará a função `handle_command` da sua `voice_app` diretamente.

```python
with st.sidebar:
    # ... (seu código existente da barra lateral)

    if st.button("🎤 Usar Comando de Voz"):
        st.session_state.voice_command = None  # Limpa o comando anterior
        st.session_state.voice_thread = threading.Thread(target=lambda: setattr(st.session_state, 'voice_command', voice_app.handle_command(voice_app.voice_handler.capture_voice_command())))
        st.session_state.voice_thread.start()
```

**4. Modificar a Lógica de Processamento de Mensagens:**

Modifique a lógica de processamento de mensagens para verificar se há um comando de voz capturado. Se houver, use esse comando como entrada para a função `process_message`.

```python
# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # ... (seu código existente para determinar o placeholder)

    # Verifica se um comando de voz foi capturado
    if 'voice_command' in st.session_state and st.session_state.voice_command:
        user_input = st.session_state.voice_command
        process_message(user_input, current_image)
        st.session_state.voice_command = None  # Limpa o comando para não repetir

    else:
        user_input = st.chat_input(placeholder)

        if user_input:
            # Processa a mensagem com a imagem (se houver) ou apenas texto
            process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)
```

**Explicação das Modificações:**

*   **Importação de `MainApp`:** Importe a classe `MainApp` da sua aplicação `voice_assistent`.
*   **Inicialização de `MainApp`:** Inicialize a aplicação `MainApp` usando o modelo `GenerativeModelHandler`. Isso deve ser feito apenas uma vez.
*   **Botão "🎤 Usar Comando de Voz":** Quando este botão é clicado, ele chama `voice_app.run()` para iniciar a captura de voz.
*   **Lógica de Processamento Modificada:** A seção de entrada de texto agora verifica se `st.session_state.voice_command` contém um valor. Se sim, ele usa esse valor como a entrada do usuário e chama `process_message`. Depois, ele limpa `st.session_state.voice_command` para evitar que o comando seja processado novamente.

**Considerações Importantes:**

*   **Cache:** Para garantir que a `MainApp` e o `GenerativeModelHandler` sejam inicializados apenas uma vez, use `@st.cache_resource`. Isso evita a reinicialização a cada interação.
*   **`handle_command`:** A função `handle_command` deve retornar o texto da resposta. Este texto é então usado como entrada para `process_message`.
*   **State Management:** O uso de `st.session_state` permite manter o estado entre as execuções da aplicação Streamlit.

Com estas modificações, seu `chat_streamlit.py` deve ser capaz de capturar comandos de voz usando a aplicação `voice_assistent` e processá-los da mesma forma que os comandos de texto. Isso adicionará a funcionalidade de controle por voz que você deseja.

User: refatore a minha aplicação inteira com as boas práticas de POO e SOLID.
Assistant: ```python
# app_config\app_config.py
from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()

    def get_root_path(self):
        return str(self.ROOT_PATH)

    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py
from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None, transcription_config=None):
        super().__init__(root_path)
        self.transcription_config = transcription_config if transcription_config else TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = self.transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])

    def get_audio_input_path(self):
        return self.AUDIO_INPUT_PATH

    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# audio_to_text\audio_to_text.py
import whisper
from audio_to_text.audio_config.audio_config import AudioConfig
from abc import ABC, abstractmethod
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class IAudioProcessor(ABC):
    @abstractmethod
    def process_audio_files(self):
        pass

class AudioToConverter(IAudioProcessor):
    def __init__(self, audio_config: AudioConfig, whisper_model_name="base"):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.get_audio_input_path()
        self.TRANSCRIPTION_INPUT_PATH = audio_config.get_transcription_input_path()
        self.model = whisper.load_model(whisper_model_name)
        self.logger = logging.getLogger(__name__)

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            self.logger.info(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                self.logger.info(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, self.model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            self.logger.info(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            self.logger.error(f"Erro ao processar o arquivo {audio_file_path}: {e}", exc_info=True)

# chat_app\chat_streamlit.py
import streamlit as st
import time
from datetime import datetime
from PIL import Image
import os
import io
from dotenv import load_dotenv
import threading  # Importa o módulo threading
from google import genai
from google.genai import types

# Core Components
from chat_app.config.config import Config
from chat_app.core.handlers.gemini_handler import GeminiHandler
from chat_app.core.rate_limiter import RateLimiter

# Services
from chat_app.services.search_files import ler_todos_arquivos_python

# Submodules (Separadas para melhor organização)
from chat_app.modules.image_handling import ImageHandler
from chat_app.modules.voice_input import VoiceInputHandler
from chat_app.modules.message_handling import MessageHandler
from chat_app.modules.image_generation import ImageGenerator


class ChatApp:
    def __init__(self):
        load_dotenv()  # Carrega as variáveis de ambiente
        Config.ensure_directories()  # Garante que os diretórios necessários existam

        self.rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

        self.gemini_handler = GeminiHandler("gemini-2.0-flash-exp")  # Inicializa GeminiHandler
        self.image_handler = ImageHandler()
        self.voice_input_handler = VoiceInputHandler()
        self.message_handler = MessageHandler()
        self.image_generator = ImageGenerator()

        self.initialize_session_state()
        self.load_chat_prompt()

    def initialize_session_state(self):
        """Inicializa os estados do session_state."""
        default_states = {
            "messages": [],
            "processing": False,
            "uploaded_image": None,
            "clipboard_image_preview": None,
            "clipboard_image_file": None,
            "last_message_time": 0,
            "file_uploader_key": "uploader_0",
            "generated_image": None,
            "image_prompt": None,
            "voice_command": None  # Estado para armazenar comandos de voz
        }

        for key, default_value in default_states.items():
            if key not in st.session_state:
                st.session_state[key] = default_value

    def load_chat_prompt(self):
        """Carrega o prompt do chat e adiciona o contexto do código fonte."""
        codigo_fonte = ler_todos_arquivos_python()
        self.chat_prompt = f"{self.load_prompt_from_file()}\n\nContexto:\n\n{codigo_fonte}"

    def load_prompt_from_file(self):
        """Carrega o prompt base do arquivo."""
        try:
            with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
                return file.read().strip()
        except FileNotFoundError:
            return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

    def process_message(self, user_input, image_data=None, generated_image=None):
        """Processa a mensagem do usuário (com ou sem imagem)."""
        st.session_state.processing = True  # Marca como processando para bloquear novos inputs
        st.session_state.current_prompt = user_input
        st.session_state.current_image = image_data
        st.session_state.current_generated_image = generated_image

        st.rerun()  # Força a reexecução para atualizar a UI e mostrar o indicador de processamento

    def execute_processing(self):
        """Executa o processamento da mensagem."""
        user_input = st.session_state.current_prompt
        image_data = st.session_state.current_image
        generated_image = st.session_state.current_generated_image

        self.rate_limiter.wait_for_slot()  # Garante que não exceda o limite de requisições

        current_time = time.time()
        time_since_last_message = current_time - st.session_state.last_message_time
        wait_time = max(0, 2 - time_since_last_message)
        time.sleep(wait_time)

        st.session_state.last_message_time = time.time()

        img_path = None
        img_display = None

        if image_data:
            img_path, img_display = self.image_handler.save_uploaded_image(image_data)
            st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
        elif generated_image:
            img_path = self.image_handler.save_generated_image(generated_image)
            st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
        else:
            st.session_state.messages.append({"role": "user", "content": user_input})

        self.message_handler.limit_message_history(st.session_state.messages)

        full_prompt = self.chat_prompt + "\n\n"  # Start with the base prompt
        full_prompt = self.message_handler.build_full_prompt(full_prompt, st.session_state.messages, user_input)

        try:
            if img_path:
                response = self.gemini_handler.generate_content(img_path, full_prompt)
            else:
                response = self.gemini_handler.generate_content(None, full_prompt)
        except Exception as e:
            response = f"❌ Erro ao gerar resposta: {str(e)}"

        st.session_state.messages.append({"role": "assistant", "content": response})
        self.message_handler.limit_message_history(st.session_state.messages)

        if img_path and os.path.exists(img_path):
            os.remove(img_path)

        st.session_state.processing = False
        st.session_state.current_prompt = None
        st.session_state.current_image = None
        st.session_state.current_generated_image = None

    def run(self):
        """Função principal para rodar a aplicação Streamlit."""
        # Executa o processamento se estiver na fila
        if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
            self.execute_processing()
            st.rerun()

        # Configuração da barra lateral
        with st.sidebar:
            st.title("Chat IA Inteligente")

            # Seção de geração de imagem
            st.markdown("### Gerar Imagem")
            image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
            if st.button("Gerar Imagem"):
                if image_prompt:
                    generated_image = self.image_generator.generate_image(image_prompt)
                    if generated_image:
                        st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                        st.session_state.generated_image = None  # Limpa para não exibir em cima

                        st.rerun()
                else:
                    st.warning("Por favor, digite um prompt para gerar a imagem.")

            # Seção de imagens (sempre visível)
            st.markdown("### Adicionar Imagem (Opcional)")
            st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

            # Layout em duas colunas para os botões de imagem
            col1, col2 = st.columns(2)

            with col1:
                # Botão para verificar a área de transferência
                if st.button("📋 Colar", use_container_width=True):
                    if self.image_handler.on_paste_click():
                        st.success("Imagem colada!")
                        st.rerun()
                    else:
                        st.warning("Nada encontrado.")

            with col2:
                # Botão para limpar a imagem atual (se houver)
                if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
                    if st.button("🗑️ Limpar", use_container_width=True):
                        self.image_handler.clear_all_images()
                        st.rerun()
                else:
                    # Placeholder para manter o layout alinhado
                    st.write("")

            # Uploader de imagem com chave dinâmica
            uploaded_file = st.file_uploader(
                "📷 Ou faça upload de imagem",
                type=["png", "jpg", "jpeg"],
                label_visibility="visible",
                key=st.session_state.file_uploader_key
            )

            # Atualiza o estado da imagem quando um arquivo é carregado
            if uploaded_file:
                st.session_state.uploaded_image = uploaded_file
                self.image_handler.on_file_upload()
                st.success("Imagem carregada!")

            # Exibe a imagem selecionada na barra lateral
            self.image_handler.display_selected_image()

            st.markdown("---")

             # Adiciona botão para comando de voz
            if st.button("🎤 Usar Comando de Voz"):
                st.session_state.voice_thread = threading.Thread(target=self.voice_input_handler.capture_voice_command)
                st.session_state.voice_thread.start()

            # Botão para limpar o histórico de conversa
            if st.button("🧹 Limpar conversa", use_container_width=True):
                st.session_state.messages = []
                st.rerun()

            st.caption("Desenvolvido com Streamlit e Gemini AI")

        # Exibição do histórico de mensagens
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                # Se houver imagem, exiba-a (se armazenada)
                if message.get("image"):
                    st.image(message["image"], use_container_width=True)
                # Exibe o conteúdo da mensagem (texto)
                st.markdown(message["content"])

        # Adiciona indicador de digitação quando estiver processando
        if st.session_state.processing:
            with st.chat_message("assistant"):
                st.markdown("Gerando resposta...")

        # Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
        if not st.session_state.processing:
            # Verifica se há uma imagem disponível
            current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

            # Adapta o placeholder com base na presença de imagem
            if current_image:
                placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
            else:
                placeholder = "Digite sua mensagem..."

            # Verifica se há um comando de voz
            if st.session_state.get('voice_command'):
                user_input = st.session_state.voice_command
                st.session_state.voice_command = None  # Limpa o comando
                self.process_message(user_input, current_image)
            else:
                user_input = st.chat_input(placeholder)

                if user_input:
                    # Processa a mensagem com a imagem (se houver) ou apenas texto
                    self.process_message(user_input, current_image)
        else:
            st.chat_input("Aguarde o processamento...", disabled=True)


if __name__ == "__main__":
    app = ChatApp()
    app.run()

# chat_app\config\config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent

    ASSETS_DIR = BASE_DIR / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"

    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"

    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"

    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"

    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}

    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR,
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py
from chat_app.services.gpt_services import GenerativeModelHandler
from chat_app.core.logger_config import logger
from chat_app.core.rate_limiter import RateLimiter

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py
import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py
import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0]
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py
from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from chat_app.config.config import Config
import os
from chat_app.core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from chat_app.core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py
import os
import time
import shutil
import json
from chat_app.config.config import Config
from chat_app.services.gpt_services import GenerativeModelHandler
from chat_app.services.document_service import DocumentService
from chat_app.services.markdown_service import MarkdownService
from chat_app.utils.file_utils import list_images
from chat_app.core.logger_config import logger
from chat_app.core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(
User: Preciso garantir que seja enviado todo o código e preciso eliminar partes redundantes e dispensáveis sem perder funcionalidades.
Assistant: Ok, aqui está a refatoração do código com boas práticas de POO e SOLID, eliminando redundâncias e garantindo que todas as funcionalidades sejam mantidas. O código foi estruturado para melhorar a organização e facilitar a manutenção.

```python
# app_config\app_config.py
from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()

    def get_root_path(self):
        return str(self.ROOT_PATH)

    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py
from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None, transcription_config=None):
        super().__init__(root_path)
        self.transcription_config = transcription_config if transcription_config else TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = self.transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])

    def get_audio_input_path(self):
        return self.AUDIO_INPUT_PATH

    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# audio_to_text\audio_to_text.py
import whisper
from audio_to_text.audio_config.audio_config import AudioConfig
from abc import ABC, abstractmethod
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class IAudioProcessor(ABC):
    @abstractmethod
    def process_audio_files(self):
        pass

class AudioToConverter(IAudioProcessor):
    def __init__(self, audio_config: AudioConfig, whisper_model_name="base"):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.get_audio_input_path()
        self.TRANSCRIPTION_INPUT_PATH = audio_config.get_transcription_input_path()
        self.model = whisper.load_model(whisper_model_name)
        self.logger = logging.getLogger(__name__)

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            self.logger.info(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                self.logger.info(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, self.model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            self.logger.info(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            self.logger.error(f"Erro ao processar o arquivo {audio_file_path}: {e}", exc_info=True)

# chat_app\chat_streamlit.py
import streamlit as st
import time
from datetime import datetime
from PIL import Image
import os
import io
from dotenv import load_dotenv
import threading  # Importa o módulo threading
from google import genai
from google.genai import types

# Core Components
from chat_app.config.config import Config
from chat_app.core.handlers.gemini_handler import GeminiHandler
from chat_app.core.rate_limiter import RateLimiter

# Services
from chat_app.services.search_files import ler_todos_arquivos_python

# Submodules (Separadas para melhor organização)
from chat_app.modules.image_handling import ImageHandler
from chat_app.modules.voice_input import VoiceInputHandler
from chat_app.modules.message_handling import MessageHandler
from chat_app.modules.image_generation import ImageGenerator


class ChatApp:
    def __init__(self):
        load_dotenv()  # Carrega as variáveis de ambiente
        Config.ensure_directories()  # Garante que os diretórios necessários existam

        self.rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

        self.gemini_handler = GeminiHandler("gemini-2.0-flash-exp")  # Inicializa GeminiHandler
        self.image_handler = ImageHandler(self)  # Passa a instância de ChatApp
        self.voice_input_handler = VoiceInputHandler(self)  # Passa a instância de ChatApp
        self.message_handler = MessageHandler()
        self.image_generator = ImageGenerator()

        self.initialize_session_state()
        self.load_chat_prompt()

    def initialize_session_state(self):
        """Inicializa os estados do session_state."""
        default_states = {
            "messages": [],
            "processing": False,
            "uploaded_image": None,
            "clipboard_image_preview": None,
            "clipboard_image_file": None,
            "last_message_time": 0,
            "file_uploader_key": "uploader_0",
            "generated_image": None,
            "image_prompt": None,
            "voice_command": None  # Estado para armazenar comandos de voz
        }

        for key, default_value in default_states.items():
            if key not in st.session_state:
                st.session_state[key] = default_value

    def load_chat_prompt(self):
        """Carrega o prompt do chat e adiciona o contexto do código fonte."""
        codigo_fonte = ler_todos_arquivos_python()
        self.chat_prompt = f"{self.load_prompt_from_file()}\n\nContexto:\n\n{codigo_fonte}"

    def load_prompt_from_file(self):
        """Carrega o prompt base do arquivo."""
        try:
            with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
                return file.read().strip()
        except FileNotFoundError:
            return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

    def process_message(self, user_input, image_data=None, generated_image=None):
        """Processa a mensagem do usuário (com ou sem imagem)."""
        st.session_state.processing = True  # Marca como processando para bloquear novos inputs
        st.session_state.current_prompt = user_input
        st.session_state.current_image = image_data
        st.session_state.current_generated_image = generated_image

        st.rerun()  # Força a reexecução para atualizar a UI e mostrar o indicador de processamento

    def execute_processing(self):
        """Executa o processamento da mensagem."""
        user_input = st.session_state.current_prompt
        image_data = st.session_state.current_image
        generated_image = st.session_state.current_generated_image

        self.rate_limiter.wait_for_slot()  # Garante que não exceda o limite de requisições

        current_time = time.time()
        time_since_last_message = current_time - st.session_state.last_message_time
        wait_time = max(0, 2 - time_since_last_message)
        time.sleep(wait_time)

        st.session_state.last_message_time = time.time()

        img_path = None
        img_display = None

        if image_data:
            img_path, img_display = self.image_handler.save_uploaded_image(image_data)
            st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
        elif generated_image:
            img_path = self.image_handler.save_generated_image(generated_image)
            st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
        else:
            st.session_state.messages.append({"role": "user", "content": user_input})

        self.message_handler.limit_message_history(st.session_state.messages)

        full_prompt = self.chat_prompt + "\n\n"  # Start with the base prompt
        full_prompt = self.message_handler.build_full_prompt(full_prompt, st.session_state.messages, user_input)

        try:
            if img_path:
                response = self.gemini_handler.generate_content(img_path, full_prompt)
            else:
                response = self.gemini_handler.generate_content(None, full_prompt)
        except Exception as e:
            response = f"❌ Erro ao gerar resposta: {str(e)}"

        st.session_state.messages.append({"role": "assistant", "content": response})
        self.message_handler.limit_message_history(st.session_state.messages)

        if img_path and os.path.exists(img_path):
            os.remove(img_path)

        st.session_state.processing = False
        st.session_state.current_prompt = None
        st.session_state.current_image = None
        st.session_state.current_generated_image = None

    def run(self):
        """Função principal para rodar a aplicação Streamlit."""
        # Executa o processamento se estiver na fila
        if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
            self.execute_processing()
            st.rerun()

        # Configuração da barra lateral
        with st.sidebar:
            st.title("Chat IA Inteligente")

            # Seção de geração de imagem
            st.markdown("### Gerar Imagem")
            image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
            if st.button("Gerar Imagem"):
                if image_prompt:
                    generated_image = self.image_generator.generate_image(image_prompt)
                    if generated_image:
                        st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                        st.session_state.generated_image = None  # Limpa para não exibir em cima

                        st.rerun()
                else:
                    st.warning("Por favor, digite um prompt para gerar a imagem.")

            # Seção de imagens (sempre visível)
            st.markdown("### Adicionar Imagem (Opcional)")
            st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

            # Layout em duas colunas para os botões de imagem
            col1, col2 = st.columns(2)

            with col1:
                # Botão para verificar a área de transferência
                if st.button("📋 Colar", use_container_width=True):
                    if self.image_handler.on_paste_click():
                        st.success("Imagem colada!")
                        st.rerun()
                    else:
                        st.warning("Nada encontrado.")

            with col2:
                # Botão para limpar a imagem atual (se houver)
                if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
                    if st.button("🗑️ Limpar", use_container_width=True):
                        self.image_handler.clear_all_images()
                        st.rerun()
                else:
                    # Placeholder para manter o layout alinhado
                    st.write("")

            # Uploader de imagem com chave dinâmica
            uploaded_file = st.file_uploader(
                "📷 Ou faça upload de imagem",
                type=["png", "jpg", "jpeg"],
                label_visibility="visible",
                key=st.session_state.file_uploader_key
            )

            # Atualiza o estado da imagem quando um arquivo é carregado
            if uploaded_file:
                st.session_state.uploaded_image = uploaded_file
                self.image_handler.on_file_upload()
                st.success("Imagem carregada!")

            # Exibe a imagem selecionada na barra lateral
            self.image_handler.display_selected_image()

            st.markdown("---")

            # Adiciona botão para comando de voz
            if st.button("🎤 Usar Comando de Voz"):
                st.session_state.voice_thread = threading.Thread(target=self.voice_input_handler.capture_voice_command)
                st.session_state.voice_thread.start()

            # Botão para limpar o histórico de conversa
            if st.button("🧹 Limpar conversa", use_container_width=True):
                st.session_state.messages = []
                st.rerun()

            st.caption("Desenvolvido com Streamlit e Gemini AI")

        # Exibição do histórico de mensagens
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                # Se houver imagem, exiba-a (se armazenada)
                if message.get("image"):
                    st.image(message["image"], use_container_width=True)
                # Exibe o conteúdo da mensagem (texto)
                st.markdown(message["content"])

        # Adiciona indicador de digitação quando estiver processando
        if st.session_state.processing:
            with st.chat_message("assistant"):
                st.markdown("Gerando resposta...")

        # Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
        if not st.session_state.processing:
            # Verifica se há uma imagem disponível
            current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

            # Adapta o placeholder com base na presença de imagem
            if current_image:
                placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
            else:
                placeholder = "Digite sua mensagem..."

            # Verifica se há um comando de voz
            if st.session_state.get('voice_command'):
                user_input = st.session_state.voice_command
                st.session_state.voice_command = None  # Limpa o comando
                self.process_message(user_input, current_image)
            else:
                user_input = st.chat_input(placeholder)

                if user_input:
                    # Processa a mensagem com a imagem (se houver) ou apenas texto
                    self.process_message(user_input, current_image)
        else:
            st.chat_input("Aguarde o processamento...", disabled=True)


if __name__ == "__main__":
    app = ChatApp()
    app.run()

# chat_app\config\config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent

    ASSETS_DIR = BASE_DIR / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"

    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"

    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"

    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"

    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}

    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR,
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py
from chat_app.services.gpt_services import GenerativeModelHandler
from chat_app.core.logger_config import logger
from chat_app.core.rate_limiter import RateLimiter

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py
import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py
import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0]
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py
from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from chat_app.config.config import Config
import os
from chat_app.core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from chat_app.core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py
import os
import time
import shutil
import json
from chat_app.config.config import Config
from chat_app.services.gpt_services import GenerativeModelHandler
from chat_app.services.document_service import DocumentService
from chat_app.services.markdown_service import MarkdownService
from chat_app.utils.file_utils import list_images
from chat_app.core.logger_config import logger
from chat_app.core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response
User: Pode enviar apenas um arquivo de cada vez, bem separado  que precisa alterar de cada vez para implementar na interface streamlit o comando de voz ??? 
2025-04-10 20:21:08,041 [INFO] Resposta da IA (texto): Ok, aqui está o arquivo `chat_app\chat_streamlit.py` com as modificações necessárias para integrar o comando de voz, seguindo as melhores práticas de POO e SOLID. As modificações estão concentradas e explicadas para facilitar a implementação.

```python
# chat_app\chat_streamlit.py
import streamlit as st
import time
from datetime import datetime
from PIL import Image
import os
import io
from dotenv import load_dotenv
import threading  # Importa o módulo threading
from google import genai
from google.genai import types

# Core Components
from chat_app.config.config import Config
from chat_app.core.handlers.gemini_handler import GeminiHandler
from chat_app.core.rate_limiter import RateLimiter

# Services
from chat_app.services.search_files import ler_todos_arquivos_python

# Submodules (Separadas para melhor organização)
from chat_app.modules.image_handling import ImageHandler
from chat_app.modules.voice_input import VoiceInputHandler
from chat_app.modules.message_handling import MessageHandler
from chat_app.modules.image_generation import ImageGenerator


class ChatApp:
    def __init__(self):
        load_dotenv()  # Carrega as variáveis de ambiente
        Config.ensure_directories()  # Garante que os diretórios necessários existam

        self.rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

        self.gemini_handler = GeminiHandler("gemini-2.0-flash-exp")  # Inicializa GeminiHandler
        self.image_handler = ImageHandler(self)  # Passa a instância de ChatApp
        self.voice_input_handler = VoiceInputHandler(self)  # Passa a instância de ChatApp
        self.message_handler = MessageHandler()
        self.image_generator = ImageGenerator()

        self.initialize_session_state()
        self.load_chat_prompt()

    def initialize_session_state(self):
        """Inicializa os estados do session_state."""
        default_states = {
            "messages": [],
            "processing": False,
            "uploaded_image": None,
            "clipboard_image_preview": None,
            "clipboard_image_file": None,
            "last_message_time": 0,
            "file_uploader_key": "uploader_0",
            "generated_image": None,
            "image_prompt": None,
            "voice_command": None  # Estado para armazenar comandos de voz
        }

        for key, default_value in default_states.items():
            if key not in st.session_state:
                st.session_state[key] = default_value

    def load_chat_prompt(self):
        """Carrega o prompt do chat e adiciona o contexto do código fonte."""
        codigo_fonte = ler_todos_arquivos_python()
        self.chat_prompt = f"{self.load_prompt_from_file()}\n\nContexto:\n\n{codigo_fonte}"

    def load_prompt_from_file(self):
        """Carrega o prompt base do arquivo."""
        try:
            with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
                return file.read().strip()
        except FileNotFoundError:
            return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

    def process_message(self, user_input, image_data=None, generated_image=None):
        """Processa a mensagem do usuário (com ou sem imagem)."""
        st.session_state.processing = True  # Marca como processando para bloquear novos inputs
        st.session_state.current_prompt = user_input
        st.session_state.current_image = image_data
        st.session_state.current_generated_image = generated_image

        st.rerun()  # Força a reexecução para atualizar a UI e mostrar o indicador de processamento

    def execute_processing(self):
        """Executa o processamento da mensagem."""
        user_input = st.session_state.current_prompt
        image_data = st.session_state.current_image
        generated_image = st.session_state.current_generated_image

        self.rate_limiter.wait_for_slot()  # Garante que não exceda o limite de requisições

        current_time = time.time()
        time_since_last_message = current_time - st.session_state.last_message_time
        wait_time = max(0, 2 - time_since_last_message)
        time.sleep(wait_time)

        st.session_state.last_message_time = time.time()

        img_path = None
        img_display = None

        if image_data:
            img_path, img_display = self.image_handler.save_uploaded_image(image_data)
            st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
        elif generated_image:
            img_path = self.image_handler.save_generated_image(generated_image)
            st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
        else:
            st.session_state.messages.append({"role": "user", "content": user_input})

        self.message_handler.limit_message_history(st.session_state.messages)

        full_prompt = self.chat_prompt + "\n\n"  # Start with the base prompt
        full_prompt = self.message_handler.build_full_prompt(full_prompt, st.session_state.messages, user_input)

        try:
            if img_path:
                response = self.gemini_handler.generate_content(img_path, full_prompt)
            else:
                response = self.gemini_handler.generate_content(None, full_prompt)
        except Exception as e:
            response = f"❌ Erro ao gerar resposta: {str(e)}"

        st.session_state.messages.append({"role": "assistant", "content": response})
        self.message_handler.limit_message_history(st.session_state.messages)

        if img_path and os.path.exists(img_path):
            os.remove(img_path)

        st.session_state.processing = False
        st.session_state.current_prompt = None
        st.session_state.current_image = None
        st.session_state.current_generated_image = None

    def run(self):
        """Função principal para rodar a aplicação Streamlit."""
        # Executa o processamento se estiver na fila
        if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
            self.execute_processing()
            st.rerun()

        # Configuração da barra lateral
        with st.sidebar:
            st.title("Chat IA Inteligente")

            # Seção de geração de imagem
            st.markdown("### Gerar Imagem")
            image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
            if st.button("Gerar Imagem"):
                if image_prompt:
                    generated_image = self.image_generator.generate_image(image_prompt)
                    if generated_image:
                        st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                        st.session_state.generated_image = None  # Limpa para não exibir em cima

                        st.rerun()
                else:
                    st.warning("Por favor, digite um prompt para gerar a imagem.")

            # Seção de imagens (sempre visível)
            st.markdown("### Adicionar Imagem (Opcional)")
            st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

            # Layout em duas colunas para os botões de imagem
            col1, col2 = st.columns(2)

            with col1:
                # Botão para verificar a área de transferência
                if st.button("📋 Colar", use_container_width=True):
                    if self.image_handler.on_paste_click():
                        st.success("Imagem colada!")
                        st.rerun()
                    else:
                        st.warning("Nada encontrado.")

            with col2:
                # Botão para limpar a imagem atual (se houver)
                if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
                    if st.button("🗑️ Limpar", use_container_width=True):
                        self.image_handler.clear_all_images()
                        st.rerun()
                else:
                    # Placeholder para manter o layout alinhado
                    st.write("")

            # Uploader de imagem com chave dinâmica
            uploaded_file = st.file_uploader(
                "📷 Ou faça upload de imagem",
                type=["png", "jpg", "jpeg"],
                label_visibility="visible",
                key=st.session_state.file_uploader_key
            )

            # Atualiza o estado da imagem quando um arquivo é carregado
            if uploaded_file:
                st.session_state.uploaded_image = uploaded_file
                self.image_handler.on_file_upload()
                st.success("Imagem carregada!")

            # Exibe a imagem selecionada na barra lateral
            self.image_handler.display_selected_image()

            st.markdown("---")

            # Adiciona botão para comando de voz
            # Modificação 1: Adiciona botão para ativar o comando de voz
            if st.button("🎤 Usar Comando de Voz"):
                st.session_state.voice_thread = threading.Thread(target=self.voice_input_handler.capture_voice_command)
                st.session_state.voice_thread.start()

            # Botão para limpar o histórico de conversa
            if st.button("🧹 Limpar conversa", use_container_width=True):
                st.session_state.messages = []
                st.rerun()

            st.caption("Desenvolvido com Streamlit e Gemini AI")

        # Exibição do histórico de mensagens
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                # Se houver imagem, exiba-a (se armazenada)
                if message.get("image"):
                    st.image(message["image"], use_container_width=True)
                # Exibe o conteúdo da mensagem (texto)
                st.markdown(message["content"])

        # Adiciona indicador de digitação quando estiver processando
        if st.session_state.processing:
            with st.chat_message("assistant"):
                st.markdown("Gerando resposta...")

        # Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
        if not st.session_state.processing:
            # Verifica se há uma imagem disponível
            current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

            # Adapta o placeholder com base na presença de imagem
            if current_image:
                placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
            else:
                placeholder = "Digite sua mensagem..."

            # Modificação 2: Verifica se há um comando de voz
            if st.session_state.get('voice_command'):
                user_input = st.session_state.voice_command
                st.session_state.voice_command = None  # Limpa o comando
                self.process_message(user_input, current_image)
            else:
                user_input = st.chat_input(placeholder)

                if user_input:
                    # Processa a mensagem com a imagem (se houver) ou apenas texto
                    self.process_message(user_input, current_image)
        else:
            st.chat_input("Aguarde o processamento...", disabled=True)


if __name__ == "__main__":
    app = ChatApp()
    app.run()
```

**Explicação das Modificações:**

1.  **Importa o módulo `threading`:**

    *   Certifique-se de que o módulo `threading` está importado no início do arquivo:

    ```python
    import threading
    ```

2.  **Adiciona botão para comando de voz:**

    *   Adiciona um botão na barra lateral para ativar o comando de voz. Este botão inicia uma thread que executa a função `capture_voice_command` da classe `VoiceInputHandler`.

    ```python
    with st.sidebar:
        # ... (seu código existente da barra lateral)

        # Adiciona botão para comando de voz
        if st.button("🎤 Usar Comando de Voz"):
            st.session_state.voice_thread = threading.Thread(target=self.voice_input_handler.capture_voice_command)
            st.session_state.voice_thread.start()
    ```

3.  **Verifica se há um comando de voz:**

    *   Modifica a lógica de processamento de mensagens para verificar se há um comando de voz capturado em `st.session_state.voice_command`. Se houver, usa esse comando como entrada e limpa o estado para evitar repetições.

    ```python
    if not st.session_state.processing:
        # ... (seu código existente para determinar o placeholder)

        # Verifica se há um comando de voz
        if st.session_state.get('voice_command'):
            user_input = st.session_state.voice_command
            st.session_state.voice_command = None  # Limpa o comando
            self.process_message(user_input, current_image)
        else:
            user_input = st.chat_input(placeholder)

            if user_input:
                # Processa a mensagem com a imagem (se houver) ou apenas texto
                self.process_message(user_input, current_image)
    ```

Com estas modificações, o arquivo `chat_streamlit.py` deve estar pronto para receber comandos de voz. Lembre-se de que os arquivos `VoiceInputHandler`, e outras classes devem estar corretamente definidos para que esta integração funcione.

2025-04-10 20:21:08,202 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-10 20:21:08,204 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-10 20:21:08,206 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-10 20:21:08,208 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-10 20:21:08,210 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-10 20:21:08,212 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-10 20:21:08,215 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-10 20:21:08,217 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-10 20:21:08,219 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-10 20:21:08,221 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-10 20:21:08,224 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-10 20:21:08,226 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-10 20:21:08,228 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-10 20:21:08,231 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-10 20:21:08,234 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-10 20:21:08,236 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-10 20:21:08,238 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-10 20:21:08,240 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-10 20:21:08,242 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-10 20:21:08,244 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-10 20:21:08,247 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-10 20:21:08,249 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 20:21:08,251 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-10 20:21:08,253 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-10 20:21:08,255 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-10 20:21:08,257 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-10 20:21:08,258 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-10 20:21:08,261 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-10 20:21:08,263 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-10 20:21:08,266 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-10 20:21:08,267 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-10 20:21:08,269 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-10 20:21:08,270 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-10 20:21:08,273 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-10 20:21:08,275 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-10 20:21:08,277 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-10 20:21:08,279 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-10 20:21:08,281 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-10 20:21:08,283 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-10 20:21:08,285 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-10 20:21:08,287 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-10 20:21:08,289 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-10 20:21:08,291 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-10 20:21:08,294 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-10 20:21:08,296 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-10 20:21:08,299 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-10 20:21:08,301 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-10 20:21:08,302 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-10 20:21:08,304 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-10 20:21:08,306 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-10 20:21:08,308 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-10 20:21:08,310 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-10 20:21:08,313 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-10 20:21:08,316 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-10 20:27:49,306 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-10 20:27:49,308 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-10 20:27:49,309 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-10 20:27:49,311 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-10 20:27:49,312 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-10 20:27:49,314 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-10 20:27:49,315 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-10 20:27:49,316 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-10 20:27:49,319 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-10 20:27:49,321 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-10 20:27:49,326 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-10 20:27:49,328 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-10 20:27:49,330 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-10 20:27:49,332 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-10 20:27:49,338 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-10 20:27:49,341 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-10 20:27:49,343 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-10 20:27:49,345 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-10 20:27:49,348 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-10 20:27:49,349 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-10 20:27:49,350 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-10 20:27:49,355 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 20:27:49,356 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-10 20:27:49,358 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-10 20:27:49,359 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-10 20:27:49,360 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-10 20:27:49,361 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-10 20:27:49,363 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-10 20:27:49,364 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-10 20:27:49,365 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-10 20:27:49,366 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-10 20:27:49,369 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-10 20:27:49,372 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-10 20:27:49,374 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-10 20:27:49,375 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-10 20:27:49,376 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-10 20:27:49,377 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-10 20:27:49,378 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-10 20:27:49,380 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-10 20:27:49,381 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-10 20:27:49,382 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-10 20:27:49,384 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-10 20:27:49,387 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-10 20:27:49,389 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-10 20:27:49,390 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-10 20:27:49,391 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-10 20:27:49,392 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-10 20:27:49,393 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-10 20:27:49,395 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-10 20:27:49,396 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-10 20:27:49,397 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-10 20:27:49,399 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-10 20:27:49,401 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-10 20:27:49,405 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-10 20:27:49,505 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-10 20:27:49,507 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-10 20:27:49,508 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-10 20:27:49,511 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-10 20:27:49,513 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-10 20:27:49,515 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-10 20:27:49,516 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-10 20:27:49,519 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-10 20:27:49,522 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-10 20:27:49,524 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-10 20:27:49,526 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-10 20:27:49,528 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-10 20:27:49,530 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-10 20:27:49,532 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-10 20:27:49,533 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-10 20:27:49,538 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-10 20:27:49,541 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-10 20:27:49,543 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-10 20:27:49,547 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-10 20:27:49,549 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-10 20:27:49,553 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-10 20:27:49,558 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 20:27:49,560 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-10 20:27:49,562 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-10 20:27:49,564 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-10 20:27:49,566 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-10 20:27:49,569 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-10 20:27:49,574 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-10 20:27:49,578 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-10 20:27:49,580 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-10 20:27:49,582 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-10 20:27:49,589 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-10 20:27:49,592 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-10 20:27:49,594 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-10 20:27:49,597 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-10 20:27:49,599 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-10 20:27:49,600 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-10 20:27:49,605 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-10 20:27:49,608 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-10 20:27:49,609 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-10 20:27:49,611 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-10 20:27:49,613 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-10 20:27:49,614 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-10 20:27:49,616 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-10 20:27:49,619 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-10 20:27:49,621 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-10 20:27:49,623 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-10 20:27:49,625 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-10 20:27:49,626 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-10 20:27:49,628 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-10 20:27:49,630 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-10 20:27:49,632 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-10 20:27:49,633 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-10 20:27:49,636 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-10 20:28:19,622 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-10 20:28:19,624 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-10 20:28:19,625 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-10 20:28:19,627 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-10 20:28:19,629 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-10 20:28:19,631 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-10 20:28:19,633 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-10 20:28:19,635 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-10 20:28:19,637 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-10 20:28:19,640 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-10 20:28:19,643 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-10 20:28:19,645 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-10 20:28:19,647 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-10 20:28:19,649 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-10 20:28:19,650 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-10 20:28:19,652 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-10 20:28:19,656 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-10 20:28:19,658 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-10 20:28:19,661 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-10 20:28:19,663 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-10 20:28:19,664 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-10 20:28:19,665 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 20:28:19,667 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-10 20:28:19,668 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-10 20:28:19,670 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-10 20:28:19,674 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-10 20:28:19,676 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-10 20:28:19,678 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-10 20:28:19,679 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-10 20:28:19,680 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-10 20:28:19,682 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-10 20:28:19,684 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-10 20:28:19,685 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-10 20:28:19,687 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-10 20:28:19,691 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-10 20:28:19,693 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-10 20:28:19,694 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-10 20:28:19,696 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-10 20:28:19,697 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-10 20:28:19,699 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-10 20:28:19,700 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-10 20:28:19,702 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-10 20:28:19,703 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-10 20:28:19,707 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-10 20:28:19,709 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-10 20:28:19,711 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-10 20:28:19,712 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-10 20:28:19,714 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-10 20:28:19,715 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-10 20:28:19,717 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-10 20:28:19,718 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-10 20:28:19,720 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-10 20:28:19,723 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-10 20:28:19,726 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-10 20:28:19,850 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-10 20:28:19,851 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-10 20:28:19,853 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-10 20:28:19,856 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-10 20:28:19,859 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-10 20:28:19,861 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-10 20:28:19,863 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-10 20:28:19,865 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-10 20:28:19,867 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-10 20:28:19,869 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-10 20:28:19,873 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-10 20:28:19,875 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-10 20:28:19,877 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-10 20:28:19,879 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-10 20:28:19,881 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-10 20:28:19,882 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-10 20:28:19,884 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-10 20:28:19,885 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-10 20:28:19,889 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-10 20:28:19,892 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-10 20:28:19,893 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-10 20:28:19,895 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 20:28:19,896 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-10 20:28:19,898 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-10 20:28:19,899 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-10 20:28:19,901 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-10 20:28:19,903 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-10 20:28:19,906 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-10 20:28:19,908 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-10 20:28:19,910 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-10 20:28:19,911 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-10 20:28:19,913 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-10 20:28:19,914 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-10 20:28:19,916 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-10 20:28:19,918 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-10 20:28:19,919 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-10 20:28:19,922 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-10 20:28:19,923 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-10 20:28:19,926 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-10 20:28:19,928 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-10 20:28:19,929 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-10 20:28:19,931 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-10 20:28:19,932 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-10 20:28:19,934 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-10 20:28:19,935 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-10 20:28:19,937 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-10 20:28:19,940 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-10 20:28:19,941 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-10 20:28:19,943 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-10 20:28:19,945 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-10 20:28:19,946 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-10 20:28:19,948 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-10 20:28:19,949 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-10 20:28:19,951 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-10 20:28:19,953 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diretório atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Definição dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Criação dos diretórios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configuração do layout do Streamlit
st.set_page_config(
    page_title="Análise de Ações",
    layout="wide",
    page_icon="📈"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscilações

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Período', 'Oscilação'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("📊 Dashboard de Análise de Ações")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma ação", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"📌 Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("🔍 Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descrição', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa ação.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("💰 Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("📉 Oscilações")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("📈 Tabela Geral de Ações")
        st.dataframe(self.acoes)

# Execução
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para análise fundamentalista de ações da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from config.common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"Não foram encontrados arquivos .txt no diretório {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conteúdo do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conteúdo HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conteúdo HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se está correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # Configuração de vídeos
    video_config = VideoConfig(root_path=root_path)
    video_processor = VideoToAudioConverter(video_config=video_config)
    video_processor.process_videos()
    
    # # Configuração de áudios
    audio_config = AudioConfig(root_path=root_path)
    audio_processor = AudioToConverter(audio_config=audio_config)
    audio_processor.process_audio_files()
    
    # # Processamento de transcrições e envio de embeddings
    # embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    # embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diretório
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diretório.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do último arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conteúdo do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimensões do embedding:", embedding.shape)

# Caminho do diretório de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o último embedding
load_and_print_embedding(embedding_directory)


# test_whisper.py

from transformers import pipeline
from common_paths import CommonPaths

commmom_paths = CommonPaths()

# Função para ler a transcrição de um arquivo
def read_transcription(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()
    return lines

# Função para converter a pontuação em estrelas
def score_to_stars(score):
    if score >= 0.75:
        return 5
    elif score >= 0.55:
        return 4
    elif score >= 0.35:
        return 3
    elif score >= 0.15:
        return 2
    else:
        return 1

# Inicializar o pipeline de análise de sentimento
sentiment_pipeline = pipeline("sentiment-analysis", model="neuralmind/bert-base-portuguese-cased")

# Caminho para o arquivo de transcrição
file_path = commmom_paths.TRANSCRIPTION_OUTPUT_PATH / 'feedback.txt'

# Ler a transcrição
lines = read_transcription(file_path)

# Analisar sentimento para cada feedback
for line in lines:
    sentiment = sentiment_pipeline(line.strip())
    score = sentiment[0]['score']
    stars = score_to_stars(score)
    
    # Ajuste manual para feedbacks com aspectos positivos e negativos
    if any(word in line.lower() for word in ["mas", "porém", "no entanto"]):
        stars = max(1, stars - 1)  # Reduz uma estrela se houver contradição
    
    # Ajuste adicional baseado em palavras-chave
    positive_keywords = ["excelente", "ótimo", "perfeito", "rápido", "eficiente", "satisfeito", "clara", "eficaz", "bom trabalho"]
    negative_keywords = ["ruim", "péssimo", "horrível", "poderia", "não resolveu", "insatisfatória", "confusa", "inadequado", "lento", "não houve seguimento", "não atendeu", "não funcionou", "demorado"]
    
    positive_count = sum(word in line.lower() for word in positive_keywords)
    negative_count = sum(word in line.lower() for word in negative_keywords)
    
    # Ajuste de estrelas baseado na contagem de palavras-chave
    if positive_count > negative_count:
        stars = min(5, stars + 1)
    elif negative_count > positive_count:
        stars = max(1, stars - 1)
    
    print(f"Texto: {line.strip()}")
    print(f"Sentimento: {stars} estrelas")
    print(f"Pontuação: {score:.2f}")
    print('---')


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configuração de transcrições e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diretório onde está procurando as transcrições
        print(f"Diretório de entrada das transcrições: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcrição no diretório de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcrição encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo não encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcrição do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} está vazio.")
                    return

            # Gerar o embedding da transcrição
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcrição e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de áudio
    with sr.Microphone() as source:
        print("Diga algo em português...")

        while True:
            try:
                # Escuta o áudio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {texto_portugues}")

                # Traduz o texto para o inglês
                traducao = translator.translate(texto_portugues)
                print(f"Tradução para o inglês: {traducao}")

            except sr.UnknownValueError:
                print("Não foi possível entender o áudio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usuário")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configurações de áudio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de áudio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em português...")

try:
    audio_buffer = []

    while True:
        # Lê dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o áudio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o áudio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradução
            print(f"Tradução para o inglês: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usuário")

    # Fecha o stream de áudio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O vídeo {video_path} não contém áudio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de vídeo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de vídeo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de vídeo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Conversão de vídeo para áudio concluída!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configurações da API
handler = genai('gemini-1.5-flash')

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Função para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conteúdo com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares únicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcrição obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' não é uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuniões: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de reuniões: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|tópicos da|pontos (relevantes|principais)|análise)\b.*\b(reunião|última (reunião|conversa|sessão))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como análise de reunião.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padrão.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que são muito similares ao comando atual para evitar redundância.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padrão: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # Métodos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de análise de reunião.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conteúdo usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inválida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar variáveis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conteúdo com base no prompt, contexto e reunião"""
        try:
            # Supondo que a API espera um dicionário com os parâmetros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisição para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar variáveis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Criação da conclusão do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que você acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["faça um resumo da última reunião.", "tópicos da última reunião", "resuma a última reunião", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Você é responsável por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - Não acrescentar título nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informações acima e forneça uma solução detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Você é responsável analisar com detalhes a reunião de [str_meeting], e fornecer uma longa estória sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precisão da resposta.
            - Não acrescente título nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e faça uma pesquisa online para uma resposta mais precisa das questões de [str_text]
        - Não acrescente título nas respostas.
        
        ------
        [context]: Regras Básicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("Áudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum áudio detectado.")
                return None
            except sr.UnknownValueError:
                print("Não foi possível entender o áudio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configurações da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicionário
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta não está no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API: {e}")
        return None

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: Analise meus códigos e me envie as partes que posso eliminar e os códigos duplicados.
2025-04-10 20:28:25,464 [INFO] Resposta da IA (texto): Com base no seu pedido, analisei os códigos fornecidos e identifiquei as seguintes partes que podem ser eliminadas e os códigos duplicados:

**Partes que podem ser eliminadas:**

*   **Arquivos não utilizados:**
    *   `fundamentus_api\setup.py`: Este arquivo é usado para configurar e instalar o pacote `fundamentalvision`. Se você já instalou o pacote e não pretende distribuí-lo, pode remover este arquivo.
    *   `translate\translator_to_english.py`: Este arquivo contém código para traduzir voz do português para o inglês. Se você não precisa dessa funcionalidade, pode remover este arquivo.
    *    `voice_assistent\template.py`: Parece ser um modelo ou código de teste. Se não for usado, pode ser removido.
    *   `test_whisper.py`: Este arquivo parece ser um script de teste para análise de sentimento usando o modelo "neuralmind/bert-base-portuguese-cased". Se você não precisa dessa funcionalidade, pode remover este arquivo.
    *   `send_embeddings_database\verify_last_enbedding.py`: Este arquivo parece ser um script para carregar e imprimir o último embedding salvo em um diretório. Se você não precisa dessa funcionalidade, pode remover este arquivo.

*   **Código comentado ou desnecessário:**
    *   Em vários arquivos, procure por blocos de código comentados que não são mais necessários e podem ser removidos.
    *   Verifique se há logs excessivos ou mensagens de depuração que podem ser removidas após a conclusão do desenvolvimento.

**Códigos duplicados:**

*   **Funções de extração de dados JSON:**
    *   As funções `extract_values_from_json` estão definidas em `voice_assistent\template.py` e `voice_assistent\class_voice_assistent\text_processor.py`. Você deve manter apenas uma definição dessa função em um local comum e reutilizá-la onde for necessário.
*    **Carregar variáveis do ambiente**
    *   O modelo de GenerativeModelHandler carrega e configura a API em `chat_app\services\gpt_services.py` e  `voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py`.

**Observações:**

*   Antes de eliminar qualquer código, certifique-se de fazer um backup para evitar a perda de funcionalidades importantes.
*   Remova arquivos e partes de código desnecessárias para manter o projeto organizado e eficiente.
*   Considere refatorar códigos duplicados para melhorar a manutenção e evitar erros.
2025-04-10 20:28:25,647 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-10 20:28:25,649 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-10 20:28:25,650 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-10 20:28:25,652 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-10 20:28:25,653 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-10 20:28:25,658 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-10 20:28:25,660 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-10 20:28:25,661 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-10 20:28:25,663 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-10 20:28:25,664 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-10 20:28:25,666 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-10 20:28:25,667 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-10 20:28:25,669 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-10 20:28:25,670 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-10 20:28:25,674 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-10 20:28:25,675 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-10 20:28:25,677 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-10 20:28:25,678 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-10 20:28:25,679 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-10 20:28:25,680 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-10 20:28:25,682 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-10 20:28:25,683 [INFO] Arquivo lido com sucesso: main.py
2025-04-10 20:28:25,685 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-10 20:28:25,686 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-10 20:28:25,688 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-10 20:28:25,692 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-10 20:28:25,693 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-10 20:28:25,694 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-10 20:28:25,696 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-10 20:28:25,697 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-10 20:28:25,698 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-10 20:28:25,699 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-10 20:28:25,701 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-10 20:28:25,702 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-10 20:28:25,703 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-10 20:28:25,706 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-10 20:28:25,709 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-10 20:28:25,710 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-10 20:28:25,711 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-10 20:28:25,713 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-10 20:28:25,715 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-10 20:28:25,716 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-10 20:28:25,718 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-10 20:28:25,720 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-10 20:28:25,723 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-10 20:28:25,725 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-10 20:28:25,727 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-10 20:28:25,729 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-10 20:28:25,732 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-10 20:28:25,733 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-10 20:28:25,735 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-10 20:28:25,736 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-10 20:28:25,739 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-10 20:28:25,742 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-11 10:18:06,178 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-11 10:18:06,187 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-11 10:18:06,193 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-11 10:18:06,200 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-11 10:18:06,206 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-11 10:18:06,219 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-11 10:18:06,224 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-11 10:18:06,229 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-11 10:18:06,235 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-11 10:18:06,242 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-11 10:18:06,249 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-11 10:18:06,255 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-11 10:18:06,261 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-11 10:18:06,267 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-11 10:18:06,272 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-11 10:18:06,277 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-11 10:18:06,282 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-11 10:18:06,284 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-11 10:18:06,290 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-11 10:18:06,294 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-11 10:18:06,299 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-11 10:18:06,304 [INFO] Arquivo lido com sucesso: main.py
2025-04-11 10:18:06,309 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-11 10:18:06,313 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-11 10:18:06,320 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-11 10:18:06,326 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-11 10:18:06,331 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-11 10:18:06,337 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-11 10:18:06,341 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-11 10:18:06,345 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-11 10:18:06,351 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-11 10:18:06,356 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-11 10:18:06,362 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-11 10:18:06,367 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-11 10:18:06,372 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-11 10:18:06,376 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-11 10:18:06,380 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-11 10:18:06,382 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-11 10:18:06,386 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-11 10:18:06,392 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-11 10:18:06,396 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-11 10:18:06,401 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-11 10:18:06,405 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-11 10:18:06,410 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-11 10:18:06,414 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-11 10:18:06,418 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-11 10:18:06,422 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-11 10:18:06,427 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-11 10:18:06,431 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-11 10:18:06,435 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-11 10:18:06,439 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-11 10:18:06,444 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-11 10:18:06,448 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-11 10:18:06,452 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-11 10:18:07,014 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-11 10:18:07,017 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-11 10:18:07,019 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-11 10:18:07,021 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-11 10:18:07,023 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-11 10:18:07,024 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-11 10:18:07,026 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-11 10:18:07,028 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-11 10:18:07,030 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-11 10:18:07,033 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-11 10:18:07,036 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-11 10:18:07,037 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-11 10:18:07,039 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-11 10:18:07,040 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-11 10:18:07,042 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-11 10:18:07,044 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-11 10:18:07,046 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-11 10:18:07,048 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-11 10:18:07,051 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-11 10:18:07,053 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-11 10:18:07,055 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-11 10:18:07,057 [INFO] Arquivo lido com sucesso: main.py
2025-04-11 10:18:07,058 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-11 10:18:07,061 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-11 10:18:07,063 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-11 10:18:07,065 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-11 10:18:07,068 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-11 10:18:07,070 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-11 10:18:07,071 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-11 10:18:07,073 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-11 10:18:07,076 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-11 10:18:07,077 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-11 10:18:07,079 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-11 10:18:07,082 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-11 10:18:07,084 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-11 10:18:07,086 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-11 10:18:07,089 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-11 10:18:07,091 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-11 10:18:07,092 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-11 10:18:07,094 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-11 10:18:07,096 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-11 10:18:07,098 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-11 10:18:07,101 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-11 10:18:07,103 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-11 10:18:07,105 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-11 10:18:07,107 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-11 10:18:07,109 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-11 10:18:07,110 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-11 10:18:07,112 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-11 10:18:07,113 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-11 10:18:07,116 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-11 10:18:07,119 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-11 10:18:07,120 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-11 10:18:07,123 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-11 10:18:17,680 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-11 10:18:17,682 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-11 10:18:17,684 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-11 10:18:17,686 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-11 10:18:17,688 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-11 10:18:17,690 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-11 10:18:17,691 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-11 10:18:17,692 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-11 10:18:17,694 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-11 10:18:17,695 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-11 10:18:17,697 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-11 10:18:17,698 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-11 10:18:17,700 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-11 10:18:17,702 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-11 10:18:17,703 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-11 10:18:17,705 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-11 10:18:17,707 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-11 10:18:17,709 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-11 10:18:17,711 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-11 10:18:17,713 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-11 10:18:17,715 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-11 10:18:17,718 [INFO] Arquivo lido com sucesso: main.py
2025-04-11 10:18:17,720 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-11 10:18:17,722 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-11 10:18:17,723 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-11 10:18:17,725 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-11 10:18:17,726 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-11 10:18:17,727 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-11 10:18:17,729 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-11 10:18:17,731 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-11 10:18:17,733 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-11 10:18:17,735 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-11 10:18:17,736 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-11 10:18:17,738 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-11 10:18:17,739 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-11 10:18:17,741 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-11 10:18:17,742 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-11 10:18:17,743 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-11 10:18:17,745 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-11 10:18:17,748 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-11 10:18:17,750 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-11 10:18:17,752 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-11 10:18:17,754 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-11 10:18:17,756 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-11 10:18:17,758 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-11 10:18:17,759 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-11 10:18:17,761 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-11 10:18:17,763 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-11 10:18:17,765 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-11 10:18:17,766 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-11 10:18:17,767 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-11 10:18:17,769 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-11 10:18:17,770 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-11 10:18:17,772 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-11 10:18:17,891 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-11 10:18:17,893 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-11 10:18:17,895 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-11 10:18:17,897 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-11 10:18:17,899 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-11 10:18:17,901 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-11 10:18:17,903 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-11 10:18:17,905 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-11 10:18:17,907 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-11 10:18:17,909 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-11 10:18:17,910 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-11 10:18:17,913 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-11 10:18:17,916 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-11 10:18:17,918 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-11 10:18:17,920 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-11 10:18:17,922 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-11 10:18:17,924 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-11 10:18:17,925 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-11 10:18:17,927 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-11 10:18:17,928 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-11 10:18:17,931 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-11 10:18:17,932 [INFO] Arquivo lido com sucesso: main.py
2025-04-11 10:18:17,934 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-11 10:18:17,937 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-11 10:18:17,939 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-11 10:18:17,941 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-11 10:18:17,942 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-11 10:18:17,944 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-11 10:18:17,946 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-11 10:18:17,947 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-11 10:18:17,949 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-11 10:18:17,951 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-11 10:18:17,952 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-11 10:18:17,953 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-11 10:18:17,955 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-11 10:18:17,956 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-11 10:18:17,957 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-11 10:18:17,958 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-11 10:18:17,960 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-11 10:18:17,961 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-11 10:18:17,963 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-11 10:18:17,965 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-11 10:18:17,967 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-11 10:18:17,968 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-11 10:18:17,970 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-11 10:18:17,972 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-11 10:18:17,973 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-11 10:18:17,975 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-11 10:18:17,977 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-11 10:18:17,978 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-11 10:18:17,980 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-11 10:18:17,982 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-11 10:18:17,984 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-11 10:18:17,985 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-11 10:18:18,004 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\agents_ia\comandAI\assets\20250411101817_clipboard_20250411101806.png, Prompt: Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# app_config\app_config.py

from pathlib import Path

class AppConfig:
    def __init__(self, root_path=None):
        self.ROOT_PATH = Path(root_path) if root_path else Path.cwd()
    
    def get_root_path(self):
        return str(self.ROOT_PATH)
    
    def create_directories(self, paths):
        for path in paths:
            path.mkdir(parents=True, exist_ok=True)


# audio_to_text\audio_config\audio_config.py

from app_config.app_config import AppConfig
from transcriptions.transcriptions_config import TranscriptionConfig

class AudioConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        transcription_config = TranscriptionConfig(root_path)
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.TRANSCRIPTION_INPUT_PATH = transcription_config.get_transcription_input_path()
        self.create_directories([self.AUDIO_INPUT_PATH])


# audio_to_text\audio_to_text.py

import whisper
from audio_to_text.audio_config.audio_config import AudioConfig

class AudioToConverter:
    def __init__(self, audio_config: AudioConfig):
        self.audio_config = audio_config
        self.AUDIO_INPUT_PATH = audio_config.AUDIO_INPUT_PATH
        self.TRANSCRIPTION_INPUT_PATH = audio_config.TRANSCRIPTION_INPUT_PATH

    def process_audio_files(self):
        audio_files = list(self.AUDIO_INPUT_PATH.glob('*'))

        if not audio_files:
            print(f"Não foram encontrados arquivos de áudio no diretório {self.AUDIO_INPUT_PATH}.")
            return

        model = whisper.load_model("base")

        for audio_file_path in audio_files:
            if audio_file_path.is_file():
                print(f"Processando arquivo: {audio_file_path}")
                self.process_audio_file(audio_file_path, model)

    def process_audio_file(self, audio_file_path, model):
        try:
            result = model.transcribe(str(audio_file_path))

            output_file_path = self.TRANSCRIPTION_INPUT_PATH / audio_file_path.with_suffix('.txt').name

            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(result['text'])

            print(f"Transcrição salva em: {output_file_path}")
        except Exception as e:
            print(f"Erro ao processar o arquivo {audio_file_path}: {e}")


# chat_app\chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# chat_app\config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# chat_app\core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# chat_app\core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("🚨 Processamento interrompido pelo usuário.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# chat_app\core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# chat_app\core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o próximo slot disponível, ajustando a espera conforme necessário."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no número de requisições feitas
            # tempo necessário para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista não está vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se não houver requisições

            # Aguarda o tempo necessário para garantir que a próxima requisição pode ser feita
            time.sleep(remaining_time)

# chat_app\services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# chat_app\services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# chat_app\services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# chat_app\services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# chat_app\services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# chat_app\services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# chat_app\utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

# common_paths\common_paths.py

from pathlib import Path

class CommonPaths:
    def __init__(self):
        # Diretório atual do script
        self.ROOT_PATH = Path(__file__).resolve().parent

        # Definição dos caminhos comuns
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'output'
        self.AUDIO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.AUDIO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio'
        self.TRANSCRIPTION_OUTPUT_PATH = self.ROOT_PATH / 'data'
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'data'

        # Criação dos diretórios
        self.create_directories()

    def create_directories(self):
        self.VIDEO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_INPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.AUDIO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.VIDEO_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
        self.TRANSCRIPTION_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)



# fundamentus_api\fundamentus\__init__.py



# fundamentus_api\fundamentus\dados_b3.py

import locale
import pandas as pd
import streamlit as st
import requests
import fundamentus
import os
import plotly.express as px
from bs4 import BeautifulSoup
from fundamentus.detalhes import get_papel
import logging

# Configura localidade
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')

# Configuração do layout do Streamlit
st.set_page_config(
    page_title="Análise de Ações",
    layout="wide",
    page_icon="📈"
)

class Acao:
    def __init__(self, papel):
        self.papel = papel
        self.dados_fundamentais = None
        self.proventos = None
        self.detalhes = None
        self.oscilacoes = None  # Adicionando um atributo para oscilações

    def carregar_dados_fundamentais(self):
        self.dados_fundamentais = fundamentus.get_resultado().loc[[self.papel]]  # Use colchetes duplos para garantir que seja um DataFrame
        self.remover_formatacao()

    def obter_detalhes(self):
        self.detalhes = get_papel(self.papel)
        if self.detalhes is None or self.detalhes.empty:
            logging.warning(f"Nenhum detalhe encontrado para o papel: {self.papel}")

    def obter_proventos(self):
        url = f"https://www.fundamentus.com.br/proventos.php?papel={self.papel}&tipo=2"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        tabela = soup.find('table', {'id': 'resultado'})

        if not tabela:
            return pd.DataFrame()

        dados = []
        for linha in tabela.find_all('tr')[1:]:
            colunas = linha.find_all('td')
            try:
                valor = float(colunas[1].text.strip().replace(',', '.'))
            except ValueError:
                valor = None  # Se der erro, coloca None para evitar crash

            dados.append([colunas[0].text.strip(), valor, colunas[2].text.strip()])
        
        self.proventos = pd.DataFrame(dados, columns=['Data', 'Valor', 'Tipo'])
        return self.proventos

    def obter_oscilacoes(self):
        url = f"https://www.fundamentus.com.br/detalhes.php?papel={self.papel}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return pd.DataFrame()

        soup = BeautifulSoup(response.text, 'html.parser')
        conteudo_div = soup.find('div', class_='conteudo clearfix')

        if conteudo_div is None:
            return pd.DataFrame()

        oscilacoes_data = []
        oscilacoes_section = conteudo_div.find('td', class_='nivel1', colspan='2')
        
        if oscilacoes_section:
            labels = oscilacoes_section.find_all_next('td', class_='label w1')
            dados = oscilacoes_section.find_all_next('td', class_='data w1')

            for label, dado in zip(labels, dados):
                label_text = label.get_text(strip=True)
                valor_text = dado.find('span', class_='oscil').get_text(strip=True)
                oscilacoes_data.append([label_text, valor_text])

        self.oscilacoes = pd.DataFrame(oscilacoes_data, columns=['Período', 'Oscilação'])
        return self.oscilacoes

    def remover_formatacao(self):
        colunas_percentuais = ['dy', 'mrgebit', 'mrgliq', 'roic', 'roe', 'c5y']
        for coluna in colunas_percentuais:
            if coluna in self.dados_fundamentais:
                try:
                    self.dados_fundamentais[coluna] = self.dados_fundamentais[coluna].astype(float)
                except ValueError as e:
                    logging.error(f"Erro ao converter coluna {coluna} para float: {e}")

    def formatar_moeda(self, valor):
        return locale.currency(valor, symbol=True, grouping=True)

class Aplicacao:
    def __init__(self):
        self.acoes = fundamentus.get_resultado()

    def ajustar_tipos_dataframe(self, df):
        for coluna in df.columns:
            if df[coluna].dtype == 'object':
                try:
                    df[coluna] = df[coluna].astype(float)
                except ValueError:
                    df[coluna] = df[coluna].astype(str)
            elif df[coluna].dtype in ['int64', 'float64']:
                df[coluna] = df[coluna].astype(float)
        return df

    def exibir_dashboard(self):
        st.sidebar.title("📊 Dashboard de Análise de Ações")
        st.sidebar.write("Selecione um papel para visualizar detalhes.")

        papel_selecionado = st.sidebar.selectbox("Escolha uma ação", self.acoes.index)

        acao = Acao(papel_selecionado)
        acao.carregar_dados_fundamentais()
        acao.obter_proventos()
        acao.obter_detalhes()
        acao.obter_oscilacoes()

        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader(f"📌 Dados Fundamentais - {papel_selecionado}")
            dados_fundamentais_df = self.ajustar_tipos_dataframe(acao.dados_fundamentais.T)
            st.dataframe(dados_fundamentais_df, width=400)

        with col2:
            st.subheader("🔍 Detalhes")
            if acao.detalhes is not None and not acao.detalhes.empty:
                detalhes_df = pd.DataFrame(acao.detalhes).T.reset_index()
                detalhes_df.columns = ['Descrição', 'Valor']
                detalhes_df = self.ajustar_tipos_dataframe(detalhes_df)

                st.subheader("Tabela de Detalhes")
                st.dataframe(detalhes_df, width=800)
            else:
                st.warning("Nenhum detalhe encontrado para essa ação.")

        col_dividendos, col_oscilacoes = st.columns([1, 2])

        with col_dividendos:
            st.subheader("💰 Dividendos")
            if not acao.proventos.empty:
                proventos_df = self.ajustar_tipos_dataframe(acao.proventos)
                st.write(proventos_df)

        with col_oscilacoes:
            st.subheader("📉 Oscilações")
            if acao.oscilacoes is not None and not acao.oscilacoes.empty:
                oscilacoes_df = self.ajustar_tipos_dataframe(acao.oscilacoes)
                st.write(oscilacoes_df)

        st.subheader("📈 Tabela Geral de Ações")
        st.dataframe(self.acoes)

# Execução
if __name__ == "__main__":
    app = Aplicacao()
    app.exibir_dashboard()

# fundamentus_api\setup.py

from setuptools import setup, find_packages

setup(
    name='fundamentalvision ',
    version='0.1.0',
    author='Joel FerreiraHeanna dos Reis',
    author_email='heannareis@gmail.com',
    description='Um pacote para análise fundamentalista de ações da Bolsa B3 do Brasil.',
    packages=find_packages(),
    install_requires=[
        'pandas',
        'requests',
        'beautifulsoup4',
        'streamlit',
        'plotly',
        'fundamentus'
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)

# ia_generator.py

import requests
from pathlib import Path
import webbrowser
from config.common_paths import TRANSCRIPTION_OUTPUT_PATH

apiKey = "6UlOOoY/kkmprunma/qNDg"

str_personas = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'personas.txt'
str_contexto = TRANSCRIPTION_OUTPUT_PATH / 'input' / 'contexto.txt'

url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

txt_files = list(TRANSCRIPTION_OUTPUT_PATH.glob('*.txt'))

css_styles = """
<style>
body {
    font-family: Arial, sans-serif;
    margin: 20px;
}

h1, h2, h3 {
    color: #FF8C00;
}

li, strong, p {
    color: #008000;
}

h1 {
    font-size: 24px;
    margin-bottom: 20px;
}

h2 {
    font-size: 20px;
    margin-top: 20px;
    margin-bottom: 10px;
}

ul {
    list-style-type: disc;
    margin-left: 40px;
}

li {
    margin-bottom: 10px;
}

p {
    line-height: 1.6;
}
</style>
"""

if not txt_files:
    print(f"Não foram encontrados arquivos .txt no diretório {TRANSCRIPTION_OUTPUT_PATH}.")
else:
    for txt_file in txt_files:
        if txt_file.is_file():
            print(f"Lendo o arquivo: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as file:
                str_reuniao = file.read()

            print(f"Enviando o conteúdo do arquivo {txt_file.name} para a API...")
            data = {
                "inputs": {
                    "str_reuniao": str_reuniao,
                    "str_personas": str_personas.read_text(encoding='utf-8'),
                    "str_contexto": str_contexto.read_text(encoding='utf-8'),
                }
            }

            response = requests.post(f"{url}/api/templates/668de04202493d3063a9d7fa/execute", json=data, headers=headers)
            if response.status_code == 200:
                print(f"Resultado para o arquivo {txt_file.name} recebido.")
                html_content = response.text
                print(response.text)

                # Incluir o CSS no conteúdo HTML
                html_with_css = f"<html><head>{css_styles}</head><body>{html_content}</body></html>"

                # Salvar o conteúdo HTML em um arquivo
                output_file = TRANSCRIPTION_OUTPUT_PATH / f"{txt_file.stem}_output.html"
                with open(output_file, 'w', encoding='utf-8') as html_file:
                    html_file.write(html_with_css)

                # Abrir o arquivo HTML no navegador
                webbrowser.open(f"file://{output_file.resolve()}")
            else:
                print(f"Erro ao processar o arquivo {txt_file.name}: {response.status_code}")


# main.py

from video_to_audio.video_to_audio import VideoConfig, VideoToAudioConverter
from audio_to_text.audio_to_text import AudioToConverter
from audio_to_text.audio_config.audio_config import AudioConfig
from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from transcriptions.transcriptions_config import TranscriptionConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from text_to_embedding.embedding_processing import EmbeddingProcessorWrapper
from pathlib import Path

def main():
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    root_path = str(PROJECT_ROOT)
    print(f"Root path: {root_path}")  # Para verificar se está correto
    api_url = "http://localhost:8081/api/meetings/transcriptions"
    
    # # Configuração de vídeos
    video_config = VideoConfig(root_path=root_path)
    video_processor = VideoToAudioConverter(video_config=video_config)
    video_processor.process_videos()
    
    # # Configuração de áudios
    audio_config = AudioConfig(root_path=root_path)
    audio_processor = AudioToConverter(audio_config=audio_config)
    audio_processor.process_audio_files()
    
    # # Processamento de transcrições e envio de embeddings
    # embedding_processor_wrapper = EmbeddingProcessorWrapper(root_path=root_path, api_url=api_url)
    # embedding_processor_wrapper.process_transcriptions()

if __name__ == "__main__":
    main()


# send_embeddings_database\embedding_config\embedding_config.py

from app_config.app_config import AppConfig

class EmbeddingConfig(AppConfig):
    def __init__(self, root_path=None, transcription_input_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = transcription_input_path
        self.EMBEDDING_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'embeddings' / 'output'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH, self.EMBEDDING_OUTPUT_PATH])


# send_embeddings_database\verify_last_enbedding.py

import os
import numpy as np

def get_latest_file(directory):
    # Listar todos os arquivos no diretório
    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    
    if not files:
        raise FileNotFoundError("Nenhum arquivo encontrado no diretório.")

    # Encontrar o arquivo mais recente
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_and_print_embedding(directory):
    # Obter o caminho do último arquivo de embedding
    embedding_file_path = get_latest_file(directory)
    
    # Carregar o embedding
    embedding = np.load(embedding_file_path)
    
    # Exibir o conteúdo do embedding
    print("Embedding carregado:")
    print(embedding)
    print("Dimensões do embedding:", embedding.shape)

# Caminho do diretório de embeddings
embedding_directory = 'C:/Users/HeannarReis/Documents/bsa_atacadao/assets/embeddings/output'

# Carregar e exibir o último embedding
load_and_print_embedding(embedding_directory)


# test_whisper.py

from transformers import pipeline
from common_paths import CommonPaths

commmom_paths = CommonPaths()

# Função para ler a transcrição de um arquivo
def read_transcription(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()
    return lines

# Função para converter a pontuação em estrelas
def score_to_stars(score):
    if score >= 0.75:
        return 5
    elif score >= 0.55:
        return 4
    elif score >= 0.35:
        return 3
    elif score >= 0.15:
        return 2
    else:
        return 1

# Inicializar o pipeline de análise de sentimento
sentiment_pipeline = pipeline("sentiment-analysis", model="neuralmind/bert-base-portuguese-cased")

# Caminho para o arquivo de transcrição
file_path = commmom_paths.TRANSCRIPTION_OUTPUT_PATH / 'feedback.txt'

# Ler a transcrição
lines = read_transcription(file_path)

# Analisar sentimento para cada feedback
for line in lines:
    sentiment = sentiment_pipeline(line.strip())
    score = sentiment[0]['score']
    stars = score_to_stars(score)
    
    # Ajuste manual para feedbacks com aspectos positivos e negativos
    if any(word in line.lower() for word in ["mas", "porém", "no entanto"]):
        stars = max(1, stars - 1)  # Reduz uma estrela se houver contradição
    
    # Ajuste adicional baseado em palavras-chave
    positive_keywords = ["excelente", "ótimo", "perfeito", "rápido", "eficiente", "satisfeito", "clara", "eficaz", "bom trabalho"]
    negative_keywords = ["ruim", "péssimo", "horrível", "poderia", "não resolveu", "insatisfatória", "confusa", "inadequado", "lento", "não houve seguimento", "não atendeu", "não funcionou", "demorado"]
    
    positive_count = sum(word in line.lower() for word in positive_keywords)
    negative_count = sum(word in line.lower() for word in negative_keywords)
    
    # Ajuste de estrelas baseado na contagem de palavras-chave
    if positive_count > negative_count:
        stars = min(5, stars + 1)
    elif negative_count > positive_count:
        stars = max(1, stars - 1)
    
    print(f"Texto: {line.strip()}")
    print(f"Sentimento: {stars} estrelas")
    print(f"Pontuação: {score:.2f}")
    print('---')


# text_to_embedding\embedding_processing.py

from send_embeddings_database.embedding_config.embedding_config import EmbeddingConfig
from text_to_embedding.texto_to_embedding import EmbeddingProcessor
from transcriptions.transcriptions_config import TranscriptionConfig
from transcriptions.transciption_sender_database import TranscriptionSenderDatabase

class EmbeddingProcessorWrapper:
    def __init__(self, root_path, api_url):
        # Configuração de transcrições e embeddings
        transcription_config = TranscriptionConfig(root_path=root_path)
        embedding_config = EmbeddingConfig(root_path=root_path, transcription_input_path=transcription_config.get_transcription_input_path())

        self.embedding_processor = EmbeddingProcessor(embedding_config)
        self.transcription_sender = TranscriptionSenderDatabase(api_url)
    
    def process_transcriptions(self):
        # Mostrar o diretório onde está procurando as transcrições
        print(f"Diretório de entrada das transcrições: {self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH}")
        
        # Listar todos os arquivos de transcrição no diretório de entrada
        transcription_files = list(self.embedding_processor.embedding_config.TRANSCRIPTION_INPUT_PATH.glob('*.txt'))
        if not transcription_files:
            print("Nenhum arquivo de transcrição encontrado.")
        for transcription_file_path in transcription_files:
            if transcription_file_path.is_file():
                print(f"Processando arquivo: {transcription_file_path}")
                self.process_and_send_transcription(transcription_file_path)
            else:
                print(f"Arquivo não encontrado: {transcription_file_path}")

    def process_and_send_transcription(self, transcription_file_path):
        try:
            # Ler a transcrição do arquivo de texto
            with open(transcription_file_path, 'r', encoding='utf-8') as f:
                transcription_text = f.read()
                if not transcription_text:
                    print(f"Arquivo {transcription_file_path} está vazio.")
                    return

            # Gerar o embedding da transcrição
            embedding = self.embedding_processor.generate_embedding(transcription_text)
            if embedding is None:
                print(f"Falha ao gerar embedding para o arquivo {transcription_file_path}.")
                return

            # Salvar o embedding em um arquivo .npy
            self.embedding_processor.save_embedding(transcription_file_path, embedding)

            # Enviar os dados para a API
            self.transcription_sender.send_transcription(transcription_text, embedding)

        except Exception as e:
            print(f"Erro ao processar o arquivo {transcription_file_path}: {e}")


# text_to_embedding\texto_to_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingProcessor:
    def __init__(self, embedding_config):
        self.embedding_config = embedding_config
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def generate_embedding(self, transcription_text):
        return self.embedding_model.encode(transcription_text)

    def save_embedding(self, transcription_file_path, embedding):
        embedding_file_path = self.embedding_config.EMBEDDING_OUTPUT_PATH / transcription_file_path.with_suffix('.npy').name
        np.save(embedding_file_path, embedding)
        print(f"Embedding salvo em: {embedding_file_path}")
        return embedding_file_path


# transcriptions\transciption_sender_database.py

import requests

class TranscriptionSenderDatabase:
    def __init__(self, api_url):
        self.api_url = api_url

    def send_transcription(self, transcription_text, embedding):
        data = {
            'transcriptionText': transcription_text,
            'embedding': embedding.tolist()
        }

        response = requests.post(self.api_url, json=data)

        if response.status_code == 201:
            print("Transcrição e embedding enviados com sucesso.")
        else:
            print(f"Erro ao enviar dados: {response.status_code}")
            print("Resposta da API:")
            print(response.text)


# transcriptions\transcriptions_config.py

from app_config.app_config import AppConfig

class TranscriptionConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.TRANSCRIPTION_INPUT_PATH = self.ROOT_PATH / 'assets' / 'transcriptions' / 'input'
        self.create_directories([self.TRANSCRIPTION_INPUT_PATH])
    
    def get_transcription_input_path(self):
        return self.TRANSCRIPTION_INPUT_PATH


# translate\translator_to_english.py

import speech_recognition as sr
from translate import Translator

def ouvir_e_traduzir():
    # Inicializa o reconhecedor de fala
    recognizer = sr.Recognizer()

    # Configura o tradutor
    translator = Translator(to_lang="en", from_lang="pt")

    # Usa o microfone como fonte de áudio
    with sr.Microphone() as source:
        print("Diga algo em português...")

        while True:
            try:
                # Escuta o áudio do microfone
                audio = recognizer.listen(source)
                
                # Reconhece a fala usando o Google Web Speech API
                texto_portugues = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {texto_portugues}")

                # Traduz o texto para o inglês
                traducao = translator.translate(texto_portugues)
                print(f"Tradução para o inglês: {traducao}")

            except sr.UnknownValueError:
                print("Não foi possível entender o áudio")
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")

if __name__ == "__main__":
    try:
        ouvir_e_traduzir()
    except KeyboardInterrupt:
        print("Interrompido pelo usuário")


# translate\whispert_translator.py

import whisper
import pyaudio
import numpy as np

# Inicializa o modelo Whisper
model = whisper.load_model("base")

# Configurações de áudio
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024

# Inicializa o PyAudio
audio = pyaudio.PyAudio()

# Abre o stream de áudio
stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

print("Diga algo em português...")

try:
    audio_buffer = []

    while True:
        # Lê dados do microfone
        data = stream.read(CHUNK)
        audio_buffer.append(np.frombuffer(data, dtype=np.int16).flatten().astype(np.float32) / 32768.0)

        # Processa o áudio a cada 5 segundos
        if len(audio_buffer) * CHUNK / RATE >= 5:
            audio_data = np.concatenate(audio_buffer)
            audio_buffer = []

            # Transcreve e traduz o áudio usando Whisper
            result = model.transcribe(audio_data, task="translate", language="pt")

            # Exibe a tradução
            print(f"Tradução para o inglês: {result['text']}")

except KeyboardInterrupt:
    print("Interrompido pelo usuário")

    # Fecha o stream de áudio
    stream.stop_stream()
    stream.close()
    audio.terminate()


# video_to_audio\video_config\video_config.py

from app_config.app_config import AppConfig

class VideoConfig(AppConfig):
    def __init__(self, root_path=None):
        super().__init__(root_path)
        self.VIDEO_INPUT_PATH = self.ROOT_PATH / 'assets' / 'video' / 'input'
        self.VIDEO_OUTPUT_PATH = self.ROOT_PATH / 'assets' / 'audio' / 'input'
        self.create_directories([self.VIDEO_INPUT_PATH, self.VIDEO_OUTPUT_PATH])

# video_to_audio\video_to_audio.py

from moviepy import VideoFileClip
import glob
import os
from .video_config.video_config import VideoConfig

class VideoToAudioConverter:
    def __init__(self, video_config: VideoConfig):
        self.video_config = video_config

    def convert_video_to_audio(self, video_path, audio_path):
        try:
            video = VideoFileClip(video_path)
            if video.audio:
                video.audio.write_audiofile(audio_path, fps=44100)
                print(f"Convertido {video_path} para {audio_path}")
            else:
                print(f"Aviso: O vídeo {video_path} não contém áudio!")
        except Exception as e:
            print(f"Erro ao converter {video_path}: {e}")

    def process_videos(self):
        input_directory = self.video_config.VIDEO_INPUT_PATH
        output_directory = self.video_config.VIDEO_OUTPUT_PATH

        os.makedirs(output_directory, exist_ok=True)

        # Busca qualquer arquivo de vídeo (formatos comuns)
        video_files = glob.glob(os.path.join(input_directory, "*.*"))  # Pega todos os arquivos

        # Filtra apenas arquivos de vídeo
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv"}  
        video_files = [f for f in video_files if os.path.splitext(f)[1].lower() in video_extensions]

        if not video_files:
            print(f"Nenhum arquivo de vídeo encontrado em: {input_directory}")
            return

        for video_file in video_files:
            base_name = os.path.basename(video_file)
            audio_file = os.path.join(output_directory, os.path.splitext(base_name)[0] + ".wav")
            self.convert_video_to_audio(video_file, audio_file)

        print("Conversão de vídeo para áudio concluída!")


# voice_assistent\assistent.py

import speech_recognition as sr
import pyttsx3
import re
from collections import deque
import spacy
import requests
import os
import webbrowser
from class_voice_assistent.prompt import create_prompt
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# Configurações da API
handler = genai('gemini-1.5-flash')

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

voices = engine.getProperty('voices')
engine.setProperty('rate', 180)
print("\nLista de Vozes...")
for indice, vozes in enumerate(voices):
    print(indice, vozes.name)

voz = 1
engine.setProperty('voice', voices[voz].id)

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

# Função para extrair texto de HTML
def extract_text_from_html(html):
    if not html.strip().startswith('<'):
        print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
        return html
    soup = BeautifulSoup(html, 'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

def get_text_response(prompt, context, feedback):
    # Gere o conteúdo com base no prompt usando a classe GenerativeModelHandler
    response = handler.generate_content(prompt)
    return response

# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/api/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


# voice_assistent\class_voice_assistent\api_client.py

import requests


class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, meeting):
        try:
            response_text = self.model.generate_content(prompt, context, meeting)
            return response_text
        except Exception as e:
            print(f"Erro inesperado: {e}")
            return None

    def find_similar_embeddings(self, embedding):
        try:
            print(f"Buscando embeddings similares para: {embedding}")
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            similar_embeddings = response.json()

            # Ordenar por similaridade (assumindo que a API retorna com similaridade em ordem decrescente)
            # Remover duplicatas baseadas na pergunta
            seen_questions = set()
            unique_embeddings = []
            for embedding in similar_embeddings:
                question = embedding['question'].strip().lower()
                if question not in seen_questions:
                    unique_embeddings.append(embedding)
                    seen_questions.add(question)
            print(f"Embeddings similares únicos encontrados: {unique_embeddings}")
            return unique_embeddings
        except requests.RequestException as e:
            print(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            # Converter embeddings de numpy arrays para listas
            if hasattr(question_embedding, 'tolist'):
                question_embedding = question_embedding.tolist()
            if hasattr(answer_embedding, 'tolist'):
                answer_embedding = answer_embedding.tolist()
            
            data = {
                "question": question,
                "questionEmbedding": question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding
            }
            
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                print("Pergunta e resposta salvas com sucesso.")
            else:
                print(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            print(f"Erro em save_question_answer: {e}")


    def fetch_all_contexts(self):
        try:
            response = requests.get("http://localhost:8081/api/contexts/all")
            if response.status_code == 200:
                data = response.json()
                contexts = data.get('contexts', [])
                if isinstance(contexts, list):
                    print(f"Contexto obtido da API: {contexts}")
                    return contexts
                else:
                    print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                    return []
            else:
                print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de contextos: {e}")
            return []

    def fetch_last_meeting(self):
        try:
            response = requests.get("http://localhost:8081/api/meetings/last")
            if response.status_code == 200:
                data = response.json()
                transcription_text = data.get('transcriptionText', "")
                if isinstance(transcription_text, str):
                    print(f"Texto da transcrição obtido da API: {transcription_text}")
                    return transcription_text
                else:
                    print(f"Erro: 'transcriptionText' não é uma string. Dados retornados: {data}")
                    return ""
            else:
                print(f"Erro ao acessar a API de reuniões: {response.status_code}, {response.text}")
                return ""
        except requests.RequestException as e:
            print(f"Erro ao fazer requisição para a API de reuniões: {e}")
            return ""


# voice_assistent\class_voice_assistent\command_interpreter.py

import spacy
from prompt_generator.online_prompt import OnlineResearchPromptGenerator
from prompt_generator.meeting_prompt import MeetingPromptGenerator
from prompt_generator.default_prompt_generator import DefaultPromptGenerator
import re

# Carregar o modelo de linguagem natural
nlp = spacy.load("pt_core_news_sm")

class CommandInterpreter:
    def __init__(self, api_client, question_answer_service, context_manager, max_similar=3):
        self.api_client = api_client
        self.question_answer_service = question_answer_service
        self.context_manager = context_manager
        self.max_similar = max_similar  # Limite de contextos similares

    def interpret_command(self, command, meeting):
        print(f"Interpretando comando: {command}")
        contexts = self.api_client.fetch_all_contexts()
        context_str = "\n".join([context['context'] for context in contexts])

        # Gerar embedding para a pergunta e buscar embeddings similares
        question_embedding = self.question_answer_service.convert_text_to_embedding(command)
        similar_embeddings = self.api_client.find_similar_embeddings(question_embedding)

        # Filtrar para evitar respostas redundantes
        unique_responses = self._filter_unique_responses(similar_embeddings, command)
        similar_context = "\n".join([f"Pergunta: {embedding['question']}\nResposta: {embedding['answer']}" for embedding in unique_responses[:self.max_similar]])

        # Detectar tipo de comando usando regex
        if re.search(r'\b(pesquise|pesquisar|procure)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como pesquisa online.")
            response = self.get_online_research_response(command, context_str, similar_context)
        elif re.search(r'\b(contexto)\b', command, re.IGNORECASE):
            print(f"\nComando identificado como busca de contexto.")
            response = self.get_project_response(command, meeting, context_str, similar_context)
        elif re.search(r'\b(resumo?|tópicos da|pontos (relevantes|principais)|análise)\b.*\b(reunião|última (reunião|conversa|sessão))\b', command, re.IGNORECASE):
            print(f"\nComando identificado como análise de reunião.")
            meeting = self.api_client.fetch_last_meeting()
            response = self.get_meeting_analysis_response(command, context_str, meeting)
        else:
            print(f"\nComando identificado como comando padrão.")
            response = self.handle_default_command(command, context_str, meeting, similar_context)

        if response:
            answer_embedding = self.question_answer_service.convert_text_to_embedding(response)
            self.api_client.save_question_answer(command, question_embedding, response, answer_embedding)
            self.context_manager.add_context(command, response)

        return response

    def _filter_unique_responses(self, similar_embeddings, current_command):
        """
        Filtra respostas semelhantes que são muito similares ao comando atual para evitar redundância.
        """
        filtered = []
        for embedding in similar_embeddings:
            if embedding['question'].lower() != current_command.lower():
                filtered.append(embedding)
        return filtered

    def handle_default_command(self, command, context_str, meeting, similar_context):
        print(f"\nTratando comando padrão: {command}")
        # Combinar o contexto atual com os contextos similares para enriquecer a resposta
        combined_context = f"{context_str}\n{similar_context}"
        prompt = DefaultPromptGenerator().generate_prompt(command, combined_context, meeting)
        response = self.api_client.get_text_response(prompt, combined_context, meeting)
        return response

    # Métodos get_project_response, get_meeting_analysis_response, get_online_research_response permanecem inalterados

    def get_project_response(self, command, meeting, context_str, similar_context):
        print(f"\nGerando prompt de projeto.")
        prompt = DefaultPromptGenerator().generate_prompt(command, context_str, meeting, similar_context)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_meeting_analysis_response(self, command, context_str, meeting):
        print(f"\nGerando prompt de análise de reunião.")
        prompt = MeetingPromptGenerator().generate_prompt(command, context_str, meeting)
        return self.api_client.get_text_response(prompt, context_str, meeting)

    def get_online_research_response(self, command, context_str, similar_context):
        print(f"\nGerando prompt de pesquisa online.")
        prompt = OnlineResearchPromptGenerator().generate_prompt(command, context_str, similar_context)
        return self.api_client.get_text_response(prompt, context_str, None)


# voice_assistent\class_voice_assistent\context_manager.py

from collections import deque

class ContextManager:
    def __init__(self, maxlen=10):
        self.recent_context = deque(maxlen=maxlen)

    def add_context(self, command, response):
        self.recent_context.append((command, response))

    def get_context(self):
        return "\n".join([context for context, _ in self.recent_context])


# voice_assistent\class_voice_assistent\conversation_history.py



# voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py

import requests
import logging
import google.generativeai as genai

# Configure o logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class APIClient:
    def __init__(self, similarity_url, save_url, model):
        self.similarity_url = similarity_url
        self.save_url = save_url
        self.model = model

    def get_text_response(self, prompt, context, feedback):
        try:
            # Gerando o conteúdo usando a nova API
            response = self.model.generate_content(prompt)
            if response and hasattr(response, 'text'):
                return prompt, response.text
            else:
                logger.error("Resposta inválida da API")
                return prompt, None
        except Exception as e:
            logger.error(f"Erro em get_text_response: {e}")
            return prompt, None

    def find_similar_embeddings(self, embedding):
        try:
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            data = embedding
            logger.info(f"Enviando dados para a API de embeddings similares: {data}")
            response = requests.post(f"{self.similarity_url}/api/question_answers/similar", json=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Erro em find_similar_embeddings: {e}")
            return []

    def save_question_answer(self, question, question_embedding, answer, answer_embedding):
        try:
            data = {
                "question": question,
                "questionEmbedding": question_embedding.tolist() if hasattr(question_embedding, 'tolist') else question_embedding,
                "answer": answer,
                "answerEmbedding": answer_embedding.tolist() if hasattr(answer_embedding, 'tolist') else answer_embedding
            }
            response = requests.post(self.save_url, json=data)
            response.raise_for_status()
            if response.status_code == 201:
                logger.info("Pergunta e resposta salvas com sucesso.")
            else:
                logger.warning(f"Falha ao salvar pergunta e resposta. Código de status: {response.status_code}")
        except requests.RequestException as e:
            logger.error(f"Erro em save_question_answer: {e}")


# voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        """Carregar variáveis do arquivo .env"""
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        """Configurar a chave da API"""
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        """Inicializar o modelo generativo"""
        try:
            self.model = genai.GenerativeModel(self.model_name)
        except Exception as e:  
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content(self, prompt: str, context: str, meeting: str) -> str:
        """Gerar conteúdo com base no prompt, contexto e reunião"""
        try:
            # Supondo que a API espera um dicionário com os parâmetros
            request_data = f'''
                "prompt": {prompt},
                "context": {context},
                "meeting": {meeting}
            '''
            print(f"Enviando requisição para a API GenAI: {request_data}")

            response = self.model.generate_content(request_data)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py

import os
from dotenv import load_dotenv
from groq import Groq

# Carregar variáveis do arquivo .env
load_dotenv()

# Recuperar a chave da API
api_key = os.getenv("GROQ_API_KEY")

# Verificar se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API Key is missing. Please set the GROQ_API_KEY in the .env file.")

# Configurar o cliente com a chave da API
client = Groq(api_key=api_key)

# Criação da conclusão do chat
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "De acordo com nossas conversas anteriores, o que você acha do meu uso de IA ?",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)


# voice_assistent\class_voice_assistent\main.py

import os
from context_manager import ContextManager
from api_client import APIClient
from command_interpreter import CommandInterpreter
from text_command_hendler import TextCommandHandler
from text_processor import TextProcessor
from text_to_speech import TextToSpeech
from voice_command_hendler import VoiceCommandHandler
from question_answers_service import QuestionAnswerService
from gpt_communication.gemini_gpt import GenerativeModelHandler

class MainApp:
    def __init__(self, model):
        self.voice_handler = VoiceCommandHandler()
        self.text_handler = TextCommandHandler()
        self.tts = TextToSpeech()
        self.text_processor = TextProcessor()
        self.api_client = APIClient(
            similarity_url="http://localhost:8081",
            save_url="http://localhost:8081/api/question_answers/save",
            model=model
        )
        self.context_manager = ContextManager()
        self.question_answer_service = QuestionAnswerService()
        self.command_interpreter = CommandInterpreter(
            self.api_client,
            self.question_answer_service,
            self.context_manager
        )

    def handle_command(self, command, meeting=""):
        if command:
            print(f"Pergunta recebida: {command}")
            text_response = self.command_interpreter.interpret_command(command, meeting)
            if text_response:
                print(f"Resposta: {text_response}")
                self.tts.speak_text(text_response)
                self.context_manager.add_context(command, text_response)
                return text_response
        else:
            print("Nenhum comando detectado.")
            return None

    def run(self):
        meeting = ""
        while True:
            try:
                input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
                if input_type == 'v':
                    command = self.voice_handler.capture_voice_command()
                elif input_type == 't':
                    command = self.text_handler.capture_text_command()
                else:
                    print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
                    continue

                response = self.handle_command(command, meeting)
                if response:
                    print(f"Resposta: {response}")
            except Exception as e:
                print(f"Ocorreu um erro: {e}")

if __name__ == "__main__":
    model = GenerativeModelHandler('gemini-1.5-flash')
    app = MainApp(model)
    app.run()

# voice_assistent\class_voice_assistent\prompt.py

def create_prompt(command, context, meeting):
    keywords = ["faça um resumo da última reunião.", "tópicos da última reunião", "resuma a última reunião", "pesquise", "pesquisar", "procure"]
    if any(keyword in command.lower() for keyword in keywords):
        return f"""
        Regras de Meeting:
        - Você é responsável por analisar, debater, sugerir e informar melhorias.
        - Resuma de forma clara e Objetiva.
        - Não acrescentar título nas respostas.

        [context]: {context}
        -------
        [meeting]: {meeting}
        -------
        [str_texto]: {command}
        """
    else:
        return f"""
        [context]: {context}
        -------
        [str_texto]: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py

class DefaultPromptGenerator:
    def generate_prompt(self, command, combined_context, meeting):
        prompt = (
            f"Comando: {command}\n"
            f"Contexto Anterior:\n{combined_context}\n"
            f"Baseie sua resposta nas informações acima e forneça uma solução detalhada."
        )
        return prompt

# voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class MeetingPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting):
        return f"""
        Regras de Meeting com respostas inteligentes:
        - Responda a pergunta de [str_texto] com base nas diretrizes abaixo...
            - Você é responsável analisar com detalhes a reunião de [str_meeting], e fornecer uma longa estória sobre o assunto.
            - observe os nomes das personas mencionadas no texto de meeting para aprender e melhorar a precisão da resposta.
            - Não acrescente título nas respostas.
        
        ------
        [str_texto]: Responda a pergunta de: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py

from prompt_generator.prompt_generator import PromptGenerator

class OnlineResearchPromptGenerator(PromptGenerator):
    def generate_prompt(self, command, context, meeting, similar_context):
        return f"""
        Regras de Pesquisa Online Inteligente:
        - Utilize similar_context e faça uma pesquisa online para uma resposta mais precisa das questões de [str_text]
        - Não acrescente título nas respostas.
        
        ------
        [context]: Regras Básicas {context}
        ------
        [similar_context]:
        Perguntas e respostas anteriores.{similar_context}
        ------
        [str_texto]: Responda seguinte pergunta: {command}
        """

# voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py

from abc import ABC, abstractmethod

class PromptGenerator(ABC):
    @abstractmethod
    def generate_prompt(self, command, context, meeting, similar_context):
        pass

# voice_assistent\class_voice_assistent\question_answers_service.py

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

class QuestionAnswerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)

    def convert_text_to_embedding(self, text):
        embedding = self.embedding_model.encode(text)
        #print(f"Embedding gerado para '{text}': {embedding[0]:.16f}") # Adicionado para verificar o embedding gerado
        return embedding


# voice_assistent\class_voice_assistent\text_command_hendler.py

class TextCommandHandler:
    def capture_text_command(self):
        command = input("Digite o seu comando: ")
        return command


# voice_assistent\class_voice_assistent\text_processor.py

from bs4 import BeautifulSoup

class TextProcessor:
    def extract_values_from_json(self, data):
        if isinstance(data, dict):
            return ' '.join([str(value) for value in data.values()])
        elif isinstance(data, list):
            return ' '.join([self.extract_values_from_json(item) for item in data])
        return str(data)

    def extract_text_from_html(self, html):
        if not html.strip().startswith('<'):
            print("Aviso: A entrada parece um caminho de arquivo, não um conteúdo HTML.")
            return html
        soup = BeautifulSoup(html, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        return text


# voice_assistent\class_voice_assistent\text_to_speech.py

import pyttsx3

class TextToSpeech:
    def __init__(self):
        self.engine = pyttsx3.init()

    def speak_text(self, text):
        cleaned_text = self.clean_text(text)
        self.engine.say(cleaned_text)
        self.engine.runAndWait()

    def clean_text(self, text):
        import re
        return re.sub(r'[\*\_\#]', '', text)


# voice_assistent\class_voice_assistent\voice_command_hendler.py

import speech_recognition as sr

class VoiceCommandHandler:
    def capture_voice_command(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Por favor, fale o seu comando:")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                print("Áudio capturado com sucesso.")
                command = recognizer.recognize_google(audio, language='pt-BR')
                print(f"Você disse: {command}")
                return command
            except sr.WaitTimeoutError:
                print("Tempo de espera expirado. Nenhum áudio detectado.")
                return None
            except sr.UnknownValueError:
                print("Não foi possível entender o áudio.")
                return None
            except sr.RequestError as e:
                print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
                return None


# voice_assistent\config.py

# config.py
import pyttsx3
import spacy
from collections import deque

class APIConfig:
    apiKey = "API_KEY"
    url = "https://gpt-templates.saiapplications.com"
    headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")


# voice_assistent\template.py

import speech_recognition as sr
import requests
import pyttsx3
import re
from collections import deque
import spacy
import os
import webbrowser
from voice_assistent.prompt import create_prompt

# Configurações da API
apiKey = "6UlOOoY/kkmprunma/qNDg"
url = "https://gpt-templates.saiapplications.com"
headers = {"X-Api-Key": apiKey}

# Inicialização do motor de texto para voz
engine = pyttsx3.init()

# Inicializa o contexto como uma deque para manter as últimas interações
recent_context = deque(maxlen=10)

# Inicialização do modelo de linguagem
nlp = spacy.load("pt_core_news_sm")

# Função para capturar e processar comandos de voz
def capture_voice_command():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("Por favor, fale o seu comando:")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
            print("Áudio capturado com sucesso.")
            command = recognizer.recognize_google(audio, language='pt-BR')
            print(f"Você disse: {command}")
            return command
        except sr.WaitTimeoutError:
            print("Tempo de espera expirado. Nenhum áudio detectado.")
            return None
        except sr.UnknownValueError:
            print("Não foi possível entender o áudio.")
            return None
        except sr.RequestError as e:
            print(f"Erro ao solicitar resultados do serviço de reconhecimento de fala; {e}")
            return None

# Função para capturar comandos de texto
def capture_text_command():
    command = input("Digite o seu comando: ")
    return command

# Função para converter texto em fala
def speak_text(text):
    if isinstance(text, dict):
        text = extract_values_from_json(text)  # Extrai os valores do dicionário
    cleaned_text = clean_text(text)
    engine.say(cleaned_text)
    engine.runAndWait()

# Função para remover caracteres especiais do texto
def clean_text(text):
    return re.sub(r'[\*\_]', '', text)

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)

def get_text_response(prompt, context, feedback):
    data = {
        "inputs": {
            "str_texto": prompt,
            "str_contexto": context,
            "str_feedback": feedback
        }
    }
    print(f"Enviando dados para a API: {data}")
    try:
        response = requests.post(f"{url}/api/templates/6691e223802f95c2b394a8bd/execute", json=data, headers=headers)
        print(f"Status da resposta: {response.status_code}")
        if response.status_code == 200:
            try:
                response_data = response.html()  # Tente converter a resposta para JSON
                print("Resposta HTML recebida.")
                return extract_values_from_json(response_data)  # Extrai os valores do JSON
            except ValueError:
                print("A resposta não está no formato JSON esperado. Tratando como texto simples.")
                return response.text  # Retorna o texto bruto da resposta
        else:
            print(f"Erro ao acessar a API: {response.status_code}, {response.text}")
            return None
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API: {e}")
        return None

# Função para extrair valores do JSON
def extract_values_from_json(data):
    if isinstance(data, dict):
        return ' '.join([str(value) for value in data.values()])
    elif isinstance(data, list):
        return ' '.join([extract_values_from_json(item) for item in data])
    return str(data)


# Função para consultar todos os contextos da API
def fetch_all_contexts():
    try:
        response = requests.get("http://localhost:8081/contexts/all")
        # Verifica o status da resposta
        if response.status_code == 200:
            data = response.json()  # Obtemos o JSON completo

            # Imprime o JSON completo para verificar o retorno bruto
            print(f"Dados brutos da API: {data}")

            # Acessa a lista de contextos e imprime o tipo de dados
            contexts = data.get('contexts', [])
            print(f"Tipo de dados de 'contexts': {type(contexts)}")
            
            if isinstance(contexts, list):  # Verificamos se é uma lista
                context_str = "\n".join([context['context'] for context in contexts])
                print(f"Contexto obtido da API: {context_str}")  # Adiciona um print para verificar o contexto
                return contexts  # Retorna a lista completa de contextos
            else:
                print(f"Erro: 'contexts' não é uma lista. Dados retornados: {data}")
                return []
        else:
            print(f"Erro ao acessar a API de contextos: {response.status_code}, {response.text}")
            return []
    except requests.RequestException as e:
        print(f"Erro ao fazer requisição para a API de contextos: {e}")
        return []

# Função para interpretar comandos e delegar tarefas
def interpret_command(command, feedback):
    # Atualiza o contexto com base na API antes de elaborar a resposta
    contexts = fetch_all_contexts()
    
    doc = nlp(command)
    if "abrir" in command:
        if "navegador" in command:
            webbrowser.open("http://www.google.com")
            return "Abrindo navegador"
        elif "arquivo" in command or "pasta" in command:
            # Extraia o nome do arquivo ou pasta do comando
            for token in doc:
                if token.pos_ == "NOUN":
                    path = token.text
                    if os.path.exists(path):
                        os.startfile(path)
                        return f"Abrindo {path}"
                    else:
                        return f"Arquivo ou pasta {path} não encontrado"
    elif any(keyword in command.lower() for keyword in ["faça análise", "sentimento", "feedbacks", "feedback"]):
        return get_feedback_analysis_response(command, feedback)
    elif any(keyword in command.lower() for keyword in ["pesquise", "pesquisar", "procure"]):
        return get_online_research_response(command)
    else:
        context_str = "\n".join([context['context'] for context in contexts])  # Converter o contexto para string
        return get_project_response(command, context_str, feedback)

# Função para responder perguntas sobre o projeto
def get_project_response(command, context, feedback):
    prompt = create_prompt(command, context, feedback)
    print(f"Prompt enviado para a API GPT: {prompt}")  # Adiciona um print para verificar o prompt
    return get_text_response(prompt, context, feedback)

# Função para fazer pesquisas online
def get_online_research_response(command):
    prompt = create_prompt(command, "", "")
    return get_text_response(prompt, "", "")

# Função para análise de feedbacks
def get_feedback_analysis_response(command, feedback):
    prompt = create_prompt(command, "", feedback)
    return get_text_response(prompt, "", feedback)

# Loop principal para interação contínua, incluindo o contexto
def main():
    feedback = ""  # Inicializa o feedback como uma string vazia
    while True:
        input_type = input("Você quer usar voz ou texto? (v/t): ").strip().lower()
        if input_type == 'v':
            command = capture_voice_command()
        elif input_type == 't':
            command = capture_text_command()
        else:
            print("Opção inválida. Por favor, escolha 'v' para voz ou 't' para texto.")
            continue

        if command:
            text_response = interpret_command(command, feedback)
            if text_response:
                print(f"Resposta: {text_response}")
                speak_text(text_response)
                # Adiciona a interação recente ao contexto
                recent_context.append((command, text_response))
        else:
            print("Nenhum comando detectado. Aguardando novamente...")
            continue

if __name__ == "__main__":
    main()


User: Analise meus códigos e me envie as partes que posso eliminar e os códigos duplicados.
Assistant: Com base no seu pedido, analisei os códigos fornecidos e identifiquei as seguintes partes que podem ser eliminadas e os códigos duplicados:

**Partes que podem ser eliminadas:**

*   **Arquivos não utilizados:**
    *   `fundamentus_api\setup.py`: Este arquivo é usado para configurar e instalar o pacote `fundamentalvision`. Se você já instalou o pacote e não pretende distribuí-lo, pode remover este arquivo.
    *   `translate\translator_to_english.py`: Este arquivo contém código para traduzir voz do português para o inglês. Se você não precisa dessa funcionalidade, pode remover este arquivo.
    *    `voice_assistent\template.py`: Parece ser um modelo ou código de teste. Se não for usado, pode ser removido.
    *   `test_whisper.py`: Este arquivo parece ser um script de teste para análise de sentimento usando o modelo "neuralmind/bert-base-portuguese-cased". Se você não precisa dessa funcionalidade, pode remover este arquivo.
    *   `send_embeddings_database\verify_last_enbedding.py`: Este arquivo parece ser um script para carregar e imprimir o último embedding salvo em um diretório. Se você não precisa dessa funcionalidade, pode remover este arquivo.

*   **Código comentado ou desnecessário:**
    *   Em vários arquivos, procure por blocos de código comentados que não são mais necessários e podem ser removidos.
    *   Verifique se há logs excessivos ou mensagens de depuração que podem ser removidas após a conclusão do desenvolvimento.

**Códigos duplicados:**

*   **Funções de extração de dados JSON:**
    *   As funções `extract_values_from_json` estão definidas em `voice_assistent\template.py` e `voice_assistent\class_voice_assistent\text_processor.py`. Você deve manter apenas uma definição dessa função em um local comum e reutilizá-la onde for necessário.
*    **Carregar variáveis do ambiente**
    *   O modelo de GenerativeModelHandler carrega e configura a API em `chat_app\services\gpt_services.py` e  `voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py`.

**Observações:**

*   Antes de eliminar qualquer código, certifique-se de fazer um backup para evitar a perda de funcionalidades importantes.
*   Remova arquivos e partes de código desnecessárias para manter o projeto organizado e eficiente.
*   Considere refatorar códigos duplicados para melhorar a manutenção e evitar erros.
User: Faça um resumo do chamado dessa imagem.
2025-04-11 10:18:21,166 [INFO] Resposta da IA (imagem): Com certeza. O chamado na imagem relata um problema com o pedido de compra 302165 (PR2900473), onde a equipe de TI informa que não consegue cadastrar o PIP devido a um erro no tipo de patrimônio que sai do Ariba. É mencionado que a espécie, subespécie e tipo de patrimônio estão errados, e que essa combinação não existe no cadastro do portal. Aparentemente, alguém precisa corrigir as informações no Ariba para que o pedido possa prosseguir.

2025-04-11 10:18:21,267 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-11 10:18:21,270 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-11 10:18:21,272 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-11 10:18:21,273 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-11 10:18:21,275 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-11 10:18:21,277 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-11 10:18:21,278 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-11 10:18:21,279 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-11 10:18:21,281 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-11 10:18:21,283 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-11 10:18:21,285 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-11 10:18:21,286 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-11 10:18:21,288 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-11 10:18:21,290 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-11 10:18:21,291 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-11 10:18:21,293 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-11 10:18:21,294 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-11 10:18:21,295 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-11 10:18:21,297 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-11 10:18:21,298 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-11 10:18:21,300 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-11 10:18:21,301 [INFO] Arquivo lido com sucesso: main.py
2025-04-11 10:18:21,303 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-11 10:18:21,305 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-11 10:18:21,306 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-11 10:18:21,308 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-11 10:18:21,310 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-11 10:18:21,311 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-11 10:18:21,313 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-11 10:18:21,314 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-11 10:18:21,316 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-11 10:18:21,318 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-11 10:18:21,320 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-11 10:18:21,321 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-11 10:18:21,323 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-11 10:18:21,325 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-11 10:18:21,327 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-11 10:18:21,328 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-11 10:18:21,330 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-11 10:18:21,334 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-11 10:18:21,336 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-11 10:18:21,338 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-11 10:18:21,340 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-11 10:18:21,341 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-11 10:18:21,343 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-11 10:18:21,344 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-11 10:18:21,346 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-11 10:18:21,348 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-11 10:18:21,350 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-11 10:18:21,351 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-11 10:18:21,353 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-11 10:18:21,354 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-11 10:18:21,355 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-11 10:18:21,357 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-11 10:27:06,993 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-11 10:27:06,994 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-11 10:27:06,996 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-11 10:27:06,997 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-11 10:27:06,999 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-11 10:27:07,001 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-11 10:27:07,003 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-11 10:27:07,005 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-11 10:27:07,007 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-11 10:27:07,008 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-11 10:27:07,009 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-11 10:27:07,011 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-11 10:27:07,013 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-11 10:27:07,015 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-11 10:27:07,017 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-11 10:27:07,019 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-11 10:27:07,023 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-11 10:27:07,025 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-11 10:27:07,026 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-11 10:27:07,028 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-11 10:27:07,029 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-11 10:27:07,030 [INFO] Arquivo lido com sucesso: main.py
2025-04-11 10:27:07,032 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-11 10:27:07,033 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-11 10:27:07,034 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-11 10:27:07,036 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-11 10:27:07,039 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-11 10:27:07,040 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-11 10:27:07,041 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-11 10:27:07,043 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-11 10:27:07,044 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-11 10:27:07,046 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-11 10:27:07,047 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-11 10:27:07,048 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-11 10:27:07,050 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-11 10:27:07,051 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-11 10:27:07,053 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-11 10:27:07,054 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-11 10:27:07,056 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-11 10:27:07,057 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-11 10:27:07,059 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-11 10:27:07,060 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-11 10:27:07,062 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-11 10:27:07,064 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-11 10:27:07,065 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-11 10:27:07,067 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-11 10:27:07,068 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-11 10:27:07,070 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-11 10:27:07,071 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-11 10:27:07,072 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-11 10:27:07,074 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-11 10:27:07,075 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-11 10:27:07,076 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-11 10:27:07,078 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
2025-04-11 10:27:07,188 [INFO] Arquivo lido com sucesso: app_config\app_config.py
2025-04-11 10:27:07,193 [INFO] Arquivo lido com sucesso: audio_to_text\audio_config\audio_config.py
2025-04-11 10:27:07,196 [INFO] Arquivo lido com sucesso: audio_to_text\audio_to_text.py
2025-04-11 10:27:07,198 [INFO] Arquivo lido com sucesso: chat_app\chat_streamlit.py
2025-04-11 10:27:07,200 [INFO] Arquivo lido com sucesso: chat_app\config\config.py
2025-04-11 10:27:07,202 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\gemini_handler.py
2025-04-11 10:27:07,205 [INFO] Arquivo lido com sucesso: chat_app\core\handlers\signal_handler.py
2025-04-11 10:27:07,207 [INFO] Arquivo lido com sucesso: chat_app\core\logger_config.py
2025-04-11 10:27:07,212 [INFO] Arquivo lido com sucesso: chat_app\core\rate_limiter.py
2025-04-11 10:27:07,213 [INFO] Arquivo lido com sucesso: chat_app\services\document_service.py
2025-04-11 10:27:07,215 [INFO] Arquivo lido com sucesso: chat_app\services\gpt_services.py
2025-04-11 10:27:07,218 [INFO] Arquivo lido com sucesso: chat_app\services\image_processor.py
2025-04-11 10:27:07,220 [INFO] Arquivo lido com sucesso: chat_app\services\image_services.py
2025-04-11 10:27:07,222 [INFO] Arquivo lido com sucesso: chat_app\services\markdown_service.py
2025-04-11 10:27:07,224 [INFO] Arquivo lido com sucesso: chat_app\services\search_files.py
2025-04-11 10:27:07,226 [INFO] Arquivo lido com sucesso: chat_app\utils\file_utils.py
2025-04-11 10:27:07,227 [INFO] Arquivo lido com sucesso: common_paths\common_paths.py
2025-04-11 10:27:07,229 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\__init__.py
2025-04-11 10:27:07,230 [INFO] Arquivo lido com sucesso: fundamentus_api\fundamentus\dados_b3.py
2025-04-11 10:27:07,232 [INFO] Arquivo lido com sucesso: fundamentus_api\setup.py
2025-04-11 10:27:07,234 [INFO] Arquivo lido com sucesso: ia_generator.py
2025-04-11 10:27:07,235 [INFO] Arquivo lido com sucesso: main.py
2025-04-11 10:27:07,237 [INFO] Arquivo lido com sucesso: send_embeddings_database\embedding_config\embedding_config.py
2025-04-11 10:27:07,239 [INFO] Arquivo lido com sucesso: send_embeddings_database\verify_last_enbedding.py
2025-04-11 10:27:07,241 [INFO] Arquivo lido com sucesso: test_whisper.py
2025-04-11 10:27:07,244 [INFO] Arquivo lido com sucesso: text_to_embedding\embedding_processing.py
2025-04-11 10:27:07,246 [INFO] Arquivo lido com sucesso: text_to_embedding\texto_to_embedding.py
2025-04-11 10:27:07,247 [INFO] Arquivo lido com sucesso: transcriptions\transciption_sender_database.py
2025-04-11 10:27:07,248 [INFO] Arquivo lido com sucesso: transcriptions\transcriptions_config.py
2025-04-11 10:27:07,249 [INFO] Arquivo lido com sucesso: translate\translator_to_english.py
2025-04-11 10:27:07,251 [INFO] Arquivo lido com sucesso: translate\whispert_translator.py
2025-04-11 10:27:07,252 [INFO] Arquivo lido com sucesso: video_to_audio\video_config\video_config.py
2025-04-11 10:27:07,255 [INFO] Arquivo lido com sucesso: video_to_audio\video_to_audio.py
2025-04-11 10:27:07,257 [INFO] Arquivo lido com sucesso: voice_assistent\assistent.py
2025-04-11 10:27:07,259 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\api_client.py
2025-04-11 10:27:07,261 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\command_interpreter.py
2025-04-11 10:27:07,263 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\context_manager.py
2025-04-11 10:27:07,265 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\conversation_history.py
2025-04-11 10:27:07,267 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_api_client.py
2025-04-11 10:27:07,269 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\gemini_gpt.py
2025-04-11 10:27:07,272 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\gpt_communication\groq._gpt.py
2025-04-11 10:27:07,274 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\main.py
2025-04-11 10:27:07,275 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt.py
2025-04-11 10:27:07,277 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\default_prompt_generator.py
2025-04-11 10:27:07,279 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\meeting_prompt.py
2025-04-11 10:27:07,281 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\online_prompt.py
2025-04-11 10:27:07,282 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\prompt_generator\prompt_generator.py
2025-04-11 10:27:07,283 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\question_answers_service.py
2025-04-11 10:27:07,285 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_command_hendler.py
2025-04-11 10:27:07,287 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_processor.py
2025-04-11 10:27:07,288 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\text_to_speech.py
2025-04-11 10:27:07,290 [INFO] Arquivo lido com sucesso: voice_assistent\class_voice_assistent\voice_command_hendler.py
2025-04-11 10:27:07,291 [INFO] Arquivo lido com sucesso: voice_assistent\config.py
2025-04-11 10:27:07,293 [INFO] Arquivo lido com sucesso: voice_assistent\template.py
