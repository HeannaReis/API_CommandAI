2025-04-08 00:48:03,585 [INFO] Modelo Gemini 'gemini-2.0-flash-exp' inicializado com sucesso.
2025-04-08 00:48:03,636 [INFO] Prompt carregado com sucesso: Contexto: Voc√™ est√° analisando imagens relacionadas a um passo a passo, para gerar documenta√ß√£o de processos.
As imagens ser√£o enviadas uma por vez e cada uma possue uma etapa do processo, as a√ß√µes que precisam ser documentadas est√£o com marca√ß√µes em vermelho e
com Setas vermelhas, geralmente s√£o cliques ou processo a ser executado.

Sua tarefa √© fornecer explica√ß√µes diretas e claras, sem repeti√ß√µes ou frases gen√©ricas, baseando-se no conte√∫do da imagem.

üìå Diretrizes para a Resposta
Se a imagem mostrar um processo ou configura√ß√£o, explique de forma direta o que est√° sendo feito.

Se a imagem for sobre an√°lise ou resultado, informe claramente o que foi identificado.

Evite: frases como "A imagem representa" ou qualquer introdu√ß√£o vaga.

üí° Exemplo de Resposta Esperada

### [T√≠tulo curto e direto relacionado √† imagem]
[a√ß√£o ou conceito principal], [explicar o processo de forma objetiva].

Se necess√°rio, use marcadores para destacar pontos importantes.

- Objetivo com a resposta do modelo de IA generativa, vou gerar uma documenta√ß√£o do passo a passo que estou executando para conclu√≠r um processo.
  Estou constru√≠ndo material de apoio, para disponibilizar no github.
2025-04-08 00:48:03,647 [INFO] Processando imagem 1/35: 18_Portal_azure_criar_idioma_2.png
2025-04-08 00:48:06,516 [INFO] Resposta da IA (imagem): ### Confirmar Termos e Criar Linguagem no Azure

- Marque a caixa para confirmar que voc√™ revisou e concorda com os termos do Aviso de IA Respons√°vel.

2025-04-08 00:48:06,518 [INFO] Resumo gerado para '18_Portal_azure_criar_idioma_2.png': ### Confirmar Termos e Criar Linguagem no Azure

- Marque a caixa para confirmar que voc√™ revisou e concorda com os termos do Aviso de IA Respons√°vel.

2025-04-08 00:48:06,520 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\18_Portal_azure_criar_idioma_2.png
2025-04-08 00:48:06,725 [INFO] Imagem '18_Portal_azure_criar_idioma_2.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:48:06,726 [INFO] Hist√≥rico atualizado com '18_Portal_azure_criar_idioma_2.png'.
2025-04-08 00:48:10,726 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:48:10,726 [INFO] Processando imagem 2/35: 19_Portal_azure_validacao_create.png
2025-04-08 00:48:12,657 [INFO] Resposta da IA (imagem): ### Criar Linguagem no Azure

- Clique no bot√£o "Create" para iniciar a cria√ß√£o da linguagem no Azure.

2025-04-08 00:48:12,659 [INFO] Resumo gerado para '19_Portal_azure_validacao_create.png': ### Criar Linguagem no Azure

- Clique no bot√£o "Create" para iniciar a cria√ß√£o da linguagem no Azure.

2025-04-08 00:48:12,661 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\19_Portal_azure_validacao_create.png
2025-04-08 00:48:12,863 [INFO] Imagem '19_Portal_azure_validacao_create.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:48:12,863 [INFO] Hist√≥rico atualizado com '19_Portal_azure_validacao_create.png'.
2025-04-08 00:48:16,864 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:48:16,864 [INFO] Processando imagem 3/35: 1_acessar_configuracoes.png
2025-04-08 00:48:19,505 [INFO] Resposta da IA (imagem): ### Acessar Configura√ß√µes do Speech Studio

- Clique no √≠cone de engrenagem para acessar as configura√ß√µes do Speech Studio.

2025-04-08 00:48:19,507 [INFO] Resumo gerado para '1_acessar_configuracoes.png': ### Acessar Configura√ß√µes do Speech Studio

- Clique no √≠cone de engrenagem para acessar as configura√ß√µes do Speech Studio.

2025-04-08 00:48:19,508 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\1_acessar_configuracoes.png
2025-04-08 00:48:19,726 [INFO] Imagem '1_acessar_configuracoes.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:48:19,727 [INFO] Hist√≥rico atualizado com '1_acessar_configuracoes.png'.
2025-04-08 00:48:23,728 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:48:23,728 [INFO] Processando imagem 4/35: 20_azure_studio_select_resources.png
2025-04-08 00:48:28,167 [INFO] Resposta da IA (imagem): ### Selecionar um Recurso do Azure no Language Studio

- Escolha o diret√≥rio do Azure em "Azure directory".
- Selecione a assinatura do Azure em "Azure subscription".
- Escolha o tipo de recurso como "Language" em "Resource type".
- Selecione um recurso existente ou crie um novo em "Resource name".
2025-04-08 00:48:28,168 [INFO] Resumo gerado para '20_azure_studio_select_resources.png': ### Selecionar um Recurso do Azure no Language Studio

- Escolha o diret√≥rio do Azure em "Azure directory".
- Selecione a assinatura do Azure em "Azure subscription".
- Escolha o tipo de recurso como "Language" em "Resource type".
- Selecione um recurso existente ou crie um novo em "Resource name".
2025-04-08 00:48:28,169 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\20_azure_studio_select_resources.png
2025-04-08 00:48:28,366 [INFO] Imagem '20_azure_studio_select_resources.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:48:28,367 [INFO] Hist√≥rico atualizado com '20_azure_studio_select_resources.png'.
2025-04-08 00:48:32,367 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:48:32,368 [INFO] Processando imagem 5/35: 21_azure_studio_criar_nova_solucao.png
2025-04-08 00:48:38,011 [INFO] Resposta da IA (imagem): ### Criar Novo Projeto no Language Studio

- Clique em "Create new" para iniciar a cria√ß√£o de um novo projeto no Language Studio.

2025-04-08 00:48:38,013 [INFO] Resumo gerado para '21_azure_studio_criar_nova_solucao.png': ### Criar Novo Projeto no Language Studio

- Clique em "Create new" para iniciar a cria√ß√£o de um novo projeto no Language Studio.

2025-04-08 00:48:38,014 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\21_azure_studio_criar_nova_solucao.png
2025-04-08 00:48:38,234 [INFO] Imagem '21_azure_studio_criar_nova_solucao.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:48:38,235 [INFO] Hist√≥rico atualizado com '21_azure_studio_criar_nova_solucao.png'.
2025-04-08 00:48:42,236 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:48:42,237 [INFO] Processando imagem 6/35: 22_azure_studio_select_resources_done.png
2025-04-08 00:48:43,978 [INFO] Resposta da IA (imagem): ### Selecionar Recurso do Azure no Language Studio

- Clique em "Done" para concluir a sele√ß√£o do recurso.
2025-04-08 00:48:43,980 [INFO] Resumo gerado para '22_azure_studio_select_resources_done.png': ### Selecionar Recurso do Azure no Language Studio

- Clique em "Done" para concluir a sele√ß√£o do recurso.
2025-04-08 00:48:43,981 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\22_azure_studio_select_resources_done.png
2025-04-08 00:48:44,189 [INFO] Imagem '22_azure_studio_select_resources_done.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:48:44,189 [INFO] Hist√≥rico atualizado com '22_azure_studio_select_resources_done.png'.
2025-04-08 00:48:48,190 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:48:48,192 [INFO] Processando imagem 7/35: 23_azure_studio_-recursos_disponiveis.png
2025-04-08 00:48:51,039 [INFO] Resposta da IA (imagem): ### Criar Novo Recurso no Language Studio

- Clique em "Create new" para selecionar o tipo de recurso a ser criado no Language Studio.
- As op√ß√µes dispon√≠veis s√£o:
  - Conversational language understanding (compreens√£o de linguagem conversacional).
  - Orchestration workflow (fluxo de trabalho de orquestra√ß√£o).
  - Custom question answering (personaliza√ß√£o de respostas a perguntas).
  - Custom text classification (classifica√ß√£o de texto personalizada).
  - Custom named entity recognition (reconhecimento de entidade nomeada personalizada).
  - Custom conversation summarization (sumariza√ß√£o de conversas personalizada).
  - Custom abstractive summarization (sumariza√ß√£o abstrata personalizada).
2025-04-08 00:48:51,043 [INFO] Resumo gerado para '23_azure_studio_-recursos_disponiveis.png': ### Criar Novo Recurso no Language Studio

- Clique em "Create new" para selecionar o tipo de recurso a ser criado no Language Studio.
- As op√ß√µes dispon√≠veis s√£o:
  - Conversational language understanding (compreens√£o de linguagem conversacional).
  - Orchestration workflow (fluxo de trabalho de orquestra√ß√£o).
  - Custom question answering (personaliza√ß√£o de respostas a perguntas).
  - Custom text classification (classifica√ß√£o de texto personalizada).
  - Custom named entity recognition (reconhecimento de entidade nomeada personalizada).
  - Custom conversation summarization (sumariza√ß√£o de conversas personalizada).
  - Custom abstractive summarization (sumariza√ß√£o abstrata personalizada).
2025-04-08 00:48:51,047 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\23_azure_studio_-recursos_disponiveis.png
2025-04-08 00:48:51,253 [INFO] Imagem '23_azure_studio_-recursos_disponiveis.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:48:51,253 [INFO] Hist√≥rico atualizado com '23_azure_studio_-recursos_disponiveis.png'.
2025-04-08 00:48:55,255 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:48:55,256 [INFO] Processando imagem 8/35: 24_azure_studio_-escolha_modelo.png
2025-04-08 00:48:57,464 [INFO] Resposta da IA (imagem): ### Selecionar Modelo de Linguagem no Language Studio
Clique em "Classify text" para iniciar a configura√ß√£o do modelo de linguagem.
2025-04-08 00:48:57,466 [INFO] Resumo gerado para '24_azure_studio_-escolha_modelo.png': ### Selecionar Modelo de Linguagem no Language Studio
Clique em "Classify text" para iniciar a configura√ß√£o do modelo de linguagem.
2025-04-08 00:48:57,467 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\24_azure_studio_-escolha_modelo.png
2025-04-08 00:48:57,678 [INFO] Imagem '24_azure_studio_-escolha_modelo.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:48:57,678 [INFO] Hist√≥rico atualizado com '24_azure_studio_-escolha_modelo.png'.
2025-04-08 00:49:01,679 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:49:01,681 [INFO] Processando imagem 9/35: 25_azure_studio_digitar_mensagem.png
2025-04-08 00:49:04,002 [INFO] Resposta da IA (imagem): ### Inserir Texto para An√°lise no Language Studio

- Selecione o idioma do texto em "Select text language".
- Escolha o recurso do Azure em "Select your Azure resource".
- Digite ou cole o texto a ser analisado no campo de texto.
- Ative ou desative a op√ß√£o "Enable opinion mining" conforme necess√°rio.
2025-04-08 00:49:04,004 [INFO] Resumo gerado para '25_azure_studio_digitar_mensagem.png': ### Inserir Texto para An√°lise no Language Studio

- Selecione o idioma do texto em "Select text language".
- Escolha o recurso do Azure em "Select your Azure resource".
- Digite ou cole o texto a ser analisado no campo de texto.
- Ative ou desative a op√ß√£o "Enable opinion mining" conforme necess√°rio.
2025-04-08 00:49:04,006 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\25_azure_studio_digitar_mensagem.png
2025-04-08 00:49:04,215 [INFO] Imagem '25_azure_studio_digitar_mensagem.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:49:04,215 [INFO] Hist√≥rico atualizado com '25_azure_studio_digitar_mensagem.png'.
2025-04-08 00:49:08,216 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:49:08,217 [INFO] Processando imagem 10/35: 26_azure_studio_run_model.png
2025-04-08 00:49:10,346 [INFO] Resposta da IA (imagem): ### Executar An√°lise de Sentimento no Language Studio

- Marque a caixa para confirmar que voc√™ est√° ciente dos custos de uso do Azure.
- Clique em "Run" para executar a an√°lise de sentimento no texto inserido.
2025-04-08 00:49:10,347 [INFO] Resumo gerado para '26_azure_studio_run_model.png': ### Executar An√°lise de Sentimento no Language Studio

- Marque a caixa para confirmar que voc√™ est√° ciente dos custos de uso do Azure.
- Clique em "Run" para executar a an√°lise de sentimento no texto inserido.
2025-04-08 00:49:10,347 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\26_azure_studio_run_model.png
2025-04-08 00:49:10,537 [INFO] Imagem '26_azure_studio_run_model.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:49:10,538 [INFO] Hist√≥rico atualizado com '26_azure_studio_run_model.png'.
2025-04-08 00:49:14,539 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:49:14,540 [INFO] Processando imagem 11/35: 27_azure_studio_resultado1.png
2025-04-08 00:49:18,034 [INFO] Resposta da IA (imagem): ### An√°lise de Sentimento do Texto

- O texto foi classificado como majoritariamente negativo, com uma confian√ßa de 96%.
- A frase "Tired hotel with poor service" foi identificada com an√°lise de sentimento, mostrando a associa√ß√£o de "hotel" com o aspecto negativo de "poor service".
2025-04-08 00:49:18,035 [INFO] Resumo gerado para '27_azure_studio_resultado1.png': ### An√°lise de Sentimento do Texto

- O texto foi classificado como majoritariamente negativo, com uma confian√ßa de 96%.
- A frase "Tired hotel with poor service" foi identificada com an√°lise de sentimento, mostrando a associa√ß√£o de "hotel" com o aspecto negativo de "poor service".
2025-04-08 00:49:18,036 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\27_azure_studio_resultado1.png
2025-04-08 00:49:18,219 [INFO] Imagem '27_azure_studio_resultado1.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:49:18,219 [INFO] Hist√≥rico atualizado com '27_azure_studio_resultado1.png'.
2025-04-08 00:49:22,220 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:49:22,221 [INFO] Processando imagem 12/35: 28_azure_studio_resultado2.png
2025-04-08 00:49:29,858 [INFO] Resposta da IA (imagem): ### An√°lise de Sentimento Detalhada no Azure Language Studio

- A an√°lise de sentimento da frase resultou em:
  - Negativo: 98.00%
  - Neutro: 1.00%
  - Positivo: 0.00%
- Identifica√ß√£o de "hotel" como alvo de "tired" (negativo, 99.00%) e "service" como alvo de "poor" (negativo, 99.00%).

2025-04-08 00:49:29,859 [INFO] Resumo gerado para '28_azure_studio_resultado2.png': ### An√°lise de Sentimento Detalhada no Azure Language Studio

- A an√°lise de sentimento da frase resultou em:
  - Negativo: 98.00%
  - Neutro: 1.00%
  - Positivo: 0.00%
- Identifica√ß√£o de "hotel" como alvo de "tired" (negativo, 99.00%) e "service" como alvo de "poor" (negativo, 99.00%).

2025-04-08 00:49:29,860 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\28_azure_studio_resultado2.png
2025-04-08 00:49:30,042 [INFO] Imagem '28_azure_studio_resultado2.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:49:30,042 [INFO] Hist√≥rico atualizado com '28_azure_studio_resultado2.png'.
2025-04-08 00:49:34,043 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:49:34,043 [INFO] Processando imagem 13/35: 29_azure_studio_resultado3.png
2025-04-08 00:49:35,625 [INFO] Resposta da IA (imagem): ### An√°lise de Sentimento Detalhada no Azure Language Studio

- Continua√ß√£o da an√°lise de sentimento da frase.
- Indica que mais an√°lises podem ser realizadas em outras frases ("Sentence 2" e "Sentence 3").
2025-04-08 00:49:35,628 [INFO] Resumo gerado para '29_azure_studio_resultado3.png': ### An√°lise de Sentimento Detalhada no Azure Language Studio

- Continua√ß√£o da an√°lise de sentimento da frase.
- Indica que mais an√°lises podem ser realizadas em outras frases ("Sentence 2" e "Sentence 3").
2025-04-08 00:49:35,630 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\29_azure_studio_resultado3.png
2025-04-08 00:49:35,838 [INFO] Imagem '29_azure_studio_resultado3.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:49:35,838 [INFO] Hist√≥rico atualizado com '29_azure_studio_resultado3.png'.
2025-04-08 00:49:39,839 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:49:39,840 [INFO] Processando imagem 14/35: 2_criar_novo_recurso.png
2025-04-08 00:49:41,404 [INFO] Resposta da IA (imagem): ### Criar Novo Recurso no Azure Speech Studio
Clique em "+ Criar novo recurso" para iniciar a cria√ß√£o de um novo recurso.
2025-04-08 00:49:41,405 [INFO] Resumo gerado para '2_criar_novo_recurso.png': ### Criar Novo Recurso no Azure Speech Studio
Clique em "+ Criar novo recurso" para iniciar a cria√ß√£o de um novo recurso.
2025-04-08 00:49:41,405 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\2_criar_novo_recurso.png
2025-04-08 00:49:41,591 [INFO] Imagem '2_criar_novo_recurso.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:49:41,591 [INFO] Hist√≥rico atualizado com '2_criar_novo_recurso.png'.
2025-04-08 00:49:45,592 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:49:45,592 [INFO] Processando imagem 15/35: 30_azure_studio_resultado4.png
2025-04-08 00:49:47,416 [INFO] Resposta da IA (imagem): ### An√°lise de Sentimento e Opini√£o do Texto no Language Studio

- Identifica√ß√£o do termo "internet" como o alvo ("target") da avalia√ß√£o.
- A avalia√ß√£o ("assessment") relacionada a "internet" √© que "didn't work".
2025-04-08 00:49:47,418 [INFO] Resumo gerado para '30_azure_studio_resultado4.png': ### An√°lise de Sentimento e Opini√£o do Texto no Language Studio

- Identifica√ß√£o do termo "internet" como o alvo ("target") da avalia√ß√£o.
- A avalia√ß√£o ("assessment") relacionada a "internet" √© que "didn't work".
2025-04-08 00:49:47,420 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\30_azure_studio_resultado4.png
2025-04-08 00:49:47,672 [INFO] Imagem '30_azure_studio_resultado4.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:49:47,673 [INFO] Hist√≥rico atualizado com '30_azure_studio_resultado4.png'.
2025-04-08 00:49:51,674 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:49:51,675 [INFO] Processando imagem 16/35: 31_azure_proximos_passos.png
2025-04-08 00:49:53,875 [INFO] Resposta da IA (imagem): ### Pr√≥ximos Passos para a Cria√ß√£o de Linguagem no Azure

- Verifique se voc√™ tem uma conta do Azure com recursos de Linguagem ou Servi√ßos Cognitivos pagos.
- Selecione um recurso de Linguagem existente ou crie um novo.
- Regi√£o: eastus
- Endpoint da linguagem: https://idiomalanguage.cognitiveservices.azure.com/
- Ap√≥s criar o recurso, configure o ambiente e instale o SDK da Linguagem.
- Clique em "Open the quick start" para seguir o guia de in√≠cio r√°pido ou "View samples on GitHub" para ver exemplos de c√≥digo.
2025-04-08 00:49:53,876 [INFO] Resumo gerado para '31_azure_proximos_passos.png': ### Pr√≥ximos Passos para a Cria√ß√£o de Linguagem no Azure

- Verifique se voc√™ tem uma conta do Azure com recursos de Linguagem ou Servi√ßos Cognitivos pagos.
- Selecione um recurso de Linguagem existente ou crie um novo.
- Regi√£o: eastus
- Endpoint da linguagem: https://idiomalanguage.cognitiveservices.azure.com/
- Ap√≥s criar o recurso, configure o ambiente e instale o SDK da Linguagem.
- Clique em "Open the quick start" para seguir o guia de in√≠cio r√°pido ou "View samples on GitHub" para ver exemplos de c√≥digo.
2025-04-08 00:49:53,877 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\31_azure_proximos_passos.png
2025-04-08 00:49:54,088 [INFO] Imagem '31_azure_proximos_passos.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:49:54,089 [INFO] Hist√≥rico atualizado com '31_azure_proximos_passos.png'.
2025-04-08 00:49:58,090 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:49:58,091 [INFO] Processando imagem 17/35: 32_azure_analise_codigo.png
2025-04-08 00:50:00,058 [INFO] Resposta da IA (imagem): ### Obter SDK e Amostras de C√≥digo
Para come√ßar a utilizar a An√°lise de Sentimento, siga os passos abaixo:

- Clique em "Get SDK" para obter o SDK necess√°rio para integrar a funcionalidade em sua aplica√ß√£o.
- Clique em "View samples on GitHub" para acessar exemplos de c√≥digo que podem auxiliar no desenvolvimento.
2025-04-08 00:50:00,060 [INFO] Resumo gerado para '32_azure_analise_codigo.png': ### Obter SDK e Amostras de C√≥digo
Para come√ßar a utilizar a An√°lise de Sentimento, siga os passos abaixo:

- Clique em "Get SDK" para obter o SDK necess√°rio para integrar a funcionalidade em sua aplica√ß√£o.
- Clique em "View samples on GitHub" para acessar exemplos de c√≥digo que podem auxiliar no desenvolvimento.
2025-04-08 00:50:00,061 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\32_azure_analise_codigo.png
2025-04-08 00:50:00,365 [INFO] Imagem '32_azure_analise_codigo.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:50:00,365 [INFO] Hist√≥rico atualizado com '32_azure_analise_codigo.png'.
2025-04-08 00:50:04,366 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:50:04,366 [INFO] Processando imagem 18/35: 33_azure_cobranca.png
2025-04-08 00:50:06,697 [INFO] Resposta da IA (imagem): ### Acessar Informa√ß√µes de Pre√ßos do Azure Language Studio
Para verificar os pre√ßos dos servi√ßos de linguagem no Azure Language Studio, siga os passos:
- Clique em "See pricing" no menu lateral.
- Para obter informa√ß√µes detalhadas sobre os pre√ßos, visite "Azure Language services pricing" ou utilize a calculadora de pre√ßos do Azure em "Azure pricing calculator".
- Para solicitar um or√ßamento de pre√ßos, clique em "Talk to a sales specialist".
2025-04-08 00:50:06,699 [INFO] Resumo gerado para '33_azure_cobranca.png': ### Acessar Informa√ß√µes de Pre√ßos do Azure Language Studio
Para verificar os pre√ßos dos servi√ßos de linguagem no Azure Language Studio, siga os passos:
- Clique em "See pricing" no menu lateral.
- Para obter informa√ß√µes detalhadas sobre os pre√ßos, visite "Azure Language services pricing" ou utilize a calculadora de pre√ßos do Azure em "Azure pricing calculator".
- Para solicitar um or√ßamento de pre√ßos, clique em "Talk to a sales specialist".
2025-04-08 00:50:06,700 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\33_azure_cobranca.png
2025-04-08 00:50:06,961 [INFO] Imagem '33_azure_cobranca.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:50:06,962 [INFO] Hist√≥rico atualizado com '33_azure_cobranca.png'.
2025-04-08 00:50:10,962 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:50:10,963 [INFO] Processando imagem 19/35: 34_azure_resposabilidade.png
2025-04-08 00:50:13,224 [INFO] Resposta da IA (imagem): 34_azure_resposabilidade.png: ### Responsabilidade no Uso da IA no Azure Language Studio

- Selecione "Responsible use of AI" para obter informa√ß√µes sobre o uso respons√°vel da IA com os Cognitive Services.
- Saiba mais sobre os princ√≠pios de IA da Microsoft.

2025-04-08 00:50:13,225 [INFO] Resumo gerado para '34_azure_resposabilidade.png': 34_azure_resposabilidade.png: ### Responsabilidade no Uso da IA no Azure Language Studio

- Selecione "Responsible use of AI" para obter informa√ß√µes sobre o uso respons√°vel da IA com os Cognitive Services.
- Saiba mais sobre os princ√≠pios de IA da Microsoft.

2025-04-08 00:50:13,225 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\34_azure_resposabilidade.png
2025-04-08 00:50:13,535 [INFO] Imagem '34_azure_resposabilidade.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:50:13,536 [INFO] Hist√≥rico atualizado com '34_azure_resposabilidade.png'.
2025-04-08 00:50:17,536 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:50:17,537 [INFO] Processando imagem 20/35: 3_criar_recurso_fala.png
2025-04-08 00:50:20,328 [INFO] Resposta da IA (imagem): 3_criar_recurso_fala.png: ### Criar Recurso de Fala no Azure

- Preencha os dados para criar um recurso de fala no Azure:
  - Nome do novo recurso: test_azure_speech
  - Assinatura: Assinatura do Azure 1
  - Regi√£o: Leste dos EUA
  - Tipo de pre√ßo: Gratuito F0
  - Grupo de recursos: NetworkWatcherRG
- Clique em "Criar um recurso" para finalizar.
2025-04-08 00:50:20,331 [INFO] Resumo gerado para '3_criar_recurso_fala.png': 3_criar_recurso_fala.png: ### Criar Recurso de Fala no Azure

- Preencha os dados para criar um recurso de fala no Azure:
  - Nome do novo recurso: test_azure_speech
  - Assinatura: Assinatura do Azure 1
  - Regi√£o: Leste dos EUA
  - Tipo de pre√ßo: Gratuito F0
  - Grupo de recursos: NetworkWatcherRG
- Clique em "Criar um recurso" para finalizar.
2025-04-08 00:50:20,333 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\3_criar_recurso_fala.png
2025-04-08 00:50:20,539 [INFO] Imagem '3_criar_recurso_fala.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:50:20,539 [INFO] Hist√≥rico atualizado com '3_criar_recurso_fala.png'.
2025-04-08 00:50:24,540 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:50:24,541 [INFO] Processando imagem 21/35: 4_recursos_criados_em_uso.png
2025-04-08 00:50:28,068 [INFO] Resposta da IA (imagem): 4_recursos_criados_em_uso.png: ### Recursos Criados e Em Uso no Azure
Lista os recursos criados no Azure:

- Selecione o diret√≥rio, recurso e geral a serem usados.
- Clique em "fechar" para finalizar a sele√ß√£o.
- Selecione o nome do recurso, no caso "aiassistspeech".
- Clique em "Usar o recurso".
2025-04-08 00:50:28,069 [INFO] Resumo gerado para '4_recursos_criados_em_uso.png': 4_recursos_criados_em_uso.png: ### Recursos Criados e Em Uso no Azure
Lista os recursos criados no Azure:

- Selecione o diret√≥rio, recurso e geral a serem usados.
- Clique em "fechar" para finalizar a sele√ß√£o.
- Selecione o nome do recurso, no caso "aiassistspeech".
- Clique em "Usar o recurso".
2025-04-08 00:50:28,069 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\4_recursos_criados_em_uso.png
2025-04-08 00:50:28,269 [INFO] Imagem '4_recursos_criados_em_uso.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:50:28,269 [INFO] Hist√≥rico atualizado com '4_recursos_criados_em_uso.png'.
2025-04-08 00:50:32,270 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:50:32,271 [INFO] Processando imagem 22/35: 5_selecionar_modelo.png
2025-04-08 00:50:36,237 [INFO] Resposta da IA (imagem): ### Selecionar Modelo de Convers√£o de Fala em Texto

- Clique em "Convers√£o de fala em texto em tempo real" para testar a transcri√ß√£o de √°udio ao vivo sem c√≥digo.

2025-04-08 00:50:36,239 [INFO] Resumo gerado para '5_selecionar_modelo.png': ### Selecionar Modelo de Convers√£o de Fala em Texto

- Clique em "Convers√£o de fala em texto em tempo real" para testar a transcri√ß√£o de √°udio ao vivo sem c√≥digo.

2025-04-08 00:50:36,240 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\5_selecionar_modelo.png
2025-04-08 00:50:36,434 [INFO] Imagem '5_selecionar_modelo.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:50:36,435 [INFO] Hist√≥rico atualizado com '5_selecionar_modelo.png'.
2025-04-08 00:50:40,435 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:50:40,436 [INFO] Processando imagem 23/35: 6_configuracoes_modelo.png
2025-04-08 00:50:42,796 [INFO] Resposta da IA (imagem): ### Configura√ß√µes do Modelo de Fala em Texto em Tempo Real

- Configurar a Convers√£o de fala em texto em tempo real.
- Para testar, grave √°udio em um ponto de extremidade de reconhecimento de fala sem escrever c√≥digo.
- √â poss√≠vel ver a documenta√ß√£o, exibir exemplos de c√≥digo ou usar a CLI de fala.
- Opcionalmente, configurar para usar um recurso espec√≠fico, selecionando "Reconhe√ßo que este aplicativo usa o recurso aiassistspeech e incorrer√° no uso da minha conta".
- √â poss√≠vel tamb√©m escolher o idioma para Ingl√™s (Estados Unidos) e mostrar as op√ß√µes avan√ßadas.
- As plataformas dispon√≠veis s√£o Cont√™iner do Docker e Nuvem.
2025-04-08 00:50:42,799 [INFO] Resumo gerado para '6_configuracoes_modelo.png': ### Configura√ß√µes do Modelo de Fala em Texto em Tempo Real

- Configurar a Convers√£o de fala em texto em tempo real.
- Para testar, grave √°udio em um ponto de extremidade de reconhecimento de fala sem escrever c√≥digo.
- √â poss√≠vel ver a documenta√ß√£o, exibir exemplos de c√≥digo ou usar a CLI de fala.
- Opcionalmente, configurar para usar um recurso espec√≠fico, selecionando "Reconhe√ßo que este aplicativo usa o recurso aiassistspeech e incorrer√° no uso da minha conta".
- √â poss√≠vel tamb√©m escolher o idioma para Ingl√™s (Estados Unidos) e mostrar as op√ß√µes avan√ßadas.
- As plataformas dispon√≠veis s√£o Cont√™iner do Docker e Nuvem.
2025-04-08 00:50:42,801 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\6_configuracoes_modelo.png
2025-04-08 00:50:43,017 [INFO] Imagem '6_configuracoes_modelo.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:50:43,018 [INFO] Hist√≥rico atualizado com '6_configuracoes_modelo.png'.
2025-04-08 00:50:47,018 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:50:47,019 [INFO] Processando imagem 24/35: 7_playground_modelo.png
2025-04-08 00:50:48,842 [INFO] Resposta da IA (imagem): ### Escolher e Testar Arquivos de √Åudio no Speech Studio

- Arraste e solte arquivos de √°udio ou grave diretamente.
- Os resultados da convers√£o de fala em texto ser√£o exibidos assim que um √°udio for carregado.
2025-04-08 00:50:48,844 [INFO] Resumo gerado para '7_playground_modelo.png': ### Escolher e Testar Arquivos de √Åudio no Speech Studio

- Arraste e solte arquivos de √°udio ou grave diretamente.
- Os resultados da convers√£o de fala em texto ser√£o exibidos assim que um √°udio for carregado.
2025-04-08 00:50:48,845 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\7_playground_modelo.png
2025-04-08 00:50:49,051 [INFO] Imagem '7_playground_modelo.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:50:49,052 [INFO] Hist√≥rico atualizado com '7_playground_modelo.png'.
2025-04-08 00:50:53,052 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:50:53,054 [INFO] Processando imagem 25/35: 8_resultado_audio_para_texto.png
2025-04-08 00:50:55,229 [INFO] Resposta da IA (imagem): ### Exibir Texto Transcrito do √Åudio no Speech Studio

- O texto transcrito do arquivo de √°udio selecionado ("Be Simple Past.mp3") √© exibido na √°rea de resultados, mostrando a convers√£o da fala em texto.
2025-04-08 00:50:55,230 [INFO] Resumo gerado para '8_resultado_audio_para_texto.png': ### Exibir Texto Transcrito do √Åudio no Speech Studio

- O texto transcrito do arquivo de √°udio selecionado ("Be Simple Past.mp3") √© exibido na √°rea de resultados, mostrando a convers√£o da fala em texto.
2025-04-08 00:50:55,231 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\8_resultado_audio_para_texto.png
2025-04-08 00:50:55,479 [INFO] Imagem '8_resultado_audio_para_texto.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:50:55,480 [INFO] Hist√≥rico atualizado com '8_resultado_audio_para_texto.png'.
2025-04-08 00:50:59,480 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:50:59,481 [INFO] Processando imagem 26/35: 9_proximas_etapas_introducao.png
2025-04-08 00:51:01,815 [INFO] Resposta da IA (imagem): 9_proximas_etapas_introducao.png: ### Pr√≥ximos Passos e Sele√ß√£o de Recurso de Fala no Azure

- Selecione um recurso do Servi√ßo Cognitivo do Azure para Fala.
- Escolha um recurso para exibir chaves e regi√£o para configura√ß√£o de fala.
- √â poss√≠vel selecionar entre um recurso existente ou criar um novo.
- Ap√≥s criar o recurso, siga o in√≠cio r√°pido para executar o c√≥digo de exemplo.
- Para acessar o guia de in√≠cio r√°pido, clique em "Abrir o in√≠cio r√°pido" ou "Veja o c√≥digo de exemplo no GitHub".
2025-04-08 00:51:01,815 [INFO] Resumo gerado para '9_proximas_etapas_introducao.png': 9_proximas_etapas_introducao.png: ### Pr√≥ximos Passos e Sele√ß√£o de Recurso de Fala no Azure

- Selecione um recurso do Servi√ßo Cognitivo do Azure para Fala.
- Escolha um recurso para exibir chaves e regi√£o para configura√ß√£o de fala.
- √â poss√≠vel selecionar entre um recurso existente ou criar um novo.
- Ap√≥s criar o recurso, siga o in√≠cio r√°pido para executar o c√≥digo de exemplo.
- Para acessar o guia de in√≠cio r√°pido, clique em "Abrir o in√≠cio r√°pido" ou "Veja o c√≥digo de exemplo no GitHub".
2025-04-08 00:51:01,817 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\9_proximas_etapas_introducao.png
2025-04-08 00:51:02,031 [INFO] Imagem '9_proximas_etapas_introducao.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:51:02,031 [INFO] Hist√≥rico atualizado com '9_proximas_etapas_introducao.png'.
2025-04-08 00:51:06,033 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:51:06,034 [INFO] Processando imagem 27/35: taxas.png
2025-04-08 00:51:08,284 [INFO] Resposta da IA (imagem): ### Visualiza√ß√£o de Pre√ßos e Recursos Gratuitos no Azure

- A imagem apresenta uma tabela comparativa de recursos gratuitos oferecidos pelo Azure.
- A tabela detalha as categorias (Convers√£o de Fala em Texto, Convers√£o de Texto em Fala, Tradu√ß√£o de Fala, Reconhecimento do Locutor), funcionalidades (Padr√£o, Personalizado, Neural, Verifica√ß√£o/Identifica√ß√£o do Locutor, Armazenamento do Perfil de Voz), e os pre√ßos, indicando as quantidades gratuitas por m√™s para cada funcionalidade.
- Informa√ß√µes adicionais sobre cotas, limites e instru√ß√µes para aumentar solicita√ß√µes simult√¢neas podem ser encontradas na documenta√ß√£o.

2025-04-08 00:51:08,285 [INFO] Resumo gerado para 'taxas.png': ### Visualiza√ß√£o de Pre√ßos e Recursos Gratuitos no Azure

- A imagem apresenta uma tabela comparativa de recursos gratuitos oferecidos pelo Azure.
- A tabela detalha as categorias (Convers√£o de Fala em Texto, Convers√£o de Texto em Fala, Tradu√ß√£o de Fala, Reconhecimento do Locutor), funcionalidades (Padr√£o, Personalizado, Neural, Verifica√ß√£o/Identifica√ß√£o do Locutor, Armazenamento do Perfil de Voz), e os pre√ßos, indicando as quantidades gratuitas por m√™s para cada funcionalidade.
- Informa√ß√µes adicionais sobre cotas, limites e instru√ß√µes para aumentar solicita√ß√µes simult√¢neas podem ser encontradas na documenta√ß√£o.

2025-04-08 00:51:08,285 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\taxas.png
2025-04-08 00:51:08,595 [INFO] Imagem 'taxas.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:51:08,596 [INFO] Hist√≥rico atualizado com 'taxas.png'.
2025-04-08 00:51:12,599 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:51:12,602 [INFO] Processando imagem 28/35: 10_proximas_etapas_cenarios_cumuns.png
2025-04-08 00:51:15,253 [INFO] Resposta da IA (imagem): 10_proximas_etapas_cenarios_cumuns.png: ### Pr√≥ximos Passos em Cen√°rios Comuns do Speech Studio

- Clique em "Cen√°rios comuns" para explorar os cen√°rios de convers√£o de fala em texto.
- Explore "Tente legendar com a convers√£o de fala em texto" para usar o Servi√ßo Cognitivo do Azure para Fala para legendar v√≠deos automaticamente.
- Explore "Transcri√ß√£o e an√°lise de call center com o Fala e Idioma do Azure" para transcrever e analisar grava√ß√µes de call center.

2025-04-08 00:51:15,254 [INFO] Resumo gerado para '10_proximas_etapas_cenarios_cumuns.png': 10_proximas_etapas_cenarios_cumuns.png: ### Pr√≥ximos Passos em Cen√°rios Comuns do Speech Studio

- Clique em "Cen√°rios comuns" para explorar os cen√°rios de convers√£o de fala em texto.
- Explore "Tente legendar com a convers√£o de fala em texto" para usar o Servi√ßo Cognitivo do Azure para Fala para legendar v√≠deos automaticamente.
- Explore "Transcri√ß√£o e an√°lise de call center com o Fala e Idioma do Azure" para transcrever e analisar grava√ß√µes de call center.

2025-04-08 00:51:15,255 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\10_proximas_etapas_cenarios_cumuns.png
2025-04-08 00:51:15,466 [INFO] Imagem '10_proximas_etapas_cenarios_cumuns.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:51:15,467 [INFO] Hist√≥rico atualizado com '10_proximas_etapas_cenarios_cumuns.png'.
2025-04-08 00:51:19,467 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:51:19,468 [INFO] Processando imagem 29/35: 11_proximas_etapas_servicos_relacionados.png
2025-04-08 00:51:22,416 [INFO] Resposta da IA (imagem): ### Pr√≥ximos Passos em Servi√ßos Relacionados do Speech Studio

- Clique em "Servi√ßos relacionados" para explorar os servi√ßos relacionados √† convers√£o de fala em texto.
  - Fala Personalizada: Melhora o reconhecimento de fala com a Fala Personalizada, adaptando-a ao vocabul√°rio e estilo de fala dos seus usu√°rios.
  - Galeria de Servi√ßo de Voz: Encontre o locutor perfeito para seu projeto, com mais de 150 vozes em 500 idiomas e variantes.
  - Avalia√ß√£o de Pron√∫ncia: Receba coment√°rios instant√¢neos sobre a precis√£o e flu√™ncia da pron√∫ncia para aumentar a fala e fazer apresenta√ß√µes com confian√ßa.

2025-04-08 00:51:22,417 [INFO] Resumo gerado para '11_proximas_etapas_servicos_relacionados.png': ### Pr√≥ximos Passos em Servi√ßos Relacionados do Speech Studio

- Clique em "Servi√ßos relacionados" para explorar os servi√ßos relacionados √† convers√£o de fala em texto.
  - Fala Personalizada: Melhora o reconhecimento de fala com a Fala Personalizada, adaptando-a ao vocabul√°rio e estilo de fala dos seus usu√°rios.
  - Galeria de Servi√ßo de Voz: Encontre o locutor perfeito para seu projeto, com mais de 150 vozes em 500 idiomas e variantes.
  - Avalia√ß√£o de Pron√∫ncia: Receba coment√°rios instant√¢neos sobre a precis√£o e flu√™ncia da pron√∫ncia para aumentar a fala e fazer apresenta√ß√µes com confian√ßa.

2025-04-08 00:51:22,418 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\11_proximas_etapas_servicos_relacionados.png
2025-04-08 00:51:22,669 [INFO] Imagem '11_proximas_etapas_servicos_relacionados.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:51:22,669 [INFO] Hist√≥rico atualizado com '11_proximas_etapas_servicos_relacionados.png'.
2025-04-08 00:51:26,670 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:51:26,671 [INFO] Processando imagem 30/35: 12_proximas_etapas_avaliar_preco.png
2025-04-08 00:51:28,622 [INFO] Resposta da IA (imagem): 12_proximas_etapas_avaliar_preco.png: ### Avaliar Pre√ßos da Convers√£o de Fala em Texto no Azure

- Para obter informa√ß√µes sobre pre√ßos, visite "Pre√ßos dos servi√ßos do Fala do Azure" ou use a "Calculadora de pre√ßos do Azure".
- Para solicitar um or√ßamento, clique em "Converse com um especialista de vendas".
2025-04-08 00:51:28,625 [INFO] Resumo gerado para '12_proximas_etapas_avaliar_preco.png': 12_proximas_etapas_avaliar_preco.png: ### Avaliar Pre√ßos da Convers√£o de Fala em Texto no Azure

- Para obter informa√ß√µes sobre pre√ßos, visite "Pre√ßos dos servi√ßos do Fala do Azure" ou use a "Calculadora de pre√ßos do Azure".
- Para solicitar um or√ßamento, clique em "Converse com um especialista de vendas".
2025-04-08 00:51:28,626 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\12_proximas_etapas_avaliar_preco.png
2025-04-08 00:51:28,893 [INFO] Imagem '12_proximas_etapas_avaliar_preco.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:51:28,893 [INFO] Hist√≥rico atualizado com '12_proximas_etapas_avaliar_preco.png'.
2025-04-08 00:51:32,894 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:51:32,895 [INFO] Processando imagem 31/35: 13_proximas_etapas_uso_responsavel_ia.png
2025-04-08 00:51:35,727 [INFO] Resposta da IA (imagem): ### Responsabilidade no Uso da IA no Azure Language Studio

- Selecione "Responsible use of AI" para obter informa√ß√µes sobre o uso respons√°vel da IA com os Cognitive Services.
- Saiba mais sobre os princ√≠pios de IA da Microsoft.
2025-04-08 00:51:35,728 [INFO] Resumo gerado para '13_proximas_etapas_uso_responsavel_ia.png': ### Responsabilidade no Uso da IA no Azure Language Studio

- Selecione "Responsible use of AI" para obter informa√ß√µes sobre o uso respons√°vel da IA com os Cognitive Services.
- Saiba mais sobre os princ√≠pios de IA da Microsoft.
2025-04-08 00:51:35,729 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\13_proximas_etapas_uso_responsavel_ia.png
2025-04-08 00:51:35,986 [INFO] Imagem '13_proximas_etapas_uso_responsavel_ia.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:51:35,986 [INFO] Hist√≥rico atualizado com '13_proximas_etapas_uso_responsavel_ia.png'.
2025-04-08 00:51:39,987 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:51:39,988 [INFO] Processando imagem 32/35: 14_Portal_azure_servico_inicial.png
2025-04-08 00:51:42,117 [INFO] Resposta da IA (imagem): ### Acessar Azure AI services no Azure

- Clique em "Azure AI services" para acessar os recursos relacionados √† intelig√™ncia artificial no Azure.
2025-04-08 00:51:42,118 [INFO] Resumo gerado para '14_Portal_azure_servico_inicial.png': ### Acessar Azure AI services no Azure

- Clique em "Azure AI services" para acessar os recursos relacionados √† intelig√™ncia artificial no Azure.
2025-04-08 00:51:42,118 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\14_Portal_azure_servico_inicial.png
2025-04-08 00:51:42,362 [INFO] Imagem '14_Portal_azure_servico_inicial.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:51:42,362 [INFO] Hist√≥rico atualizado com '14_Portal_azure_servico_inicial.png'.
2025-04-08 00:51:46,362 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:51:46,363 [INFO] Processando imagem 33/35: 15_Portal_azure_servico_linguagem.png
2025-04-08 00:51:48,824 [INFO] Resposta da IA (imagem): 15_Portal_azure_servico_linguagem.png: ### Criar Linguagem no Azure
Clique em "Create" na op√ß√£o Language Service para criar uma linguagem no Azure.
2025-04-08 00:51:48,826 [INFO] Resumo gerado para '15_Portal_azure_servico_linguagem.png': 15_Portal_azure_servico_linguagem.png: ### Criar Linguagem no Azure
Clique em "Create" na op√ß√£o Language Service para criar uma linguagem no Azure.
2025-04-08 00:51:48,828 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\15_Portal_azure_servico_linguagem.png
2025-04-08 00:51:49,080 [INFO] Imagem '15_Portal_azure_servico_linguagem.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:51:49,080 [INFO] Hist√≥rico atualizado com '15_Portal_azure_servico_linguagem.png'.
2025-04-08 00:51:53,081 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:51:53,082 [INFO] Processando imagem 34/35: 16_Portal_azure_funcoes_criacao.png
2025-04-08 00:51:54,911 [INFO] Resposta da IA (imagem): ### Continuar Cria√ß√£o de Recursos no Azure
Clique em "Continue to create your resource" para prosseguir com a cria√ß√£o dos recursos selecionados.
2025-04-08 00:51:54,913 [INFO] Resumo gerado para '16_Portal_azure_funcoes_criacao.png': ### Continuar Cria√ß√£o de Recursos no Azure
Clique em "Continue to create your resource" para prosseguir com a cria√ß√£o dos recursos selecionados.
2025-04-08 00:51:54,914 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\16_Portal_azure_funcoes_criacao.png
2025-04-08 00:51:55,560 [INFO] Imagem '16_Portal_azure_funcoes_criacao.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:51:55,561 [INFO] Hist√≥rico atualizado com '16_Portal_azure_funcoes_criacao.png'.
2025-04-08 00:51:59,562 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 00:51:59,563 [INFO] Processando imagem 35/35: 17_Portal_azure_criar_idioma.png
2025-04-08 00:52:01,360 [INFO] Resposta da IA (imagem): ### Configurar Detalhes do Projeto e Inst√¢ncia para Linguagem no Azure

- Preencha os detalhes do projeto, incluindo:
  - Assinatura
  - Grupo de recursos
- Configure os detalhes da inst√¢ncia, como:
  - Regi√£o
  - Nome
  - Plano de pre√ßos
2025-04-08 00:52:01,365 [INFO] Resumo gerado para '17_Portal_azure_criar_idioma.png': ### Configurar Detalhes do Projeto e Inst√¢ncia para Linguagem no Azure

- Preencha os detalhes do projeto, incluindo:
  - Assinatura
  - Grupo de recursos
- Configure os detalhes da inst√¢ncia, como:
  - Regi√£o
  - Nome
  - Plano de pre√ßos
2025-04-08 00:52:01,366 [INFO] Caminho da imagem para o Word: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images\17_Portal_azure_criar_idioma.png
2025-04-08 00:52:01,635 [INFO] Imagem '17_Portal_azure_criar_idioma.png' movida para 'C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\processed_images'.
2025-04-08 00:52:01,636 [INFO] Hist√≥rico atualizado com '17_Portal_azure_criar_idioma.png'.
2025-04-08 00:52:05,636 [INFO] Preparando a pr√≥xima an√°lise...
2025-04-08 02:05:51,354 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 02:05:51,355 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 02:05:51,356 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 02:05:51,358 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 02:05:51,360 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 02:05:51,362 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 02:05:51,363 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 02:05:51,364 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 02:05:51,365 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 02:05:51,367 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 02:05:51,368 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 02:05:51,369 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 02:05:51,371 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 02:05:51,372 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 02:05:51,373 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 02:05:51,836 [INFO] Modelo Gemini 'gemini-2.0-flash-exp' inicializado com sucesso.
2025-04-08 02:06:23,367 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 02:06:23,369 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 02:06:23,370 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 02:06:23,372 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 02:06:23,373 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 02:06:23,375 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 02:06:23,377 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 02:06:23,381 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 02:06:23,385 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 02:06:23,387 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 02:06:23,389 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 02:06:23,390 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 02:06:23,392 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 02:06:23,394 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 02:06:23,396 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 02:06:23,497 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 02:06:23,499 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 02:06:23,503 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 02:06:23,505 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 02:06:23,507 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 02:06:23,508 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 02:06:23,510 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 02:06:23,512 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 02:06:23,516 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 02:06:23,517 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 02:06:23,519 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 02:06:23,521 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 02:06:23,522 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 02:06:23,524 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 02:06:23,525 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 02:06:23,529 [INFO] Enviando para IA - Prompt (sem imagem): prompt = f"""
Voc√™ √© um assistente inteligente especializado em revis√£o e melhoria de c√≥digo.

Seu objetivo √©:
- Analisar o c√≥digo-fonte contido no projeto enviado
- Identificar melhorias em legibilidade, performance e seguran√ßa
- Sugerir pr√°ticas recomendadas de design e arquitetura
- Indicar partes com d√≠vidas t√©cnicas ou complexidade desnecess√°ria

Informa√ß√µes adicionais:
- Estrutura do projeto: segue o padr√£o de pastas iniciando por src/

"""

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
import base64
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.generativeai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: gere uma documenta√ß√£o em markdown do meu c√≥digo atual, para um README do github.
2025-04-08 02:06:33,776 [INFO] Resposta da IA (texto): Com certeza! Ap√≥s analisar o c√≥digo, aqui est√£o algumas sugest√µes para documentar seu projeto em um README do GitHub, juntamente com sugest√µes de melhorias no c√≥digo:

**Estrutura do README (sugest√£o):**

```markdown
# Nome do Projeto (Ex: Chat IA Inteligente)

Uma breve descri√ß√£o do que o projeto faz.

## Funcionalidades Principais

*   Chat interativo com IA (Gemini).
*   An√°lise de imagens (upload ou √°rea de transfer√™ncia).
*   Gera√ß√£o de imagens a partir de prompts.
*   Hist√≥rico de conversas.
*   Limita√ß√£o de taxa de requisi√ß√µes (Rate Limiting).

## Como Usar

Instru√ß√µes detalhadas sobre como executar o projeto (incluindo depend√™ncias e vari√°veis de ambiente).

Exemplo:

1.  Clone o reposit√≥rio.
2.  Crie um ambiente virtual: `python -m venv venv`
3.  Ative o ambiente virtual: `source venv/bin/activate` (Linux/macOS) ou `venv\Scripts\activate` (Windows)
4.  Instale as depend√™ncias: `pip install -r requirements.txt`
5.  Configure a vari√°vel de ambiente `API_KEY_GEMINI` no arquivo `.env`.
6.  Execute o aplicativo: `streamlit run src/chat_streamlit.py`

## Arquitetura do Projeto

*   `src/`: Cont√©m o c√≥digo fonte principal.
    *   `chat_streamlit.py`: Interface Streamlit para o chat.
    *   `config/`: Configura√ß√µes do projeto.
    *   `core/`: L√≥gica principal (handlers, logger, rate limiting).
    *   `services/`: Servi√ßos (IA, documentos, markdown, busca de arquivos).
    *   `utils/`: Utilit√°rios.
    *   `main.py`: Ponto de entrada para processamento de imagens em lote.

## Diagrama de Classes (Opcional)

(Adicione um diagrama UML aqui, se poss√≠vel, para ilustrar as rela√ß√µes entre as classes.)

## Contribui√ß√µes

Como contribuir para o projeto (pull requests, etc.).

## Licen√ßa

(Ex: MIT License)

## Autores

[Seu Nome]
```

**Documenta√ß√£o Adicional:**

*   **`chat_streamlit.py`:**
    *   Explicar o uso do `st.session_state` para manter o estado da aplica√ß√£o.
    *   Detalhar como as fun√ß√µes `process_message`, `check_clipboard`, `generate_image` funcionam.
    *   Mencionar como o prompt base √© carregado do arquivo `PROMPT_CHAT_FILE` e como o contexto do c√≥digo √© adicionado.
*   **`config/config.py`:**
    *   Explicar como os caminhos s√£o configurados e a import√¢ncia de `ensure_directories()`.
    *   Documentar a configura√ß√£o do Rate Limiting.
*   **`core/`:**
    *   `GeminiHandler`: Descrever como ele encapsula a intera√ß√£o com o modelo Gemini.
    *   `RateLimiter`: Explicar como a classe limita as requisi√ß√µes para evitar exceder as quotas da API.
*   **`services/`:**
    *   `ImageProcessor`: Detalhar como as imagens s√£o processadas, resumidas e como o hist√≥rico √© mantido.
    *   `DocumentService` e `MarkdownService`: Explicar como os resumos s√£o formatados e salvos em diferentes formatos.

**Sugest√µes de Melhorias no C√≥digo:**

*   **`chat_streamlit.py`:**
    *   **Modulariza√ß√£o:** Quebrar a fun√ß√£o `process_message` em fun√ß√µes menores e mais focadas.
    *   **Gerenciamento de Erros:** Implementar tratamento de erros mais robusto, com mensagens de erro mais claras para o usu√°rio.
    *   **Coment√°rios:** Adicionar mais coment√°rios para explicar a l√≥gica de partes complexas do c√≥digo.
*   **`core/RateLimiter.py`:**
    *   **Logging:** Adicionar logs para registrar quando as requisi√ß√µes s√£o limitadas e por quanto tempo.
*   **`services/ImageProcessor.py`:**
    *   **Paraleliza√ß√£o:** Considerar o uso de threads ou processos para processar m√∫ltiplas imagens simultaneamente (respeitando o Rate Limiting).
    *   **Abstra√ß√£o:** Criar classes para representar prompts e respostas da IA, para facilitar a manuten√ß√£o e testes.
*   **`utils/file_utils.py`:**
    *   **Tratamento de Exce√ß√µes:** Adicionar tratamento de exce√ß√µes ao listar imagens, caso o diret√≥rio n√£o exista ou ocorra um erro de permiss√£o.
*   **Configura√ß√£o e Vari√°veis de Ambiente:**
    *   **Valida√ß√£o:** Adicionar valida√ß√£o para garantir que as vari√°veis de ambiente necess√°rias est√£o configuradas corretamente.
*   **Padroniza√ß√£o:**
    *   **Formata√ß√£o:** Usar um formatador de c√≥digo como Black ou autopep8 para manter a consist√™ncia na formata√ß√£o do c√≥digo.
    *   **Linters:** Usar um linter como pylint ou flake8 para identificar poss√≠veis problemas no c√≥digo.

**D√≠vidas T√©cnicas:**

*   **Tratamento de Erros Gen√©rico:** Muitos blocos `except Exception as e` capturam todos os tipos de erros, o que pode dificultar a identifica√ß√£o de problemas espec√≠ficos. √â recomend√°vel capturar tipos de erros mais espec√≠ficos e trat√°-los adequadamente.
*   **C√≥digo Duplicado:** Identificar e refatorar trechos de c√≥digo duplicados, como o carregamento do hist√≥rico em `ImageProcessor`.
*   **Testes:** Adicionar testes unit√°rios e de integra√ß√£o para garantir a qualidade do c√≥digo e evitar regress√µes.

**Observa√ß√µes:**

*   O uso de `st.rerun()` pode ser problem√°tico em algumas situa√ß√µes. Considere outras formas de atualizar a interface do Streamlit, como usar callbacks ou reatividade.
*   A l√≥gica de limpar as imagens da √°rea de transfer√™ncia e do uploader pode ser simplificada usando uma √∫nica fun√ß√£o.
*   A classe `RateLimiter` parece estar funcionando corretamente, mas √© importante monitorar seu desempenho em um ambiente de produ√ß√£o para garantir que ela est√° limitando as requisi√ß√µes conforme o esperado.

Espero que isto ajude a criar uma documenta√ß√£o abrangente e a melhorar a qualidade do seu c√≥digo!

2025-04-08 02:06:33,850 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 02:06:33,852 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 02:06:33,854 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 02:06:33,855 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 02:06:33,857 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 02:06:33,858 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 02:06:33,860 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 02:06:33,861 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 02:06:33,862 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 02:06:33,864 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 02:06:33,865 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 02:06:33,866 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 02:06:33,868 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 02:06:33,869 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 02:06:33,870 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 02:08:50,523 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 02:08:50,525 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 02:08:50,527 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 02:08:50,528 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 02:08:50,530 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 02:08:50,532 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 02:08:50,533 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 02:08:50,535 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 02:08:50,538 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 02:08:50,540 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 02:08:50,542 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 02:08:50,544 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 02:08:50,546 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 02:08:50,548 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 02:08:50,550 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 02:08:50,653 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 02:08:50,655 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 02:08:50,658 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 02:08:50,661 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 02:08:50,664 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 02:08:50,665 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 02:08:50,668 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 02:08:50,670 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 02:08:50,671 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 02:08:50,673 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 02:08:50,675 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 02:08:50,678 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 02:08:50,679 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 02:08:50,681 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 02:08:50,686 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 02:08:50,695 [INFO] Enviando para IA - Prompt (sem imagem): prompt = f"""
Voc√™ √© um assistente inteligente especializado em revis√£o e melhoria de c√≥digo.

Seu objetivo √©:
- Analisar o c√≥digo-fonte contido no projeto enviado
- Identificar melhorias em legibilidade, performance e seguran√ßa
- Sugerir pr√°ticas recomendadas de design e arquitetura
- Indicar partes com d√≠vidas t√©cnicas ou complexidade desnecess√°ria

Informa√ß√µes adicionais:
- Estrutura do projeto: segue o padr√£o de pastas iniciando por src/

"""

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
import base64
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.generativeai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: gere uma documenta√ß√£o em markdown do meu c√≥digo atual, para um README do github.
Assistant: Com certeza! Ap√≥s analisar o c√≥digo, aqui est√£o algumas sugest√µes para documentar seu projeto em um README do GitHub, juntamente com sugest√µes de melhorias no c√≥digo:

**Estrutura do README (sugest√£o):**

```markdown
# Nome do Projeto (Ex: Chat IA Inteligente)

Uma breve descri√ß√£o do que o projeto faz.

## Funcionalidades Principais

*   Chat interativo com IA (Gemini).
*   An√°lise de imagens (upload ou √°rea de transfer√™ncia).
*   Gera√ß√£o de imagens a partir de prompts.
*   Hist√≥rico de conversas.
*   Limita√ß√£o de taxa de requisi√ß√µes (Rate Limiting).

## Como Usar

Instru√ß√µes detalhadas sobre como executar o projeto (incluindo depend√™ncias e vari√°veis de ambiente).

Exemplo:

1.  Clone o reposit√≥rio.
2.  Crie um ambiente virtual: `python -m venv venv`
3.  Ative o ambiente virtual: `source venv/bin/activate` (Linux/macOS) ou `venv\Scripts\activate` (Windows)
4.  Instale as depend√™ncias: `pip install -r requirements.txt`
5.  Configure a vari√°vel de ambiente `API_KEY_GEMINI` no arquivo `.env`.
6.  Execute o aplicativo: `streamlit run src/chat_streamlit.py`

## Arquitetura do Projeto

*   `src/`: Cont√©m o c√≥digo fonte principal.
    *   `chat_streamlit.py`: Interface Streamlit para o chat.
    *   `config/`: Configura√ß√µes do projeto.
    *   `core/`: L√≥gica principal (handlers, logger, rate limiting).
    *   `services/`: Servi√ßos (IA, documentos, markdown, busca de arquivos).
    *   `utils/`: Utilit√°rios.
    *   `main.py`: Ponto de entrada para processamento de imagens em lote.

## Diagrama de Classes (Opcional)

(Adicione um diagrama UML aqui, se poss√≠vel, para ilustrar as rela√ß√µes entre as classes.)

## Contribui√ß√µes

Como contribuir para o projeto (pull requests, etc.).

## Licen√ßa

(Ex: MIT License)

## Autores

[Seu Nome]
```

**Documenta√ß√£o Adicional:**

*   **`chat_streamlit.py`:**
    *   Explicar o uso do `st.session_state` para manter o estado da aplica√ß√£o.
    *   Detalhar como as fun√ß√µes `process_message`, `check_clipboard`, `generate_image` funcionam.
    *   Mencionar como o prompt base √© carregado do arquivo `PROMPT_CHAT_FILE` e como o contexto do c√≥digo √© adicionado.
*   **`config/config.py`:**
    *   Explicar como os caminhos s√£o configurados e a import√¢ncia de `ensure_directories()`.
    *   Documentar a configura√ß√£o do Rate Limiting.
*   **`core/`:**
    *   `GeminiHandler`: Descrever como ele encapsula a intera√ß√£o com o modelo Gemini.
    *   `RateLimiter`: Explicar como a classe limita as requisi√ß√µes para evitar exceder as quotas da API.
*   **`services/`:**
    *   `ImageProcessor`: Detalhar como as imagens s√£o processadas, resumidas e como o hist√≥rico √© mantido.
    *   `DocumentService` e `MarkdownService`: Explicar como os resumos s√£o formatados e salvos em diferentes formatos.

**Sugest√µes de Melhorias no C√≥digo:**

*   **`chat_streamlit.py`:**
    *   **Modulariza√ß√£o:** Quebrar a fun√ß√£o `process_message` em fun√ß√µes menores e mais focadas.
    *   **Gerenciamento de Erros:** Implementar tratamento de erros mais robusto, com mensagens de erro mais claras para o usu√°rio.
    *   **Coment√°rios:** Adicionar mais coment√°rios para explicar a l√≥gica de partes complexas do c√≥digo.
*   **`core/RateLimiter.py`:**
    *   **Logging:** Adicionar logs para registrar quando as requisi√ß√µes s√£o limitadas e por quanto tempo.
*   **`services/ImageProcessor.py`:**
    *   **Paraleliza√ß√£o:** Considerar o uso de threads ou processos para processar m√∫ltiplas imagens simultaneamente (respeitando o Rate Limiting).
    *   **Abstra√ß√£o:** Criar classes para representar prompts e respostas da IA, para facilitar a manuten√ß√£o e testes.
*   **`utils/file_utils.py`:**
    *   **Tratamento de Exce√ß√µes:** Adicionar tratamento de exce√ß√µes ao listar imagens, caso o diret√≥rio n√£o exista ou ocorra um erro de permiss√£o.
*   **Configura√ß√£o e Vari√°veis de Ambiente:**
    *   **Valida√ß√£o:** Adicionar valida√ß√£o para garantir que as vari√°veis de ambiente necess√°rias est√£o configuradas corretamente.
*   **Padroniza√ß√£o:**
    *   **Formata√ß√£o:** Usar um formatador de c√≥digo como Black ou autopep8 para manter a consist√™ncia na formata√ß√£o do c√≥digo.
    *   **Linters:** Usar um linter como pylint ou flake8 para identificar poss√≠veis problemas no c√≥digo.

**D√≠vidas T√©cnicas:**

*   **Tratamento de Erros Gen√©rico:** Muitos blocos `except Exception as e` capturam todos os tipos de erros, o que pode dificultar a identifica√ß√£o de problemas espec√≠ficos. √â recomend√°vel capturar tipos de erros mais espec√≠ficos e trat√°-los adequadamente.
*   **C√≥digo Duplicado:** Identificar e refatorar trechos de c√≥digo duplicados, como o carregamento do hist√≥rico em `ImageProcessor`.
*   **Testes:** Adicionar testes unit√°rios e de integra√ß√£o para garantir a qualidade do c√≥digo e evitar regress√µes.

**Observa√ß√µes:**

*   O uso de `st.rerun()` pode ser problem√°tico em algumas situa√ß√µes. Considere outras formas de atualizar a interface do Streamlit, como usar callbacks ou reatividade.
*   A l√≥gica de limpar as imagens da √°rea de transfer√™ncia e do uploader pode ser simplificada usando uma √∫nica fun√ß√£o.
*   A classe `RateLimiter` parece estar funcionando corretamente, mas √© importante monitorar seu desempenho em um ambiente de produ√ß√£o para garantir que ela est√° limitando as requisi√ß√µes conforme o esperado.

Espero que isto ajude a criar uma documenta√ß√£o abrangente e a melhorar a qualidade do seu c√≥digo!

User: gere um artigo com a minha aplica√ß√£o para apresentar no linkedin.
2025-04-08 02:08:57,938 [INFO] Resposta da IA (texto): Assistant: Com certeza! Aqui est√° um rascunho de artigo para o LinkedIn, destacando sua aplica√ß√£o e convidando a comunidade a explorar:

**T√≠tulo:** Apresentando [Nome da Aplica√ß√£o]: Sua Nova Ferramenta Inteligente para [Prop√≥sito Principal]! üöÄ

**Imagem:** (Anexe uma imagem ou GIF atraente da sua aplica√ß√£o em a√ß√£o. Pode ser uma captura de tela do chat, um exemplo de imagem gerada ou uma representa√ß√£o visual da arquitetura.)

**Texto:**

---

Ol√°, comunidade do LinkedIn! üëã

Estou muito animado para compartilhar com voc√™s um projeto que tenho trabalhado recentemente: [Nome da Aplica√ß√£o]!

[Nome da Aplica√ß√£o] √© uma aplica√ß√£o inovadora constru√≠da com Streamlit e alimentada pela intelig√™ncia artificial do Gemini, que visa facilitar [Prop√≥sito Principal da Aplica√ß√£o]. Seja voc√™ [p√∫blico-alvo], esta ferramenta pode te ajudar a [Benef√≠cios Principais].

**Funcionalidades em Destaque:**

*   üí¨ **Chat Inteligente:** Converse com um assistente de IA vers√°til para obter respostas, insights e suporte.
*   üñºÔ∏è **An√°lise de Imagens:** Fa√ßa upload de imagens ou cole da √°rea de transfer√™ncia para obter an√°lises detalhadas e informa√ß√µes relevantes.
*   ‚ú® **Gera√ß√£o de Imagens:** Crie imagens impressionantes a partir de prompts de texto, liberando sua criatividade.
*   üìö **Aprendizado Cont√≠nuo:** A aplica√ß√£o est√° sempre aprendendo e evoluindo, com atualiza√ß√µes regulares e novos recursos.

**Como Funciona:**

[D√™ um breve resumo de como a aplica√ß√£o funciona. Por exemplo:]

1.  Acesse a interface intuitiva via Streamlit.
2.  Comece a conversar com a IA, fa√ßa upload de uma imagem ou insira um prompt para gerar uma imagem.
3.  Receba respostas r√°pidas, an√°lises detalhadas e imagens criativas em segundos.

**Por Tr√°s da Aplica√ß√£o:**

Constru√≠ [Nome da Aplica√ß√£o] usando as seguintes tecnologias:

*   **Streamlit:** Para uma interface de usu√°rio interativa e amig√°vel.
*   **Gemini (Google AI):** Para capacidades avan√ßadas de processamento de linguagem natural e gera√ß√£o de imagens.
*   **Python:** A linguagem de programa√ß√£o por tr√°s de toda a m√°gica.

**Por Que Eu Criei Isso?**

[Compartilhe sua motiva√ß√£o por tr√°s da cria√ß√£o da aplica√ß√£o. Por exemplo:]

Eu sempre fui fascinado pelo poder da IA para resolver problemas e melhorar a vida das pessoas. Com [Nome da Aplica√ß√£o], eu queria criar uma ferramenta acess√≠vel e f√°cil de usar que pudesse ajudar [p√∫blico-alvo] a [Benef√≠cios Principais].

**Convite √† A√ß√£o:**

Gostaria muito de convidar voc√™ a experimentar [Nome da Aplica√ß√£o] e compartilhar seus coment√°rios! Se voc√™ √© [p√∫blico-alvo], acredito que esta ferramenta pode te ajudar a [Benef√≠cios Principais].

‚û°Ô∏è [Link para o reposit√≥rio no GitHub ou para a aplica√ß√£o online (se dispon√≠vel)]

**Pe√ßo a sua ajuda!**

Se voc√™ gostou deste projeto, por favor, curta, compartilhe e comente abaixo. Sua opini√£o √© muito importante para mim!

#IA #InteligenciaArtificial #Streamlit #Python #Gemini #Inovacao #Desenvolvimento #OpenSource #Compartilhe

---

**Dicas Adicionais:**

*   **Adapte o texto:** Personalize o texto com detalhes espec√≠ficos sobre sua aplica√ß√£o, seu p√∫blico-alvo e sua motiva√ß√£o.
*   **Use emojis:** Emojis podem tornar seu artigo mais atraente e expressivo.
*   **Seja conciso:** Mantenha o texto curto e direto ao ponto.
*   **Interaja com os coment√°rios:** Responda aos coment√°rios e perguntas para criar um di√°logo com a comunidade.
*   **Compartilhe em grupos relevantes:** Publique o artigo em grupos do LinkedIn relacionados a IA, desenvolvimento de software e sua √°rea de atua√ß√£o.

Espero que este rascunho te ajude a criar um artigo de sucesso para o LinkedIn! Se precisar de mais alguma ajuda, √© s√≥ me avisar.
2025-04-08 02:08:58,091 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 02:08:58,094 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 02:08:58,096 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 02:08:58,099 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 02:08:58,101 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 02:08:58,103 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 02:08:58,105 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 02:08:58,107 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 02:08:58,111 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 02:08:58,113 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 02:08:58,115 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 02:08:58,117 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 02:08:58,118 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 02:08:58,119 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 02:08:58,121 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 02:10:19,492 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 02:10:19,494 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 02:10:19,495 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 02:10:19,496 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 02:10:19,498 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 02:10:19,500 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 02:10:19,502 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 02:10:19,504 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 02:10:19,506 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 02:10:19,508 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 02:10:19,510 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 02:10:19,511 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 02:10:19,512 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 02:10:19,514 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 02:10:19,515 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 02:10:19,616 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 02:10:19,618 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 02:10:19,622 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 02:10:19,625 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 02:10:19,628 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 02:10:19,629 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 02:10:19,631 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 02:10:19,633 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 02:10:19,634 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 02:10:19,637 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 02:10:19,638 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 02:10:19,641 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 02:10:19,642 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 02:10:19,644 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 02:10:19,646 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 02:10:19,648 [INFO] Enviando para IA - Prompt (sem imagem): prompt = f"""
Voc√™ √© um assistente inteligente especializado em revis√£o e melhoria de c√≥digo.

Seu objetivo √©:
- Analisar o c√≥digo-fonte contido no projeto enviado
- Identificar melhorias em legibilidade, performance e seguran√ßa
- Sugerir pr√°ticas recomendadas de design e arquitetura
- Indicar partes com d√≠vidas t√©cnicas ou complexidade desnecess√°ria

Informa√ß√µes adicionais:
- Estrutura do projeto: segue o padr√£o de pastas iniciando por src/

"""

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
import base64
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.generativeai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: gere uma documenta√ß√£o em markdown do meu c√≥digo atual, para um README do github.
Assistant: Com certeza! Ap√≥s analisar o c√≥digo, aqui est√£o algumas sugest√µes para documentar seu projeto em um README do GitHub, juntamente com sugest√µes de melhorias no c√≥digo:

**Estrutura do README (sugest√£o):**

```markdown
# Nome do Projeto (Ex: Chat IA Inteligente)

Uma breve descri√ß√£o do que o projeto faz.

## Funcionalidades Principais

*   Chat interativo com IA (Gemini).
*   An√°lise de imagens (upload ou √°rea de transfer√™ncia).
*   Gera√ß√£o de imagens a partir de prompts.
*   Hist√≥rico de conversas.
*   Limita√ß√£o de taxa de requisi√ß√µes (Rate Limiting).

## Como Usar

Instru√ß√µes detalhadas sobre como executar o projeto (incluindo depend√™ncias e vari√°veis de ambiente).

Exemplo:

1.  Clone o reposit√≥rio.
2.  Crie um ambiente virtual: `python -m venv venv`
3.  Ative o ambiente virtual: `source venv/bin/activate` (Linux/macOS) ou `venv\Scripts\activate` (Windows)
4.  Instale as depend√™ncias: `pip install -r requirements.txt`
5.  Configure a vari√°vel de ambiente `API_KEY_GEMINI` no arquivo `.env`.
6.  Execute o aplicativo: `streamlit run src/chat_streamlit.py`

## Arquitetura do Projeto

*   `src/`: Cont√©m o c√≥digo fonte principal.
    *   `chat_streamlit.py`: Interface Streamlit para o chat.
    *   `config/`: Configura√ß√µes do projeto.
    *   `core/`: L√≥gica principal (handlers, logger, rate limiting).
    *   `services/`: Servi√ßos (IA, documentos, markdown, busca de arquivos).
    *   `utils/`: Utilit√°rios.
    *   `main.py`: Ponto de entrada para processamento de imagens em lote.

## Diagrama de Classes (Opcional)

(Adicione um diagrama UML aqui, se poss√≠vel, para ilustrar as rela√ß√µes entre as classes.)

## Contribui√ß√µes

Como contribuir para o projeto (pull requests, etc.).

## Licen√ßa

(Ex: MIT License)

## Autores

[Seu Nome]
```

**Documenta√ß√£o Adicional:**

*   **`chat_streamlit.py`:**
    *   Explicar o uso do `st.session_state` para manter o estado da aplica√ß√£o.
    *   Detalhar como as fun√ß√µes `process_message`, `check_clipboard`, `generate_image` funcionam.
    *   Mencionar como o prompt base √© carregado do arquivo `PROMPT_CHAT_FILE` e como o contexto do c√≥digo √© adicionado.
*   **`config/config.py`:**
    *   Explicar como os caminhos s√£o configurados e a import√¢ncia de `ensure_directories()`.
    *   Documentar a configura√ß√£o do Rate Limiting.
*   **`core/`:**
    *   `GeminiHandler`: Descrever como ele encapsula a intera√ß√£o com o modelo Gemini.
    *   `RateLimiter`: Explicar como a classe limita as requisi√ß√µes para evitar exceder as quotas da API.
*   **`services/`:**
    *   `ImageProcessor`: Detalhar como as imagens s√£o processadas, resumidas e como o hist√≥rico √© mantido.
    *   `DocumentService` e `MarkdownService`: Explicar como os resumos s√£o formatados e salvos em diferentes formatos.

**Sugest√µes de Melhorias no C√≥digo:**

*   **`chat_streamlit.py`:**
    *   **Modulariza√ß√£o:** Quebrar a fun√ß√£o `process_message` em fun√ß√µes menores e mais focadas.
    *   **Gerenciamento de Erros:** Implementar tratamento de erros mais robusto, com mensagens de erro mais claras para o usu√°rio.
    *   **Coment√°rios:** Adicionar mais coment√°rios para explicar a l√≥gica de partes complexas do c√≥digo.
*   **`core/RateLimiter.py`:**
    *   **Logging:** Adicionar logs para registrar quando as requisi√ß√µes s√£o limitadas e por quanto tempo.
*   **`services/ImageProcessor.py`:**
    *   **Paraleliza√ß√£o:** Considerar o uso de threads ou processos para processar m√∫ltiplas imagens simultaneamente (respeitando o Rate Limiting).
    *   **Abstra√ß√£o:** Criar classes para representar prompts e respostas da IA, para facilitar a manuten√ß√£o e testes.
*   **`utils/file_utils.py`:**
    *   **Tratamento de Exce√ß√µes:** Adicionar tratamento de exce√ß√µes ao listar imagens, caso o diret√≥rio n√£o exista ou ocorra um erro de permiss√£o.
*   **Configura√ß√£o e Vari√°veis de Ambiente:**
    *   **Valida√ß√£o:** Adicionar valida√ß√£o para garantir que as vari√°veis de ambiente necess√°rias est√£o configuradas corretamente.
*   **Padroniza√ß√£o:**
    *   **Formata√ß√£o:** Usar um formatador de c√≥digo como Black ou autopep8 para manter a consist√™ncia na formata√ß√£o do c√≥digo.
    *   **Linters:** Usar um linter como pylint ou flake8 para identificar poss√≠veis problemas no c√≥digo.

**D√≠vidas T√©cnicas:**

*   **Tratamento de Erros Gen√©rico:** Muitos blocos `except Exception as e` capturam todos os tipos de erros, o que pode dificultar a identifica√ß√£o de problemas espec√≠ficos. √â recomend√°vel capturar tipos de erros mais espec√≠ficos e trat√°-los adequadamente.
*   **C√≥digo Duplicado:** Identificar e refatorar trechos de c√≥digo duplicados, como o carregamento do hist√≥rico em `ImageProcessor`.
*   **Testes:** Adicionar testes unit√°rios e de integra√ß√£o para garantir a qualidade do c√≥digo e evitar regress√µes.

**Observa√ß√µes:**

*   O uso de `st.rerun()` pode ser problem√°tico em algumas situa√ß√µes. Considere outras formas de atualizar a interface do Streamlit, como usar callbacks ou reatividade.
*   A l√≥gica de limpar as imagens da √°rea de transfer√™ncia e do uploader pode ser simplificada usando uma √∫nica fun√ß√£o.
*   A classe `RateLimiter` parece estar funcionando corretamente, mas √© importante monitorar seu desempenho em um ambiente de produ√ß√£o para garantir que ela est√° limitando as requisi√ß√µes conforme o esperado.

Espero que isto ajude a criar uma documenta√ß√£o abrangente e a melhorar a qualidade do seu c√≥digo!

User: gere um artigo com a minha aplica√ß√£o para apresentar no linkedin.
Assistant: Assistant: Com certeza! Aqui est√° um rascunho de artigo para o LinkedIn, destacando sua aplica√ß√£o e convidando a comunidade a explorar:

**T√≠tulo:** Apresentando [Nome da Aplica√ß√£o]: Sua Nova Ferramenta Inteligente para [Prop√≥sito Principal]! üöÄ

**Imagem:** (Anexe uma imagem ou GIF atraente da sua aplica√ß√£o em a√ß√£o. Pode ser uma captura de tela do chat, um exemplo de imagem gerada ou uma representa√ß√£o visual da arquitetura.)

**Texto:**

---

Ol√°, comunidade do LinkedIn! üëã

Estou muito animado para compartilhar com voc√™s um projeto que tenho trabalhado recentemente: [Nome da Aplica√ß√£o]!

[Nome da Aplica√ß√£o] √© uma aplica√ß√£o inovadora constru√≠da com Streamlit e alimentada pela intelig√™ncia artificial do Gemini, que visa facilitar [Prop√≥sito Principal da Aplica√ß√£o]. Seja voc√™ [p√∫blico-alvo], esta ferramenta pode te ajudar a [Benef√≠cios Principais].

**Funcionalidades em Destaque:**

*   üí¨ **Chat Inteligente:** Converse com um assistente de IA vers√°til para obter respostas, insights e suporte.
*   üñºÔ∏è **An√°lise de Imagens:** Fa√ßa upload de imagens ou cole da √°rea de transfer√™ncia para obter an√°lises detalhadas e informa√ß√µes relevantes.
*   ‚ú® **Gera√ß√£o de Imagens:** Crie imagens impressionantes a partir de prompts de texto, liberando sua criatividade.
*   üìö **Aprendizado Cont√≠nuo:** A aplica√ß√£o est√° sempre aprendendo e evoluindo, com atualiza√ß√µes regulares e novos recursos.

**Como Funciona:**

[D√™ um breve resumo de como a aplica√ß√£o funciona. Por exemplo:]

1.  Acesse a interface intuitiva via Streamlit.
2.  Comece a conversar com a IA, fa√ßa upload de uma imagem ou insira um prompt para gerar uma imagem.
3.  Receba respostas r√°pidas, an√°lises detalhadas e imagens criativas em segundos.

**Por Tr√°s da Aplica√ß√£o:**

Constru√≠ [Nome da Aplica√ß√£o] usando as seguintes tecnologias:

*   **Streamlit:** Para uma interface de usu√°rio interativa e amig√°vel.
*   **Gemini (Google AI):** Para capacidades avan√ßadas de processamento de linguagem natural e gera√ß√£o de imagens.
*   **Python:** A linguagem de programa√ß√£o por tr√°s de toda a m√°gica.

**Por Que Eu Criei Isso?**

[Compartilhe sua motiva√ß√£o por tr√°s da cria√ß√£o da aplica√ß√£o. Por exemplo:]

Eu sempre fui fascinado pelo poder da IA para resolver problemas e melhorar a vida das pessoas. Com [Nome da Aplica√ß√£o], eu queria criar uma ferramenta acess√≠vel e f√°cil de usar que pudesse ajudar [p√∫blico-alvo] a [Benef√≠cios Principais].

**Convite √† A√ß√£o:**

Gostaria muito de convidar voc√™ a experimentar [Nome da Aplica√ß√£o] e compartilhar seus coment√°rios! Se voc√™ √© [p√∫blico-alvo], acredito que esta ferramenta pode te ajudar a [Benef√≠cios Principais].

‚û°Ô∏è [Link para o reposit√≥rio no GitHub ou para a aplica√ß√£o online (se dispon√≠vel)]

**Pe√ßo a sua ajuda!**

Se voc√™ gostou deste projeto, por favor, curta, compartilhe e comente abaixo. Sua opini√£o √© muito importante para mim!

#IA #InteligenciaArtificial #Streamlit #Python #Gemini #Inovacao #Desenvolvimento #OpenSource #Compartilhe

---

**Dicas Adicionais:**

*   **Adapte o texto:** Personalize o texto com detalhes espec√≠ficos sobre sua aplica√ß√£o, seu p√∫blico-alvo e sua motiva√ß√£o.
*   **Use emojis:** Emojis podem tornar seu artigo mais atraente e expressivo.
*   **Seja conciso:** Mantenha o texto curto e direto ao ponto.
*   **Interaja com os coment√°rios:** Responda aos coment√°rios e perguntas para criar um di√°logo com a comunidade.
*   **Compartilhe em grupos relevantes:** Publique o artigo em grupos do LinkedIn relacionados a IA, desenvolvimento de software e sua √°rea de atua√ß√£o.

Espero que este rascunho te ajude a criar um artigo de sucesso para o LinkedIn! Se precisar de mais alguma ajuda, √© s√≥ me avisar.
User: voc√™ consegue detalhar o que essa aplica√ß√£o faz ?
2025-04-08 02:10:27,558 [INFO] Resposta da IA (texto): Assistant: Com certeza! Vamos detalhar o que essa aplica√ß√£o faz, explorando seus componentes e funcionalidades principais:

**Prop√≥sito Geral:**

A aplica√ß√£o √© um assistente inteligente e vers√°til, constru√≠do para interagir com usu√°rios de diversas maneiras, combinando capacidades de chat, an√°lise de imagens e gera√ß√£o de imagens. O objetivo √© fornecer uma experi√™ncia rica e integrada, onde o usu√°rio pode obter informa√ß√µes, insights e criar conte√∫do visual de forma intuitiva.

**Funcionalidades Detalhadas:**

1.  **Chat Interativo com IA:**
    *   **Motor:** Utiliza o modelo Gemini (do Google AI) para processamento de linguagem natural.
    *   **Funcionalidade:** Permite que os usu√°rios conversem livremente com a IA sobre uma variedade de t√≥picos.
    *   **Contexto:**
        *   Carrega um prompt base a partir de um arquivo externo (`PROMPT_CHAT_FILE`) para definir a personalidade e o comportamento da IA.
        *   Adiciona o c√≥digo-fonte do projeto como contexto para que a IA possa responder perguntas sobre a estrutura, a l√≥gica e o funcionamento interno da aplica√ß√£o.
        *   Mant√©m um hist√≥rico das conversas para que a IA possa entender o contexto das perguntas e fornecer respostas mais relevantes.
    *   **Casos de Uso:**
        *   Obter informa√ß√µes sobre diversos assuntos.
        *   Pedir ajuda para resolver problemas de programa√ß√£o.
        *   Explorar ideias criativas.
        *   Aprender sobre o funcionamento da pr√≥pria aplica√ß√£o.

2.  **An√°lise de Imagens:**
    *   **Fontes de Imagem:**
        *   Upload de arquivos de imagem (PNG, JPG, JPEG).
        *   Colagem de imagens da √°rea de transfer√™ncia.
    *   **Funcionalidade:** Permite que os usu√°rios enviem imagens para que a IA as analise e forne√ßa informa√ß√µes sobre o conte√∫do da imagem.
    *   **Processamento:**
        *   As imagens s√£o salvas temporariamente no disco para serem processadas pelo modelo Gemini.
        *   O prompt base √© combinado com a imagem para instruir a IA sobre o tipo de an√°lise a ser realizada.
        *   O hist√≥rico de conversas tamb√©m √© inclu√≠do no prompt para fornecer contexto adicional.
    *   **Casos de Uso:**
        *   Obter descri√ß√µes de imagens.
        *   Identificar objetos em imagens.
        *   Analisar gr√°ficos e diagramas.
        *   Pedir opini√µes sobre fotos.

3.  **Gera√ß√£o de Imagens:**
    *   **Motor:** Utiliza o modelo Gemini para gera√ß√£o de imagens.
    *   **Funcionalidade:** Permite que os usu√°rios criem imagens a partir de prompts de texto.
    *   **Processamento:**
        *   O usu√°rio fornece um prompt descrevendo a imagem desejada.
        *   O modelo Gemini gera uma imagem com base no prompt.
        *   A imagem gerada √© exibida ao usu√°rio e pode ser salva.
    *   **Casos de Uso:**
        *   Criar ilustra√ß√µes para apresenta√ß√µes.
        *   Gerar imagens para redes sociais.
        *   Visualizar conceitos abstratos.
        *   Experimentar com diferentes estilos art√≠sticos.

4.  **Interface do Usu√°rio (Streamlit):**
    *   **Layout:**
        *   Barra lateral para configura√ß√µes, upload de imagens e gera√ß√£o de imagens.
        *   √Årea principal para exibi√ß√£o do hist√≥rico de conversas.
    *   **Componentes:**
        *   Chat input para entrada de texto.
        *   File uploader para upload de imagens.
        *   Bot√µes para colar da √°rea de transfer√™ncia e limpar imagens.
        *   Bot√£o para limpar o hist√≥rico de conversas.
        *   Display do hist√≥rico de conversas com mensagens do usu√°rio e da IA.
    *   **Estado:** Utiliza `st.session_state` para manter o estado da aplica√ß√£o entre as intera√ß√µes do usu√°rio.

5.  **Gerenciamento de Recursos:**
    *   **Rate Limiting:** Implementa um sistema de limita√ß√£o de taxa de requisi√ß√µes para evitar sobrecarregar a API do Gemini e garantir um uso justo dos recursos.
    *   **Logs:** Utiliza um sistema de logging para registrar eventos importantes, erros e informa√ß√µes de depura√ß√£o.
    *   **Configura√ß√£o:** Carrega configura√ß√µes a partir de arquivos externos para facilitar a personaliza√ß√£o e a manuten√ß√£o da aplica√ß√£o.

6.  **Persist√™ncia:**
    *   **Hist√≥rico de An√°lises:** A aplica√ß√£o mant√©m um hist√≥rico das an√°lises de imagens, incluindo o nome da imagem e o resumo gerado pela IA.
    *   **Exporta√ß√£o:** Os resumos das an√°lises podem ser exportados para arquivos DOCX (Word) e Markdown.

**Fluxo de Dados:**

1.  O usu√°rio interage com a interface do Streamlit.
2.  A entrada do usu√°rio (texto ou imagem) √© enviada para a IA (Gemini).
3.  A IA processa a entrada e gera uma resposta.
4.  A resposta da IA √© exibida ao usu√°rio na interface do Streamlit.
5.  O hist√≥rico de conversas √© atualizado.
6.  As an√°lises de imagens s√£o adicionadas ao hist√≥rico e podem ser exportadas.

**Em resumo, a aplica√ß√£o √© um assistente inteligente multifuncional que combina chat, an√°lise de imagens e gera√ß√£o de imagens para fornecer uma experi√™ncia de usu√°rio rica e vers√°til.** Ela pode ser usada para uma variedade de prop√≥sitos, desde obter informa√ß√µes e insights at√© criar conte√∫do visual.

2025-04-08 02:10:27,636 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 02:10:27,638 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 02:10:27,640 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 02:10:27,643 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 02:10:27,645 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 02:10:27,648 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 02:10:27,650 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 02:10:27,652 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 02:10:27,654 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 02:10:27,657 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 02:10:27,659 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 02:10:27,661 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 02:10:27,662 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 02:10:27,664 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 02:10:27,665 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 02:10:42,863 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 02:10:42,865 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 02:10:42,867 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 02:10:42,869 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 02:10:42,871 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 02:10:42,875 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 02:10:42,876 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 02:10:42,878 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 02:10:42,880 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 02:10:42,881 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 02:10:42,883 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 02:10:42,884 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 02:10:42,886 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 02:10:42,887 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 02:10:42,889 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 02:10:42,994 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 02:10:42,996 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 02:10:42,998 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 02:10:43,000 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 02:10:43,002 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 02:10:43,004 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 02:10:43,008 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 02:10:43,011 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 02:10:43,013 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 02:10:43,014 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 02:10:43,016 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 02:10:43,017 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 02:10:43,019 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 02:10:43,020 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 02:10:43,023 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 02:10:43,027 [INFO] Enviando para IA - Prompt (sem imagem): prompt = f"""
Voc√™ √© um assistente inteligente especializado em revis√£o e melhoria de c√≥digo.

Seu objetivo √©:
- Analisar o c√≥digo-fonte contido no projeto enviado
- Identificar melhorias em legibilidade, performance e seguran√ßa
- Sugerir pr√°ticas recomendadas de design e arquitetura
- Indicar partes com d√≠vidas t√©cnicas ou complexidade desnecess√°ria

Informa√ß√µes adicionais:
- Estrutura do projeto: segue o padr√£o de pastas iniciando por src/

"""

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
import base64
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.generativeai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: gere uma documenta√ß√£o em markdown do meu c√≥digo atual, para um README do github.
Assistant: Com certeza! Ap√≥s analisar o c√≥digo, aqui est√£o algumas sugest√µes para documentar seu projeto em um README do GitHub, juntamente com sugest√µes de melhorias no c√≥digo:

**Estrutura do README (sugest√£o):**

```markdown
# Nome do Projeto (Ex: Chat IA Inteligente)

Uma breve descri√ß√£o do que o projeto faz.

## Funcionalidades Principais

*   Chat interativo com IA (Gemini).
*   An√°lise de imagens (upload ou √°rea de transfer√™ncia).
*   Gera√ß√£o de imagens a partir de prompts.
*   Hist√≥rico de conversas.
*   Limita√ß√£o de taxa de requisi√ß√µes (Rate Limiting).

## Como Usar

Instru√ß√µes detalhadas sobre como executar o projeto (incluindo depend√™ncias e vari√°veis de ambiente).

Exemplo:

1.  Clone o reposit√≥rio.
2.  Crie um ambiente virtual: `python -m venv venv`
3.  Ative o ambiente virtual: `source venv/bin/activate` (Linux/macOS) ou `venv\Scripts\activate` (Windows)
4.  Instale as depend√™ncias: `pip install -r requirements.txt`
5.  Configure a vari√°vel de ambiente `API_KEY_GEMINI` no arquivo `.env`.
6.  Execute o aplicativo: `streamlit run src/chat_streamlit.py`

## Arquitetura do Projeto

*   `src/`: Cont√©m o c√≥digo fonte principal.
    *   `chat_streamlit.py`: Interface Streamlit para o chat.
    *   `config/`: Configura√ß√µes do projeto.
    *   `core/`: L√≥gica principal (handlers, logger, rate limiting).
    *   `services/`: Servi√ßos (IA, documentos, markdown, busca de arquivos).
    *   `utils/`: Utilit√°rios.
    *   `main.py`: Ponto de entrada para processamento de imagens em lote.

## Diagrama de Classes (Opcional)

(Adicione um diagrama UML aqui, se poss√≠vel, para ilustrar as rela√ß√µes entre as classes.)

## Contribui√ß√µes

Como contribuir para o projeto (pull requests, etc.).

## Licen√ßa

(Ex: MIT License)

## Autores

[Seu Nome]
```

**Documenta√ß√£o Adicional:**

*   **`chat_streamlit.py`:**
    *   Explicar o uso do `st.session_state` para manter o estado da aplica√ß√£o.
    *   Detalhar como as fun√ß√µes `process_message`, `check_clipboard`, `generate_image` funcionam.
    *   Mencionar como o prompt base √© carregado do arquivo `PROMPT_CHAT_FILE` e como o contexto do c√≥digo √© adicionado.
*   **`config/config.py`:**
    *   Explicar como os caminhos s√£o configurados e a import√¢ncia de `ensure_directories()`.
    *   Documentar a configura√ß√£o do Rate Limiting.
*   **`core/`:**
    *   `GeminiHandler`: Descrever como ele encapsula a intera√ß√£o com o modelo Gemini.
    *   `RateLimiter`: Explicar como a classe limita as requisi√ß√µes para evitar exceder as quotas da API.
*   **`services/`:**
    *   `ImageProcessor`: Detalhar como as imagens s√£o processadas, resumidas e como o hist√≥rico √© mantido.
    *   `DocumentService` e `MarkdownService`: Explicar como os resumos s√£o formatados e salvos em diferentes formatos.

**Sugest√µes de Melhorias no C√≥digo:**

*   **`chat_streamlit.py`:**
    *   **Modulariza√ß√£o:** Quebrar a fun√ß√£o `process_message` em fun√ß√µes menores e mais focadas.
    *   **Gerenciamento de Erros:** Implementar tratamento de erros mais robusto, com mensagens de erro mais claras para o usu√°rio.
    *   **Coment√°rios:** Adicionar mais coment√°rios para explicar a l√≥gica de partes complexas do c√≥digo.
*   **`core/RateLimiter.py`:**
    *   **Logging:** Adicionar logs para registrar quando as requisi√ß√µes s√£o limitadas e por quanto tempo.
*   **`services/ImageProcessor.py`:**
    *   **Paraleliza√ß√£o:** Considerar o uso de threads ou processos para processar m√∫ltiplas imagens simultaneamente (respeitando o Rate Limiting).
    *   **Abstra√ß√£o:** Criar classes para representar prompts e respostas da IA, para facilitar a manuten√ß√£o e testes.
*   **`utils/file_utils.py`:**
    *   **Tratamento de Exce√ß√µes:** Adicionar tratamento de exce√ß√µes ao listar imagens, caso o diret√≥rio n√£o exista ou ocorra um erro de permiss√£o.
*   **Configura√ß√£o e Vari√°veis de Ambiente:**
    *   **Valida√ß√£o:** Adicionar valida√ß√£o para garantir que as vari√°veis de ambiente necess√°rias est√£o configuradas corretamente.
*   **Padroniza√ß√£o:**
    *   **Formata√ß√£o:** Usar um formatador de c√≥digo como Black ou autopep8 para manter a consist√™ncia na formata√ß√£o do c√≥digo.
    *   **Linters:** Usar um linter como pylint ou flake8 para identificar poss√≠veis problemas no c√≥digo.

**D√≠vidas T√©cnicas:**

*   **Tratamento de Erros Gen√©rico:** Muitos blocos `except Exception as e` capturam todos os tipos de erros, o que pode dificultar a identifica√ß√£o de problemas espec√≠ficos. √â recomend√°vel capturar tipos de erros mais espec√≠ficos e trat√°-los adequadamente.
*   **C√≥digo Duplicado:** Identificar e refatorar trechos de c√≥digo duplicados, como o carregamento do hist√≥rico em `ImageProcessor`.
*   **Testes:** Adicionar testes unit√°rios e de integra√ß√£o para garantir a qualidade do c√≥digo e evitar regress√µes.

**Observa√ß√µes:**

*   O uso de `st.rerun()` pode ser problem√°tico em algumas situa√ß√µes. Considere outras formas de atualizar a interface do Streamlit, como usar callbacks ou reatividade.
*   A l√≥gica de limpar as imagens da √°rea de transfer√™ncia e do uploader pode ser simplificada usando uma √∫nica fun√ß√£o.
*   A classe `RateLimiter` parece estar funcionando corretamente, mas √© importante monitorar seu desempenho em um ambiente de produ√ß√£o para garantir que ela est√° limitando as requisi√ß√µes conforme o esperado.

Espero que isto ajude a criar uma documenta√ß√£o abrangente e a melhorar a qualidade do seu c√≥digo!

User: gere um artigo com a minha aplica√ß√£o para apresentar no linkedin.
Assistant: Assistant: Com certeza! Aqui est√° um rascunho de artigo para o LinkedIn, destacando sua aplica√ß√£o e convidando a comunidade a explorar:

**T√≠tulo:** Apresentando [Nome da Aplica√ß√£o]: Sua Nova Ferramenta Inteligente para [Prop√≥sito Principal]! üöÄ

**Imagem:** (Anexe uma imagem ou GIF atraente da sua aplica√ß√£o em a√ß√£o. Pode ser uma captura de tela do chat, um exemplo de imagem gerada ou uma representa√ß√£o visual da arquitetura.)

**Texto:**

---

Ol√°, comunidade do LinkedIn! üëã

Estou muito animado para compartilhar com voc√™s um projeto que tenho trabalhado recentemente: [Nome da Aplica√ß√£o]!

[Nome da Aplica√ß√£o] √© uma aplica√ß√£o inovadora constru√≠da com Streamlit e alimentada pela intelig√™ncia artificial do Gemini, que visa facilitar [Prop√≥sito Principal da Aplica√ß√£o]. Seja voc√™ [p√∫blico-alvo], esta ferramenta pode te ajudar a [Benef√≠cios Principais].

**Funcionalidades em Destaque:**

*   üí¨ **Chat Inteligente:** Converse com um assistente de IA vers√°til para obter respostas, insights e suporte.
*   üñºÔ∏è **An√°lise de Imagens:** Fa√ßa upload de imagens ou cole da √°rea de transfer√™ncia para obter an√°lises detalhadas e informa√ß√µes relevantes.
*   ‚ú® **Gera√ß√£o de Imagens:** Crie imagens impressionantes a partir de prompts de texto, liberando sua criatividade.
*   üìö **Aprendizado Cont√≠nuo:** A aplica√ß√£o est√° sempre aprendendo e evoluindo, com atualiza√ß√µes regulares e novos recursos.

**Como Funciona:**

[D√™ um breve resumo de como a aplica√ß√£o funciona. Por exemplo:]

1.  Acesse a interface intuitiva via Streamlit.
2.  Comece a conversar com a IA, fa√ßa upload de uma imagem ou insira um prompt para gerar uma imagem.
3.  Receba respostas r√°pidas, an√°lises detalhadas e imagens criativas em segundos.

**Por Tr√°s da Aplica√ß√£o:**

Constru√≠ [Nome da Aplica√ß√£o] usando as seguintes tecnologias:

*   **Streamlit:** Para uma interface de usu√°rio interativa e amig√°vel.
*   **Gemini (Google AI):** Para capacidades avan√ßadas de processamento de linguagem natural e gera√ß√£o de imagens.
*   **Python:** A linguagem de programa√ß√£o por tr√°s de toda a m√°gica.

**Por Que Eu Criei Isso?**

[Compartilhe sua motiva√ß√£o por tr√°s da cria√ß√£o da aplica√ß√£o. Por exemplo:]

Eu sempre fui fascinado pelo poder da IA para resolver problemas e melhorar a vida das pessoas. Com [Nome da Aplica√ß√£o], eu queria criar uma ferramenta acess√≠vel e f√°cil de usar que pudesse ajudar [p√∫blico-alvo] a [Benef√≠cios Principais].

**Convite √† A√ß√£o:**

Gostaria muito de convidar voc√™ a experimentar [Nome da Aplica√ß√£o] e compartilhar seus coment√°rios! Se voc√™ √© [p√∫blico-alvo], acredito que esta ferramenta pode te ajudar a [Benef√≠cios Principais].

‚û°Ô∏è [Link para o reposit√≥rio no GitHub ou para a aplica√ß√£o online (se dispon√≠vel)]

**Pe√ßo a sua ajuda!**

Se voc√™ gostou deste projeto, por favor, curta, compartilhe e comente abaixo. Sua opini√£o √© muito importante para mim!

#IA #InteligenciaArtificial #Streamlit #Python #Gemini #Inovacao #Desenvolvimento #OpenSource #Compartilhe

---

**Dicas Adicionais:**

*   **Adapte o texto:** Personalize o texto com detalhes espec√≠ficos sobre sua aplica√ß√£o, seu p√∫blico-alvo e sua motiva√ß√£o.
*   **Use emojis:** Emojis podem tornar seu artigo mais atraente e expressivo.
*   **Seja conciso:** Mantenha o texto curto e direto ao ponto.
*   **Interaja com os coment√°rios:** Responda aos coment√°rios e perguntas para criar um di√°logo com a comunidade.
*   **Compartilhe em grupos relevantes:** Publique o artigo em grupos do LinkedIn relacionados a IA, desenvolvimento de software e sua √°rea de atua√ß√£o.

Espero que este rascunho te ajude a criar um artigo de sucesso para o LinkedIn! Se precisar de mais alguma ajuda, √© s√≥ me avisar.
User: voc√™ consegue detalhar o que essa aplica√ß√£o faz ?
Assistant: Assistant: Com certeza! Vamos detalhar o que essa aplica√ß√£o faz, explorando seus componentes e funcionalidades principais:

**Prop√≥sito Geral:**

A aplica√ß√£o √© um assistente inteligente e vers√°til, constru√≠do para interagir com usu√°rios de diversas maneiras, combinando capacidades de chat, an√°lise de imagens e gera√ß√£o de imagens. O objetivo √© fornecer uma experi√™ncia rica e integrada, onde o usu√°rio pode obter informa√ß√µes, insights e criar conte√∫do visual de forma intuitiva.

**Funcionalidades Detalhadas:**

1.  **Chat Interativo com IA:**
    *   **Motor:** Utiliza o modelo Gemini (do Google AI) para processamento de linguagem natural.
    *   **Funcionalidade:** Permite que os usu√°rios conversem livremente com a IA sobre uma variedade de t√≥picos.
    *   **Contexto:**
        *   Carrega um prompt base a partir de um arquivo externo (`PROMPT_CHAT_FILE`) para definir a personalidade e o comportamento da IA.
        *   Adiciona o c√≥digo-fonte do projeto como contexto para que a IA possa responder perguntas sobre a estrutura, a l√≥gica e o funcionamento interno da aplica√ß√£o.
        *   Mant√©m um hist√≥rico das conversas para que a IA possa entender o contexto das perguntas e fornecer respostas mais relevantes.
    *   **Casos de Uso:**
        *   Obter informa√ß√µes sobre diversos assuntos.
        *   Pedir ajuda para resolver problemas de programa√ß√£o.
        *   Explorar ideias criativas.
        *   Aprender sobre o funcionamento da pr√≥pria aplica√ß√£o.

2.  **An√°lise de Imagens:**
    *   **Fontes de Imagem:**
        *   Upload de arquivos de imagem (PNG, JPG, JPEG).
        *   Colagem de imagens da √°rea de transfer√™ncia.
    *   **Funcionalidade:** Permite que os usu√°rios enviem imagens para que a IA as analise e forne√ßa informa√ß√µes sobre o conte√∫do da imagem.
    *   **Processamento:**
        *   As imagens s√£o salvas temporariamente no disco para serem processadas pelo modelo Gemini.
        *   O prompt base √© combinado com a imagem para instruir a IA sobre o tipo de an√°lise a ser realizada.
        *   O hist√≥rico de conversas tamb√©m √© inclu√≠do no prompt para fornecer contexto adicional.
    *   **Casos de Uso:**
        *   Obter descri√ß√µes de imagens.
        *   Identificar objetos em imagens.
        *   Analisar gr√°ficos e diagramas.
        *   Pedir opini√µes sobre fotos.

3.  **Gera√ß√£o de Imagens:**
    *   **Motor:** Utiliza o modelo Gemini para gera√ß√£o de imagens.
    *   **Funcionalidade:** Permite que os usu√°rios criem imagens a partir de prompts de texto.
    *   **Processamento:**
        *   O usu√°rio fornece um prompt descrevendo a imagem desejada.
        *   O modelo Gemini gera uma imagem com base no prompt.
        *   A imagem gerada √© exibida ao usu√°rio e pode ser salva.
    *   **Casos de Uso:**
        *   Criar ilustra√ß√µes para apresenta√ß√µes.
        *   Gerar imagens para redes sociais.
        *   Visualizar conceitos abstratos.
        *   Experimentar com diferentes estilos art√≠sticos.

4.  **Interface do Usu√°rio (Streamlit):**
    *   **Layout:**
        *   Barra lateral para configura√ß√µes, upload de imagens e gera√ß√£o de imagens.
        *   √Årea principal para exibi√ß√£o do hist√≥rico de conversas.
    *   **Componentes:**
        *   Chat input para entrada de texto.
        *   File uploader para upload de imagens.
        *   Bot√µes para colar da √°rea de transfer√™ncia e limpar imagens.
        *   Bot√£o para limpar o hist√≥rico de conversas.
        *   Display do hist√≥rico de conversas com mensagens do usu√°rio e da IA.
    *   **Estado:** Utiliza `st.session_state` para manter o estado da aplica√ß√£o entre as intera√ß√µes do usu√°rio.

5.  **Gerenciamento de Recursos:**
    *   **Rate Limiting:** Implementa um sistema de limita√ß√£o de taxa de requisi√ß√µes para evitar sobrecarregar a API do Gemini e garantir um uso justo dos recursos.
    *   **Logs:** Utiliza um sistema de logging para registrar eventos importantes, erros e informa√ß√µes de depura√ß√£o.
    *   **Configura√ß√£o:** Carrega configura√ß√µes a partir de arquivos externos para facilitar a personaliza√ß√£o e a manuten√ß√£o da aplica√ß√£o.

6.  **Persist√™ncia:**
    *   **Hist√≥rico de An√°lises:** A aplica√ß√£o mant√©m um hist√≥rico das an√°lises de imagens, incluindo o nome da imagem e o resumo gerado pela IA.
    *   **Exporta√ß√£o:** Os resumos das an√°lises podem ser exportados para arquivos DOCX (Word) e Markdown.

**Fluxo de Dados:**

1.  O usu√°rio interage com a interface do Streamlit.
2.  A entrada do usu√°rio (texto ou imagem) √© enviada para a IA (Gemini).
3.  A IA processa a entrada e gera uma resposta.
4.  A resposta da IA √© exibida ao usu√°rio na interface do Streamlit.
5.  O hist√≥rico de conversas √© atualizado.
6.  As an√°lises de imagens s√£o adicionadas ao hist√≥rico e podem ser exportadas.

**Em resumo, a aplica√ß√£o √© um assistente inteligente multifuncional que combina chat, an√°lise de imagens e gera√ß√£o de imagens para fornecer uma experi√™ncia de usu√°rio rica e vers√°til.** Ela pode ser usada para uma variedade de prop√≥sitos, desde obter informa√ß√µes e insights at√© criar conte√∫do visual.

User:  
2025-04-08 02:10:50,420 [INFO] Resposta da IA (texto): Com base na sua solicita√ß√£o para uma revis√£o e melhoria do c√≥digo, aqui est√£o algumas sugest√µes e √°reas a serem consideradas:

**Arquitetura e Design:**

*   **Separa√ß√£o de Responsabilidades:** O arquivo `chat_streamlit.py` est√° sobrecarregado com l√≥gica de UI, gerenciamento de estado, e intera√ß√£o com a IA. Considere mover a l√≥gica de gerenciamento de estado para uma classe separada ou usar um padr√£o como o Model-View-Presenter (MVP) para separar a apresenta√ß√£o da l√≥gica.
*   **Abstra√ß√£o da Interface do Usu√°rio:** Isolar a l√≥gica espec√≠fica do Streamlit em m√≥dulos separados pode facilitar a substitui√ß√£o da interface do usu√°rio por outra tecnologia no futuro.
*   **Centraliza√ß√£o da Configura√ß√£o:** Agrupar todas as configura√ß√µes em `config.py`, incluindo chaves de API e limites de taxa. Isso facilita o gerenciamento e a manuten√ß√£o.

**Legibilidade e Manutenibilidade:**

*   **Coment√°rios e Documenta√ß√£o:** Adicionar mais coment√°rios para explicar partes complexas do c√≥digo, especialmente em `chat_streamlit.py`.
*   **Nomes Descritivos:** Certificar-se de que todas as vari√°veis e fun√ß√µes tenham nomes claros e descritivos.
*   **Redu√ß√£o da Complexidade:**
    *   Quebrar fun√ß√µes grandes como `process_message` em fun√ß√µes menores e mais focadas.
    *   Simplificar as condicionais e loops sempre que poss√≠vel.
*   **Strings M√°gicas:** Substituir strings literais por constantes nomeadas para facilitar a altera√ß√£o e evitar erros de digita√ß√£o.

**Performance:**

*   **Otimiza√ß√£o de Imagens:** Considerar a otimiza√ß√£o de imagens antes de envi√°-las para a IA (redimensionamento, compress√£o) para reduzir o tempo de processamento e o uso da largura de banda.
*   **Processamento Ass√≠ncrono:** Se poss√≠vel, usar threads ou processos para executar tarefas demoradas (como a gera√ß√£o de imagens) em segundo plano, sem bloquear a interface do usu√°rio. (CUIDADO COM O RATE LIMIT)

**Seguran√ßa:**

*   **Valida√ß√£o de Dados:** Validar todos os dados de entrada do usu√°rio para evitar ataques de inje√ß√£o e outros problemas de seguran√ßa.
*   **Gerenciamento de Chaves de API:** Certificar-se de que as chaves de API sejam armazenadas com seguran√ßa e n√£o sejam expostas no c√≥digo ou em logs. (J√° est√° feito pelo .env)

**Tratamento de Erros:**

*   **Tipos de Exce√ß√£o Espec√≠ficos:** Evitar o uso de `except Exception as e` gen√©rico. Capturar tipos de exce√ß√£o espec√≠ficos e trat√°-los adequadamente.
*   **Mensagens de Erro √öteis:** Fornecer mensagens de erro claras e √∫teis para o usu√°rio, com informa√ß√µes sobre como resolver o problema.
*   **Logging de Erros:** Registrar todos os erros em um arquivo de log para facilitar a depura√ß√£o.

**D√≠vidas T√©cnicas:**

*   **`st.rerun()`:** O uso de `st.rerun()` pode causar comportamentos inesperados e deve ser evitado sempre que poss√≠vel. Considere usar outras formas de atualizar a interface do Streamlit, como callbacks ou reatividade.
*   **Duplica√ß√£o de C√≥digo:** Identificar e refatorar trechos de c√≥digo duplicados.
*   **Falta de Testes:** Adicionar testes unit√°rios e de integra√ß√£o para garantir a qualidade do c√≥digo e evitar regress√µes.

**Sugest√µes Espec√≠ficas:**

*   **`chat_streamlit.py`:**
    *   Considere usar um padr√£o de mensagens para o hist√≥rico do chat (por exemplo, uma classe `Message` com atributos como `role`, `content`, `image`).
    *   A l√≥gica para resetar o uploader e limpar as imagens pode ser simplificada.
*   **`core/RateLimiter.py`:**
    *   Adicione logs para registrar quando as requisi√ß√µes s√£o limitadas e por quanto tempo.
*   **`services/ImageProcessor.py`:**
    *   A l√≥gica para carregar e salvar o hist√≥rico pode ser simplificada.

**Padroniza√ß√£o:**

*   **Formata√ß√£o:** Usar um formatador de c√≥digo como Black ou autopep8 para manter a consist√™ncia na formata√ß√£o do c√≥digo.
*   **Linters:** Usar um linter como pylint ou flake8 para identificar poss√≠veis problemas no c√≥digo.

**Recursos Adicionais:**

*   **Documenta√ß√£o do Streamlit:** [https://docs.streamlit.io/](https://docs.streamlit.io/)
*   **Guia de Estilo do Python (PEP 8):** [https://peps.python.org/pep-0008/](https://peps.python.org/pep-0008/)
*   **Ferramentas de Linter e Formata√ß√£o:** Black, pylint, flake8

Lembre-se de que estas s√£o apenas sugest√µes. A melhor forma de melhorar o c√≥digo depende das suas necessidades espec√≠ficas e dos seus objetivos para o projeto.

2025-04-08 02:10:50,536 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 02:10:50,539 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 02:10:50,541 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 02:10:50,543 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 02:10:50,546 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 02:10:50,549 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 02:10:50,551 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 02:10:50,553 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 02:10:50,554 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 02:10:50,556 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 02:10:50,558 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 02:10:50,561 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 02:10:50,563 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 02:10:50,564 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 02:10:50,566 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 02:12:22,577 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 02:12:22,579 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 02:12:22,580 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 02:12:22,582 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 02:12:22,584 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 02:12:22,586 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 02:12:22,588 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 02:12:22,590 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 02:12:22,591 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 02:12:22,593 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 02:12:22,594 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 02:12:22,596 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 02:12:22,597 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 02:12:22,599 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 02:12:22,601 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 02:12:22,701 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 02:12:22,703 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 02:12:22,704 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 02:12:22,705 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 02:12:22,707 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 02:12:22,709 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 02:12:22,714 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 02:12:22,719 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 02:12:22,721 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 02:12:22,723 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 02:12:22,724 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 02:12:22,726 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 02:12:22,728 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 02:12:22,730 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 02:12:22,731 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 02:12:22,736 [INFO] Enviando para IA - Prompt (sem imagem): prompt = f"""
Voc√™ √© um assistente inteligente especializado em revis√£o e melhoria de c√≥digo.

Seu objetivo √©:
- Analisar o c√≥digo-fonte contido no projeto enviado
- Identificar melhorias em legibilidade, performance e seguran√ßa
- Sugerir pr√°ticas recomendadas de design e arquitetura
- Indicar partes com d√≠vidas t√©cnicas ou complexidade desnecess√°ria

Informa√ß√µes adicionais:
- Estrutura do projeto: segue o padr√£o de pastas iniciando por src/

"""

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
import base64
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.generativeai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: gere uma documenta√ß√£o em markdown do meu c√≥digo atual, para um README do github.
Assistant: Com certeza! Ap√≥s analisar o c√≥digo, aqui est√£o algumas sugest√µes para documentar seu projeto em um README do GitHub, juntamente com sugest√µes de melhorias no c√≥digo:

**Estrutura do README (sugest√£o):**

```markdown
# Nome do Projeto (Ex: Chat IA Inteligente)

Uma breve descri√ß√£o do que o projeto faz.

## Funcionalidades Principais

*   Chat interativo com IA (Gemini).
*   An√°lise de imagens (upload ou √°rea de transfer√™ncia).
*   Gera√ß√£o de imagens a partir de prompts.
*   Hist√≥rico de conversas.
*   Limita√ß√£o de taxa de requisi√ß√µes (Rate Limiting).

## Como Usar

Instru√ß√µes detalhadas sobre como executar o projeto (incluindo depend√™ncias e vari√°veis de ambiente).

Exemplo:

1.  Clone o reposit√≥rio.
2.  Crie um ambiente virtual: `python -m venv venv`
3.  Ative o ambiente virtual: `source venv/bin/activate` (Linux/macOS) ou `venv\Scripts\activate` (Windows)
4.  Instale as depend√™ncias: `pip install -r requirements.txt`
5.  Configure a vari√°vel de ambiente `API_KEY_GEMINI` no arquivo `.env`.
6.  Execute o aplicativo: `streamlit run src/chat_streamlit.py`

## Arquitetura do Projeto

*   `src/`: Cont√©m o c√≥digo fonte principal.
    *   `chat_streamlit.py`: Interface Streamlit para o chat.
    *   `config/`: Configura√ß√µes do projeto.
    *   `core/`: L√≥gica principal (handlers, logger, rate limiting).
    *   `services/`: Servi√ßos (IA, documentos, markdown, busca de arquivos).
    *   `utils/`: Utilit√°rios.
    *   `main.py`: Ponto de entrada para processamento de imagens em lote.

## Diagrama de Classes (Opcional)

(Adicione um diagrama UML aqui, se poss√≠vel, para ilustrar as rela√ß√µes entre as classes.)

## Contribui√ß√µes

Como contribuir para o projeto (pull requests, etc.).

## Licen√ßa

(Ex: MIT License)

## Autores

[Seu Nome]
```

**Documenta√ß√£o Adicional:**

*   **`chat_streamlit.py`:**
    *   Explicar o uso do `st.session_state` para manter o estado da aplica√ß√£o.
    *   Detalhar como as fun√ß√µes `process_message`, `check_clipboard`, `generate_image` funcionam.
    *   Mencionar como o prompt base √© carregado do arquivo `PROMPT_CHAT_FILE` e como o contexto do c√≥digo √© adicionado.
*   **`config/config.py`:**
    *   Explicar como os caminhos s√£o configurados e a import√¢ncia de `ensure_directories()`.
    *   Documentar a configura√ß√£o do Rate Limiting.
*   **`core/`:**
    *   `GeminiHandler`: Descrever como ele encapsula a intera√ß√£o com o modelo Gemini.
    *   `RateLimiter`: Explicar como a classe limita as requisi√ß√µes para evitar exceder as quotas da API.
*   **`services/`:**
    *   `ImageProcessor`: Detalhar como as imagens s√£o processadas, resumidas e como o hist√≥rico √© mantido.
    *   `DocumentService` e `MarkdownService`: Explicar como os resumos s√£o formatados e salvos em diferentes formatos.

**Sugest√µes de Melhorias no C√≥digo:**

*   **`chat_streamlit.py`:**
    *   **Modulariza√ß√£o:** Quebrar a fun√ß√£o `process_message` em fun√ß√µes menores e mais focadas.
    *   **Gerenciamento de Erros:** Implementar tratamento de erros mais robusto, com mensagens de erro mais claras para o usu√°rio.
    *   **Coment√°rios:** Adicionar mais coment√°rios para explicar a l√≥gica de partes complexas do c√≥digo.
*   **`core/RateLimiter.py`:**
    *   **Logging:** Adicionar logs para registrar quando as requisi√ß√µes s√£o limitadas e por quanto tempo.
*   **`services/ImageProcessor.py`:**
    *   **Paraleliza√ß√£o:** Considerar o uso de threads ou processos para processar m√∫ltiplas imagens simultaneamente (respeitando o Rate Limiting).
    *   **Abstra√ß√£o:** Criar classes para representar prompts e respostas da IA, para facilitar a manuten√ß√£o e testes.
*   **`utils/file_utils.py`:**
    *   **Tratamento de Exce√ß√µes:** Adicionar tratamento de exce√ß√µes ao listar imagens, caso o diret√≥rio n√£o exista ou ocorra um erro de permiss√£o.
*   **Configura√ß√£o e Vari√°veis de Ambiente:**
    *   **Valida√ß√£o:** Adicionar valida√ß√£o para garantir que as vari√°veis de ambiente necess√°rias est√£o configuradas corretamente.
*   **Padroniza√ß√£o:**
    *   **Formata√ß√£o:** Usar um formatador de c√≥digo como Black ou autopep8 para manter a consist√™ncia na formata√ß√£o do c√≥digo.
    *   **Linters:** Usar um linter como pylint ou flake8 para identificar poss√≠veis problemas no c√≥digo.

**D√≠vidas T√©cnicas:**

*   **Tratamento de Erros Gen√©rico:** Muitos blocos `except Exception as e` capturam todos os tipos de erros, o que pode dificultar a identifica√ß√£o de problemas espec√≠ficos. √â recomend√°vel capturar tipos de erros mais espec√≠ficos e trat√°-los adequadamente.
*   **C√≥digo Duplicado:** Identificar e refatorar trechos de c√≥digo duplicados, como o carregamento do hist√≥rico em `ImageProcessor`.
*   **Testes:** Adicionar testes unit√°rios e de integra√ß√£o para garantir a qualidade do c√≥digo e evitar regress√µes.

**Observa√ß√µes:**

*   O uso de `st.rerun()` pode ser problem√°tico em algumas situa√ß√µes. Considere outras formas de atualizar a interface do Streamlit, como usar callbacks ou reatividade.
*   A l√≥gica de limpar as imagens da √°rea de transfer√™ncia e do uploader pode ser simplificada usando uma √∫nica fun√ß√£o.
*   A classe `RateLimiter` parece estar funcionando corretamente, mas √© importante monitorar seu desempenho em um ambiente de produ√ß√£o para garantir que ela est√° limitando as requisi√ß√µes conforme o esperado.

Espero que isto ajude a criar uma documenta√ß√£o abrangente e a melhorar a qualidade do seu c√≥digo!

User: gere um artigo com a minha aplica√ß√£o para apresentar no linkedin.
Assistant: Assistant: Com certeza! Aqui est√° um rascunho de artigo para o LinkedIn, destacando sua aplica√ß√£o e convidando a comunidade a explorar:

**T√≠tulo:** Apresentando [Nome da Aplica√ß√£o]: Sua Nova Ferramenta Inteligente para [Prop√≥sito Principal]! üöÄ

**Imagem:** (Anexe uma imagem ou GIF atraente da sua aplica√ß√£o em a√ß√£o. Pode ser uma captura de tela do chat, um exemplo de imagem gerada ou uma representa√ß√£o visual da arquitetura.)

**Texto:**

---

Ol√°, comunidade do LinkedIn! üëã

Estou muito animado para compartilhar com voc√™s um projeto que tenho trabalhado recentemente: [Nome da Aplica√ß√£o]!

[Nome da Aplica√ß√£o] √© uma aplica√ß√£o inovadora constru√≠da com Streamlit e alimentada pela intelig√™ncia artificial do Gemini, que visa facilitar [Prop√≥sito Principal da Aplica√ß√£o]. Seja voc√™ [p√∫blico-alvo], esta ferramenta pode te ajudar a [Benef√≠cios Principais].

**Funcionalidades em Destaque:**

*   üí¨ **Chat Inteligente:** Converse com um assistente de IA vers√°til para obter respostas, insights e suporte.
*   üñºÔ∏è **An√°lise de Imagens:** Fa√ßa upload de imagens ou cole da √°rea de transfer√™ncia para obter an√°lises detalhadas e informa√ß√µes relevantes.
*   ‚ú® **Gera√ß√£o de Imagens:** Crie imagens impressionantes a partir de prompts de texto, liberando sua criatividade.
*   üìö **Aprendizado Cont√≠nuo:** A aplica√ß√£o est√° sempre aprendendo e evoluindo, com atualiza√ß√µes regulares e novos recursos.

**Como Funciona:**

[D√™ um breve resumo de como a aplica√ß√£o funciona. Por exemplo:]

1.  Acesse a interface intuitiva via Streamlit.
2.  Comece a conversar com a IA, fa√ßa upload de uma imagem ou insira um prompt para gerar uma imagem.
3.  Receba respostas r√°pidas, an√°lises detalhadas e imagens criativas em segundos.

**Por Tr√°s da Aplica√ß√£o:**

Constru√≠ [Nome da Aplica√ß√£o] usando as seguintes tecnologias:

*   **Streamlit:** Para uma interface de usu√°rio interativa e amig√°vel.
*   **Gemini (Google AI):** Para capacidades avan√ßadas de processamento de linguagem natural e gera√ß√£o de imagens.
*   **Python:** A linguagem de programa√ß√£o por tr√°s de toda a m√°gica.

**Por Que Eu Criei Isso?**

[Compartilhe sua motiva√ß√£o por tr√°s da cria√ß√£o da aplica√ß√£o. Por exemplo:]

Eu sempre fui fascinado pelo poder da IA para resolver problemas e melhorar a vida das pessoas. Com [Nome da Aplica√ß√£o], eu queria criar uma ferramenta acess√≠vel e f√°cil de usar que pudesse ajudar [p√∫blico-alvo] a [Benef√≠cios Principais].

**Convite √† A√ß√£o:**

Gostaria muito de convidar voc√™ a experimentar [Nome da Aplica√ß√£o] e compartilhar seus coment√°rios! Se voc√™ √© [p√∫blico-alvo], acredito que esta ferramenta pode te ajudar a [Benef√≠cios Principais].

‚û°Ô∏è [Link para o reposit√≥rio no GitHub ou para a aplica√ß√£o online (se dispon√≠vel)]

**Pe√ßo a sua ajuda!**

Se voc√™ gostou deste projeto, por favor, curta, compartilhe e comente abaixo. Sua opini√£o √© muito importante para mim!

#IA #InteligenciaArtificial #Streamlit #Python #Gemini #Inovacao #Desenvolvimento #OpenSource #Compartilhe

---

**Dicas Adicionais:**

*   **Adapte o texto:** Personalize o texto com detalhes espec√≠ficos sobre sua aplica√ß√£o, seu p√∫blico-alvo e sua motiva√ß√£o.
*   **Use emojis:** Emojis podem tornar seu artigo mais atraente e expressivo.
*   **Seja conciso:** Mantenha o texto curto e direto ao ponto.
*   **Interaja com os coment√°rios:** Responda aos coment√°rios e perguntas para criar um di√°logo com a comunidade.
*   **Compartilhe em grupos relevantes:** Publique o artigo em grupos do LinkedIn relacionados a IA, desenvolvimento de software e sua √°rea de atua√ß√£o.

Espero que este rascunho te ajude a criar um artigo de sucesso para o LinkedIn! Se precisar de mais alguma ajuda, √© s√≥ me avisar.
User: voc√™ consegue detalhar o que essa aplica√ß√£o faz ?
Assistant: Assistant: Com certeza! Vamos detalhar o que essa aplica√ß√£o faz, explorando seus componentes e funcionalidades principais:

**Prop√≥sito Geral:**

A aplica√ß√£o √© um assistente inteligente e vers√°til, constru√≠do para interagir com usu√°rios de diversas maneiras, combinando capacidades de chat, an√°lise de imagens e gera√ß√£o de imagens. O objetivo √© fornecer uma experi√™ncia rica e integrada, onde o usu√°rio pode obter informa√ß√µes, insights e criar conte√∫do visual de forma intuitiva.

**Funcionalidades Detalhadas:**

1.  **Chat Interativo com IA:**
    *   **Motor:** Utiliza o modelo Gemini (do Google AI) para processamento de linguagem natural.
    *   **Funcionalidade:** Permite que os usu√°rios conversem livremente com a IA sobre uma variedade de t√≥picos.
    *   **Contexto:**
        *   Carrega um prompt base a partir de um arquivo externo (`PROMPT_CHAT_FILE`) para definir a personalidade e o comportamento da IA.
        *   Adiciona o c√≥digo-fonte do projeto como contexto para que a IA possa responder perguntas sobre a estrutura, a l√≥gica e o funcionamento interno da aplica√ß√£o.
        *   Mant√©m um hist√≥rico das conversas para que a IA possa entender o contexto das perguntas e fornecer respostas mais relevantes.
    *   **Casos de Uso:**
        *   Obter informa√ß√µes sobre diversos assuntos.
        *   Pedir ajuda para resolver problemas de programa√ß√£o.
        *   Explorar ideias criativas.
        *   Aprender sobre o funcionamento da pr√≥pria aplica√ß√£o.

2.  **An√°lise de Imagens:**
    *   **Fontes de Imagem:**
        *   Upload de arquivos de imagem (PNG, JPG, JPEG).
        *   Colagem de imagens da √°rea de transfer√™ncia.
    *   **Funcionalidade:** Permite que os usu√°rios enviem imagens para que a IA as analise e forne√ßa informa√ß√µes sobre o conte√∫do da imagem.
    *   **Processamento:**
        *   As imagens s√£o salvas temporariamente no disco para serem processadas pelo modelo Gemini.
        *   O prompt base √© combinado com a imagem para instruir a IA sobre o tipo de an√°lise a ser realizada.
        *   O hist√≥rico de conversas tamb√©m √© inclu√≠do no prompt para fornecer contexto adicional.
    *   **Casos de Uso:**
        *   Obter descri√ß√µes de imagens.
        *   Identificar objetos em imagens.
        *   Analisar gr√°ficos e diagramas.
        *   Pedir opini√µes sobre fotos.

3.  **Gera√ß√£o de Imagens:**
    *   **Motor:** Utiliza o modelo Gemini para gera√ß√£o de imagens.
    *   **Funcionalidade:** Permite que os usu√°rios criem imagens a partir de prompts de texto.
    *   **Processamento:**
        *   O usu√°rio fornece um prompt descrevendo a imagem desejada.
        *   O modelo Gemini gera uma imagem com base no prompt.
        *   A imagem gerada √© exibida ao usu√°rio e pode ser salva.
    *   **Casos de Uso:**
        *   Criar ilustra√ß√µes para apresenta√ß√µes.
        *   Gerar imagens para redes sociais.
        *   Visualizar conceitos abstratos.
        *   Experimentar com diferentes estilos art√≠sticos.

4.  **Interface do Usu√°rio (Streamlit):**
    *   **Layout:**
        *   Barra lateral para configura√ß√µes, upload de imagens e gera√ß√£o de imagens.
        *   √Årea principal para exibi√ß√£o do hist√≥rico de conversas.
    *   **Componentes:**
        *   Chat input para entrada de texto.
        *   File uploader para upload de imagens.
        *   Bot√µes para colar da √°rea de transfer√™ncia e limpar imagens.
        *   Bot√£o para limpar o hist√≥rico de conversas.
        *   Display do hist√≥rico de conversas com mensagens do usu√°rio e da IA.
    *   **Estado:** Utiliza `st.session_state` para manter o estado da aplica√ß√£o entre as intera√ß√µes do usu√°rio.

5.  **Gerenciamento de Recursos:**
    *   **Rate Limiting:** Implementa um sistema de limita√ß√£o de taxa de requisi√ß√µes para evitar sobrecarregar a API do Gemini e garantir um uso justo dos recursos.
    *   **Logs:** Utiliza um sistema de logging para registrar eventos importantes, erros e informa√ß√µes de depura√ß√£o.
    *   **Configura√ß√£o:** Carrega configura√ß√µes a partir de arquivos externos para facilitar a personaliza√ß√£o e a manuten√ß√£o da aplica√ß√£o.

6.  **Persist√™ncia:**
    *   **Hist√≥rico de An√°lises:** A aplica√ß√£o mant√©m um hist√≥rico das an√°lises de imagens, incluindo o nome da imagem e o resumo gerado pela IA.
    *   **Exporta√ß√£o:** Os resumos das an√°lises podem ser exportados para arquivos DOCX (Word) e Markdown.

**Fluxo de Dados:**

1.  O usu√°rio interage com a interface do Streamlit.
2.  A entrada do usu√°rio (texto ou imagem) √© enviada para a IA (Gemini).
3.  A IA processa a entrada e gera uma resposta.
4.  A resposta da IA √© exibida ao usu√°rio na interface do Streamlit.
5.  O hist√≥rico de conversas √© atualizado.
6.  As an√°lises de imagens s√£o adicionadas ao hist√≥rico e podem ser exportadas.

**Em resumo, a aplica√ß√£o √© um assistente inteligente multifuncional que combina chat, an√°lise de imagens e gera√ß√£o de imagens para fornecer uma experi√™ncia de usu√°rio rica e vers√°til.** Ela pode ser usada para uma variedade de prop√≥sitos, desde obter informa√ß√µes e insights at√© criar conte√∫do visual.

User:  
Assistant: Com base na sua solicita√ß√£o para uma revis√£o e melhoria do c√≥digo, aqui est√£o algumas sugest√µes e √°reas a serem consideradas:

**Arquitetura e Design:**

*   **Separa√ß√£o de Responsabilidades:** O arquivo `chat_streamlit.py` est√° sobrecarregado com l√≥gica de UI, gerenciamento de estado, e intera√ß√£o com a IA. Considere mover a l√≥gica de gerenciamento de estado para uma classe separada ou usar um padr√£o como o Model-View-Presenter (MVP) para separar a apresenta√ß√£o da l√≥gica.
*   **Abstra√ß√£o da Interface do Usu√°rio:** Isolar a l√≥gica espec√≠fica do Streamlit em m√≥dulos separados pode facilitar a substitui√ß√£o da interface do usu√°rio por outra tecnologia no futuro.
*   **Centraliza√ß√£o da Configura√ß√£o:** Agrupar todas as configura√ß√µes em `config.py`, incluindo chaves de API e limites de taxa. Isso facilita o gerenciamento e a manuten√ß√£o.

**Legibilidade e Manutenibilidade:**

*   **Coment√°rios e Documenta√ß√£o:** Adicionar mais coment√°rios para explicar partes complexas do c√≥digo, especialmente em `chat_streamlit.py`.
*   **Nomes Descritivos:** Certificar-se de que todas as vari√°veis e fun√ß√µes tenham nomes claros e descritivos.
*   **Redu√ß√£o da Complexidade:**
    *   Quebrar fun√ß√µes grandes como `process_message` em fun√ß√µes menores e mais focadas.
    *   Simplificar as condicionais e loops sempre que poss√≠vel.
*   **Strings M√°gicas:** Substituir strings literais por constantes nomeadas para facilitar a altera√ß√£o e evitar erros de digita√ß√£o.

**Performance:**

*   **Otimiza√ß√£o de Imagens:** Considerar a otimiza√ß√£o de imagens antes de envi√°-las para a IA (redimensionamento, compress√£o) para reduzir o tempo de processamento e o uso da largura de banda.
*   **Processamento Ass√≠ncrono:** Se poss√≠vel, usar threads ou processos para executar tarefas demoradas (como a gera√ß√£o de imagens) em segundo plano, sem bloquear a interface do usu√°rio. (CUIDADO COM O RATE LIMIT)

**Seguran√ßa:**

*   **Valida√ß√£o de Dados:** Validar todos os dados de entrada do usu√°rio para evitar ataques de inje√ß√£o e outros problemas de seguran√ßa.
*   **Gerenciamento de Chaves de API:** Certificar-se de que as chaves de API sejam armazenadas com seguran√ßa e n√£o sejam expostas no c√≥digo ou em logs. (J√° est√° feito pelo .env)

**Tratamento de Erros:**

*   **Tipos de Exce√ß√£o Espec√≠ficos:** Evitar o uso de `except Exception as e` gen√©rico. Capturar tipos de exce√ß√£o espec√≠ficos e trat√°-los adequadamente.
*   **Mensagens de Erro √öteis:** Fornecer mensagens de erro claras e √∫teis para o usu√°rio, com informa√ß√µes sobre como resolver o problema.
*   **Logging de Erros:** Registrar todos os erros em um arquivo de log para facilitar a depura√ß√£o.

**D√≠vidas T√©cnicas:**

*   **`st.rerun()`:** O uso de `st.rerun()` pode causar comportamentos inesperados e deve ser evitado sempre que poss√≠vel. Considere usar outras formas de atualizar a interface do Streamlit, como callbacks ou reatividade.
*   **Duplica√ß√£o de C√≥digo:** Identificar e refatorar trechos de c√≥digo duplicados.
*   **Falta de Testes:** Adicionar testes unit√°rios e de integra√ß√£o para garantir a qualidade do c√≥digo e evitar regress√µes.

**Sugest√µes Espec√≠ficas:**

*   **`chat_streamlit.py`:**
    *   Considere usar um padr√£o de mensagens para o hist√≥rico do chat (por exemplo, uma classe `Message` com atributos como `role`, `content`, `image`).
    *   A l√≥gica para resetar o uploader e limpar as imagens pode ser simplificada.
*   **`core/RateLimiter.py`:**
    *   Adicione logs para registrar quando as requisi√ß√µes s√£o limitadas e por quanto tempo.
*   **`services/ImageProcessor.py`:**
    *   A l√≥gica para carregar e salvar o hist√≥rico pode ser simplificada.

**Padroniza√ß√£o:**

*   **Formata√ß√£o:** Usar um formatador de c√≥digo como Black ou autopep8 para manter a consist√™ncia na formata√ß√£o do c√≥digo.
*   **Linters:** Usar um linter como pylint ou flake8 para identificar poss√≠veis problemas no c√≥digo.

**Recursos Adicionais:**

*   **Documenta√ß√£o do Streamlit:** [https://docs.streamlit.io/](https://docs.streamlit.io/)
*   **Guia de Estilo do Python (PEP 8):** [https://peps.python.org/pep-0008/](https://peps.python.org/pep-0008/)
*   **Ferramentas de Linter e Formata√ß√£o:** Black, pylint, flake8

Lembre-se de que estas s√£o apenas sugest√µes. A melhor forma de melhorar o c√≥digo depende das suas necessidades espec√≠ficas e dos seus objetivos para o projeto.

User: Existe alguma aplica√ß√£o que tem um chat que recebe como contexto todo o c√≥digo para que possa analisar em tempo real e propor melhorias, debug e solu√ß√µes ?
2025-04-08 02:12:28,056 [INFO] Resposta da IA (texto): Assistant: Sim, existem algumas aplica√ß√µes e IDEs (Integrated Development Environments) que oferecem funcionalidades de chat com contexto do c√≥digo para an√°lise, debug e sugest√µes de melhorias em tempo real. Aqui est√£o algumas op√ß√µes:

**1. IDEs com Integra√ß√£o de IA:**

*   **GitHub Copilot:** √â um assistente de IA desenvolvido pela GitHub e OpenAI, integrado em diversos IDEs como VS Code, Neovim, JetBrains, etc. Ele oferece sugest√µes de c√≥digo em tempo real, autocompletar, explica√ß√µes de c√≥digo e pode at√© mesmo gerar trechos de c√≥digo completos com base em coment√°rios ou contexto existente. O Copilot tem acesso ao seu c√≥digo no editor, o que lhe permite fornecer sugest√µes e an√°lises contextuais.
*   **Amazon CodeWhisperer:** Similar ao GitHub Copilot, o CodeWhisperer √© um assistente de IA da Amazon, tamb√©m integrado em IDEs. Ele oferece sugest√µes de c√≥digo em tempo real, detec√ß√£o de vulnerabilidades e pode gerar c√≥digo com base em coment√°rios. Ele tamb√©m tem acesso ao contexto do c√≥digo que voc√™ est√° escrevendo.
*   **Tabnine:** Outra op√ß√£o popular, Tabnine, oferece sugest√µes de c√≥digo baseadas em IA e suporta uma variedade de linguagens de programa√ß√£o e IDEs. Ele tamb√©m aprende com seu c√≥digo para fornecer sugest√µes mais personalizadas.

**2. Ferramentas de An√°lise de C√≥digo com Chat:**

*   **DeepSource:** √â uma plataforma de an√°lise est√°tica de c√≥digo que tamb√©m oferece recursos de chat. Ele identifica bugs, vulnerabilidades e problemas de desempenho no seu c√≥digo, e permite que voc√™ discuta e resolva esses problemas diretamente na plataforma.
*   **SonarQube:** √â uma plataforma de an√°lise de c√≥digo open-source que ajuda a detectar bugs, vulnerabilidades e problemas de qualidade de c√≥digo. Embora n√£o tenha um chat integrado, ele permite que voc√™ comente e discuta os problemas encontrados diretamente no c√≥digo.

**3. Extens√µes de IDE com Integra√ß√£o de Chat:**

*   **Codeium:** √â uma extens√£o para VS Code e outras IDEs que utiliza IA para fornecer sugest√µes de c√≥digo, gerar c√≥digo e oferecer um chat interativo. Voc√™ pode selecionar um trecho de c√≥digo e perguntar ao Codeium sobre ele, e ele usar√° o contexto do c√≥digo para fornecer respostas relevantes.

**Como essas ferramentas funcionam:**

1.  **An√°lise do C√≥digo:** A ferramenta analisa o c√≥digo no seu editor ou reposit√≥rio para entender a estrutura, a l√≥gica e as depend√™ncias.
2.  **Contextualiza√ß√£o:** Ela usa o contexto do c√≥digo, como o nome da fun√ß√£o, os par√¢metros, os coment√°rios e o c√≥digo ao redor, para fornecer sugest√µes e an√°lises relevantes.
3.  **Chat Interativo:** Voc√™ pode fazer perguntas sobre o c√≥digo, pedir explica√ß√µes, pedir sugest√µes de melhorias ou pedir ajuda para depurar problemas.
4.  **Sugest√µes em Tempo Real:** A ferramenta oferece sugest√µes de c√≥digo, autocompletar, detec√ß√£o de erros e outras funcionalidades em tempo real, enquanto voc√™ est√° escrevendo o c√≥digo.

**Observa√ß√µes:**

*   A maioria dessas ferramentas requer uma assinatura paga para acessar todos os recursos.
*   A qualidade das sugest√µes e an√°lises depende da qualidade do modelo de IA e da quantidade de dados com que foi treinado.
*   √â importante revisar as sugest√µes da IA com cuidado, pois nem sempre s√£o perfeitas e podem conter erros.

Espero que esta lista te ajude a encontrar a ferramenta certa para suas necessidades!
2025-04-08 02:12:28,200 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 02:12:28,202 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 02:12:28,204 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 02:12:28,206 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 02:12:28,208 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 02:12:28,211 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 02:12:28,214 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 02:12:28,217 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 02:12:28,219 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 02:12:28,221 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 02:12:28,223 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 02:12:28,227 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 02:12:28,229 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 02:12:28,231 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 02:12:28,234 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 02:15:13,041 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 02:15:13,043 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 02:15:13,045 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 02:15:13,047 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 02:15:13,049 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 02:15:13,052 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 02:15:13,055 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 02:15:13,056 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 02:15:13,058 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 02:15:13,060 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 02:15:13,061 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 02:15:13,063 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 02:15:13,065 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 02:15:13,067 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 02:15:13,069 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 02:15:13,178 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 02:15:13,180 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 02:15:13,182 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 02:15:13,184 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 02:15:13,186 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 02:15:13,188 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 02:15:13,190 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 02:15:13,192 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 02:15:13,194 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 02:15:13,196 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 02:15:13,197 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 02:15:13,201 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 02:15:13,205 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 02:15:13,207 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 02:15:13,209 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 02:15:13,211 [INFO] Enviando para IA - Prompt (sem imagem): prompt = f"""
Voc√™ √© um assistente inteligente especializado em revis√£o e melhoria de c√≥digo.

Seu objetivo √©:
- Analisar o c√≥digo-fonte contido no projeto enviado
- Identificar melhorias em legibilidade, performance e seguran√ßa
- Sugerir pr√°ticas recomendadas de design e arquitetura
- Indicar partes com d√≠vidas t√©cnicas ou complexidade desnecess√°ria

Informa√ß√µes adicionais:
- Estrutura do projeto: segue o padr√£o de pastas iniciando por src/

"""

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
import base64
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.generativeai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: gere uma documenta√ß√£o em markdown do meu c√≥digo atual, para um README do github.
Assistant: Com certeza! Ap√≥s analisar o c√≥digo, aqui est√£o algumas sugest√µes para documentar seu projeto em um README do GitHub, juntamente com sugest√µes de melhorias no c√≥digo:

**Estrutura do README (sugest√£o):**

```markdown
# Nome do Projeto (Ex: Chat IA Inteligente)

Uma breve descri√ß√£o do que o projeto faz.

## Funcionalidades Principais

*   Chat interativo com IA (Gemini).
*   An√°lise de imagens (upload ou √°rea de transfer√™ncia).
*   Gera√ß√£o de imagens a partir de prompts.
*   Hist√≥rico de conversas.
*   Limita√ß√£o de taxa de requisi√ß√µes (Rate Limiting).

## Como Usar

Instru√ß√µes detalhadas sobre como executar o projeto (incluindo depend√™ncias e vari√°veis de ambiente).

Exemplo:

1.  Clone o reposit√≥rio.
2.  Crie um ambiente virtual: `python -m venv venv`
3.  Ative o ambiente virtual: `source venv/bin/activate` (Linux/macOS) ou `venv\Scripts\activate` (Windows)
4.  Instale as depend√™ncias: `pip install -r requirements.txt`
5.  Configure a vari√°vel de ambiente `API_KEY_GEMINI` no arquivo `.env`.
6.  Execute o aplicativo: `streamlit run src/chat_streamlit.py`

## Arquitetura do Projeto

*   `src/`: Cont√©m o c√≥digo fonte principal.
    *   `chat_streamlit.py`: Interface Streamlit para o chat.
    *   `config/`: Configura√ß√µes do projeto.
    *   `core/`: L√≥gica principal (handlers, logger, rate limiting).
    *   `services/`: Servi√ßos (IA, documentos, markdown, busca de arquivos).
    *   `utils/`: Utilit√°rios.
    *   `main.py`: Ponto de entrada para processamento de imagens em lote.

## Diagrama de Classes (Opcional)

(Adicione um diagrama UML aqui, se poss√≠vel, para ilustrar as rela√ß√µes entre as classes.)

## Contribui√ß√µes

Como contribuir para o projeto (pull requests, etc.).

## Licen√ßa

(Ex: MIT License)

## Autores

[Seu Nome]
```

**Documenta√ß√£o Adicional:**

*   **`chat_streamlit.py`:**
    *   Explicar o uso do `st.session_state` para manter o estado da aplica√ß√£o.
    *   Detalhar como as fun√ß√µes `process_message`, `check_clipboard`, `generate_image` funcionam.
    *   Mencionar como o prompt base √© carregado do arquivo `PROMPT_CHAT_FILE` e como o contexto do c√≥digo √© adicionado.
*   **`config/config.py`:**
    *   Explicar como os caminhos s√£o configurados e a import√¢ncia de `ensure_directories()`.
    *   Documentar a configura√ß√£o do Rate Limiting.
*   **`core/`:**
    *   `GeminiHandler`: Descrever como ele encapsula a intera√ß√£o com o modelo Gemini.
    *   `RateLimiter`: Explicar como a classe limita as requisi√ß√µes para evitar exceder as quotas da API.
*   **`services/`:**
    *   `ImageProcessor`: Detalhar como as imagens s√£o processadas, resumidas e como o hist√≥rico √© mantido.
    *   `DocumentService` e `MarkdownService`: Explicar como os resumos s√£o formatados e salvos em diferentes formatos.

**Sugest√µes de Melhorias no C√≥digo:**

*   **`chat_streamlit.py`:**
    *   **Modulariza√ß√£o:** Quebrar a fun√ß√£o `process_message` em fun√ß√µes menores e mais focadas.
    *   **Gerenciamento de Erros:** Implementar tratamento de erros mais robusto, com mensagens de erro mais claras para o usu√°rio.
    *   **Coment√°rios:** Adicionar mais coment√°rios para explicar a l√≥gica de partes complexas do c√≥digo.
*   **`core/RateLimiter.py`:**
    *   **Logging:** Adicionar logs para registrar quando as requisi√ß√µes s√£o limitadas e por quanto tempo.
*   **`services/ImageProcessor.py`:**
    *   **Paraleliza√ß√£o:** Considerar o uso de threads ou processos para processar m√∫ltiplas imagens simultaneamente (respeitando o Rate Limiting).
    *   **Abstra√ß√£o:** Criar classes para representar prompts e respostas da IA, para facilitar a manuten√ß√£o e testes.
*   **`utils/file_utils.py`:**
    *   **Tratamento de Exce√ß√µes:** Adicionar tratamento de exce√ß√µes ao listar imagens, caso o diret√≥rio n√£o exista ou ocorra um erro de permiss√£o.
*   **Configura√ß√£o e Vari√°veis de Ambiente:**
    *   **Valida√ß√£o:** Adicionar valida√ß√£o para garantir que as vari√°veis de ambiente necess√°rias est√£o configuradas corretamente.
*   **Padroniza√ß√£o:**
    *   **Formata√ß√£o:** Usar um formatador de c√≥digo como Black ou autopep8 para manter a consist√™ncia na formata√ß√£o do c√≥digo.
    *   **Linters:** Usar um linter como pylint ou flake8 para identificar poss√≠veis problemas no c√≥digo.

**D√≠vidas T√©cnicas:**

*   **Tratamento de Erros Gen√©rico:** Muitos blocos `except Exception as e` capturam todos os tipos de erros, o que pode dificultar a identifica√ß√£o de problemas espec√≠ficos. √â recomend√°vel capturar tipos de erros mais espec√≠ficos e trat√°-los adequadamente.
*   **C√≥digo Duplicado:** Identificar e refatorar trechos de c√≥digo duplicados, como o carregamento do hist√≥rico em `ImageProcessor`.
*   **Testes:** Adicionar testes unit√°rios e de integra√ß√£o para garantir a qualidade do c√≥digo e evitar regress√µes.

**Observa√ß√µes:**

*   O uso de `st.rerun()` pode ser problem√°tico em algumas situa√ß√µes. Considere outras formas de atualizar a interface do Streamlit, como usar callbacks ou reatividade.
*   A l√≥gica de limpar as imagens da √°rea de transfer√™ncia e do uploader pode ser simplificada usando uma √∫nica fun√ß√£o.
*   A classe `RateLimiter` parece estar funcionando corretamente, mas √© importante monitorar seu desempenho em um ambiente de produ√ß√£o para garantir que ela est√° limitando as requisi√ß√µes conforme o esperado.

Espero que isto ajude a criar uma documenta√ß√£o abrangente e a melhorar a qualidade do seu c√≥digo!

User: gere um artigo com a minha aplica√ß√£o para apresentar no linkedin.
Assistant: Assistant: Com certeza! Aqui est√° um rascunho de artigo para o LinkedIn, destacando sua aplica√ß√£o e convidando a comunidade a explorar:

**T√≠tulo:** Apresentando [Nome da Aplica√ß√£o]: Sua Nova Ferramenta Inteligente para [Prop√≥sito Principal]! üöÄ

**Imagem:** (Anexe uma imagem ou GIF atraente da sua aplica√ß√£o em a√ß√£o. Pode ser uma captura de tela do chat, um exemplo de imagem gerada ou uma representa√ß√£o visual da arquitetura.)

**Texto:**

---

Ol√°, comunidade do LinkedIn! üëã

Estou muito animado para compartilhar com voc√™s um projeto que tenho trabalhado recentemente: [Nome da Aplica√ß√£o]!

[Nome da Aplica√ß√£o] √© uma aplica√ß√£o inovadora constru√≠da com Streamlit e alimentada pela intelig√™ncia artificial do Gemini, que visa facilitar [Prop√≥sito Principal da Aplica√ß√£o]. Seja voc√™ [p√∫blico-alvo], esta ferramenta pode te ajudar a [Benef√≠cios Principais].

**Funcionalidades em Destaque:**

*   üí¨ **Chat Inteligente:** Converse com um assistente de IA vers√°til para obter respostas, insights e suporte.
*   üñºÔ∏è **An√°lise de Imagens:** Fa√ßa upload de imagens ou cole da √°rea de transfer√™ncia para obter an√°lises detalhadas e informa√ß√µes relevantes.
*   ‚ú® **Gera√ß√£o de Imagens:** Crie imagens impressionantes a partir de prompts de texto, liberando sua criatividade.
*   üìö **Aprendizado Cont√≠nuo:** A aplica√ß√£o est√° sempre aprendendo e evoluindo, com atualiza√ß√µes regulares e novos recursos.

**Como Funciona:**

[D√™ um breve resumo de como a aplica√ß√£o funciona. Por exemplo:]

1.  Acesse a interface intuitiva via Streamlit.
2.  Comece a conversar com a IA, fa√ßa upload de uma imagem ou insira um prompt para gerar uma imagem.
3.  Receba respostas r√°pidas, an√°lises detalhadas e imagens criativas em segundos.

**Por Tr√°s da Aplica√ß√£o:**

Constru√≠ [Nome da Aplica√ß√£o] usando as seguintes tecnologias:

*   **Streamlit:** Para uma interface de usu√°rio interativa e amig√°vel.
*   **Gemini (Google AI):** Para capacidades avan√ßadas de processamento de linguagem natural e gera√ß√£o de imagens.
*   **Python:** A linguagem de programa√ß√£o por tr√°s de toda a m√°gica.

**Por Que Eu Criei Isso?**

[Compartilhe sua motiva√ß√£o por tr√°s da cria√ß√£o da aplica√ß√£o. Por exemplo:]

Eu sempre fui fascinado pelo poder da IA para resolver problemas e melhorar a vida das pessoas. Com [Nome da Aplica√ß√£o], eu queria criar uma ferramenta acess√≠vel e f√°cil de usar que pudesse ajudar [p√∫blico-alvo] a [Benef√≠cios Principais].

**Convite √† A√ß√£o:**

Gostaria muito de convidar voc√™ a experimentar [Nome da Aplica√ß√£o] e compartilhar seus coment√°rios! Se voc√™ √© [p√∫blico-alvo], acredito que esta ferramenta pode te ajudar a [Benef√≠cios Principais].

‚û°Ô∏è [Link para o reposit√≥rio no GitHub ou para a aplica√ß√£o online (se dispon√≠vel)]

**Pe√ßo a sua ajuda!**

Se voc√™ gostou deste projeto, por favor, curta, compartilhe e comente abaixo. Sua opini√£o √© muito importante para mim!

#IA #InteligenciaArtificial #Streamlit #Python #Gemini #Inovacao #Desenvolvimento #OpenSource #Compartilhe

---

**Dicas Adicionais:**

*   **Adapte o texto:** Personalize o texto com detalhes espec√≠ficos sobre sua aplica√ß√£o, seu p√∫blico-alvo e sua motiva√ß√£o.
*   **Use emojis:** Emojis podem tornar seu artigo mais atraente e expressivo.
*   **Seja conciso:** Mantenha o texto curto e direto ao ponto.
*   **Interaja com os coment√°rios:** Responda aos coment√°rios e perguntas para criar um di√°logo com a comunidade.
*   **Compartilhe em grupos relevantes:** Publique o artigo em grupos do LinkedIn relacionados a IA, desenvolvimento de software e sua √°rea de atua√ß√£o.

Espero que este rascunho te ajude a criar um artigo de sucesso para o LinkedIn! Se precisar de mais alguma ajuda, √© s√≥ me avisar.
User: voc√™ consegue detalhar o que essa aplica√ß√£o faz ?
Assistant: Assistant: Com certeza! Vamos detalhar o que essa aplica√ß√£o faz, explorando seus componentes e funcionalidades principais:

**Prop√≥sito Geral:**

A aplica√ß√£o √© um assistente inteligente e vers√°til, constru√≠do para interagir com usu√°rios de diversas maneiras, combinando capacidades de chat, an√°lise de imagens e gera√ß√£o de imagens. O objetivo √© fornecer uma experi√™ncia rica e integrada, onde o usu√°rio pode obter informa√ß√µes, insights e criar conte√∫do visual de forma intuitiva.

**Funcionalidades Detalhadas:**

1.  **Chat Interativo com IA:**
    *   **Motor:** Utiliza o modelo Gemini (do Google AI) para processamento de linguagem natural.
    *   **Funcionalidade:** Permite que os usu√°rios conversem livremente com a IA sobre uma variedade de t√≥picos.
    *   **Contexto:**
        *   Carrega um prompt base a partir de um arquivo externo (`PROMPT_CHAT_FILE`) para definir a personalidade e o comportamento da IA.
        *   Adiciona o c√≥digo-fonte do projeto como contexto para que a IA possa responder perguntas sobre a estrutura, a l√≥gica e o funcionamento interno da aplica√ß√£o.
        *   Mant√©m um hist√≥rico das conversas para que a IA possa entender o contexto das perguntas e fornecer respostas mais relevantes.
    *   **Casos de Uso:**
        *   Obter informa√ß√µes sobre diversos assuntos.
        *   Pedir ajuda para resolver problemas de programa√ß√£o.
        *   Explorar ideias criativas.
        *   Aprender sobre o funcionamento da pr√≥pria aplica√ß√£o.

2.  **An√°lise de Imagens:**
    *   **Fontes de Imagem:**
        *   Upload de arquivos de imagem (PNG, JPG, JPEG).
        *   Colagem de imagens da √°rea de transfer√™ncia.
    *   **Funcionalidade:** Permite que os usu√°rios enviem imagens para que a IA as analise e forne√ßa informa√ß√µes sobre o conte√∫do da imagem.
    *   **Processamento:**
        *   As imagens s√£o salvas temporariamente no disco para serem processadas pelo modelo Gemini.
        *   O prompt base √© combinado com a imagem para instruir a IA sobre o tipo de an√°lise a ser realizada.
        *   O hist√≥rico de conversas tamb√©m √© inclu√≠do no prompt para fornecer contexto adicional.
    *   **Casos de Uso:**
        *   Obter descri√ß√µes de imagens.
        *   Identificar objetos em imagens.
        *   Analisar gr√°ficos e diagramas.
        *   Pedir opini√µes sobre fotos.

3.  **Gera√ß√£o de Imagens:**
    *   **Motor:** Utiliza o modelo Gemini para gera√ß√£o de imagens.
    *   **Funcionalidade:** Permite que os usu√°rios criem imagens a partir de prompts de texto.
    *   **Processamento:**
        *   O usu√°rio fornece um prompt descrevendo a imagem desejada.
        *   O modelo Gemini gera uma imagem com base no prompt.
        *   A imagem gerada √© exibida ao usu√°rio e pode ser salva.
    *   **Casos de Uso:**
        *   Criar ilustra√ß√µes para apresenta√ß√µes.
        *   Gerar imagens para redes sociais.
        *   Visualizar conceitos abstratos.
        *   Experimentar com diferentes estilos art√≠sticos.

4.  **Interface do Usu√°rio (Streamlit):**
    *   **Layout:**
        *   Barra lateral para configura√ß√µes, upload de imagens e gera√ß√£o de imagens.
        *   √Årea principal para exibi√ß√£o do hist√≥rico de conversas.
    *   **Componentes:**
        *   Chat input para entrada de texto.
        *   File uploader para upload de imagens.
        *   Bot√µes para colar da √°rea de transfer√™ncia e limpar imagens.
        *   Bot√£o para limpar o hist√≥rico de conversas.
        *   Display do hist√≥rico de conversas com mensagens do usu√°rio e da IA.
    *   **Estado:** Utiliza `st.session_state` para manter o estado da aplica√ß√£o entre as intera√ß√µes do usu√°rio.

5.  **Gerenciamento de Recursos:**
    *   **Rate Limiting:** Implementa um sistema de limita√ß√£o de taxa de requisi√ß√µes para evitar sobrecarregar a API do Gemini e garantir um uso justo dos recursos.
    *   **Logs:** Utiliza um sistema de logging para registrar eventos importantes, erros e informa√ß√µes de depura√ß√£o.
    *   **Configura√ß√£o:** Carrega configura√ß√µes a partir de arquivos externos para facilitar a personaliza√ß√£o e a manuten√ß√£o da aplica√ß√£o.

6.  **Persist√™ncia:**
    *   **Hist√≥rico de An√°lises:** A aplica√ß√£o mant√©m um hist√≥rico das an√°lises de imagens, incluindo o nome da imagem e o resumo gerado pela IA.
    *   **Exporta√ß√£o:** Os resumos das an√°lises podem ser exportados para arquivos DOCX (Word) e Markdown.

**Fluxo de Dados:**

1.  O usu√°rio interage com a interface do Streamlit.
2.  A entrada do usu√°rio (texto ou imagem) √© enviada para a IA (Gemini).
3.  A IA processa a entrada e gera uma resposta.
4.  A resposta da IA √© exibida ao usu√°rio na interface do Streamlit.
5.  O hist√≥rico de conversas √© atualizado.
6.  As an√°lises de imagens s√£o adicionadas ao hist√≥rico e podem ser exportadas.

**Em resumo, a aplica√ß√£o √© um assistente inteligente multifuncional que combina chat, an√°lise de imagens e gera√ß√£o de imagens para fornecer uma experi√™ncia de usu√°rio rica e vers√°til.** Ela pode ser usada para uma variedade de prop√≥sitos, desde obter informa√ß√µes e insights at√© criar conte√∫do visual.

User:  
Assistant: Com base na sua solicita√ß√£o para uma revis√£o e melhoria do c√≥digo, aqui est√£o algumas sugest√µes e √°reas a serem consideradas:

**Arquitetura e Design:**

*   **Separa√ß√£o de Responsabilidades:** O arquivo `chat_streamlit.py` est√° sobrecarregado com l√≥gica de UI, gerenciamento de estado, e intera√ß√£o com a IA. Considere mover a l√≥gica de gerenciamento de estado para uma classe separada ou usar um padr√£o como o Model-View-Presenter (MVP) para separar a apresenta√ß√£o da l√≥gica.
*   **Abstra√ß√£o da Interface do Usu√°rio:** Isolar a l√≥gica espec√≠fica do Streamlit em m√≥dulos separados pode facilitar a substitui√ß√£o da interface do usu√°rio por outra tecnologia no futuro.
*   **Centraliza√ß√£o da Configura√ß√£o:** Agrupar todas as configura√ß√µes em `config.py`, incluindo chaves de API e limites de taxa. Isso facilita o gerenciamento e a manuten√ß√£o.

**Legibilidade e Manutenibilidade:**

*   **Coment√°rios e Documenta√ß√£o:** Adicionar mais coment√°rios para explicar partes complexas do c√≥digo, especialmente em `chat_streamlit.py`.
*   **Nomes Descritivos:** Certificar-se de que todas as vari√°veis e fun√ß√µes tenham nomes claros e descritivos.
*   **Redu√ß√£o da Complexidade:**
    *   Quebrar fun√ß√µes grandes como `process_message` em fun√ß√µes menores e mais focadas.
    *   Simplificar as condicionais e loops sempre que poss√≠vel.
*   **Strings M√°gicas:** Substituir strings literais por constantes nomeadas para facilitar a altera√ß√£o e evitar erros de digita√ß√£o.

**Performance:**

*   **Otimiza√ß√£o de Imagens:** Considerar a otimiza√ß√£o de imagens antes de envi√°-las para a IA (redimensionamento, compress√£o) para reduzir o tempo de processamento e o uso da largura de banda.
*   **Processamento Ass√≠ncrono:** Se poss√≠vel, usar threads ou processos para executar tarefas demoradas (como a gera√ß√£o de imagens) em segundo plano, sem bloquear a interface do usu√°rio. (CUIDADO COM O RATE LIMIT)

**Seguran√ßa:**

*   **Valida√ß√£o de Dados:** Validar todos os dados de entrada do usu√°rio para evitar ataques de inje√ß√£o e outros problemas de seguran√ßa.
*   **Gerenciamento de Chaves de API:** Certificar-se de que as chaves de API sejam armazenadas com seguran√ßa e n√£o sejam expostas no c√≥digo ou em logs. (J√° est√° feito pelo .env)

**Tratamento de Erros:**

*   **Tipos de Exce√ß√£o Espec√≠ficos:** Evitar o uso de `except Exception as e` gen√©rico. Capturar tipos de exce√ß√£o espec√≠ficos e trat√°-los adequadamente.
*   **Mensagens de Erro √öteis:** Fornecer mensagens de erro claras e √∫teis para o usu√°rio, com informa√ß√µes sobre como resolver o problema.
*   **Logging de Erros:** Registrar todos os erros em um arquivo de log para facilitar a depura√ß√£o.

**D√≠vidas T√©cnicas:**

*   **`st.rerun()`:** O uso de `st.rerun()` pode causar comportamentos inesperados e deve ser evitado sempre que poss√≠vel. Considere usar outras formas de atualizar a interface do Streamlit, como callbacks ou reatividade.
*   **Duplica√ß√£o de C√≥digo:** Identificar e refatorar trechos de c√≥digo duplicados.
*   **Falta de Testes:** Adicionar testes unit√°rios e de integra√ß√£o para garantir a qualidade do c√≥digo e evitar regress√µes.

**Sugest√µes Espec√≠ficas:**

*   **`chat_streamlit.py`:**
    *   Considere usar um padr√£o de mensagens para o hist√≥rico do chat (por exemplo, uma classe `Message` com atributos como `role`, `content`, `image`).
    *   A l√≥gica para resetar o uploader e limpar as imagens pode ser simplificada.
*   **`core/RateLimiter.py`:**
    *   Adicione logs para registrar quando as requisi√ß√µes s√£o limitadas e por quanto tempo.
*   **`services/ImageProcessor.py`:**
    *   A l√≥gica para carregar e salvar o hist√≥rico pode ser simplificada.

**Padroniza√ß√£o:**

*   **Formata√ß√£o:** Usar um formatador de c√≥digo como Black ou autopep8 para manter a consist√™ncia na formata√ß√£o do c√≥digo.
*   **Linters:** Usar um linter como pylint ou flake8 para identificar poss√≠veis problemas no c√≥digo.

**Recursos Adicionais:**

*   **Documenta√ß√£o do Streamlit:** [https://docs.streamlit.io/](https://docs.streamlit.io/)
*   **Guia de Estilo do Python (PEP 8):** [https://peps.python.org/pep-0008/](https://peps.python.org/pep-0008/)
*   **Ferramentas de Linter e Formata√ß√£o:** Black, pylint, flake8

Lembre-se de que estas s√£o apenas sugest√µes. A melhor forma de melhorar o c√≥digo depende das suas necessidades espec√≠ficas e dos seus objetivos para o projeto.

User: Existe alguma aplica√ß√£o que tem um chat que recebe como contexto todo o c√≥digo para que possa analisar em tempo real e propor melhorias, debug e solu√ß√µes ?
Assistant: Assistant: Sim, existem algumas aplica√ß√µes e IDEs (Integrated Development Environments) que oferecem funcionalidades de chat com contexto do c√≥digo para an√°lise, debug e sugest√µes de melhorias em tempo real. Aqui est√£o algumas op√ß√µes:

**1. IDEs com Integra√ß√£o de IA:**

*   **GitHub Copilot:** √â um assistente de IA desenvolvido pela GitHub e OpenAI, integrado em diversos IDEs como VS Code, Neovim, JetBrains, etc. Ele oferece sugest√µes de c√≥digo em tempo real, autocompletar, explica√ß√µes de c√≥digo e pode at√© mesmo gerar trechos de c√≥digo completos com base em coment√°rios ou contexto existente. O Copilot tem acesso ao seu c√≥digo no editor, o que lhe permite fornecer sugest√µes e an√°lises contextuais.
*   **Amazon CodeWhisperer:** Similar ao GitHub Copilot, o CodeWhisperer √© um assistente de IA da Amazon, tamb√©m integrado em IDEs. Ele oferece sugest√µes de c√≥digo em tempo real, detec√ß√£o de vulnerabilidades e pode gerar c√≥digo com base em coment√°rios. Ele tamb√©m tem acesso ao contexto do c√≥digo que voc√™ est√° escrevendo.
*   **Tabnine:** Outra op√ß√£o popular, Tabnine, oferece sugest√µes de c√≥digo baseadas em IA e suporta uma variedade de linguagens de programa√ß√£o e IDEs. Ele tamb√©m aprende com seu c√≥digo para fornecer sugest√µes mais personalizadas.

**2. Ferramentas de An√°lise de C√≥digo com Chat:**

*   **DeepSource:** √â uma plataforma de an√°lise est√°tica de c√≥digo que tamb√©m oferece recursos de chat. Ele identifica bugs, vulnerabilidades e problemas de desempenho no seu c√≥digo, e permite que voc√™ discuta e resolva esses problemas diretamente na plataforma.
*   **SonarQube:** √â uma plataforma de an√°lise de c√≥digo open-source que ajuda a detectar bugs, vulnerabilidades e problemas de qualidade de c√≥digo. Embora n√£o tenha um chat integrado, ele permite que voc√™ comente e discuta os problemas encontrados diretamente no c√≥digo.

**3. Extens√µes de IDE com Integra√ß√£o de Chat:**

*   **Codeium:** √â uma extens√£o para VS Code e outras IDEs que utiliza IA para fornecer sugest√µes de c√≥digo, gerar c√≥digo e oferecer um chat interativo. Voc√™ pode selecionar um trecho de c√≥digo e perguntar ao Codeium sobre ele, e ele usar√° o contexto do c√≥digo para fornecer respostas relevantes.

**Como essas ferramentas funcionam:**

1.  **An√°lise do C√≥digo:** A ferramenta analisa o c√≥digo no seu editor ou reposit√≥rio para entender a estrutura, a l√≥gica e as depend√™ncias.
2.  **Contextualiza√ß√£o:** Ela usa o contexto do c√≥digo, como o nome da fun√ß√£o, os par√¢metros, os coment√°rios e o c√≥digo ao redor, para fornecer sugest√µes e an√°lises relevantes.
3.  **Chat Interativo:** Voc√™ pode fazer perguntas sobre o c√≥digo, pedir explica√ß√µes, pedir sugest√µes de melhorias ou pedir ajuda para depurar problemas.
4.  **Sugest√µes em Tempo Real:** A ferramenta oferece sugest√µes de c√≥digo, autocompletar, detec√ß√£o de erros e outras funcionalidades em tempo real, enquanto voc√™ est√° escrevendo o c√≥digo.

**Observa√ß√µes:**

*   A maioria dessas ferramentas requer uma assinatura paga para acessar todos os recursos.
*   A qualidade das sugest√µes e an√°lises depende da qualidade do modelo de IA e da quantidade de dados com que foi treinado.
*   √â importante revisar as sugest√µes da IA com cuidado, pois nem sempre s√£o perfeitas e podem conter erros.

Espero que esta lista te ajude a encontrar a ferramenta certa para suas necessidades!
User: n√£o quero uma ferramenta estou desenvolvendo a minha com python e streamlit como interface. recebe o c√≥digo e atrav√©s de um chat gera c√≥digo e respostas bem embasadas pois recebe a cada requisi√ß√£o o c√≥digo atual da aplica√ß√£o.
2025-04-08 02:15:21,898 [INFO] Resposta da IA (texto): prompt = f"""
Compreendi, voc√™ est√° desenvolvendo sua pr√≥pria ferramenta de an√°lise de c√≥digo com chat usando Python e Streamlit. A ideia √© que a cada requisi√ß√£o, o chat receba o c√≥digo atualizado da aplica√ß√£o para gerar respostas e c√≥digo embasados no contexto.

Aqui est√£o algumas sugest√µes para aprimorar a sua aplica√ß√£o, focando em como o chat interage com o c√≥digo:

**1. An√°lise Cont√≠nua do C√≥digo:**

*   **Parseamento do C√≥digo:** Utilize a biblioteca `ast` (Abstract Syntax Trees) do Python para parsear o c√≥digo da aplica√ß√£o. Isso permite que voc√™ analise a estrutura, as fun√ß√µes, as classes e as depend√™ncias do c√≥digo de forma program√°tica.
*   **Extra√ß√£o de Informa√ß√µes:** Extraia informa√ß√µes relevantes do c√≥digo, como:
    *   Nomes de fun√ß√µes e classes.
    *   Coment√°rios.
    *   Strings de documenta√ß√£o (docstrings).
    *   Assinaturas de fun√ß√µes (par√¢metros, tipos de retorno).
    *   Depend√™ncias entre m√≥dulos.
*   **Indexa√ß√£o:** Crie um √≠ndice do c√≥digo para facilitar a busca e o acesso r√°pido √†s informa√ß√µes.

**2. Integra√ß√£o com o Chat:**

*   **Contexto Rico:** A cada requisi√ß√£o do chat, forne√ßa um contexto rico para a IA, incluindo:
    *   O c√≥digo da aplica√ß√£o.
    *   Informa√ß√µes extra√≠das do c√≥digo (usando `ast`).
    *   O hist√≥rico da conversa.
    *   Informa√ß√µes sobre o estado atual da aplica√ß√£o (se aplic√°vel).
*   **Prompt Engineering:** Crie prompts claros e espec√≠ficos para a IA, instruindo-a sobre o tipo de an√°lise ou sugest√£o que voc√™ deseja. Por exemplo:
    *   "Analise esta fun√ß√£o e sugira melhorias na legibilidade."
    *   "Encontre poss√≠veis erros de digita√ß√£o neste c√≥digo."
    *   "Gere testes unit√°rios para esta classe."
*   **Formata√ß√£o da Resposta:** Formate a resposta da IA de forma clara e f√°cil de entender, usando Markdown ou HTML. Inclua trechos de c√≥digo formatados, links para as partes relevantes do c√≥digo e explica√ß√µes detalhadas.

**3. Gera√ß√£o de C√≥digo:**

*   **Edi√ß√£o Program√°tica:** Utilize a biblioteca `ast` para modificar o c√≥digo da aplica√ß√£o de forma program√°tica. Isso permite que a IA sugira altera√ß√µes diretamente no c√≥digo, como:
    *   Adicionar coment√°rios.
    *   Renomear vari√°veis.
    *   Refatorar fun√ß√µes.
    *   Corrigir erros de digita√ß√£o.
*   **Visualiza√ß√£o das Altera√ß√µes:** Exiba as altera√ß√µes sugeridas de forma clara para o usu√°rio, usando um diff (compara√ß√£o de c√≥digo) ou um editor de c√≥digo integrado no Streamlit.
*   **Aplica√ß√£o das Altera√ß√µes:** Permita que o usu√°rio revise e aplique as altera√ß√µes sugeridas pela IA com um clique.

**4. Melhorias Cont√≠nuas:**

*   **Feedback do Usu√°rio:** Colete feedback do usu√°rio sobre a qualidade das sugest√µes e da gera√ß√£o de c√≥digo da IA. Use esse feedback para treinar e melhorar o modelo de IA.
*   **Testes Automatizados:** Crie testes automatizados para garantir que as sugest√µes da IA n√£o introduzam erros no c√≥digo.
*   **Aprendizado Cont√≠nuo:** Mantenha-se atualizado sobre as √∫ltimas novidades em IA e aprendizado de m√°quina para incorporar novas t√©cnicas e modelos na sua aplica√ß√£o.

**Implementa√ß√£o:**

Aqui est√° um esbo√ßo de como voc√™ pode implementar essas sugest√µes:

```python
import streamlit as st
import ast
from core.handlers.gemini_handler import GeminiHandler
from services.search_files import ler_todos_arquivos_python

# Inicializa√ß√£o do GeminiHandler
gemini_handler = GeminiHandler("gemini-2.0-flash-exp")

def analisar_codigo(codigo):
    """Analisa o c√≥digo usando ast e retorna informa√ß√µes relevantes."""
    try:
        tree = ast.parse(codigo)
        funcoes = [node.name for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
        classes = [node.name for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]
        return {"funcoes": funcoes, "classes": classes}
    except SyntaxError as e:
        return {"erro": str(e)}

def gerar_prompt(codigo, informacoes_codigo, pergunta):
    """Gera o prompt para a IA com o c√≥digo e informa√ß√µes relevantes."""
    prompt = f"""Voc√™ √© um assistente de programa√ß√£o. Analise o c√≥digo abaixo e responda √† pergunta.

    C√≥digo:
    ```python
    {codigo}
    ```

    Informa√ß√µes sobre o c√≥digo:
    {informacoes_codigo}

    Pergunta:
    {pergunta}
    """
    return prompt

# Interface Streamlit
st.title("An√°lise de C√≥digo com IA")

codigo = ler_todos_arquivos_python()  # Fun√ß√£o para ler todo o c√≥digo do projeto
pergunta = st.text_input("Digite sua pergunta sobre o c√≥digo:")

if codigo and pergunta:
    informacoes_codigo = analisar_codigo(codigo)
    prompt = gerar_prompt(codigo, informacoes_codigo, pergunta)
    resposta = gemini_handler.generate_content(None, prompt)  # Envia o prompt para o Gemini

    st.write("Resposta da IA:")
    st.write(resposta)
```

Lembre-se de que este √© apenas um ponto de partida. Adapte as sugest√µes √†s suas necessidades espec√≠ficas e experimente diferentes abordagens para obter os melhores resultados.

Ao implementar essas sugest√µes, voc√™ estar√° no caminho certo para criar uma ferramenta poderosa e vers√°til para an√°lise de c√≥digo com chat.
"""

2025-04-08 02:15:22,021 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 02:15:22,023 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 02:15:22,025 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 02:15:22,027 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 02:15:22,030 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 02:15:22,033 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 02:15:22,036 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 02:15:22,038 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 02:15:22,040 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 02:15:22,041 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 02:15:22,043 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 02:15:22,044 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 02:15:22,045 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 02:15:22,047 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 02:15:22,048 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 02:19:46,401 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 02:19:46,404 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 02:19:46,406 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 02:19:46,408 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 02:19:46,411 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 02:19:46,413 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 02:19:46,415 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 02:19:46,418 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 02:19:46,419 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 02:19:46,421 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 02:19:46,423 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 02:19:46,426 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 02:19:46,427 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 02:19:46,429 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 02:19:46,431 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 02:19:46,532 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 02:19:46,533 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 02:19:46,535 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 02:19:46,536 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 02:19:46,538 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 02:19:46,539 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 02:19:46,541 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 02:19:46,542 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 02:19:46,543 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 02:19:46,545 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 02:19:46,548 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 02:19:46,550 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 02:19:46,552 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 02:19:46,554 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 02:19:46,555 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 02:19:46,558 [INFO] Enviando para IA - Prompt (sem imagem): prompt = f"""
Voc√™ √© um assistente inteligente especializado em revis√£o e melhoria de c√≥digo.

Seu objetivo √©:
- Analisar o c√≥digo-fonte contido no projeto enviado
- Identificar melhorias em legibilidade, performance e seguran√ßa
- Sugerir pr√°ticas recomendadas de design e arquitetura
- Indicar partes com d√≠vidas t√©cnicas ou complexidade desnecess√°ria

Informa√ß√µes adicionais:
- Estrutura do projeto: segue o padr√£o de pastas iniciando por src/

"""

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
import base64
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.generativeai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: gere uma documenta√ß√£o em markdown do meu c√≥digo atual, para um README do github.
Assistant: Com certeza! Ap√≥s analisar o c√≥digo, aqui est√£o algumas sugest√µes para documentar seu projeto em um README do GitHub, juntamente com sugest√µes de melhorias no c√≥digo:

**Estrutura do README (sugest√£o):**

```markdown
# Nome do Projeto (Ex: Chat IA Inteligente)

Uma breve descri√ß√£o do que o projeto faz.

## Funcionalidades Principais

*   Chat interativo com IA (Gemini).
*   An√°lise de imagens (upload ou √°rea de transfer√™ncia).
*   Gera√ß√£o de imagens a partir de prompts.
*   Hist√≥rico de conversas.
*   Limita√ß√£o de taxa de requisi√ß√µes (Rate Limiting).

## Como Usar

Instru√ß√µes detalhadas sobre como executar o projeto (incluindo depend√™ncias e vari√°veis de ambiente).

Exemplo:

1.  Clone o reposit√≥rio.
2.  Crie um ambiente virtual: `python -m venv venv`
3.  Ative o ambiente virtual: `source venv/bin/activate` (Linux/macOS) ou `venv\Scripts\activate` (Windows)
4.  Instale as depend√™ncias: `pip install -r requirements.txt`
5.  Configure a vari√°vel de ambiente `API_KEY_GEMINI` no arquivo `.env`.
6.  Execute o aplicativo: `streamlit run src/chat_streamlit.py`

## Arquitetura do Projeto

*   `src/`: Cont√©m o c√≥digo fonte principal.
    *   `chat_streamlit.py`: Interface Streamlit para o chat.
    *   `config/`: Configura√ß√µes do projeto.
    *   `core/`: L√≥gica principal (handlers, logger, rate limiting).
    *   `services/`: Servi√ßos (IA, documentos, markdown, busca de arquivos).
    *   `utils/`: Utilit√°rios.
    *   `main.py`: Ponto de entrada para processamento de imagens em lote.

## Diagrama de Classes (Opcional)

(Adicione um diagrama UML aqui, se poss√≠vel, para ilustrar as rela√ß√µes entre as classes.)

## Contribui√ß√µes

Como contribuir para o projeto (pull requests, etc.).

## Licen√ßa

(Ex: MIT License)

## Autores

[Seu Nome]
```

**Documenta√ß√£o Adicional:**

*   **`chat_streamlit.py`:**
    *   Explicar o uso do `st.session_state` para manter o estado da aplica√ß√£o.
    *   Detalhar como as fun√ß√µes `process_message`, `check_clipboard`, `generate_image` funcionam.
    *   Mencionar como o prompt base √© carregado do arquivo `PROMPT_CHAT_FILE` e como o contexto do c√≥digo √© adicionado.
*   **`config/config.py`:**
    *   Explicar como os caminhos s√£o configurados e a import√¢ncia de `ensure_directories()`.
    *   Documentar a configura√ß√£o do Rate Limiting.
*   **`core/`:**
    *   `GeminiHandler`: Descrever como ele encapsula a intera√ß√£o com o modelo Gemini.
    *   `RateLimiter`: Explicar como a classe limita as requisi√ß√µes para evitar exceder as quotas da API.
*   **`services/`:**
    *   `ImageProcessor`: Detalhar como as imagens s√£o processadas, resumidas e como o hist√≥rico √© mantido.
    *   `DocumentService` e `MarkdownService`: Explicar como os resumos s√£o formatados e salvos em diferentes formatos.

**Sugest√µes de Melhorias no C√≥digo:**

*   **`chat_streamlit.py`:**
    *   **Modulariza√ß√£o:** Quebrar a fun√ß√£o `process_message` em fun√ß√µes menores e mais focadas.
    *   **Gerenciamento de Erros:** Implementar tratamento de erros mais robusto, com mensagens de erro mais claras para o usu√°rio.
    *   **Coment√°rios:** Adicionar mais coment√°rios para explicar a l√≥gica de partes complexas do c√≥digo.
*   **`core/RateLimiter.py`:**
    *   **Logging:** Adicionar logs para registrar quando as requisi√ß√µes s√£o limitadas e por quanto tempo.
*   **`services/ImageProcessor.py`:**
    *   **Paraleliza√ß√£o:** Considerar o uso de threads ou processos para processar m√∫ltiplas imagens simultaneamente (respeitando o Rate Limiting).
    *   **Abstra√ß√£o:** Criar classes para representar prompts e respostas da IA, para facilitar a manuten√ß√£o e testes.
*   **`utils/file_utils.py`:**
    *   **Tratamento de Exce√ß√µes:** Adicionar tratamento de exce√ß√µes ao listar imagens, caso o diret√≥rio n√£o exista ou ocorra um erro de permiss√£o.
*   **Configura√ß√£o e Vari√°veis de Ambiente:**
    *   **Valida√ß√£o:** Adicionar valida√ß√£o para garantir que as vari√°veis de ambiente necess√°rias est√£o configuradas corretamente.
*   **Padroniza√ß√£o:**
    *   **Formata√ß√£o:** Usar um formatador de c√≥digo como Black ou autopep8 para manter a consist√™ncia na formata√ß√£o do c√≥digo.
    *   **Linters:** Usar um linter como pylint ou flake8 para identificar poss√≠veis problemas no c√≥digo.

**D√≠vidas T√©cnicas:**

*   **Tratamento de Erros Gen√©rico:** Muitos blocos `except Exception as e` capturam todos os tipos de erros, o que pode dificultar a identifica√ß√£o de problemas espec√≠ficos. √â recomend√°vel capturar tipos de erros mais espec√≠ficos e trat√°-los adequadamente.
*   **C√≥digo Duplicado:** Identificar e refatorar trechos de c√≥digo duplicados, como o carregamento do hist√≥rico em `ImageProcessor`.
*   **Testes:** Adicionar testes unit√°rios e de integra√ß√£o para garantir a qualidade do c√≥digo e evitar regress√µes.

**Observa√ß√µes:**

*   O uso de `st.rerun()` pode ser problem√°tico em algumas situa√ß√µes. Considere outras formas de atualizar a interface do Streamlit, como usar callbacks ou reatividade.
*   A l√≥gica de limpar as imagens da √°rea de transfer√™ncia e do uploader pode ser simplificada usando uma √∫nica fun√ß√£o.
*   A classe `RateLimiter` parece estar funcionando corretamente, mas √© importante monitorar seu desempenho em um ambiente de produ√ß√£o para garantir que ela est√° limitando as requisi√ß√µes conforme o esperado.

Espero que isto ajude a criar uma documenta√ß√£o abrangente e a melhorar a qualidade do seu c√≥digo!

User: gere um artigo com a minha aplica√ß√£o para apresentar no linkedin.
Assistant: Assistant: Com certeza! Aqui est√° um rascunho de artigo para o LinkedIn, destacando sua aplica√ß√£o e convidando a comunidade a explorar:

**T√≠tulo:** Apresentando [Nome da Aplica√ß√£o]: Sua Nova Ferramenta Inteligente para [Prop√≥sito Principal]! üöÄ

**Imagem:** (Anexe uma imagem ou GIF atraente da sua aplica√ß√£o em a√ß√£o. Pode ser uma captura de tela do chat, um exemplo de imagem gerada ou uma representa√ß√£o visual da arquitetura.)

**Texto:**

---

Ol√°, comunidade do LinkedIn! üëã

Estou muito animado para compartilhar com voc√™s um projeto que tenho trabalhado recentemente: [Nome da Aplica√ß√£o]!

[Nome da Aplica√ß√£o] √© uma aplica√ß√£o inovadora constru√≠da com Streamlit e alimentada pela intelig√™ncia artificial do Gemini, que visa facilitar [Prop√≥sito Principal da Aplica√ß√£o]. Seja voc√™ [p√∫blico-alvo], esta ferramenta pode te ajudar a [Benef√≠cios Principais].

**Funcionalidades em Destaque:**

*   üí¨ **Chat Inteligente:** Converse com um assistente de IA vers√°til para obter respostas, insights e suporte.
*   üñºÔ∏è **An√°lise de Imagens:** Fa√ßa upload de imagens ou cole da √°rea de transfer√™ncia para obter an√°lises detalhadas e informa√ß√µes relevantes.
*   ‚ú® **Gera√ß√£o de Imagens:** Crie imagens impressionantes a partir de prompts de texto, liberando sua criatividade.
*   üìö **Aprendizado Cont√≠nuo:** A aplica√ß√£o est√° sempre aprendendo e evoluindo, com atualiza√ß√µes regulares e novos recursos.

**Como Funciona:**

[D√™ um breve resumo de como a aplica√ß√£o funciona. Por exemplo:]

1.  Acesse a interface intuitiva via Streamlit.
2.  Comece a conversar com a IA, fa√ßa upload de uma imagem ou insira um prompt para gerar uma imagem.
3.  Receba respostas r√°pidas, an√°lises detalhadas e imagens criativas em segundos.

**Por Tr√°s da Aplica√ß√£o:**

Constru√≠ [Nome da Aplica√ß√£o] usando as seguintes tecnologias:

*   **Streamlit:** Para uma interface de usu√°rio interativa e amig√°vel.
*   **Gemini (Google AI):** Para capacidades avan√ßadas de processamento de linguagem natural e gera√ß√£o de imagens.
*   **Python:** A linguagem de programa√ß√£o por tr√°s de toda a m√°gica.

**Por Que Eu Criei Isso?**

[Compartilhe sua motiva√ß√£o por tr√°s da cria√ß√£o da aplica√ß√£o. Por exemplo:]

Eu sempre fui fascinado pelo poder da IA para resolver problemas e melhorar a vida das pessoas. Com [Nome da Aplica√ß√£o], eu queria criar uma ferramenta acess√≠vel e f√°cil de usar que pudesse ajudar [p√∫blico-alvo] a [Benef√≠cios Principais].

**Convite √† A√ß√£o:**

Gostaria muito de convidar voc√™ a experimentar [Nome da Aplica√ß√£o] e compartilhar seus coment√°rios! Se voc√™ √© [p√∫blico-alvo], acredito que esta ferramenta pode te ajudar a [Benef√≠cios Principais].

‚û°Ô∏è [Link para o reposit√≥rio no GitHub ou para a aplica√ß√£o online (se dispon√≠vel)]

**Pe√ßo a sua ajuda!**

Se voc√™ gostou deste projeto, por favor, curta, compartilhe e comente abaixo. Sua opini√£o √© muito importante para mim!

#IA #InteligenciaArtificial #Streamlit #Python #Gemini #Inovacao #Desenvolvimento #OpenSource #Compartilhe

---

**Dicas Adicionais:**

*   **Adapte o texto:** Personalize o texto com detalhes espec√≠ficos sobre sua aplica√ß√£o, seu p√∫blico-alvo e sua motiva√ß√£o.
*   **Use emojis:** Emojis podem tornar seu artigo mais atraente e expressivo.
*   **Seja conciso:** Mantenha o texto curto e direto ao ponto.
*   **Interaja com os coment√°rios:** Responda aos coment√°rios e perguntas para criar um di√°logo com a comunidade.
*   **Compartilhe em grupos relevantes:** Publique o artigo em grupos do LinkedIn relacionados a IA, desenvolvimento de software e sua √°rea de atua√ß√£o.

Espero que este rascunho te ajude a criar um artigo de sucesso para o LinkedIn! Se precisar de mais alguma ajuda, √© s√≥ me avisar.
User: voc√™ consegue detalhar o que essa aplica√ß√£o faz ?
Assistant: Assistant: Com certeza! Vamos detalhar o que essa aplica√ß√£o faz, explorando seus componentes e funcionalidades principais:

**Prop√≥sito Geral:**

A aplica√ß√£o √© um assistente inteligente e vers√°til, constru√≠do para interagir com usu√°rios de diversas maneiras, combinando capacidades de chat, an√°lise de imagens e gera√ß√£o de imagens. O objetivo √© fornecer uma experi√™ncia rica e integrada, onde o usu√°rio pode obter informa√ß√µes, insights e criar conte√∫do visual de forma intuitiva.

**Funcionalidades Detalhadas:**

1.  **Chat Interativo com IA:**
    *   **Motor:** Utiliza o modelo Gemini (do Google AI) para processamento de linguagem natural.
    *   **Funcionalidade:** Permite que os usu√°rios conversem livremente com a IA sobre uma variedade de t√≥picos.
    *   **Contexto:**
        *   Carrega um prompt base a partir de um arquivo externo (`PROMPT_CHAT_FILE`) para definir a personalidade e o comportamento da IA.
        *   Adiciona o c√≥digo-fonte do projeto como contexto para que a IA possa responder perguntas sobre a estrutura, a l√≥gica e o funcionamento interno da aplica√ß√£o.
        *   Mant√©m um hist√≥rico das conversas para que a IA possa entender o contexto das perguntas e fornecer respostas mais relevantes.
    *   **Casos de Uso:**
        *   Obter informa√ß√µes sobre diversos assuntos.
        *   Pedir ajuda para resolver problemas de programa√ß√£o.
        *   Explorar ideias criativas.
        *   Aprender sobre o funcionamento da pr√≥pria aplica√ß√£o.

2.  **An√°lise de Imagens:**
    *   **Fontes de Imagem:**
        *   Upload de arquivos de imagem (PNG, JPG, JPEG).
        *   Colagem de imagens da √°rea de transfer√™ncia.
    *   **Funcionalidade:** Permite que os usu√°rios enviem imagens para que a IA as analise e forne√ßa informa√ß√µes sobre o conte√∫do da imagem.
    *   **Processamento:**
        *   As imagens s√£o salvas temporariamente no disco para serem processadas pelo modelo Gemini.
        *   O prompt base √© combinado com a imagem para instruir a IA sobre o tipo de an√°lise a ser realizada.
        *   O hist√≥rico de conversas tamb√©m √© inclu√≠do no prompt para fornecer contexto adicional.
    *   **Casos de Uso:**
        *   Obter descri√ß√µes de imagens.
        *   Identificar objetos em imagens.
        *   Analisar gr√°ficos e diagramas.
        *   Pedir opini√µes sobre fotos.

3.  **Gera√ß√£o de Imagens:**
    *   **Motor:** Utiliza o modelo Gemini para gera√ß√£o de imagens.
    *   **Funcionalidade:** Permite que os usu√°rios criem imagens a partir de prompts de texto.
    *   **Processamento:**
        *   O usu√°rio fornece um prompt descrevendo a imagem desejada.
        *   O modelo Gemini gera uma imagem com base no prompt.
        *   A imagem gerada √© exibida ao usu√°rio e pode ser salva.
    *   **Casos de Uso:**
        *   Criar ilustra√ß√µes para apresenta√ß√µes.
        *   Gerar imagens para redes sociais.
        *   Visualizar conceitos abstratos.
        *   Experimentar com diferentes estilos art√≠sticos.

4.  **Interface do Usu√°rio (Streamlit):**
    *   **Layout:**
        *   Barra lateral para configura√ß√µes, upload de imagens e gera√ß√£o de imagens.
        *   √Årea principal para exibi√ß√£o do hist√≥rico de conversas.
    *   **Componentes:**
        *   Chat input para entrada de texto.
        *   File uploader para upload de imagens.
        *   Bot√µes para colar da √°rea de transfer√™ncia e limpar imagens.
        *   Bot√£o para limpar o hist√≥rico de conversas.
        *   Display do hist√≥rico de conversas com mensagens do usu√°rio e da IA.
    *   **Estado:** Utiliza `st.session_state` para manter o estado da aplica√ß√£o entre as intera√ß√µes do usu√°rio.

5.  **Gerenciamento de Recursos:**
    *   **Rate Limiting:** Implementa um sistema de limita√ß√£o de taxa de requisi√ß√µes para evitar sobrecarregar a API do Gemini e garantir um uso justo dos recursos.
    *   **Logs:** Utiliza um sistema de logging para registrar eventos importantes, erros e informa√ß√µes de depura√ß√£o.
    *   **Configura√ß√£o:** Carrega configura√ß√µes a partir de arquivos externos para facilitar a personaliza√ß√£o e a manuten√ß√£o da aplica√ß√£o.

6.  **Persist√™ncia:**
    *   **Hist√≥rico de An√°lises:** A aplica√ß√£o mant√©m um hist√≥rico das an√°lises de imagens, incluindo o nome da imagem e o resumo gerado pela IA.
    *   **Exporta√ß√£o:** Os resumos das an√°lises podem ser exportados para arquivos DOCX (Word) e Markdown.

**Fluxo de Dados:**

1.  O usu√°rio interage com a interface do Streamlit.
2.  A entrada do usu√°rio (texto ou imagem) √© enviada para a IA (Gemini).
3.  A IA processa a entrada e gera uma resposta.
4.  A resposta da IA √© exibida ao usu√°rio na interface do Streamlit.
5.  O hist√≥rico de conversas √© atualizado.
6.  As an√°lises de imagens s√£o adicionadas ao hist√≥rico e podem ser exportadas.

**Em resumo, a aplica√ß√£o √© um assistente inteligente multifuncional que combina chat, an√°lise de imagens e gera√ß√£o de imagens para fornecer uma experi√™ncia de usu√°rio rica e vers√°til.** Ela pode ser usada para uma variedade de prop√≥sitos, desde obter informa√ß√µes e insights at√© criar conte√∫do visual.

User:  
Assistant: Com base na sua solicita√ß√£o para uma revis√£o e melhoria do c√≥digo, aqui est√£o algumas sugest√µes e √°reas a serem consideradas:

**Arquitetura e Design:**

*   **Separa√ß√£o de Responsabilidades:** O arquivo `chat_streamlit.py` est√° sobrecarregado com l√≥gica de UI, gerenciamento de estado, e intera√ß√£o com a IA. Considere mover a l√≥gica de gerenciamento de estado para uma classe separada ou usar um padr√£o como o Model-View-Presenter (MVP) para separar a apresenta√ß√£o da l√≥gica.
*   **Abstra√ß√£o da Interface do Usu√°rio:** Isolar a l√≥gica espec√≠fica do Streamlit em m√≥dulos separados pode facilitar a substitui√ß√£o da interface do usu√°rio por outra tecnologia no futuro.
*   **Centraliza√ß√£o da Configura√ß√£o:** Agrupar todas as configura√ß√µes em `config.py`, incluindo chaves de API e limites de taxa. Isso facilita o gerenciamento e a manuten√ß√£o.

**Legibilidade e Manutenibilidade:**

*   **Coment√°rios e Documenta√ß√£o:** Adicionar mais coment√°rios para explicar partes complexas do c√≥digo, especialmente em `chat_streamlit.py`.
*   **Nomes Descritivos:** Certificar-se de que todas as vari√°veis e fun√ß√µes tenham nomes claros e descritivos.
*   **Redu√ß√£o da Complexidade:**
    *   Quebrar fun√ß√µes grandes como `process_message` em fun√ß√µes menores e mais focadas.
    *   Simplificar as condicionais e loops sempre que poss√≠vel.
*   **Strings M√°gicas:** Substituir strings literais por constantes nomeadas para facilitar a altera√ß√£o e evitar erros de digita√ß√£o.

**Performance:**

*   **Otimiza√ß√£o de Imagens:** Considerar a otimiza√ß√£o de imagens antes de envi√°-las para a IA (redimensionamento, compress√£o) para reduzir o tempo de processamento e o uso da largura de banda.
*   **Processamento Ass√≠ncrono:** Se poss√≠vel, usar threads ou processos para executar tarefas demoradas (como a gera√ß√£o de imagens) em segundo plano, sem bloquear a interface do usu√°rio. (CUIDADO COM O RATE LIMIT)

**Seguran√ßa:**

*   **Valida√ß√£o de Dados:** Validar todos os dados de entrada do usu√°rio para evitar ataques de inje√ß√£o e outros problemas de seguran√ßa.
*   **Gerenciamento de Chaves de API:** Certificar-se de que as chaves de API sejam armazenadas com seguran√ßa e n√£o sejam expostas no c√≥digo ou em logs. (J√° est√° feito pelo .env)

**Tratamento de Erros:**

*   **Tipos de Exce√ß√£o Espec√≠ficos:** Evitar o uso de `except Exception as e` gen√©rico. Capturar tipos de exce√ß√£o espec√≠ficos e trat√°-los adequadamente.
*   **Mensagens de Erro √öteis:** Fornecer mensagens de erro claras e √∫teis para o usu√°rio, com informa√ß√µes sobre como resolver o problema.
*   **Logging de Erros:** Registrar todos os erros em um arquivo de log para facilitar a depura√ß√£o.

**D√≠vidas T√©cnicas:**

*   **`st.rerun()`:** O uso de `st.rerun()` pode causar comportamentos inesperados e deve ser evitado sempre que poss√≠vel. Considere usar outras formas de atualizar a interface do Streamlit, como callbacks ou reatividade.
*   **Duplica√ß√£o de C√≥digo:** Identificar e refatorar trechos de c√≥digo duplicados.
*   **Falta de Testes:** Adicionar testes unit√°rios e de integra√ß√£o para garantir a qualidade do c√≥digo e evitar regress√µes.

**Sugest√µes Espec√≠ficas:**

*   **`chat_streamlit.py`:**
    *   Considere usar um padr√£o de mensagens para o hist√≥rico do chat (por exemplo, uma classe `Message` com atributos como `role`, `content`, `image`).
    *   A l√≥gica para resetar o uploader e limpar as imagens pode ser simplificada.
*   **`core/RateLimiter.py`:**
    *   Adicione logs para registrar quando as requisi√ß√µes s√£o limitadas e por quanto tempo.
*   **`services/ImageProcessor.py`:**
    *   A l√≥gica para carregar e salvar o hist√≥rico pode ser simplificada.

**Padroniza√ß√£o:**

*   **Formata√ß√£o:** Usar um formatador de c√≥digo como Black ou autopep8 para manter a consist√™ncia na formata√ß√£o do c√≥digo.
*   **Linters:** Usar um linter como pylint ou flake8 para identificar poss√≠veis problemas no c√≥digo.

**Recursos Adicionais:**

*   **Documenta√ß√£o do Streamlit:** [https://docs.streamlit.io/](https://docs.streamlit.io/)
*   **Guia de Estilo do Python (PEP 8):** [https://peps.python.org/pep-0008/](https://peps.python.org/pep-0008/)
*   **Ferramentas de Linter e Formata√ß√£o:** Black, pylint, flake8

Lembre-se de que estas s√£o apenas sugest√µes. A melhor forma de melhorar o c√≥digo depende das suas necessidades espec√≠ficas e dos seus objetivos para o projeto.

User: Existe alguma aplica√ß√£o que tem um chat que recebe como contexto todo o c√≥digo para que possa analisar em tempo real e propor melhorias, debug e solu√ß√µes ?
Assistant: Assistant: Sim, existem algumas aplica√ß√µes e IDEs (Integrated Development Environments) que oferecem funcionalidades de chat com contexto do c√≥digo para an√°lise, debug e sugest√µes de melhorias em tempo real. Aqui est√£o algumas op√ß√µes:

**1. IDEs com Integra√ß√£o de IA:**

*   **GitHub Copilot:** √â um assistente de IA desenvolvido pela GitHub e OpenAI, integrado em diversos IDEs como VS Code, Neovim, JetBrains, etc. Ele oferece sugest√µes de c√≥digo em tempo real, autocompletar, explica√ß√µes de c√≥digo e pode at√© mesmo gerar trechos de c√≥digo completos com base em coment√°rios ou contexto existente. O Copilot tem acesso ao seu c√≥digo no editor, o que lhe permite fornecer sugest√µes e an√°lises contextuais.
*   **Amazon CodeWhisperer:** Similar ao GitHub Copilot, o CodeWhisperer √© um assistente de IA da Amazon, tamb√©m integrado em IDEs. Ele oferece sugest√µes de c√≥digo em tempo real, detec√ß√£o de vulnerabilidades e pode gerar c√≥digo com base em coment√°rios. Ele tamb√©m tem acesso ao contexto do c√≥digo que voc√™ est√° escrevendo.
*   **Tabnine:** Outra op√ß√£o popular, Tabnine, oferece sugest√µes de c√≥digo baseadas em IA e suporta uma variedade de linguagens de programa√ß√£o e IDEs. Ele tamb√©m aprende com seu c√≥digo para fornecer sugest√µes mais personalizadas.

**2. Ferramentas de An√°lise de C√≥digo com Chat:**

*   **DeepSource:** √â uma plataforma de an√°lise est√°tica de c√≥digo que tamb√©m oferece recursos de chat. Ele identifica bugs, vulnerabilidades e problemas de desempenho no seu c√≥digo, e permite que voc√™ discuta e resolva esses problemas diretamente na plataforma.
*   **SonarQube:** √â uma plataforma de an√°lise de c√≥digo open-source que ajuda a detectar bugs, vulnerabilidades e problemas de qualidade de c√≥digo. Embora n√£o tenha um chat integrado, ele permite que voc√™ comente e discuta os problemas encontrados diretamente no c√≥digo.

**3. Extens√µes de IDE com Integra√ß√£o de Chat:**

*   **Codeium:** √â uma extens√£o para VS Code e outras IDEs que utiliza IA para fornecer sugest√µes de c√≥digo, gerar c√≥digo e oferecer um chat interativo. Voc√™ pode selecionar um trecho de c√≥digo e perguntar ao Codeium sobre ele, e ele usar√° o contexto do c√≥digo para fornecer respostas relevantes.

**Como essas ferramentas funcionam:**

1.  **An√°lise do C√≥digo:** A ferramenta analisa o c√≥digo no seu editor ou reposit√≥rio para entender a estrutura, a l√≥gica e as depend√™ncias.
2.  **Contextualiza√ß√£o:** Ela usa o contexto do c√≥digo, como o nome da fun√ß√£o, os par√¢metros, os coment√°rios e o c√≥digo ao redor, para fornecer sugest√µes e an√°lises relevantes.
3.  **Chat Interativo:** Voc√™ pode fazer perguntas sobre o c√≥digo, pedir explica√ß√µes, pedir sugest√µes de melhorias ou pedir ajuda para depurar problemas.
4.  **Sugest√µes em Tempo Real:** A ferramenta oferece sugest√µes de c√≥digo, autocompletar, detec√ß√£o de erros e outras funcionalidades em tempo real, enquanto voc√™ est√° escrevendo o c√≥digo.

**Observa√ß√µes:**

*   A maioria dessas ferramentas requer uma assinatura paga para acessar todos os recursos.
*   A qualidade das sugest√µes e an√°lises depende da qualidade do modelo de IA e da quantidade de dados com que foi treinado.
*   √â importante revisar as sugest√µes da IA com cuidado, pois nem sempre s√£o perfeitas e podem conter erros.

Espero que esta lista te ajude a encontrar a ferramenta certa para suas necessidades!
User: n√£o quero uma ferramenta estou desenvolvendo a minha com python e streamlit como interface. recebe o c√≥digo e atrav√©s de um chat gera c√≥digo e respostas bem embasadas pois recebe a cada requisi√ß√£o o c√≥digo atual da aplica√ß√£o.
Assistant: prompt = f"""
Compreendi, voc√™ est√° desenvolvendo sua pr√≥pria ferramenta de an√°lise de c√≥digo com chat usando Python e Streamlit. A ideia √© que a cada requisi√ß√£o, o chat receba o c√≥digo atualizado da aplica√ß√£o para gerar respostas e c√≥digo embasados no contexto.

Aqui est√£o algumas sugest√µes para aprimorar a sua aplica√ß√£o, focando em como o chat interage com o c√≥digo:

**1. An√°lise Cont√≠nua do C√≥digo:**

*   **Parseamento do C√≥digo:** Utilize a biblioteca `ast` (Abstract Syntax Trees) do Python para parsear o c√≥digo da aplica√ß√£o. Isso permite que voc√™ analise a estrutura, as fun√ß√µes, as classes e as depend√™ncias do c√≥digo de forma program√°tica.
*   **Extra√ß√£o de Informa√ß√µes:** Extraia informa√ß√µes relevantes do c√≥digo, como:
    *   Nomes de fun√ß√µes e classes.
    *   Coment√°rios.
    *   Strings de documenta√ß√£o (docstrings).
    *   Assinaturas de fun√ß√µes (par√¢metros, tipos de retorno).
    *   Depend√™ncias entre m√≥dulos.
*   **Indexa√ß√£o:** Crie um √≠ndice do c√≥digo para facilitar a busca e o acesso r√°pido √†s informa√ß√µes.

**2. Integra√ß√£o com o Chat:**

*   **Contexto Rico:** A cada requisi√ß√£o do chat, forne√ßa um contexto rico para a IA, incluindo:
    *   O c√≥digo da aplica√ß√£o.
    *   Informa√ß√µes extra√≠das do c√≥digo (usando `ast`).
    *   O hist√≥rico da conversa.
    *   Informa√ß√µes sobre o estado atual da aplica√ß√£o (se aplic√°vel).
*   **Prompt Engineering:** Crie prompts claros e espec√≠ficos para a IA, instruindo-a sobre o tipo de an√°lise ou sugest√£o que voc√™ deseja. Por exemplo:
    *   "Analise esta fun√ß√£o e sugira melhorias na legibilidade."
    *   "Encontre poss√≠veis erros de digita√ß√£o neste c√≥digo."
    *   "Gere testes unit√°rios para esta classe."
*   **Formata√ß√£o da Resposta:** Formate a resposta da IA de forma clara e f√°cil de entender, usando Markdown ou HTML. Inclua trechos de c√≥digo formatados, links para as partes relevantes do c√≥digo e explica√ß√µes detalhadas.

**3. Gera√ß√£o de C√≥digo:**

*   **Edi√ß√£o Program√°tica:** Utilize a biblioteca `ast` para modificar o c√≥digo da aplica√ß√£o de forma program√°tica. Isso permite que a IA sugira altera√ß√µes diretamente no c√≥digo, como:
    *   Adicionar coment√°rios.
    *   Renomear vari√°veis.
    *   Refatorar fun√ß√µes.
    *   Corrigir erros de digita√ß√£o.
*   **Visualiza√ß√£o das Altera√ß√µes:** Exiba as altera√ß√µes sugeridas de forma clara para o usu√°rio, usando um diff (compara√ß√£o de c√≥digo) ou um editor de c√≥digo integrado no Streamlit.
*   **Aplica√ß√£o das Altera√ß√µes:** Permita que o usu√°rio revise e aplique as altera√ß√µes sugeridas pela IA com um clique.

**4. Melhorias Cont√≠nuas:**

*   **Feedback do Usu√°rio:** Colete feedback do usu√°rio sobre a qualidade das sugest√µes e da gera√ß√£o de c√≥digo da IA. Use esse feedback para treinar e melhorar o modelo de IA.
*   **Testes Automatizados:** Crie testes automatizados para garantir que as sugest√µes da IA n√£o introduzam erros no c√≥digo.
*   **Aprendizado Cont√≠nuo:** Mantenha-se atualizado sobre as √∫ltimas novidades em IA e aprendizado de m√°quina para incorporar novas t√©cnicas e modelos na sua aplica√ß√£o.

**Implementa√ß√£o:**

Aqui est√° um esbo√ßo de como voc√™ pode implementar essas sugest√µes:

```python
import streamlit as st
import ast
from core.handlers.gemini_handler import GeminiHandler
from services.search_files import ler_todos_arquivos_python

# Inicializa√ß√£o do GeminiHandler
gemini_handler = GeminiHandler("gemini-2.0-flash-exp")

def analisar_codigo(codigo):
    """Analisa o c√≥digo usando ast e retorna informa√ß√µes relevantes."""
    try:
        tree = ast.parse(codigo)
        funcoes = [node.name for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
        classes = [node.name for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]
        return {"funcoes": funcoes, "classes": classes}
    except SyntaxError as e:
        return {"erro": str(e)}

def gerar_prompt(codigo, informacoes_codigo, pergunta):
    """Gera o prompt para a IA com o c√≥digo e informa√ß√µes relevantes."""
    prompt = f"""Voc√™ √© um assistente de programa√ß√£o. Analise o c√≥digo abaixo e responda √† pergunta.

    C√≥digo:
    ```python
    {codigo}
    ```

    Informa√ß√µes sobre o c√≥digo:
    {informacoes_codigo}

    Pergunta:
    {pergunta}
    """
    return prompt

# Interface Streamlit
st.title("An√°lise de C√≥digo com IA")

codigo = ler_todos_arquivos_python()  # Fun√ß√£o para ler todo o c√≥digo do projeto
pergunta = st.text_input("Digite sua pergunta sobre o c√≥digo:")

if codigo and pergunta:
    informacoes_codigo = analisar_codigo(codigo)
    prompt = gerar_prompt(codigo, informacoes_codigo, pergunta)
    resposta = gemini_handler.generate_content(None, prompt)  # Envia o prompt para o Gemini

    st.write("Resposta da IA:")
    st.write(resposta)
```

Lembre-se de que este √© apenas um ponto de partida. Adapte as sugest√µes √†s suas necessidades espec√≠ficas e experimente diferentes abordagens para obter os melhores resultados.

Ao implementar essas sugest√µes, voc√™ estar√° no caminho certo para criar uma ferramenta poderosa e vers√°til para an√°lise de c√≥digo com chat.
"""

User: mas j√° tenho essa aplica√ß√£o funcionando quero saber o valor dessa aplica√ß√£o para o Mundo atual.
2025-04-08 02:19:53,702 [INFO] Resposta da IA (texto): Assistant: Para avaliar o valor da sua aplica√ß√£o de an√°lise de c√≥digo com chat no mundo atual, podemos considerar diversos fatores, incluindo tend√™ncias do mercado, necessidades dos desenvolvedores, funcionalidades √∫nicas e potencial de monetiza√ß√£o.

**1. Tend√™ncias do Mercado:**

*   **Aumento da Complexidade do C√≥digo:** Os projetos de software est√£o se tornando cada vez mais complexos, com equipes maiores e ciclos de desenvolvimento mais r√°pidos. Isso torna a an√°lise e a manuten√ß√£o do c√≥digo mais desafiadoras.
*   **Populariza√ß√£o da IA:** A intelig√™ncia artificial est√° se tornando cada vez mais acess√≠vel e poderosa, com modelos de linguagem natural (como o Gemini) capazes de entender e gerar c√≥digo com alta precis√£o.
*   **Ado√ß√£o do Desenvolvimento Colaborativo:** Ferramentas de colabora√ß√£o em tempo real est√£o se tornando cada vez mais populares, permitindo que equipes de desenvolvimento trabalhem juntas de forma mais eficiente.
*   **Import√¢ncia da Qualidade do C√≥digo:** Empresas est√£o cada vez mais conscientes da import√¢ncia da qualidade do c√≥digo para a seguran√ßa, a confiabilidade e o desempenho de seus sistemas.

**2. Necessidades dos Desenvolvedores:**

*   **An√°lise de C√≥digo Automatizada:** Desenvolvedores precisam de ferramentas que possam analisar o c√≥digo de forma automatizada para identificar bugs, vulnerabilidades e problemas de estilo.
*   **Sugest√µes de Melhorias:** Desenvolvedores precisam de ferramentas que possam sugerir melhorias no c√≥digo para aumentar a legibilidade, a efici√™ncia e a seguran√ßa.
*   **Debug Facilitado:** Desenvolvedores precisam de ferramentas que possam ajud√°-los a depurar o c√≥digo de forma mais r√°pida e eficiente.
*   **Aprendizado Cont√≠nuo:** Desenvolvedores precisam de ferramentas que possam ajud√°-los a aprender novas tecnologias e a melhorar suas habilidades de programa√ß√£o.
*   **Colabora√ß√£o Eficiente:** Desenvolvedores precisam de ferramentas que facilitem a colabora√ß√£o com outros membros da equipe, permitindo que eles compartilhem c√≥digo, discutam problemas e resolvam conflitos de forma eficiente.

**3. Funcionalidades √önicas da Sua Aplica√ß√£o:**

*   **Chat Interativo:** A capacidade de interagir com a IA por meio de um chat permite que os desenvolvedores fa√ßam perguntas espec√≠ficas sobre o c√≥digo e obtenham respostas personalizadas.
*   **Contexto do C√≥digo Atualizado:** O fato de que o chat recebe o c√≥digo atualizado da aplica√ß√£o a cada requisi√ß√£o garante que as sugest√µes e an√°lises da IA sejam sempre relevantes e precisas.
*   **Gera√ß√£o de C√≥digo:** A capacidade de gerar c√≥digo automaticamente pode economizar tempo e esfor√ßo dos desenvolvedores.
*   **Interface Streamlit:** A interface Streamlit facilita o uso da aplica√ß√£o e permite que os desenvolvedores visualizem e interajam com o c√≥digo de forma intuitiva.

**4. Potencial de Monetiza√ß√£o:**

*   **Assinatura:** Voc√™ pode cobrar uma taxa de assinatura para acessar a sua aplica√ß√£o.
*   **Licenciamento:** Voc√™ pode licenciar a sua tecnologia para outras empresas.
*   **Servi√ßos:** Voc√™ pode oferecer servi√ßos de consultoria e treinamento para ajudar empresas a usar a sua aplica√ß√£o.
*   **Open Source:** Voc√™ pode tornar a sua aplica√ß√£o open source e monetiz√°-la por meio de doa√ß√µes ou patroc√≠nios.

**5. Valor no Mundo Atual:**

Considerando todos esses fatores, a sua aplica√ß√£o tem um grande potencial de valor no mundo atual:

*   **Aumento da Produtividade:** A sua aplica√ß√£o pode ajudar os desenvolvedores a escrever c√≥digo mais r√°pido, mais limpo e mais seguro.
*   **Melhoria da Qualidade do C√≥digo:** A sua aplica√ß√£o pode ajudar os desenvolvedores a identificar e corrigir bugs, vulnerabilidades e problemas de estilo.
*   **Redu√ß√£o de Custos:** A sua aplica√ß√£o pode ajudar as empresas a reduzir os custos de desenvolvimento e manuten√ß√£o de software.
*   **Acelera√ß√£o da Inova√ß√£o:** A sua aplica√ß√£o pode ajudar as empresas a inovar mais rapidamente, permitindo que elas lancem novos produtos e servi√ßos de forma mais r√°pida e eficiente.
*   **Democratiza√ß√£o do Conhecimento:** A sua aplica√ß√£o pode ajudar a democratizar o conhecimento sobre programa√ß√£o, tornando-o mais acess√≠vel a pessoas de diferentes origens e n√≠veis de experi√™ncia.

Em resumo, a sua aplica√ß√£o de an√°lise de c√≥digo com chat tem um grande potencial para causar um impacto positivo no mundo do desenvolvimento de software. Ao fornecer uma ferramenta poderosa e acess√≠vel para an√°lise, sugest√£o e gera√ß√£o de c√≥digo, voc√™ pode ajudar os desenvolvedores a serem mais produtivos, a escreverem c√≥digo de melhor qualidade e a inovarem mais rapidamente.

2025-04-08 02:19:53,792 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 02:19:53,794 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 02:19:53,797 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 02:19:53,799 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 02:19:53,801 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 02:19:53,802 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 02:19:53,804 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 02:19:53,805 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 02:19:53,807 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 02:19:53,808 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 02:19:53,809 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 02:19:53,810 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 02:19:53,812 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 02:19:53,813 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 02:19:53,815 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 02:58:14,750 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 02:58:14,753 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 02:58:14,755 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 02:58:14,757 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 02:58:14,759 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 02:58:14,762 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 02:58:14,764 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 02:58:14,765 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 02:58:14,767 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 02:58:14,769 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 02:58:14,771 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 02:58:14,775 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 02:58:14,776 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 02:58:14,778 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 02:58:14,780 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 03:01:44,097 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 03:01:44,099 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 03:01:44,101 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 03:01:44,102 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 03:01:44,104 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 03:01:44,107 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 03:01:44,109 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 03:01:44,112 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 03:01:44,114 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 03:01:44,116 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 03:01:44,118 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 03:01:44,122 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 03:01:44,125 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 03:01:44,127 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 03:01:44,130 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 03:01:44,243 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 03:01:44,244 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 03:01:44,246 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 03:01:44,247 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 03:01:44,249 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 03:01:44,250 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 03:01:44,252 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 03:01:44,254 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 03:01:44,257 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 03:01:44,260 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 03:01:44,262 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 03:01:44,267 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 03:01:44,270 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 03:01:44,274 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 03:01:44,276 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 03:01:46,122 [INFO] AFC is enabled with max remote calls: 10.
2025-04-08 03:01:50,027 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-04-08 03:01:50,999 [INFO] AFC remote call 1 is done.
2025-04-08 03:01:51,149 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 03:01:51,151 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 03:01:51,154 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 03:01:51,157 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 03:01:51,160 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 03:01:51,162 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 03:01:51,165 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 03:01:51,167 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 03:01:51,169 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 03:01:51,173 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 03:01:51,176 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 03:01:51,178 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 03:01:51,181 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 03:01:51,183 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 03:01:51,186 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 03:02:37,090 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 03:02:37,091 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 03:02:37,092 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 03:02:37,094 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 03:02:37,095 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 03:02:37,096 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 03:02:37,097 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 03:02:37,099 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 03:02:37,100 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 03:02:37,102 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 03:02:37,103 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 03:02:37,105 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 03:02:37,106 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 03:02:37,107 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 03:02:37,107 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 03:02:37,612 [INFO] Modelo Gemini 'gemini-2.0-flash-exp' inicializado com sucesso.
2025-04-08 03:02:42,348 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 03:02:42,350 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 03:02:42,351 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 03:02:42,354 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 03:02:42,356 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 03:02:42,358 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 03:02:42,361 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 03:02:42,363 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 03:02:42,365 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 03:02:42,367 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 03:02:42,369 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 03:02:42,371 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 03:02:42,372 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 03:02:42,374 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 03:02:42,375 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 03:02:42,457 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 03:02:42,459 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 03:02:42,460 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 03:02:42,462 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 03:02:42,465 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 03:02:42,467 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 03:02:42,469 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 03:02:42,471 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 03:02:42,472 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 03:02:42,474 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 03:02:42,476 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 03:02:42,479 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 03:02:42,482 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 03:02:42,484 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 03:02:42,486 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 03:02:44,074 [INFO] AFC is enabled with max remote calls: 10.
2025-04-08 03:02:46,846 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 503 Service Unavailable"
2025-04-08 03:02:59,092 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 03:02:59,094 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 03:02:59,097 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 03:02:59,100 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 03:02:59,102 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 03:02:59,104 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 03:02:59,106 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 03:02:59,108 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 03:02:59,110 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 03:02:59,112 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 03:02:59,114 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 03:02:59,116 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 03:02:59,118 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 03:02:59,120 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 03:02:59,121 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 03:03:00,573 [INFO] AFC is enabled with max remote calls: 10.
2025-04-08 03:03:05,340 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-04-08 03:03:06,568 [INFO] AFC remote call 1 is done.
2025-04-08 03:03:06,675 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 03:03:06,677 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 03:03:06,679 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 03:03:06,681 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 03:03:06,683 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 03:03:06,686 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 03:03:06,688 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 03:03:06,690 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 03:03:06,692 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 03:03:06,693 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 03:03:06,694 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 03:03:06,696 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 03:03:06,698 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 03:03:06,701 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 03:03:06,702 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 03:04:24,665 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 03:04:24,667 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 03:04:24,669 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 03:04:24,670 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 03:04:24,673 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 03:04:24,675 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 03:04:24,677 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 03:04:24,679 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 03:04:24,680 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 03:04:24,682 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 03:04:24,683 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 03:04:24,685 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 03:04:24,687 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 03:04:24,689 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 03:04:24,691 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 03:04:24,818 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 03:04:24,820 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 03:04:24,823 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 03:04:24,826 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 03:04:24,828 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 03:04:24,830 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 03:04:24,832 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 03:04:24,833 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 03:04:24,835 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 03:04:24,836 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 03:04:24,838 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 03:04:24,840 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 03:04:24,842 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 03:04:24,843 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 03:04:24,845 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 03:04:37,945 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 03:04:37,947 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 03:04:37,948 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 03:04:37,950 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 03:04:37,952 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 03:04:37,953 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 03:04:37,956 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 03:04:37,959 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 03:04:37,960 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 03:04:37,962 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 03:04:37,964 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 03:04:37,966 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 03:04:37,968 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 03:04:37,970 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 03:04:37,973 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 03:04:38,092 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 03:04:38,093 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 03:04:38,095 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 03:04:38,096 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 03:04:38,097 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 03:04:38,099 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 03:04:38,101 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 03:04:38,103 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 03:04:38,104 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 03:04:38,107 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 03:04:38,110 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 03:04:38,113 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 03:04:38,115 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 03:04:38,116 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 03:04:38,118 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 03:04:38,135 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\assets\20250408030438_clipboard_20250408030424.png, Prompt: prompt = f"""
Voc√™ √© um assistente inteligente especializado em revis√£o e melhoria de c√≥digo.

Seu objetivo √©:
- Analisar o c√≥digo-fonte contido no projeto enviado
- Identificar melhorias em legibilidade, performance e seguran√ßa
- Sugerir pr√°ticas recomendadas de design e arquitetura
- Indicar partes com d√≠vidas t√©cnicas ou complexidade desnecess√°ria

Informa√ß√µes adicionais:
- Estrutura do projeto: segue o padr√£o de pastas iniciando por src/

"""

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.generativeai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: Que erro √© esse ?
2025-04-08 03:04:50,329 [INFO] Resposta da IA (imagem): Com certeza! Este √© um relat√≥rio de an√°lise e melhorias para o c√≥digo fornecido.

**Vis√£o Geral**

O c√≥digo representa um aplicativo Streamlit para intera√ß√£o com a API Gemini da Google, permitindo tanto conversas textuais quanto an√°lise de imagens. Ele inclui funcionalidades como gera√ß√£o de imagens a partir de prompts, gerenciamento de hist√≥rico de conversas, integra√ß√£o com a √°rea de transfer√™ncia e rate limiting.

**An√°lise Detalhada e Sugest√µes de Melhoria**

1.  **Organiza√ß√£o e Modulariza√ß√£o:**

    *   **`src/`**: A estrutura b√°sica do projeto est√° boa, com a separa√ß√£o em diret√≥rios como `core`, `services`, `utils` e `config`.
    *   **Configura√ß√£o (`config/config.py`):** Excelente uso de uma classe `Config` para gerenciar caminhos e configura√ß√µes.
        *   **Melhoria:** Considerar usar vari√°veis de ambiente para configura√ß√µes sens√≠veis como chaves de API, em vez de hardcoding.
    *   **Logging (`core/logger_config.py`):** Configura√ß√£o de logging adequada, o que √© crucial para depura√ß√£o e monitoramento.
    *   **Rate Limiting (`core/rate_limiter.py`):** Implementa√ß√£o de `RateLimiter` √© boa, mas a l√≥gica de espera pode ser refinada para evitar sleeps desnecess√°rios.

2.  **Gerenciamento de Estado do Streamlit:**

    *   O c√≥digo usa `st.session_state` para manter o estado da aplica√ß√£o, o que √© a maneira correta de fazer isso no Streamlit.
    *   **Melhoria:** Simplificar o uso de chaves no `st.session_state`, consolidando nomes e evitando duplica√ß√µes.

3.  **Handlers e Servi√ßos:**

    *   **`GeminiHandler` (`core/handlers/gemini_handler.py`):** Respons√°vel por interagir com a API Gemini, o que promove a separa√ß√£o de responsabilidades.
        *   **Melhoria:** Adicionar tratamento de exce√ß√µes mais espec√≠fico para falhas na API.
    *   **`DocumentService` (`services/document_service.py`):** Lida com a cria√ß√£o e manipula√ß√£o de documentos Word.
    *   **`MarkdownService` (`services/markdown_service.py`):** Lida com a cria√ß√£o e manipula√ß√£o de arquivos Markdown.
    *   **`ImageProcessor` (`services/image_processor.py`):** Orquestra o processamento de imagens, incluindo an√°lise e gera√ß√£o de resumos.

4.  **Tratamento de Imagens:**

    *   O c√≥digo lida corretamente com o carregamento de imagens da √°rea de transfer√™ncia e do sistema de arquivos.
    *   **Melhoria:** Adicionar tratamento de erros mais robusto ao lidar com imagens corrompidas ou formatos inv√°lidos.

5.  **Gera√ß√£o de Imagens:**

    *   A fun√ß√£o `generate_image` em `services/image_services.py` usa a API Gemini para gerar imagens a partir de prompts.
    *   **Melhoria:** Adicionar logs para rastrear o sucesso ou falha na gera√ß√£o de imagens.

6.  **Fluxo da Aplica√ß√£o:**

    *   O c√≥digo usa callbacks (`on_paste_click`, `on_file_upload`, `clear_all_images`) para lidar com eventos da interface do usu√°rio.
    *   A fun√ß√£o `process_message` coordena o processamento de mensagens e imagens.
    *   **Melhoria:** Refatorar a l√≥gica de processamento de mensagens para torn√°-la mais modular e leg√≠vel.

7.  **C√≥digo Fonte e Contexto:**

    *   O c√≥digo extrai o conte√∫do de todos os arquivos Python no diret√≥rio `src/` para usar como contexto para a API Gemini.
    *   **Melhoria:** Avaliar o impacto desse contexto no desempenho e custo da API. Considerar usar um vetorizador para selecionar os trechos mais relevantes.

8.  **Rate Limiting:**

    *   O c√≥digo implementa rate limiting para evitar sobrecarregar a API Gemini.
    *   **Melhoria:** Monitorar e ajustar os limites de taxa conforme necess√°rio.

9.  **Testes:**

    *   O arquivo `test_image_generation.py` fornece um exemplo de como gerar imagens com a API Gemini.
    *   **Melhoria:** Criar testes unit√°rios e de integra√ß√£o mais abrangentes para garantir a qualidade e estabilidade do c√≥digo.

10. **Boas Pr√°ticas:**

    *   Uso de arquivos `.env` para armazenar chaves de API (embora seja recomendado n√£o commitar o arquivo).
    *   Logging adequado para depura√ß√£o e monitoramento.
    *   Separa√ß√£o de responsabilidades em diferentes m√≥dulos e classes.

**D√≠vidas T√©cnicas:**

*   **Tratamento de erros:** O c√≥digo inclui tratamento de erros, mas pode ser aprimorado para lidar com casos espec√≠ficos de falha (por exemplo, erros de rede, erros de API, etc.).
*   **Testes:** A cobertura de testes √© limitada. Testes unit√°rios e de integra√ß√£o seriam ben√©ficos.
*   **Contexto da IA:** A inclus√£o de todo o c√≥digo fonte como contexto pode ser ineficiente. Considerar usar t√©cnicas de recupera√ß√£o de informa√ß√µes para selecionar os trechos mais relevantes.

**An√°lise de Seguran√ßa:**

*   O c√≥digo usa chaves de API, que devem ser mantidas em segredo. Recomenda-se armazenar essas chaves em vari√°veis de ambiente e n√£o commitar o arquivo `.env`.
*   O c√≥digo lida com uploads de arquivos, o que pode ser uma fonte de vulnerabilidades. √â importante validar e sanitizar os arquivos antes de process√°-los.
*   O c√≥digo usa a API Gemini, que pode ser vulner√°vel a ataques de inje√ß√£o de prompt. √â importante validar e sanitizar os prompts antes de envi√°-los para a API.

**Sugest√µes:**

1.  **Use um Linter e Formatter:** Adicione um linter (como `flake8` ou `pylint`) e um formatter (como `black`) para garantir a consist√™ncia do c√≥digo.

2.  **Documenta√ß√£o:** Adicione docstrings para todas as fun√ß√µes e classes.

3.  **Monitoramento:** Implemente um sistema de monitoramento para rastrear o uso da API Gemini e detectar poss√≠veis problemas.

4.  **Feedback do Usu√°rio:** Adicione feedback visual para indicar o progresso das opera√ß√µes (por exemplo, barras de progresso).

5.  **Cache:** Implemente um sistema de cache para armazenar respostas da API Gemini e evitar requisi√ß√µes desnecess√°rias.

**Em Resumo:**

O c√≥digo tem uma boa estrutura geral, mas pode ser aprimorado em termos de tratamento de erros, testes, seguran√ßa e efici√™ncia. As sugest√µes de melhoria acima podem ajudar a tornar o c√≥digo mais robusto, seguro e f√°cil de manter.

Se precisar de ajuda para implementar alguma dessas sugest√µes, me diga!
2025-04-08 03:04:50,432 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 03:04:50,434 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 03:04:50,436 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 03:04:50,437 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 03:04:50,439 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 03:04:50,442 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 03:04:50,444 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 03:04:50,446 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 03:04:50,447 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 03:04:50,449 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 03:04:50,450 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 03:04:50,452 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 03:04:50,453 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 03:04:50,455 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 03:04:50,456 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 03:05:35,891 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 03:05:35,893 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 03:05:35,895 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 03:05:35,897 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 03:05:35,900 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 03:05:35,902 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 03:05:35,905 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 03:05:35,907 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 03:05:35,909 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 03:05:35,910 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 03:05:35,912 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 03:05:35,914 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 03:05:35,916 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 03:05:35,918 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 03:05:35,920 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 03:05:36,046 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 03:05:36,048 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 03:05:36,050 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 03:05:36,051 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 03:05:36,053 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 03:05:36,055 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 03:05:36,057 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 03:05:36,059 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 03:05:36,062 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 03:05:36,066 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 03:05:36,068 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 03:05:36,069 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 03:05:36,071 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 03:05:36,072 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 03:05:36,074 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 03:05:36,089 [INFO] Enviando para IA - Imagem: C:\Users\jfreis\Documents\azure-ai-sentiment-analysis\assets\20250408030536_clipboard_20250408030424.png, Prompt: prompt = f"""
Voc√™ √© um assistente inteligente especializado em revis√£o e melhoria de c√≥digo.

Seu objetivo √©:
- Analisar o c√≥digo-fonte contido no projeto enviado
- Identificar melhorias em legibilidade, performance e seguran√ßa
- Sugerir pr√°ticas recomendadas de design e arquitetura
- Indicar partes com d√≠vidas t√©cnicas ou complexidade desnecess√°ria

Informa√ß√µes adicionais:
- Estrutura do projeto: segue o padr√£o de pastas iniciando por src/

"""

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.generativeai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: Que erro √© esse ?
Assistant: Com certeza! Este √© um relat√≥rio de an√°lise e melhorias para o c√≥digo fornecido.

**Vis√£o Geral**

O c√≥digo representa um aplicativo Streamlit para intera√ß√£o com a API Gemini da Google, permitindo tanto conversas textuais quanto an√°lise de imagens. Ele inclui funcionalidades como gera√ß√£o de imagens a partir de prompts, gerenciamento de hist√≥rico de conversas, integra√ß√£o com a √°rea de transfer√™ncia e rate limiting.

**An√°lise Detalhada e Sugest√µes de Melhoria**

1.  **Organiza√ß√£o e Modulariza√ß√£o:**

    *   **`src/`**: A estrutura b√°sica do projeto est√° boa, com a separa√ß√£o em diret√≥rios como `core`, `services`, `utils` e `config`.
    *   **Configura√ß√£o (`config/config.py`):** Excelente uso de uma classe `Config` para gerenciar caminhos e configura√ß√µes.
        *   **Melhoria:** Considerar usar vari√°veis de ambiente para configura√ß√µes sens√≠veis como chaves de API, em vez de hardcoding.
    *   **Logging (`core/logger_config.py`):** Configura√ß√£o de logging adequada, o que √© crucial para depura√ß√£o e monitoramento.
    *   **Rate Limiting (`core/rate_limiter.py`):** Implementa√ß√£o de `RateLimiter` √© boa, mas a l√≥gica de espera pode ser refinada para evitar sleeps desnecess√°rios.

2.  **Gerenciamento de Estado do Streamlit:**

    *   O c√≥digo usa `st.session_state` para manter o estado da aplica√ß√£o, o que √© a maneira correta de fazer isso no Streamlit.
    *   **Melhoria:** Simplificar o uso de chaves no `st.session_state`, consolidando nomes e evitando duplica√ß√µes.

3.  **Handlers e Servi√ßos:**

    *   **`GeminiHandler` (`core/handlers/gemini_handler.py`):** Respons√°vel por interagir com a API Gemini, o que promove a separa√ß√£o de responsabilidades.
        *   **Melhoria:** Adicionar tratamento de exce√ß√µes mais espec√≠fico para falhas na API.
    *   **`DocumentService` (`services/document_service.py`):** Lida com a cria√ß√£o e manipula√ß√£o de documentos Word.
    *   **`MarkdownService` (`services/markdown_service.py`):** Lida com a cria√ß√£o e manipula√ß√£o de arquivos Markdown.
    *   **`ImageProcessor` (`services/image_processor.py`):** Orquestra o processamento de imagens, incluindo an√°lise e gera√ß√£o de resumos.

4.  **Tratamento de Imagens:**

    *   O c√≥digo lida corretamente com o carregamento de imagens da √°rea de transfer√™ncia e do sistema de arquivos.
    *   **Melhoria:** Adicionar tratamento de erros mais robusto ao lidar com imagens corrompidas ou formatos inv√°lidos.

5.  **Gera√ß√£o de Imagens:**

    *   A fun√ß√£o `generate_image` em `services/image_services.py` usa a API Gemini para gerar imagens a partir de prompts.
    *   **Melhoria:** Adicionar logs para rastrear o sucesso ou falha na gera√ß√£o de imagens.

6.  **Fluxo da Aplica√ß√£o:**

    *   O c√≥digo usa callbacks (`on_paste_click`, `on_file_upload`, `clear_all_images`) para lidar com eventos da interface do usu√°rio.
    *   A fun√ß√£o `process_message` coordena o processamento de mensagens e imagens.
    *   **Melhoria:** Refatorar a l√≥gica de processamento de mensagens para torn√°-la mais modular e leg√≠vel.

7.  **C√≥digo Fonte e Contexto:**

    *   O c√≥digo extrai o conte√∫do de todos os arquivos Python no diret√≥rio `src/` para usar como contexto para a API Gemini.
    *   **Melhoria:** Avaliar o impacto desse contexto no desempenho e custo da API. Considerar usar um vetorizador para selecionar os trechos mais relevantes.

8.  **Rate Limiting:**

    *   O c√≥digo implementa rate limiting para evitar sobrecarregar a API Gemini.
    *   **Melhoria:** Monitorar e ajustar os limites de taxa conforme necess√°rio.

9.  **Testes:**

    *   O arquivo `test_image_generation.py` fornece um exemplo de como gerar imagens com a API Gemini.
    *   **Melhoria:** Criar testes unit√°rios e de integra√ß√£o mais abrangentes para garantir a qualidade e estabilidade do c√≥digo.

10. **Boas Pr√°ticas:**

    *   Uso de arquivos `.env` para armazenar chaves de API (embora seja recomendado n√£o commitar o arquivo).
    *   Logging adequado para depura√ß√£o e monitoramento.
    *   Separa√ß√£o de responsabilidades em diferentes m√≥dulos e classes.

**D√≠vidas T√©cnicas:**

*   **Tratamento de erros:** O c√≥digo inclui tratamento de erros, mas pode ser aprimorado para lidar com casos espec√≠ficos de falha (por exemplo, erros de rede, erros de API, etc.).
*   **Testes:** A cobertura de testes √© limitada. Testes unit√°rios e de integra√ß√£o seriam ben√©ficos.
*   **Contexto da IA:** A inclus√£o de todo o c√≥digo fonte como contexto pode ser ineficiente. Considerar usar t√©cnicas de recupera√ß√£o de informa√ß√µes para selecionar os trechos mais relevantes.

**An√°lise de Seguran√ßa:**

*   O c√≥digo usa chaves de API, que devem ser mantidas em segredo. Recomenda-se armazenar essas chaves em vari√°veis de ambiente e n√£o commitar o arquivo `.env`.
*   O c√≥digo lida com uploads de arquivos, o que pode ser uma fonte de vulnerabilidades. √â importante validar e sanitizar os arquivos antes de process√°-los.
*   O c√≥digo usa a API Gemini, que pode ser vulner√°vel a ataques de inje√ß√£o de prompt. √â importante validar e sanitizar os prompts antes de envi√°-los para a API.

**Sugest√µes:**

1.  **Use um Linter e Formatter:** Adicione um linter (como `flake8` ou `pylint`) e um formatter (como `black`) para garantir a consist√™ncia do c√≥digo.

2.  **Documenta√ß√£o:** Adicione docstrings para todas as fun√ß√µes e classes.

3.  **Monitoramento:** Implemente um sistema de monitoramento para rastrear o uso da API Gemini e detectar poss√≠veis problemas.

4.  **Feedback do Usu√°rio:** Adicione feedback visual para indicar o progresso das opera√ß√µes (por exemplo, barras de progresso).

5.  **Cache:** Implemente um sistema de cache para armazenar respostas da API Gemini e evitar requisi√ß√µes desnecess√°rias.

**Em Resumo:**

O c√≥digo tem uma boa estrutura geral, mas pode ser aprimorado em termos de tratamento de erros, testes, seguran√ßa e efici√™ncia. As sugest√µes de melhoria acima podem ajudar a tornar o c√≥digo mais robusto, seguro e f√°cil de manter.

Se precisar de ajuda para implementar alguma dessas sugest√µes, me diga!
User: quero que analise a imagem.
2025-04-08 03:05:46,898 [INFO] Resposta da IA (imagem): Com certeza! Based on the image, it appears the user is encountering a `TypeError` when trying to use a Streamlit application that utilizes AI for image generation and analysis. The error message "Failed to fetch dynamically imported module" suggests an issue with loading a JavaScript module needed for the application's functionality.

Here's a breakdown of what's likely happening and potential solutions:

**Likely Cause of the Error**

*   **JavaScript Module Loading Issue:** The error indicates that the application is unable to load a JavaScript module (likely responsible for front-end interactivity or image processing) during runtime. This can happen due to a variety of reasons:
    *   **Incorrect Path:** The path to the JavaScript file (`http://localhost:8501/static/js/index.CJrqRt00.js`) might be incorrect.
    *   **File Not Found:** The JavaScript file may be missing or not correctly deployed in the `static/js` directory on the server.
    *   **CORS Issue:** If the application is trying to load the JavaScript module from a different domain, there might be a Cross-Origin Resource Sharing (CORS) issue blocking the request.
    *   **Browser Extension Interference:** A browser extension might be interfering with the loading of JavaScript modules.
    *   **Caching Problems:** The browser might be caching an outdated version of the JavaScript file.

**Steps to Troubleshoot and Resolve the Error**

1.  **Verify the JavaScript File Path:**
    *   **Double-Check the Path:** Ensure that the path to the JavaScript file (`http://localhost:8501/static/js/index.CJrqRt00.js`) is correct.
    *   **Confirm File Existence:** Verify that the JavaScript file actually exists in the `static/js` directory of your Streamlit application.
    *   **Check Streamlit Configuration:** Review your Streamlit application's configuration to ensure that it is correctly serving static files.

2.  **Clear Browser Cache and Cookies:**
    *   **Clear Cache:** Clear your browser's cache and cookies to ensure that you are not loading an outdated version of the JavaScript file.
    *   **Try a Different Browser:** Test the application in a different browser to see if the issue is browser-specific.

3.  **Check for CORS Issues:**
    *   **Ensure Same Origin:** If the JavaScript module is hosted on a different domain, ensure that the server hosting the module has CORS enabled to allow requests from your Streamlit application's domain.

4.  **Disable Browser Extensions:**
    *   **Disable Extensions:** Temporarily disable any browser extensions that might be interfering with the loading of JavaScript modules.
    *   **Test Again:** Reload the application to see if the error is resolved.

5.  **Inspect the Browser Console:**
    *   **Open Console:** Open your browser's developer console (usually by pressing F12).
    *   **Check for Errors:** Look for any error messages related to JavaScript module loading or network requests. The console might provide more specific information about the cause of the error.

6.  **Restart Streamlit Application:**
    *   **Restart:** Stop and restart your Streamlit application to ensure that all files are correctly loaded.

7.  **Check for Streamlit Updates:**
    *   **Update Streamlit:** Make sure you are using the latest version of Streamlit. Outdated versions might have compatibility issues or known bugs.

8.  **Examine the Streamlit Code:**

    *   Review the code for any dynamic imports or JavaScript code that might be causing the issue. Make sure that all necessary files are being served correctly.

**Example Code Snippet to check if javascript file is loaded**

```python
import streamlit as st

# Fun√ß√£o para adicionar JavaScript ao HTML
def inject_javascript(js_path):
    """Injeta um arquivo JavaScript no HTML da p√°gina Streamlit."""
    try:
        with open(js_path, 'r', encoding='utf-8') as f:
            js_code = f"<script>{f.read()}</script>"
            st.components.v1.html(js_code, height=0, scrolling=False)
        st.success(f"JavaScript file {js_path} injected successfully.")
    except FileNotFoundError:
        st.error(f"JavaScript file {js_path} not found.")
    except Exception as e:
        st.error(f"Error injecting JavaScript file {js_path}: {e}")

# Caminho para o seu arquivo JavaScript
js_file_path = "static/js/index.CJrqRt00.js"

# Injeta o JavaScript ao rodar o script Streamlit
if __name__ == "__main__":
    inject_javascript(js_file_path)

    st.title("Simple Streamlit App with JavaScript Injection")
    st.write("Check the developer console to see if the JavaScript has been injected.")

```

**Explanation of the Code Snippet**

1.  **Import `streamlit`:** Imports the Streamlit library.
2.  **`inject_javascript(js_path)` Function:**
    *   Takes the path to the JavaScript file as an argument (`js_path`).
    *   Opens and reads the JavaScript file.
    *   Embeds the JavaScript code in an HTML `<script>` tag.
    *   Uses `st.components.v1.html()` to inject the HTML (and thus the JavaScript) into the Streamlit page.
    *   The `height=0` and `scrolling=False` are used to ensure the HTML component doesn't take up space or add a scrollbar.
    *   Provides success or error messages to the Streamlit app to indicate whether the JavaScript file was successfully injected.
3.  **Main Execution Block:**
    *   Specifies the path to the JavaScript file (`js_file_path`).
    *   Calls `inject_javascript()` to inject the JavaScript file.
    *   Adds a simple title and message to the Streamlit app.

**How to Use the Code Snippet**

1.  **Save the Code:** Save the provided Python code as a file (e.g., `app.py`).
2.  **Ensure Correct JavaScript Path:** Make sure the `js_file_path` variable points to the correct path of your JavaScript file.
3.  **Run the Streamlit App:** Execute the Streamlit app by running `streamlit run app.py` in your terminal.
4.  **Check the Output:**
    *   The Streamlit app will display a title and a message.
    *   Check the Streamlit app‚Äôs success or error message to see if the JavaScript file was injected successfully.
    *   Open the browser's developer console (usually by pressing F12) and look for any JavaScript-related output or errors.

**Analyzing the Console**

*   **No Errors:** If there are no errors in the console and the success message is displayed, the JavaScript file has been injected correctly.
*   **File Not Found Error:** If the console shows a "File Not Found" error or the Streamlit app displays an error message indicating the file was not found, double-check that the file exists at the specified path.
*   **Syntax Errors or Other Issues:** If there are syntax errors or other issues in your JavaScript code, they will be displayed in the console, helping you identify and fix them.

This approach should help you verify whether your JavaScript file is being properly loaded and executed within your Streamlit application. If you can confirm that the JavaScript file is injected correctly but the issue persists, the problem might be within the JavaScript code itself.

By going through these steps, you should be able to identify the root cause of the "Failed to fetch dynamically imported module" error and resolve it.

2025-04-08 03:05:46,976 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 03:05:46,978 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 03:05:46,980 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 03:05:46,981 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 03:05:46,983 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 03:05:46,985 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 03:05:46,987 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 03:05:46,989 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 03:05:46,990 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 03:05:46,992 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 03:05:46,993 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 03:05:46,994 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 03:05:46,995 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 03:05:46,997 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 03:05:46,998 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 03:08:02,338 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 03:08:02,341 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 03:08:02,343 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 03:08:02,346 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 03:08:02,348 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 03:08:02,351 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 03:08:02,354 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 03:08:02,356 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 03:08:02,358 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 03:08:02,359 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 03:08:02,363 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 03:08:02,366 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 03:08:02,368 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 03:08:02,369 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 03:08:02,371 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 03:08:02,504 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 03:08:02,506 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 03:08:02,508 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 03:08:02,510 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 03:08:02,512 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 03:08:02,514 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 03:08:02,517 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 03:08:02,519 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 03:08:02,521 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 03:08:02,522 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 03:08:02,524 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 03:08:02,526 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 03:08:02,528 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 03:08:02,530 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 03:08:02,533 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 03:08:04,281 [INFO] AFC is enabled with max remote calls: 10.
2025-04-08 03:08:07,218 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 503 Service Unavailable"
2025-04-08 03:08:27,962 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 03:08:27,965 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 03:08:27,967 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 03:08:27,969 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 03:08:27,971 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 03:08:27,973 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 03:08:27,976 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 03:08:27,978 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 03:08:27,979 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 03:08:27,982 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 03:08:27,984 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 03:08:27,986 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 03:08:27,987 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 03:08:27,989 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 03:08:27,991 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 03:08:29,943 [INFO] AFC is enabled with max remote calls: 10.
2025-04-08 03:08:33,010 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 503 Service Unavailable"
2025-04-08 03:09:04,555 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 03:09:04,557 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 03:09:04,559 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 03:09:04,561 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 03:09:04,565 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 03:09:04,567 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 03:09:04,570 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 03:09:04,572 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 03:09:04,573 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 03:09:04,575 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 03:09:04,577 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 03:09:04,578 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 03:09:04,580 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 03:09:04,582 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 03:09:04,583 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 03:09:06,391 [INFO] AFC is enabled with max remote calls: 10.
2025-04-08 03:09:10,527 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-04-08 03:09:11,540 [INFO] AFC remote call 1 is done.
2025-04-08 03:09:11,649 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 03:09:11,654 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 03:09:11,659 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 03:09:11,666 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 03:09:11,674 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 03:09:11,681 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 03:09:11,688 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 03:09:11,694 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 03:09:11,699 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 03:09:11,703 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 03:09:11,709 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 03:09:11,713 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 03:09:11,717 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 03:09:11,720 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 03:09:11,724 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 04:27:37,454 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 04:27:37,458 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 04:27:37,460 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 04:27:37,461 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 04:27:37,463 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 04:27:37,465 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 04:27:37,467 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 04:27:37,469 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 04:27:37,472 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 04:27:37,474 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 04:27:37,476 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 04:27:37,478 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 04:27:37,481 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 04:27:37,485 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 04:27:37,488 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 04:27:37,739 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 04:27:37,741 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 04:27:37,745 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 04:27:37,747 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 04:27:37,748 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 04:27:37,751 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 04:27:37,753 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 04:27:37,755 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 04:27:37,756 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 04:27:37,758 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 04:27:37,760 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 04:27:37,764 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 04:27:37,765 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 04:27:37,767 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 04:27:37,768 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-04-08 04:27:37,773 [INFO] Enviando para IA - Prompt (sem imagem): Voc√™ √© um assistente inteligente utilizado em um chat com IA, que recebe c√≥digos de uma aplica√ß√£o completa de chat, como voc√™ funciona e tudo que 
√© programado para fazer no seu contexto.

Seu objetivo √©:
- Analisar todo o contexto e aprender com ele.
- Analisar a Pergunta do Usu√°rio.
- Identificar se possui rela√ß√£o com o contexto.
- Se houver rela√ß√£o com o c√≥digo do contexto responder a pergunta sobre o c√≥digo sempre seguindo boas pr√°ticas:
    - Analisar o c√≥digo-fonte contido no projeto enviado
    - Identificar melhorias em legibilidade, performance e seguran√ßa
    - Sugerir pr√°ticas recomendadas de design e arquitetura
    - Indicar partes com d√≠vidas t√©cnicas ou complexidade desnecess√°ria

Informa√ß√µes adicionais:
- Estrutura do projeto: segue o padr√£o de pastas iniciando por src/


## Se houver imagem no contexto, identifique se a pergunta √© sobre o c√≥digo do contexto, sobre a imagem ou sobre os dois para elaborar melhorer respostas.
## Se o usu√°rio fizer uma pergunta que n√£o tem rela√ß√£o com imagem ou contexto, use a sua base de conhecimento fora do contexto para responder de forma inteligente.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# src/config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    print(f"Base Directory: {BASE_DIR}")

    ASSETS_DIR = BASE_DIR.parent / "assets"

    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR.parent / "processed_images"
    print(PROCESSED_DIR)
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

import signal
import sys

def handler(signum, frame):
    print("üö® Processamento interrompido pelo usu√°rio.")
    sys.exit(1)

def setup_signal_handler():
    signal.signal(signal.SIGINT, handler)

# core\logger_config.py

# core/logger_config.py
import logging
import os
from datetime import datetime

LOG_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = datetime.now().strftime("log_%Y%m%d.log")
log_filepath = os.path.join(LOG_DIR, log_filename)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filepath, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# core\rate_limiter.py

import time
from collections import deque
from threading import Lock

class RateLimiter:
    def __init__(self, max_requests: int, period_seconds: int):
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        with self.lock:
            current_time = time.time()

            # Remove requests antigos fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            else:
                return False

    def wait_for_slot(self):
        """Aguarda o pr√≥ximo slot dispon√≠vel, ajustando a espera conforme necess√°rio."""
        while not self.allow_request():
            # Calcula o tempo de espera baseado no n√∫mero de requisi√ß√µes feitas
            # tempo necess√°rio para respeitar o limite
            current_time = time.time()
            if self.requests:  # Verifica se a lista n√£o est√° vazia
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera um segundo se n√£o houver requisi√ß√µes

            # Aguarda o tempo necess√°rio para garantir que a pr√≥xima requisi√ß√£o pode ser feita
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import Config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.0-flash-exp")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(Config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {Config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(Config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(Config.ASSETS_DIR, image_name)
        processed_path = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(Config.ASSETS_DIR, image_name)
        destino = os.path.join(Config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{Config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.generativeai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

Assistant: Imagem gerada com o prompt: generate image stock analysis tool, empowering investors, B3 stock exchange, rising graphs, magnifying glass on financial data, blue, green,gold colors, clean dashboard, modern, professional, minimal.
Assistant: Imagem gerada com o prompt: generate image stock analysis tool, empowering investors, B3 stock exchange, rising graphs, magnifying glass on financial data, blue, green,gold colors, clean dashboard, modern, professional, minimal.
User: ```python
import pandas as pd
import os
import streamlit as st
import plotly.express as px
import re

# Obter o diret√≥rio do script atual
current_dir = os.path.dirname(__file__)

# Definir o caminho completo do arquivo usando o diret√≥rio do script
file_path = os.path.join(current_dir, 'relatorio_final.xlsx')

# Classe para carregar dados
class DataLoader:
    def __init__(self, file_path):
        self.file_path = file_path

    def load_data(self):
        """Carrega os dados do arquivo Excel."""
        if not os.path.exists(self.file_path):
            st.error(f"Arquivo n√£o encontrado: {self.file_path}")
            st.stop()
        try:
            df = pd.read_excel(self.file_path, dtype={
                'Ticket#': str,
                'EscalationTimeWorkingTime': str,
                'Hor√°rio de Escala√ß√£o': str,
                'SolutionTime': str,
                'SolutionTimeDestinationTime': str,
                'SolutionTimeWorkingTime': str,
                'Delta de tempo de solu√ß√£o em minutos': str,
                'Tempo de solu√ß√£o em minutos': str
            })
            return df
        except Exception as e:
            st.error(f"Erro ao carregar o arquivo: {e}")
            return pd.DataFrame()

class DataProcessor:
    def __init__(self, df):
        self.df = df

    def preprocess_data(self):
        """Pr√©-processa os dados."""
        required_columns = ['Estado', 'Criado', 'Servi√ßo', 'Hora de Fechamento', '√öltima Altera√ß√£o', 'Idade']
        if not all(col in self.df.columns for col in required_columns):
            st.error(f"O arquivo n√£o cont√©m todas as colunas necess√°rias: {required_columns}")
            st.stop()

        # Remover chamados cancelados
        self.df = self.df[self.df['Estado'] != 'Cancelado']

        # Converter colunas para datetime
        self.df['Criado'] = pd.to_datetime(self.df['Criado'], errors='coerce')
        self.df['Hora de Fechamento'] = pd.to_datetime(self.df['Hora de Fechamento'], errors='coerce')
        self.df['√öltima Altera√ß√£o'] = pd.to_datetime(self.df['√öltima Altera√ß√£o'], errors='coerce')

        # Criar coluna de M√™s/Ano com base no campo Criado
        self.df['MesAno'] = self.df['Criado'].dt.to_period('M')

        # Criar coluna de M√™s/Ano com base no campo √öltima Altera√ß√£o
        self.df['MesAnoUltimaAlteracao'] = self.df['√öltima Altera√ß√£o'].dt.to_period('M')

        return self.df

    def get_oldest_tickets_by_service(self, service, n=5):
        estados_excluidos = ['Fechado', 'Solucionado', 'Cancelado']
        df_service = self.df[
            (self.df['Servi√ßo'] == service) &
            (~self.df['Estado'].isin(estados_excluidos))
        ].copy()
        df_service = df_service.sort_values(by='Criado', ascending=True).head(n)
        return df_service

    def filter_data_by_month(self, mes_selecionado):
        """Filtra os dados pelo m√™s selecionado."""
        return self.df[(self.df['Criado'].dt.to_period('M') == mes_selecionado) & (self.df['Criado'].notna())]

    def calculate_metrics(self, mes_selecionado):
        """Calcula as m√©tricas considerando todos os dados e as do m√™s selecionado."""
        # Filtrar dados do m√™s selecionado
        df_mes_selecionado = self.df[self.df['Criado'].dt.to_period('M') == mes_selecionado]

        # Total Abertos: Chamados que a Criado est√° dentro do M√™s Selecionado
        total_abertos_mes = df_mes_selecionado.shape[0]

        # Total Encerrados: Todos os Chamados que a Data de Criado est√° dentro do m√™s selecionado
        total_encerrados = self.df[self.df['Hora de Fechamento'].dt.to_period('M') == mes_selecionado].shape[0]

        # Total Sem Atendente: Todos os Chamados que n√£o t√™m um Atendente/Propriet√°rio
        total_sem_atendente = self.df[
            (self.df['Atendente/Propriet√°rio'] == '') |
            (self.df['Atendente/Propriet√°rio'] == '*a definir')
        ].shape[0]

        # Total Backlog: Todos os chamados que o Estado n√£o √© Fechado, Solucionado e Cancelado, e que o Estado seja 'Aceito'
        total_backlog = self.df[
            (self.df['Estado'] == 'Aceito')
        ].shape[0]

        # Total em Aberto: Todos os chamados que n√£o est√£o Fechados nem Solucionados
        total_em_aberto = self.df[
            ~self.df['Estado'].isin(['Fechado', 'Solucionado', 'Cancelado', 'Fechado sem solu√ß√£o'])
        ].shape[0]

        # Em Atendimento: Independente do m√™s que foi aberto, o Estado do Chamado √© "Em atendimento"
        total_em_atendimento = self.df[self.df['Estado'] == 'Em atendimento'].shape[0]

        # Aguardando Terceiros: Independente do m√™s que foi aberto, o Estado do Chamado √© "Aguardando terceiros"
        total_aguardando_terceiros = self.df[self.df['Estado'] == 'Aguardando terceiros'].shape[0]

        # Aguardando Cliente: Independente do m√™s que foi aberto, o Estado do Chamado √© "Aguardando cliente"
        total_aguardando_cliente = self.df[self.df['Estado'] == 'Aguardando cliente'].shape[0]

        # Agendado: Independente do m√™s que foi aberto, o Estado do Chamado √© "Agendado"
        total_agendado = self.df[self.df['Estado'] == 'Agendado'].shape[0]

        return (
            total_abertos_mes,
            total_encerrados,
            total_sem_atendente,
            total_backlog,
            total_em_aberto,
            total_em_atendimento,
            total_aguardando_terceiros,
            total_aguardando_cliente,
            total_agendado
        )

    def calculate_oldest_ticket_age_days(self, df):
        """Calcula a idade em dias do chamado mais antigo para cada servi√ßo."""
        backlog_abertos = df[~df['Estado'].isin(['Fechado', 'Solucionado', 'Cancelado'])]

        # Calcula a diferen√ßa em dias entre a data atual e a data de cria√ß√£o
        backlog_abertos['Tempo Aberto (Dias)'] = (pd.Timestamp('now') - backlog_abertos['Criado']).dt.days

        # Agrupa por servi√ßo e encontra o tempo aberto m√°ximo (mais antigo)
        oldest_ticket_age = backlog_abertos.groupby('Servi√ßo')['Tempo Aberto (Dias)'].max().reset_index()

        return oldest_ticket_age

# Classe base para gr√°ficos
class BasePlot:
    def __init__(self, title, total=None):
        self.title = title
        self.total = total

    def plot(self):
        raise NotImplementedError("Subclasses devem implementar o m√©todo plot.")

# Classe para gr√°ficos de barras
class BarPlot(BasePlot):
    def __init__(self, data, x, y, color, title, labels, text, total=None, barmode='group', yaxis_type='linear', showlegend=True, bar_color=None, sort_by_y=True):
        super().__init__(title, total)
        self.data = data
        self.x = x
        self.y = y
        self.color = color
        self.labels = labels
        self.text = text
        self.barmode = barmode
        self.yaxis_type = yaxis_type
        self.showlegend = showlegend
        self.bar_color = bar_color
        self.sort_by_y = sort_by_y  # Novo par√¢metro para controlar a ordena√ß√£o

    def plot(self):
        if self.total is not None:
            title_with_total = f"{self.title} (Total: {self.total})"
        else:
            title_with_total = self.title

        # Ordenar os dados pela coluna y, se sort_by_y for True
        if self.sort_by_y:
            self.data = self.data.sort_values(by=self.y, ascending=False)

        fig = px.bar(
            self.data,
            x=self.x,
            y=self.y,
            color=self.color,
            title=title_with_total,
            labels=self.labels,
            text=self.text,
            barmode=self.barmode
        )
        if self.bar_color:
            fig.update_traces(marker_color=self.bar_color)

        fig.update_layout(
            width=1200,
            height=600,
            bargap=0.1,
            showlegend=self.showlegend,
            yaxis_type=self.yaxis_type,
            xaxis=dict(
                type='category',
                tickangle=-45
            )
        )

        fig.update_traces(
            texttemplate='%{text:.0f}',
            textposition='outside'
        )

        return fig

class MetricPlot(BasePlot):
    def __init__(self, title, value):
        super().__init__(title)
        self.value = value

    def plot(self):
        st.metric(self.title, self.value)

# Classe para visualiza√ß√£o
class Visualizer:
    @staticmethod
    def plot_metric(title, value):
        metric_plot = MetricPlot(title, value)
        metric_plot.plot()

    @staticmethod
    def plot_bar(data, x, y, color, title, labels, text, yaxis_type='linear', bar_color=None, sort_by_y=True):
        bar_plot = BarPlot(
            data=data,
            x=x,
            y=y,
            color=color,
            title=title,
            labels=labels,
            text=text,
            yaxis_type=yaxis_type,
            bar_color=bar_color,
            sort_by_y = sort_by_y
        )
        fig = bar_plot.plot()
        st.plotly_chart(fig, use_container_width=True)

    @staticmethod
    def plot_line(data, x, y, title, labels):
        fig = px.line(
            data,
            x=x,
            y=y,
            title=title,
            labels=labels,
            markers=True,
            text=y  # Adiciona o valor num√©rico em cada ponto
        )

        fig.update_layout(
            width=1200,
            height=600,
            plot_bgcolor='rgba(0,0,0,0)',
            xaxis=dict(
                tickangle=-45
            )
        )

        fig.update_traces(
            line=dict(width=3),
            marker=dict(size=8),
            textposition='top center',  # posiciona o texto acima dos pontos
            texttemplate='%{text:.0f}'  # formata o texto como inteiro
        )

        st.plotly_chart(fig, use_container_width=True)

# Classe principal da aplica√ß√£o
class App:
    def __init__(self, file_path):
        self.file_path = file_path

    def run(self):
        st.set_page_config(layout="wide")

        data_loader = DataLoader(self.file_path)
        df = data_loader.load_data()

        data_processor = DataProcessor(df)
        df = data_processor.preprocess_data()

        meses_disponiveis = sorted(df['MesAno'].dropna().unique())
        mes_atual = pd.to_datetime("now").to_period('M')
        mes_selecionado = mes_atual if mes_atual in meses_disponiveis else meses_disponiveis[0]
        mes_selecionado = st.selectbox("Selecione o m√™s:", meses_disponiveis, index=list(meses_disponiveis).index(mes_selecionado))

        df_filtrado = data_processor.filter_data_by_month(mes_selecionado)

        if df_filtrado.empty:
            st.write(f"N√£o h√° dados para o m√™s de {mes_selecionado}.")
        else:
            # Calcular m√©tricas
            (
                total_abertos_mes,
                total_encerrados,
                total_sem_atendente,
                total_backlog,
                total_em_aberto,
                total_em_atendimento,
                total_aguardando_terceiros,
                total_aguardando_cliente,
                total_agendado
            ) = data_processor.calculate_metrics(mes_selecionado)

            # Primeira linha de m√©tricas
            col1, col2, col3, col4, col5 = st.columns(5)

            with col1:
                Visualizer.plot_metric("Total Abertos M√™s", total_abertos_mes)

            with col2:
                Visualizer.plot_metric("Total Encerrados", total_encerrados)

            with col3:
                Visualizer.plot_metric("Total Sem Atendente", total_sem_atendente)

            with col4:
                Visualizer.plot_metric("Total Backlog", total_backlog)

            with col5:
                Visualizer.plot_metric("Total Abertos", total_em_aberto)  # Total Abertos M√™s

            # Segunda linha de m√©tricas
            col6, col7, col8, col9 = st.columns(4)

            with col6:
                Visualizer.plot_metric("Em Atendimento", total_em_atendimento)

            with col7:
                Visualizer.plot_metric("Aguardando Terceiros", total_aguardando_terceiros)

            with col8:
                Visualizer.plot_metric("Aguardando Cliente", total_aguardando_cliente)

            with col9:
                Visualizer.plot_metric("Agendado", total_agendado)

            with st.container():
                # Filtrar os chamados que foram encerrados no m√™s selecionado
                df_chamados_solucionados_fechados = df[
                    df['Estado'].isin(['Solucionado', 'Fechado', 'Fechados sem solu√ß√£o']) &
                    (df['√öltima Altera√ß√£o'].dt.to_period('M') == mes_selecionado)
                ]

                total_por_analista = df_chamados_solucionados_fechados.groupby('Atendente/Propriet√°rio').size().reset_index(name='Total Chamados')

                total_chamados = total_por_analista['Total Chamados'].sum()

                # Ordenar os dados pela quantidade de chamados
                total_por_analista = total_por_analista.sort_values(by='Total Chamados', ascending=False)

                bar_plot = BarPlot(
                    total_por_analista,
                    x='Atendente/Propriet√°rio',
                    y='Total Chamados',
                    color=None,
                    title=f'Encerrados por Analista no M√™s Selecionado (Total: {total_chamados})',
                    labels={'Total Chamados': 'Total de Chamados'},
                    text='Total Chamados',
                    showlegend=False,
                    yaxis_type='log',
                    bar_color=['blue', '#FFDB58', 'green', 'pink', 'purple', 'red', 'orange', 'gray', 'brown']
                )

                fig = bar_plot.plot()
                st.plotly_chart(fig, use_container_width=True)

                # Criados: Filtrar por 'Criado' no m√™s selecionado
                if pd.api.types.is_datetime64_any_dtype(df_filtrado['Criado']):
                    criados_por_dia = df_filtrado[df_filtrado['Criado'].dt.to_period('M') == mes_selecionado].groupby(df_filtrado['Criado'].dt.date).size().reset_index(name='Total Criados')
                    criados_por_dia.rename(columns={'Criado': 'Data'}, inplace=True)  # Renomear a coluna para 'Data'
                else:
                    st.error("A coluna 'Criado' n√£o cont√©m valores datetime v√°lidos.")

                # Fechados: Filtrar por 'Estado' == 'Fechado' e 'Hora de Fechamento' no m√™s selecionado
                fechados_por_dia = df[df['Estado'] == 'Fechado'].copy()  # Usar o DataFrame completo
                fechados_por_dia = fechados_por_dia[fechados_por_dia['Hora de Fechamento'].notna()]  # Remover valores nulos

                if pd.api.types.is_datetime64_any_dtype(fechados_por_dia['Hora de Fechamento']):
                    fechados_por_dia = fechados_por_dia[fechados_por_dia['Hora de Fechamento'].dt.to_period('M') == mes_selecionado].groupby(fechados_por_dia['Hora de Fechamento'].dt.date).size().reset_index(name='Total Fechados')
                    fechados_por_dia.rename(columns={'Hora de Fechamento': 'Data'}, inplace=True)  # Renomear a coluna para 'Data'
                else:
                    st.error("A coluna 'Hora de Fechamento' n√£o cont√©m valores datetime v√°lidos.")

                # Solucionados: Filtrar por 'Estado' == 'Solucionado' e '√öltima Altera√ß√£o' no m√™s selecionado
                solucionados_por_dia = df[df['Estado'] == 'Solucionado'].copy()  # Usar o DataFrame completo
                solucionados_por_dia = solucionados_por_dia[solucionados_por_dia['√öltima Altera√ß√£o'].notna()]  # Remover valores nulos

                if pd.api.types.is_datetime64_any_dtype(solucionados_por_dia['√öltima Altera√ß√£o']):
                    solucionados_por_dia = solucionados_por_dia[solucionados_por_dia['√öltima Altera√ß√£o'].dt.to_period('M') == mes_selecionado].groupby(solucionados_por_dia['√öltima Altera√ß√£o'].dt.date).size().reset_index(name='Total Solucionados')
                    solucionados_por_dia.rename(columns={'√öltima Altera√ß√£o': 'Data'}, inplace=True)  # Renomear a coluna para 'Data'
                else:
                    st.error("A coluna '√öltima Altera√ß√£o' n√£o cont√©m valores datetime v√°lidos.")

                # Merge dos dados
                df_comparacao = pd.merge(criados_por_dia, fechados_por_dia, on='Data', how='outer').fillna(0)
                df_comparacao = pd.merge(df_comparacao, solucionados_por_dia, on='Data', how='outer').fillna(0)

                # Ordenar df_comparacao pela data
                df_comparacao = df_comparacao.sort_values(by='Data')

                # Criar gr√°fico
                fig_comparacao = px.bar(
                    df_comparacao,
                    x='Data',
                    y=['Total Criados', 'Total Fechados', 'Total Solucionados'],
                    title=f'Criados, Fechados e Solucionados por Dia',
                    labels={'value': 'Total', 'variable': 'Status'},
                    barmode='group'
                )

                fig_comparacao.update_traces(texttemplate='%{y:.0f}', textposition='outside')
                st.plotly_chart(fig_comparacao, use_container_width=True)

                # Definir a ordem dos estados
                ordem_estados = ['Novo', 'Aceito', 'Em atendimento', 'Aguardando cliente', 'Aguardando terceiros', 'Agendado']

                # Filtrar os dados para incluir apenas os estados desejados
                df_filtrado_atendimento = df[~df['Estado'].isin(['Aceito','Fechado', 'Solucionado', 'Cancelado', 'Fechado sem solu√ß√£o'])]

                # Agrupar os dados filtrados por 'Atendente/Propriet√°rio' e 'Estado'
                atendimento_por_analista_estado = df_filtrado_atendimento.groupby(['Atendente/Propriet√°rio', 'Estado']).size().reset_index(name='Total Atendimento')

                # Calcular o total de atendimento
                total_atendimento = atendimento_por_analista_estado['Total Atendimento'].sum()

                # Ordenar os dados de atendimento
                atendimento_por_analista_estado = atendimento_por_analista_estado.sort_values(by='Total Atendimento', ascending=False)

                # Visualizar o gr√°fico
                Visualizer.plot_bar(
                    atendimento_por_analista_estado,
                    x='Atendente/Propriet√°rio',
                    y='Total Atendimento',
                    color='Estado',
                    title=f'Em Atendimento por Analista e Estado (Total: {total_atendimento}).',
                    labels={'Total Atendimento': 'Total de Chamados'},
                    text='Total Atendimento'
                )

                # Filtrar os chamados solucionados para o m√™s selecionado com base na √öltima Altera√ß√£o
                chamados_solucionados = df[df['Estado'] == 'Solucionado']
                chamados_solucionados_mes = chamados_solucionados[
                    chamados_solucionados['√öltima Altera√ß√£o'].dt.to_period('M') == mes_selecionado
                ]

                # Agrupar chamados solucionados por dia e analista
                solucionados_por_dia_analista = chamados_solucionados_mes.groupby(
                    ['Atendente/Propriet√°rio', chamados_solucionados_mes['√öltima Altera√ß√£o'].dt.date]
                ).size().reset_index(name='Total Solucionados')

                # Calcular o total de chamados solucionados
                total_solucionados_analista = solucionados_por_dia_analista['Total Solucionados'].sum()

                # Ordenar os dados de solucionados
                solucionados_por_dia_analista = solucionados_por_dia_analista.sort_values(by='Total Solucionados', ascending=False)

                # Criar o gr√°fico
                Visualizer.plot_bar(
                    solucionados_por_dia_analista,
                    x='√öltima Altera√ß√£o',
                    y='Total Solucionados',
                    color='Atendente/Propriet√°rio',
                    title=f'Solucionados por Dia e por Analista - {mes_selecionado} (Total: {total_solucionados_analista})',
                    labels={'Total Solucionados': 'Total Solucionados'},
                    text='Total Solucionados'
                )

                backlog_abertos = df[~df['Estado'].isin(['Fechado', 'Solucionado', 'Cancelado'])]
                backlog_abertos['Servi√ßo'] = backlog_abertos['Servi√ßo'].str.replace(r'^.*?::', '', regex=True)

                # Verifique se h√° duplicatas
                print("Total de registros no backlog_abertos:", backlog_abertos.shape[0])
                print("Total de registros √∫nicos no backlog_abertos:", backlog_abertos.drop_duplicates().shape[0])

                # Total de chamados abertos por servi√ßo
                total_abertos_por_servico = backlog_abertos.groupby(['Servi√ßo']).size().reset_index(name='Total Chamados Abertos')
                total_abertos_por_servico = total_abertos_por_servico.sort_values(by='Total Chamados Abertos', ascending=False)

                total_abertos_servico = total_abertos_por_servico['Total Chamados Abertos'].sum()

                # =========================== AQUI INICIA A MAGICA ===========================

                # Calcular a idade do chamado mais antigo por servi√ßo em dias
                oldest_ticket_age = data_processor.calculate_oldest_ticket_age_days(backlog_abertos)

                # Realizar o merge dos dados
                total_abertos_por_servico = pd.merge(
                    total_abertos_por_servico,
                    oldest_ticket_age,
                    on='Servi√ßo',
                    how='left'
                )

                # Formatando o texto a ser exibido no gr√°fico
                total_abertos_por_servico['text'] = total_abertos_por_servico.apply(
                    lambda row: f"Total: {row['Total Chamados Abertos']}<br>Mais Antigo: {int(row['Tempo Aberto (Dias)'])} dias"
                    if pd.notna(row['Tempo Aberto (Dias)'])
                    else f"Total: {row['Total Chamados Abertos']}<br>Mais Antigo: N/A",
                    axis=1
                )

                # Gr√°fico de total abertos por servi√ßo
                fig_backlog_abertos = px.bar(
                    total_abertos_por_servico,
                    x='Servi√ßo',
                    y='Total Chamados Abertos',
                    title=f'Total Abertos por Servi√ßo (Total: {total_abertos_servico})',
                    barmode='group',
                    labels={'Total Chamados Abertos': 'Total de Chamados'},
                    text='text'  # Usar a coluna 'text' formatada
                )

                fig_backlog_abertos.update_layout(
                    title_font=dict(size=20, family='Arial, sans-serif'),
                    xaxis_title='Servi√ßo',
                    yaxis_title='Total de Chamados',
                    yaxis=dict(
                        title=dict(
                            text='Total de Chamados',
                            font=dict(size=14)
                        )
                    ),
                    plot_bgcolor='rgba(0,0,0,0)',
                    bargap=0.2,
                    width=1000,
                    height=550
                )

                fig_backlog_abertos.update_traces(textposition='outside', texttemplate='%{text}')  # Exibir o texto formatado
                st.plotly_chart(fig_backlog_abertos, use_container_width=True)

                # =========================== AQUI TERMINA A MAGICA ===========================

                # Filtrar chamados abertos que come√ßam com 'SAP'
                df_sap_abertos = backlog_abertos[backlog_abertos['Servi√ßo'].str.startswith('SAP', na=False)]
                distribuicao_por_servico_sap_abertos = df_sap_abertos.groupby('Servi√ßo').size().reset_index(name='Total Chamados')

                total_sap_abertos = distribuicao_por_servico_sap_abertos['Total Chamados'].sum()

                if not distribuicao_por_servico_sap_abertos.empty:
                    # Ordenar a distribui√ß√£o por servi√ßo SAP abertos
                    distribuicao_por_servico_sap_abertos = distribuicao_por_servico_sap_abertos.sort_values(by='Total Chamados', ascending=False)

                    Visualizer.plot_bar(
                        distribuicao_por_servico_sap_abertos,
                        x='Servi√ßo',
                        y='Total Chamados',
                        color=None,
                        title=f'Abertos por Servi√ßo Time "SAP" (Total: {total_sap_abertos})',
                        labels={'Total Chamados': 'Total de Chamados'},
                        text='Total Chamados',
                        bar_color=['#2684cc', '#FFDB58'] * (len(distribuicao_por_servico_sap_abertos) // 2 + 1)
                    )
                else:
                    st.write("N√£o h√° servi√ßos abertos que come√ßam com 'SAP' no m√™s selecionado.")

                # Filtrar chamados abertos que n√£o come√ßam com 'SAP'
                df_nao_sap_abertos = backlog_abertos[~backlog_abertos['Servi√ßo'].str.startswith('SAP', na=False)]
                distribuicao_por_servico_nao_sap_abertos = df_nao_sap_abertos.groupby('Servi√ßo').size().reset_index(name='Total Chamados')

                total_nao_sap_abertos = distribuicao_por_servico_nao_sap_abertos['Total Chamados'].sum()

                if not distribuicao_por_servico_nao_sap_abertos.empty:
                    # Ordenar a distribui√ß√£o por servi√ßo n√£o SAP abertos
                    distribuicao_por_servico_nao_sap_abertos = distribuicao_por_servico_nao_sap_abertos.sort_values(by='Total Chamados', ascending=False)

                    Visualizer.plot_bar(
                        distribuicao_por_servico_nao_sap_abertos,
                        x='Servi√ßo',
                        y='Total Chamados',
                        color=None,
                        title=f'Abertos por Servi√ßo Time "Atacad√£o" (Total: {total_nao_sap_abertos})',
                        labels={'Total Chamados': 'Total de Chamados'},
                        text='Total Chamados',
                        bar_color=['orange', 'green'] * (len(distribuicao_por_servico_nao_sap_abertos) // 2 + 1)
                    )
                else:
                    st.write("N√£o h√° servi√ßos abertos que n√£o come√ßam com 'SAP' no m√™s selecionado.")

                ## Chamados mais antigos por Servi√ßo
                with st.container():
                    st.subheader("Chamados Mais Antigos por Servi√ßo")

                    estados_excluidos = ['Fechado', 'Solucionado', 'Cancelado']
                    servicos_unicos = sorted(df[~df['Estado'].isin(estados_excluidos)]['Servi√ßo'].dropna().unique())

                    servico_selecionado = st.selectbox("Selecione o Servi√ßo:", servicos_unicos)

                    chamados_antigos = data_processor.get_oldest_tickets_by_service(servico_selecionado, n=5)

                    if chamados_antigos.empty:
                        st.write(f"N√£o h√° chamados ativos para o servi√ßo '{servico_selecionado}'.")
                    else:
                        # Resetar o √≠ndice antes de exibir para remover a coluna de √≠ndice
                        chamados_antigos_reset = chamados_antigos[['Ticket#', 'Titulo', 'Estado', 'Idade', 'Criado']].reset_index(drop=True)
                        st.dataframe(chamados_antigos_reset, use_container_width=True, hide_index=True)

                backlog_aceitos = df_filtrado[df_filtrado['Estado'] == 'Aceito']
                backlog_aceitos['Servi√ßo'] = backlog_aceitos['Servi√ßo'].str.replace(r'^.*?::', '', regex=True)

                total_aceitos = backlog_aceitos.groupby(['Servi√ßo', 'Estado']).size().reset_index(name='Total Aceito')
                total_aceitos_sum = total_aceitos['Total Aceito'].sum()

                # Ordenar os aceitos
                total_aceitos = total_aceitos.sort_values(by='Total Aceito', ascending=False)

                # Criar o gr√°fico de barras
                fig_backlog_aceitos = px.bar(
                    total_aceitos,
                    x='Servi√ßo',
                    y='Total Aceito',
                    color='Estado',
                    title=f'Aceitos por Servi√ßo (Total: {total_aceitos_sum})',
                    barmode='group',
                    labels={'Total Aceito': 'Total de Chamados'},
                    text='Total Aceito'
                )

                # Atualizar o layout para usar escala logar√≠tmica no eixo y
                fig_backlog_aceitos.update_layout(
                    yaxis_type='log'
                )

                # Exibir o gr√°fico
                st.plotly_chart(fig_backlog_aceitos, use_container_width=True)

                # Servicos Time SAP
                if 'Servi√ßo' in df.columns:
                    df['Servi√ßo'] = df['Servi√ßo'].str.strip()

                    df_sap_ariba = df[df['Servi√ßo'].str.contains('SAP Ariba', case=False, na=False)]
                    df_sap_ariba['Servi√ßo'] = df_sap_ariba['Servi√ßo'].str.replace(r'^Tecnologia da Informa√ß√£o::SAP Ariba::', '', regex=True)
                    df_mes_selecionado_ariba = df_sap_ariba[df_sap_ariba['Criado'].dt.to_period('M') == mes_selecionado]
                    distribuicao_por_Servi√ßo_ariba = df_mes_selecionado_ariba.groupby('Servi√ßo').size().reset_index(name='Total Chamados')

                    total_ariba = distribuicao_por_Servi√ßo_ariba['Total Chamados'].sum()

                    if not distribuicao_por_Servi√ßo_ariba.empty:
                        # Ordenar a distribui√ß√£o por servi√ßo SAP Ariba
                        distribuicao_por_Servi√ßo_ariba = distribuicao_por_Servi√ßo_ariba.sort_values(by='Total Chamados', ascending=False)

                        Visualizer.plot_bar(
                            distribuicao_por_Servi√ßo_ariba,
                            x='Servi√ßo',
                            y='Total Chamados',
                            color=None,
                            title=f'Total Criados M√™s (SAP Ariba) (Total: {total_ariba})',
                            labels={'Total Chamados': 'Total de Chamados'},
                            text='Total Chamados'
                        )
                    else:
                        st.write("N√£o h√° servi√ßos relacionados a 'SAP Ariba' no m√™s selecionado.")

                    # Servicos time Atacadao
                    df_nao_ariba = df[~df['Servi√ßo'].str.contains('SAP Ariba', case=False, na=False)]
                    df_nao_ariba['Servi√ßo'] = df_nao_ariba['Servi√ßo'].str.replace(r'^Tecnologia da Informa√ß√£o::', '', regex=True)
                    df_mes_selecionado_nao_ariba = df_nao_ariba[df_nao_ariba['Criado'].dt.to_period('M') == mes_selecionado]
                    distribuicao_por_Servi√ßo_nao_ariba = df_mes_selecionado_nao_ariba.groupby('Servi√ßo').size().reset_index(name='Total Chamados')

                    total_nao_ariba = distribuicao_por_Servi√ßo_nao_ariba['Total Chamados'].sum()

                    if not distribuicao_por_Servi√ßo_nao_ariba.empty:
                        # Ordenar a distribui√ß√£o por servi√ßo sem SAP Ariba
                        distribuicao_por_Servi√ßo_nao_ariba = distribuicao_por_Servi√ßo_nao_ariba.sort_values(by='Total Chamados', ascending=False)

                        Visualizer.plot_bar(
                            distribuicao_por_Servi√ßo_nao_ariba,
                            x='Servi√ßo',
                            y='Total Chamados',
                            color=None,
                            title=f'Distribui√ß√£o de Chamados por Servi√ßo (Sem "SAP Ariba") (Total: {total_nao_ariba})',
                            labels={'Total Chamados': 'Total de Chamados'},
                            text='Total Chamados'
                        )
                    else:
                        st.write("N√£o h√° servi√ßos sem 'SAP Ariba' com ocorr√™ncias no m√™s selecionado.")
                else:
                    st.write("A coluna 'Servi√ßo' n√£o est√° presente nos dados.")

                if 'ID do Cliente' in df.columns:
                    df_mes_selecionado_cliente = df[(df['Criado'].dt.to_period('M') == mes_selecionado)]
                    distribuicao_por_cliente = df_mes_selecionado_cliente.groupby('ID do Cliente').size().reset_index(name='Total Chamados')
                    distribuicao_por_cliente = distribuicao_por_cliente[distribuicao_por_cliente['Total Chamados'] > 5]
                    distribuicao_por_cliente['ID do Cliente'] = distribuicao_por_cliente['ID do Cliente'].astype(str)

                    total_cliente = distribuicao_por_cliente['Total Chamados'].sum()

                    if not distribuicao_por_cliente.empty:
                        # Ordenar a distribui√ß√£o por cliente
                        distribuicao_por_cliente = distribuicao_por_cliente.sort_values(by='Total Chamados', ascending=False)

                        Visualizer.plot_bar(
                        distribuicao_por_cliente,
                        x='ID do Cliente',
                        y='Total Chamados',
                        color=None,
                        title=f'Distribui√ß√£o de Chamados por Filial (Total: {total_cliente})',
                        labels={'Total Chamados': 'Total de Chamados'},
                        text='Total Chamados',
                        yaxis_type='log'
                    )
                else:
                    st.write("A coluna 'ID do Cliente' n√£o est√° presente nos dados.")

                # Agrupar chamados criados por m√™s
                chamados_criados_por_mes = df.groupby('MesAno').size().reset_index(name='Total Criados')

                # Converter MesAno para string para melhor visualiza√ß√£o no gr√°fico
                chamados_criados_por_mes['MesAno'] = chamados_criados_por_mes['MesAno'].astype(str)

                # Ordenar cronologicamente
                chamados_criados_por_mes = chamados_criados_por_mes.sort_values(by='MesAno')

                # Plotar gr√°fico de linha
                Visualizer.plot_line(
                    data=chamados_criados_por_mes,
                    x='MesAno',
                    y='Total Criados',
                    title='Total de Chamados Criados por M√™s',
                    labels={'MesAno': 'M√™s/Ano', 'Total Criados': 'Total de Chamados Criados'}
)

                df['Grupo'] = df['Servi√ßo'].apply(
                    lambda x: 'S2C' if isinstance(x, str) and re.match(r'^Tecnologia da Informa√ß√£o::SAP Ariba::(SLP|S2C):', x) else 'P2P'
                )

                # Agrupar os chamados por m√™s e grupo
                chamados_por_grupo_mes = df.groupby(['MesAno', 'Grupo']).size().reset_index(name='Total Chamados')

                # Converter 'MesAno' para o tipo Period para garantir a ordena√ß√£o cronol√≥gica
                chamados_por_grupo_mes['MesAno'] = chamados_por_grupo_mes['MesAno'].astype(str)
                chamados_por_grupo_mes['MesAno'] = pd.to_datetime(chamados_por_grupo_mes['MesAno'], format='%Y-%m').dt.to_period('M')

                # Ordenar os dados pela coluna 'MesAno' em ordem crescente
                chamados_por_grupo_mes = chamados_por_grupo_mes.sort_values(by='MesAno')

                # Converter 'MesAno' de volta para string para exibi√ß√£o no gr√°fico
                chamados_por_grupo_mes['MesAno'] = chamados_por_grupo_mes['MesAno'].astype(str)

                Visualizer.plot_bar(
                    chamados_por_grupo_mes,
                    x='MesAno',
                    y='Total Chamados',
                    color='Grupo',
                    title='Quantidade de Chamados por Grupo (S2C e P2P) por M√™s',
                    labels={'Total Chamados': 'Total de Chamados', 'MesAno': 'M√™s/Ano'},
                    text='Total Chamados',
                    sort_by_y=False
                )

# Executar a aplica√ß√£o
app = App(file_path)
app.run()
```
===========================
modularizar esse c√≥digo.
2025-04-08 04:28:04,534 [INFO] Resposta da IA (texto): Com certeza! Analisei o c√≥digo e preparei algumas sugest√µes para modulariza√ß√£o, legibilidade, performance e seguran√ßa, juntamente com pr√°ticas recomendadas de design e arquitetura.

### An√°lise Geral
O c√≥digo parece bem estruturado, com classes para carregar, processar e visualizar os dados. No entanto, algumas √°reas podem ser melhoradas para aumentar a modularidade, legibilidade e testabilidade.

### Sugest√µes de Melhorias

1.  **Separa√ß√£o de Responsabilidades (Single Responsibility Principle)**:

    *   A classe `DataProcessor` est√° fazendo muitas coisas (preprocessamento, filtragem e c√°lculo de m√©tricas). Considere dividi-la em classes menores com responsabilidades √∫nicas.
    *   A classe `Visualizer` est√° diretamente usando `st.plotly_chart`. Isso dificulta o teste e a reutiliza√ß√£o. Isole a l√≥gica de plotagem em fun√ß√µes separadas.

2.  **Inje√ß√£o de Depend√™ncia**:

    *   Em vez de instanciar `DataLoader` e `DataProcessor` dentro da classe `App`, passe-os como argumentos no construtor. Isso facilita a substitui√ß√£o por mocks em testes.

3.  **Configura√ß√£o Centralizada**:

    *   Mova as configura√ß√µes (nomes de colunas, valores de estado, etc.) para um arquivo de configura√ß√£o separado ou use vari√°veis de ambiente.

4.  **Tratamento de Erros**:

    *   O tratamento de erros est√° bom, mas pode ser aprimorado com logs mais detalhados e mensagens de erro mais amig√°veis.

5.  **Modulariza√ß√£o de Gr√°ficos**:

    *   A classe `BarPlot` e `MetricPlot` s√£o boas, mas a cria√ß√£o dos gr√°ficos em si poderia ser movida para fun√ß√µes separadas para facilitar a manuten√ß√£o e reutiliza√ß√£o.

6.  **Remo√ß√£o de Duplica√ß√£o de C√≥digo**:

    *   H√° v√°rias partes do c√≥digo que se repetem, como a l√≥gica de filtragem por m√™s. Crie fun√ß√µes para evitar a duplica√ß√£o.

### C√≥digo Refatorado

Aqui est√° uma vers√£o refatorada do c√≥digo com algumas das melhorias sugeridas:

```python
# config.py
import os

class Config:
    FILE_PATH = os.path.join(os.path.dirname(__file__), 'relatorio_final.xlsx')
    REQUIRED_COLUMNS = ['Estado', 'Criado', 'Servi√ßo', 'Hora de Fechamento', '√öltima Altera√ß√£o', 'Idade']
    EXCLUDED_STATES = ['Fechado', 'Solucionado', 'Cancelado']
    DEFAULT_SERVICE_GROUP = 'P2P'
    SERVICE_GROUP_MAPPING = {
        'S2C': r'^Tecnologia da Informa√ß√£o::SAP Ariba::(SLP|S2C):'
    }

# utils.py
import pandas as pd
import streamlit as st

def load_data(file_path: str) -> pd.DataFrame:
    """Loads data from the specified Excel file."""
    try:
        df = pd.read_excel(file_path, dtype={
            'Ticket#': str,
            'EscalationTimeWorkingTime': str,
            'Hor√°rio de Escala√ß√£o': str,
            'SolutionTime': str,
            'SolutionTimeDestinationTime': str,
            'SolutionTimeWorkingTime': str,
            'Delta de tempo de solu√ß√£o em minutos': str,
            'Tempo de solu√ß√£o em minutos': str
        })
        return df
    except FileNotFoundError:
        st.error(f"File not found: {file_path}")
        st.stop()
    except Exception as e:
        st.error(f"Error loading file: {e}")
        return pd.DataFrame()

def filter_data_by_month(df: pd.DataFrame, month: pd.Period) -> pd.DataFrame:
    """Filters the DataFrame by the specified month."""
    return df[(df['Criado'].dt.to_period('M') == month) & (df['Criado'].notna())]

# data_processor.py
import pandas as pd
import streamlit as st
from config import Config

class DataPreprocessor:
    def __init__(self, df: pd.DataFrame):
        self.df = df.copy()  # Avoid modifying the original DataFrame

    def preprocess_data(self) -> pd.DataFrame:
        """Preprocesses the data."""
        if not all(col in self.df.columns for col in Config.REQUIRED_COLUMNS):
            st.error(f"Missing required columns: {Config.REQUIRED_COLUMNS}")
            st.stop()

        self.df = self.df[~self.df['Estado'].isin(Config.EXCLUDED_STATES)].copy()
        self.df['Criado'] = pd.to_datetime(self.df['Criado'], errors='coerce')
        self.df['Hora de Fechamento'] = pd.to_datetime(self.df['Hora de Fechamento'], errors='coerce')
        self.df['√öltima Altera√ß√£o'] = pd.to_datetime(self.df['√öltima Altera√ß√£o'], errors='coerce')
        self.df['MesAno'] = self.df['Criado'].dt.to_period('M')
        self.df['MesAnoUltimaAlteracao'] = self.df['√öltima Altera√ß√£o'].dt.to_period('M')
        
        return self.df

class MetricsCalculator:
    def __init__(self, df: pd.DataFrame):
        self.df = df

    def calculate_metrics(self, month: pd.Period) -> tuple:
        """Calculates key metrics for the specified month."""
        month_data = self.df[self.df['Criado'].dt.to_period('M') == month]
        
        total_opened = month_data.shape[0]
        total_closed = self.df[self.df['Hora de Fechamento'].dt.to_period('M') == month].shape[0]
        total_no_assignee = self.df[(self.df['Atendente/Propriet√°rio'] == '') | (self.df['Atendente/Propriet√°rio'] == '*a definir')].shape[0]
        total_backlog = self.df[self.df['Estado'] == 'Aceito'].shape[0]
        total_open = self.df[~self.df['Estado'].isin(['Fechado', 'Solucionado', 'Cancelado', 'Fechado sem solu√ß√£o'])].shape[0]
        total_in_progress = self.df[self.df['Estado'] == 'Em atendimento'].shape[0]
        total_waiting_third_party = self.df[self.df['Estado'] == 'Aguardando terceiros'].shape[0]
        total_waiting_customer = self.df[self.df['Estado'] == 'Aguardando cliente'].shape[0]
        total_scheduled = self.df[self.df['Estado'] == 'Agendado'].shape[0]
        
        return (total_opened, total_closed, total_no_assignee, total_backlog, total_open, 
                total_in_progress, total_waiting_third_party, total_waiting_customer, total_scheduled)

    def calculate_oldest_ticket_age_days(self, df: pd.DataFrame) -> pd.DataFrame:
        """Calculates the age in days of the oldest ticket for each service."""
        open_backlog = df[~df['Estado'].isin(['Fechado', 'Solucionado', 'Cancelado'])].copy()
        open_backlog['Tempo Aberto (Dias)'] = (pd.Timestamp('now') - open_backlog['Criado']).dt.days
        oldest_ticket_age = open_backlog.groupby('Servi√ßo')['Tempo Aberto (Dias)'].max().reset_index()
        return oldest_ticket_age

class ServiceAnalyzer:
    def __init__(self, df: pd.DataFrame):
        self.df = df

    def get_oldest_tickets_by_service(self, service: str, n: int = 5) -> pd.DataFrame:
        """Retrieves the oldest active tickets for a given service."""
        excluded_states = ['Fechado', 'Solucionado', 'Cancelado']
        df_service = self.df[(self.df['Servi√ßo'] == service) & (~self.df['Estado'].isin(excluded_states))].copy()
        df_service = df_service.sort_values(by='Criado', ascending=True).head(n)
        return df_service
        
    def assign_service_group(self, service: str) -> str:
        """Assigns a service group based on predefined mappings."""
        for group, pattern in Config.SERVICE_GROUP_MAPPING.items():
            if pd.notna(service) and re.match(pattern, service):
                return group
        return Config.DEFAULT_SERVICE_GROUP

# visualizations.py
import pandas as pd
import streamlit as st
import plotly.express as px

class BasePlot:
    def __init__(self, title: str, total: int | None = None):
        self.title = f"{title} (Total: {total})" if total is not None else title

    def show(self):
        raise NotImplementedError("Subclasses must implement the show method.")

class MetricPlot(BasePlot):
    def __init__(self, title: str, value: int):
        super().__init__(title)
        self.value = value

    def show(self):
        st.metric(self.title, self.value)

class BarPlot(BasePlot):
    def __init__(self, data: pd.DataFrame, x: str, y: str, color: str | None, title: str, 
                 labels: dict, text: str, barmode: str = 'group', yaxis_type: str = 'linear', 
                 showlegend: bool = True, bar_color: list[str] | None = None, sort_by_y: bool = True):
        super().__init__(title)
        self.data = data.sort_values(by=y, ascending=False) if sort_by_y else data
        self.x = x
        self.y = y
        self.color = color
        self.labels = labels
        self.text = text
        self.barmode = barmode
        self.yaxis_type = yaxis_type
        self.showlegend = showlegend
        self.bar_color = bar_color

    def show(self):
        fig = px.bar(
            self.data, x=self.x, y=self.y, color=self.color, title=self.title,
            labels=self.labels, text=self.text, barmode=self.barmode
        )

        if self.bar_color:
            fig.update_traces(marker_color=self.bar_color)

        fig.update_layout(width=1200, height=600, bargap=0.1, showlegend=self.showlegend, yaxis_type=self.yaxis_type,
            xaxis=dict(type='category', tickangle=-45)
        )

        fig.update_traces(texttemplate='%{text:.0f}', textposition='outside')
        st.plotly_chart(fig, use_container_width=True)

class LinePlot(BasePlot):
    def __init__(self, data: pd.DataFrame, x: str, y: str, title: str, labels: dict):
        super().__init__(title)
        self.data = data
        self.x = x
        self.y = y
        self.labels = labels

    def show(self):
        fig = px.line(
            self.data, x=self.x, y=self.y, title=self.title, labels=self.labels, markers=True, text=self.y
        )

        fig.update_layout(width=1200, height=600, plot_bgcolor='rgba(0,0,0,0)',
            xaxis=dict(tickangle=-45)
        )

        fig.update_traces(line=dict(width=3), marker=dict(size=8), textposition='top center', texttemplate='%{text:.0f}')
        st.plotly_chart(fig, use_container_width=True)

# app.py
import pandas as pd
import streamlit as st
from config import Config
from utils import load_data, filter_data_by_month
from data_processor import DataPreprocessor, MetricsCalculator, ServiceAnalyzer
from visualizations import MetricPlot, BarPlot, LinePlot

class App:
    def __init__(self, file_path: str):
        self.file_path = file_path

    def run(self):
        st.set_page_config(layout="wide")
        df = load_data(self.file_path)
        
        preprocessor = DataPreprocessor(df)
        df = preprocessor.preprocess_data()
        
        metrics_calculator = MetricsCalculator(df)
        service_analyzer = ServiceAnalyzer(df)
        
        available_months = sorted(df['MesAno'].dropna().unique())
        current_month = pd.to_datetime("now").to_period('M')
        selected_month = current_month if current_month in available_months else available_months[0]
        selected_month = st.selectbox("Select Month:", available_months, index=list(available_months).index(selected_month))
        
        filtered_df = filter_data_by_month(df, selected_month)
        
        if filtered_df.empty:
            st.write(f"No data for {selected_month}.")
            return

        (total_opened, total_closed, total_no_assignee, total_backlog, total_open,
         total_in_progress, total_waiting_third_party, total_waiting_customer, total_scheduled) = metrics_calculator.calculate_metrics(selected_month)

        col1, col2, col3, col4, col5 = st.columns(5)
        with col1:
            MetricPlot("Total Opened Month", total_opened).show()
        with col2:
            MetricPlot("Total Closed", total_closed).show()
        with col3:
            MetricPlot("Total No Assignee", total_no_assignee).show()
        with col4:
            MetricPlot("Total Backlog", total_backlog).show()
        with col5:
            MetricPlot("Total Opened", total_open).show()

        col6, col7, col8, col9 = st.columns(4)
        with col6:
            MetricPlot("In Progress", total_in_progress).show()
        with col7:
            MetricPlot("Waiting Third Party", total_waiting_third_party).show()
        with col8:
            MetricPlot("Waiting Customer", total_waiting_customer).show()
        with col9:
            MetricPlot("Scheduled", total_scheduled).show()

        with st.container():
            # Closed by Analyst
            closed_tickets = df[df['Estado'].isin(['Solucionado', 'Fechado', 'Fechados sem solu√ß√£o']) & (df['√öltima Altera√ß√£o'].dt.to_period('M') == selected_month)]
            total_per_analyst = closed_tickets.groupby('Atendente/Propriet√°rio').size().reset_index(name='Total Tickets')
            total_per_analyst = total_per_analyst.sort_values(by='Total Tickets', ascending=False)
            total_tickets = total_per_analyst['Total Tickets'].sum()
            
            BarPlot(total_per_analyst, x='Atendente/Propriet√°rio', y='Total Tickets', color=None,
                    title=f'Closed by Analyst in Selected Month (Total: {total_tickets})',
                    labels={'Total Tickets': 'Total Tickets'}, text='Total Tickets', showlegend=False, yaxis_type='log',
                    bar_color=['blue', '#FFDB58', 'green', 'pink', 'purple', 'red', 'orange', 'gray', 'brown']).show()

            # Created, Closed, Solved Comparison
            created_by_day = filtered_df.groupby(filtered_df['Criado'].dt.date).size().reset_index(name='Total Created')
            closed_by_day = df[df['Estado'] == 'Fechado'].groupby(df['Hora de Fechamento'].dt.date).size().reset_index(name='Total Closed')
            solved_by_day = df[df['Estado'] == 'Solucionado'].groupby(df['√öltima Altera√ß√£o'].dt.date).size().reset_index(name='Total Solved')
            
            comparison_df = created_by_day.merge(closed_by_day, left_on='Criado', right_on='Hora de Fechamento', how='outer').merge(solved_by_day, left_on='Criado', right_on='√öltima Altera√ß√£o', how='outer').fillna(0)
            comparison_df = comparison_df.sort_values(by='Criado')

            fig_comparison = px.bar(
                comparison_df,
                x='Criado',
                y=['Total Created', 'Total Closed', 'Total Solved'],
                title=f'Created, Closed and Solved by Day',
                labels={'value': 'Total', 'variable': 'Status'},
                barmode='group'
            )

            fig_comparison.update_traces(texttemplate='%{y:.0f}', textposition='outside')
            st.plotly_chart(fig_comparison, use_container_width=True)
            
            # Service Assignment
            estados_excluidos = ['Fechado', 'Solucionado', 'Cancelado']
            servicos_unicos = sorted(df[~df['Estado'].isin(estados_excluidos)]['Servi√ßo'].dropna().unique())

            servico_selecionado = st.selectbox("Select Service:", servicos_unicos)

            chamados_antigos = service_analyzer.get_oldest_tickets_by_service(servico_selecionado, n=5)

            if chamados_antigos.empty:
                st.write(f"N√£o h√° chamados ativos para o servi√ßo '{servico_selecionado}'.")
            else:
                # Resetar o √≠ndice antes de exibir para remover a coluna de √≠ndice
                chamados_antigos_reset = chamados_antigos[['Ticket#', 'Titulo', 'Estado', 'Idade', 'Criado']].reset_index(drop=True)
                st.dataframe(chamados_antigos_reset, use_container_width=True, hide_index=True)

# Main execution
if __name__ == "__main__":
    app = App(Config.FILE_PATH)
    app.run()
```

### Principais Altera√ß√µes:

*   **Arquivo de Configura√ß√£o (`config.py`):** Mant√©m todas as configura√ß√µes em um s√≥ lugar.
*   **Fun√ß√µes Utilit√°rias (`utils.py`):** Fun√ß√µes para carregar e filtrar dados.
*   **Classes de Processamento de Dados (`data_processor.py`):** Divididas em `DataPreprocessor`, `MetricsCalculator` e `ServiceAnalyzer` para melhor organiza√ß√£o.
*   **Classes de Visualiza√ß√£o (`visualizations.py`):** Implementadas como classes separadas.
*   **Classe Principal da Aplica√ß√£o (`app.py`):** Coordena o carregamento, processamento e visualiza√ß√£o dos dados.

### Pr√≥ximos Passos

*   **Testes Unit√°rios:** Escreva testes unit√°rios para cada classe e fun√ß√£o para garantir que funcionem corretamente.
*   **Logs:** Adicione logs para rastrear o fluxo da aplica√ß√£o e ajudar na depura√ß√£o.
*   **Documenta√ß√£o:** Documente cada classe e fun√ß√£o para facilitar a manuten√ß√£o e o uso.

Este refactoring deve tornar o c√≥digo mais modular, leg√≠vel e test√°vel.
2025-04-08 04:28:04,693 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-04-08 04:28:04,695 [INFO] Arquivo lido com sucesso: config\config.py
2025-04-08 04:28:04,698 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-04-08 04:28:04,700 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-04-08 04:28:04,703 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-04-08 04:28:04,704 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-04-08 04:28:04,707 [INFO] Arquivo lido com sucesso: main.py
2025-04-08 04:28:04,709 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-04-08 04:28:04,711 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-04-08 04:28:04,713 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-04-08 04:28:04,716 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-04-08 04:28:04,718 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-04-08 04:28:04,720 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-04-08 04:28:04,722 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-04-08 04:28:04,724 [INFO] Arquivo lido com sucesso: utils\file_utils.py
